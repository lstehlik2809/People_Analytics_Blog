[
  {
    "path": "posts/2023-11-22-gender-gap-in-hiring-decisions/",
    "title": "Evidence on the presence of gender bias in selection settings",
    "description": "Interesting results from a pre-registered meta-analysis of 44 years of field experiments on gender gaps in hiring decisions.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-22",
    "categories": [
      "gender discrimination",
      "field experiments",
      "meta-analysis",
      "open science",
      "forecasting"
    ],
    "contents": "\r\nSchaerer et al. (2023) conducted a pre-registered meta-analysis of 44 years of field experiments on gender gaps in hiring decisions with quite interesting results:\r\nDiscrimination against women for male-typed and balanced jobs decreased across time.\r\nDiscrimination against men for female-typed jobs remained stable across time.\r\nAverage effect is small - the average odds of male applicants to receive a callback is 0.91 times the odds of equally qualified female applicants (with 95% CI from 0.86 to 0.97).\r\nHeterogeneity of true effects is very high, specifically 83% of total variance across studies can be attributed to heterogeneity rather than random chance, so the average effect is not so telling.\r\nIn addition to the meta-analysis, the study also included a forecasting challenge in which researchers and laypeople attempted to accurately estimate both time-trends and the current pervasiveness of gender biases in selection settings. Forecasters expected observed decline, but overestimated the degree of remaining bias.\r\n\r\n\r\n\r\nNote on the attached chart: In all figures, odds ratios above 1 indicate a greater preference for male applicants and odds ratios below 1 indicate greater preference for female applicants.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-22-gender-gap-in-hiring-decisions/./gendergap.jpg",
    "last_modified": "2023-11-26T11:42:26+01:00",
    "input_file": "gender-gap-in-hiring-decisions.knit.md"
  },
  {
    "path": "posts/2023-11-18-job-demands-job-control-wellbeing/",
    "title": "Surprising finding on the impact of job demands and control on workers’ well-being",
    "description": "I just came across an interesting and surprising result from a Bayesian meta-analysis on the effect of the interaction between job demands and job control on worker well-being.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-18",
    "categories": [
      "well-being",
      "job demands",
      "job control",
      "meta-analysis"
    ],
    "contents": "\r\nAccording to Huth & Chung-Yan (2023), contrary to the position of many theories in the field of occupational health and stress, job control does not reduce the negative impact of job demands on workers’ well-being.\r\nAs you can see from the table below, the data provided strong evidence for the absence of the interaction between job demands and control.\r\n\r\n\r\n\r\nAt the same time, however, the authors themselves emphasize that “[these] findings do not suggest that job demands and job control are not important work design features when considering the well-being of workers. Their direct effects on worker well-being are well-established in past research.”\r\n“The important conclusion of [the] study is that increased job control cannot offset the deleterious impact that high workloads have on workers. [This means that assuming] that employee well-being is a priority, workload should be restricted irrespective of the positive benefits of increasing employee control.”\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-18-job-demands-job-control-wellbeing/./pic.png",
    "last_modified": "2023-11-19T12:35:40+01:00",
    "input_file": "job-demands-job-control-wellbeing.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-11-09-personas-based-on-ml-local-interpretation-algos/",
    "title": "Personas based on ML local interpretation algorithms",
    "description": "A demonstration of one method useful for sharing insights from fitted ML models.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-10",
    "categories": [
      "machine learning",
      "interpretability",
      "personas",
      "storytelling",
      "r",
      "python"
    ],
    "contents": "\r\nThere is one very useful albeit relatively underused method of sharing insights from fitted ML models, at least in my professional bubble.\r\nIt’s a method of identifying personas based on outputs from ML local interpretation algorithms, which provide information about the specific drivers of predictions for individual observations.\r\nIts implementation is pretty straightforward:\r\nFit a ML model.\r\nGenerate predictions for each observation using the fitted model.\r\nIdentify drivers of predictions for individual observation units using a ML local interpretation algorithm, e.g., LIME or SHAP.\r\nUse the data points from steps 2 and 3 to identify clusters with similar characteristics.\r\nName and describe personas corresponding to the identified clusters.\r\nLet’s see this in action using the Python code below. First, we need to create an artificial dataset on which we will demonstrate the method described above. We’ll create a classification dataset - imagine, for example, that we’re trying to predict sales performance based on some collaboration metrics, but feel free to imagine any scenario you like - and prepare a training and testing set to train our ML.\r\n\r\n\r\nShow code\r\n\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\n\r\n\r\n# defining the number of samples and features for the dataset\r\nn_samples = 10000\r\nn_features = 10\r\n\r\n# creating the dataset\r\nX, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=6, n_redundant=4, n_clusters_per_class=3, flip_y=0.27, class_sep=1, random_state=1979)\r\n\r\n# creating a df from X and y\r\ndf = pd.DataFrame(X, columns=['feature_{}'.format(i) for i in range(n_features)])\r\ndf['criterion'] = y\r\n\r\n# splitting the dataset into training and test sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1979)\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\nlibrary(reticulate)\r\n\r\n# table dataviz\r\nDT::datatable(\r\n  py$df,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nNow we can fit and fine-tune our ML (XGBoost) model.\r\n\r\n\r\nShow code\r\nfrom xgboost import XGBClassifier\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\n# fitting a XGBoost model with hyperparameter tuning and 10-fold cross-validation\r\n# initializing the XGBoost classifier\r\nxgb_model = XGBClassifier(random_state=1979)\r\n\r\n# defining the parameter grid for hyperparameter tuning\r\nparam_grid = {\r\n  'n_estimators': [100, 200],\r\n  'max_depth': [3, 5, 7],\r\n  'learning_rate': [0.01, 0.1, 0.2]\r\n}\r\n\r\n# setting up the grid search with 10-fold cross-validation\r\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=10, scoring='f1')\r\n\r\n# fitting the model with hyperparameter tuning\r\ngrid_search.fit(X_train, y_train)\r\nGridSearchCV(cv=10,\r\n             estimator=XGBClassifier(base_score=None, booster=None,\r\n                                     callbacks=None, colsample_bylevel=None,\r\n                                     colsample_bynode=None,\r\n                                     colsample_bytree=None,\r\n                                     early_stopping_rounds=None,\r\n                                     enable_categorical=False, eval_metric=None,\r\n                                     gamma=None, gpu_id=None, grow_policy=None,\r\n                                     importance_type=None,\r\n                                     interaction_constraints=None,\r\n                                     learning_rate=None, max_bin=None,\r\n                                     max_cat_to_onehot=None,\r\n                                     max_delta_step=None, max_depth=None,\r\n                                     max_leaves=None, min_child_weight=None,\r\n                                     missing=nan, monotone_constraints=None,\r\n                                     n_estimators=100, n_jobs=None,\r\n                                     num_parallel_tree=None, predictor=None,\r\n                                     random_state=1979, reg_alpha=None,\r\n                                     reg_lambda=None, ...),\r\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\r\n                         'max_depth': [3, 5, 7], 'n_estimators': [100, 200]},\r\n             scoring='f1')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10,\r\n             estimator=XGBClassifier(base_score=None, booster=None,\r\n                                     callbacks=None, colsample_bylevel=None,\r\n                                     colsample_bynode=None,\r\n                                     colsample_bytree=None,\r\n                                     early_stopping_rounds=None,\r\n                                     enable_categorical=False, eval_metric=None,\r\n                                     gamma=None, gpu_id=None, grow_policy=None,\r\n                                     importance_type=None,\r\n                                     interaction_constraints=None,\r\n                                     learning_rate=None, max_bin=None,\r\n                                     max_cat_to_onehot=None,\r\n                                     max_delta_step=None, max_depth=None,\r\n                                     max_leaves=None, min_child_weight=None,\r\n                                     missing=nan, monotone_constraints=None,\r\n                                     n_estimators=100, n_jobs=None,\r\n                                     num_parallel_tree=None, predictor=None,\r\n                                     random_state=1979, reg_alpha=None,\r\n                                     reg_lambda=None, ...),\r\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\r\n                         'max_depth': [3, 5, 7], 'n_estimators': [100, 200]},\r\n             scoring='f1')estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\r\n              colsample_bylevel=None, colsample_bynode=None,\r\n              colsample_bytree=None, early_stopping_rounds=None,\r\n              enable_categorical=False, eval_metric=None, gamma=None,\r\n              gpu_id=None, grow_policy=None, importance_type=None,\r\n              interaction_constraints=None, learning_rate=None, max_bin=None,\r\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\r\n              max_leaves=None, min_child_weight=None, missing=nan,\r\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\r\n              num_parallel_tree=None, predictor=None, random_state=1979,\r\n              reg_alpha=None, reg_lambda=None, ...)XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\r\n              colsample_bylevel=None, colsample_bynode=None,\r\n              colsample_bytree=None, early_stopping_rounds=None,\r\n              enable_categorical=False, eval_metric=None, gamma=None,\r\n              gpu_id=None, grow_policy=None, importance_type=None,\r\n              interaction_constraints=None, learning_rate=None, max_bin=None,\r\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\r\n              max_leaves=None, min_child_weight=None, missing=nan,\r\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\r\n              num_parallel_tree=None, predictor=None, random_state=1979,\r\n              reg_alpha=None, reg_lambda=None, ...)\r\n\r\nShow code\r\n\r\n# getting the best estimator\r\nbest_xgb_model = grid_search.best_estimator_\r\n\r\nThe classification performance metrics below show that the fitted model performs well on the test data, so we can safely proceed further.\r\n\r\n\r\nShow code\r\nfrom sklearn.metrics import classification_report, roc_auc_score\r\nimport numpy as np\r\n\r\n# predictions on the test set\r\ny_pred = best_xgb_model.predict(X_test)\r\ny_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1] \r\n\r\n# classification report\r\nreport = classification_report(y_test, y_pred)\r\nprint(report)\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.81      0.78      0.80       977\r\n           1       0.80      0.83      0.81      1023\r\n\r\n    accuracy                           0.81      2000\r\n   macro avg       0.81      0.80      0.80      2000\r\nweighted avg       0.81      0.81      0.80      2000\r\n\r\nShow code\r\n# ROC AUC score\r\nroc_auc = roc_auc_score(y_test, y_pred_proba)\r\nprint('ROC AUC score: ', np.round(roc_auc, 2))\r\nROC AUC score:  0.85\r\n\r\nNow we will generate LIME explanations for each observation in the testing dataset and standardize them for later analysis and visualization (including the predicted probabilities of the positive class).\r\n\r\n\r\nShow code\r\n\r\nimport lime.lime_tabular\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\n# using LIME for local interpretation\r\n# initializing the LIME explainer\r\nexplainer = lime.lime_tabular.LimeTabularExplainer(\r\n    training_data=X_train,\r\n    feature_names=['Feature_{}'.format(i) for i in range(X_train.shape[1])],\r\n    class_names=['Low Performance', 'High Performance'],\r\n    mode='classification',\r\n    random_state=1234\r\n)\r\n\r\n\r\n# df for storing the LIME explanations for all observation\r\nexplanations_df = pd.DataFrame()\r\nfeature_names = df.columns[:-1].tolist()\r\n\r\n# generating LIME explanations for each observation in the test set\r\nfor i in range(X_test.shape[0]):\r\n    \r\n    # predicted probability for the positive class\r\n    predicted_class_proba = y_pred_proba[i]\r\n    \r\n    # generating the LIME explanation\r\n    exp = explainer.explain_instance(X_test[i], best_xgb_model.predict_proba, num_features=X_train.shape[1])\r\n    exp_list = exp.as_list()\r\n    \r\n    feature_values = {name: 0 for name in feature_names}\r\n    # looping through the employee's conditions and updating feature_values accordingly\r\n    for condition, value in exp_list:\r\n        for feature_name in feature_names:\r\n            if feature_name in condition.lower():\r\n                feature_values[feature_name] = value\r\n                break\r\n\r\n    # adding the predicted probability for the positive class\r\n    feature_values['predicted_class_proba'] = predicted_class_proba\r\n\r\n    supp_df = pd.DataFrame(feature_values, index=[0])  \r\n    explanations_df = pd.concat([explanations_df, supp_df], ignore_index=True) \r\n\r\n  \r\n# standardizing all features (including the probability for the positive class)\r\nscaler = StandardScaler()\r\nexplanations_scaled = scaler.fit_transform(explanations_df)\r\nexplanations_scaled_df = pd.DataFrame(explanations_scaled)\r\nexplanations_scaled_df.columns = explanations_df.columns\r\n\r\nUsing the UMAP 2D projection of the prediction explanations and predicted probabilities, we can see that that are several clusters of observations with similar predicted probabilities and their drivers.\r\n\r\n\r\nShow code\r\nimport umap\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"white\")\r\n\r\n# visualizing the personas using UMAP\r\n# initializing and fitting UMAP\r\nreducer = umap.UMAP(n_components=2, n_neighbors=50, min_dist=0.01, metric='euclidean', random_state=1979, n_jobs=1)\r\nembedding = reducer.fit_transform(explanations_scaled)  \r\n\r\n# plotting the explanations and predicted probability in 2D scatterplot \r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c='lightblue', s=50, alpha=0.5)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\nplt.show()\r\n\r\n\r\nA clustering algorithm, such as HDBSCAN, can help us identify the clusters.\r\n\r\n\r\nShow code\r\nimport hdbscan\r\nimport matplotlib.patches as mpatches\r\n\r\n# HDBSCAN clustering\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=10, cluster_selection_epsilon=0.3, prediction_data=True)\r\nclusterer.fit(embedding)\r\nHDBSCAN(cluster_selection_epsilon=0.3, min_cluster_size=25, min_samples=10,\r\n        prediction_data=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HDBSCANHDBSCAN(cluster_selection_epsilon=0.3, min_cluster_size=25, min_samples=10,\r\n        prediction_data=True)\r\n\r\nShow code\r\nclusters = clusterer.labels_\r\n\r\n# adding the cluster labels to the dataframe\r\nexplanations_df['cluster'] = clusters\r\n\r\n# plotting the clusters\r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\ncmap = plt.cm.get_cmap('tab20')\r\nnorm = plt.Normalize(clusters.min(), clusters.max())\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=50, cmap=cmap, norm=norm, alpha=0.5)\r\npatches = [mpatches.Patch(color=cmap(norm(i)), label=f'Cluster {i}') for i in np.unique(clusters)]\r\nplt.legend(handles=patches, fontsize=12)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities. The clusters were identified using HDBSCAN.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\nplt.show()\r\n\r\n\r\nThere seem to be about eight clusters and some outliers (cluster -1). Let’s look at how they differ in terms of predicted probabilities. According to the chart below, there appear to be four clusters with increased predicted probabilities (clusters 3, 5, 6, and 7), three with decreased predicted probabilities (clusters 0, 2, and 4), and one with more mixed predictions (cluster 1).\r\n\r\n\r\nShow code\r\nimport matplotlib.colorbar as colorbar\r\n\r\n# plotting the distribution of probability of positive classes\r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=y_pred_proba, cmap='viridis', s=50, alpha = 0.5)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\ncbar = plt.colorbar(scatter)\r\ncbar.set_label('Predicted class probability', rotation=270, labelpad=15)\r\nplt.show()\r\n\r\n\r\nNow we can check for the selected clusters which features and in which direction most affect their respective predicted probabilities. For example, we can see from the table below that the clusters with lower predicted probabilities (clusters 0, 2 and 4) are driven either by the respective values in features 6 and 9 (cluster 0), or by the respective values in features 0, 3, 7 and 9 (cluster 2), or by the respective values in feature 0 (cluster 4).\r\n\r\n\r\nShow code\r\n\r\n# creating a summary for each cluster across all feature drivers of predicted probabilities of the positive class\r\ntab1 <- py$explanations_df %>% \r\n  dplyr::group_by(cluster) %>% \r\n  dplyr::summarise_all(~median(., na.rm = TRUE))\r\n\r\n# tab dataviz\r\nDT::datatable(\r\n  round(tab1,2),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 10, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  formatStyle(\r\n    names(tab1 %>% dplyr::select(-cluster, -predicted_class_proba)),\r\n    background = styleColorBar(range(tab1 %>% dplyr::select(-cluster, -predicted_class_proba)), 'lightblue'),\r\n    backgroundSize = '98% 90%',\r\n    backgroundRepeat = 'no-repeat',\r\n    backgroundPosition = 'center'\r\n  ) \r\n\r\n\r\n\r\nCombined with information on the median values of these specific features, we can get a good idea of the people who tend to under-perform and “why”. For example, people in cluster 0 score too high in feature 6 and too low in feature 9; people in cluster 2 score too low in features 0, 3, 7 and 9; and people in cluster 4 score too low in features 0. Given these differences, it would be useful to consider different approaches to try to improve the sales performance of people based on information about which persona they belong to. We could also repeat a similar analysis for clusters with higher predicted probabilities to see which combination of features tends to be associated with higher performance.\r\n\r\n\r\nShow code\r\n\r\n# creating a df with X and y from testing part of the dataset\r\ntest_df = pd.DataFrame(X_test, columns=['feature_{}'.format(i) for i in range(n_features)])\r\ntest_df['predicted_class_proba'] = y_pred_proba\r\ntest_df['cluster'] = clusters\r\n\r\n\r\n\r\nShow code\r\n\r\n# creating a summary for each cluster across all raw data and predicted probability of the positive class\r\ntab2 <- py$test_df %>% \r\n  dplyr::group_by(cluster) %>% \r\n  dplyr::summarise_all(~median(., na.rm = TRUE))\r\n\r\n# tab dataviz\r\nDT::datatable(\r\n  round(tab2,2),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 10, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  formatStyle(\r\n    names(tab2 %>% dplyr::select(-cluster, -predicted_class_proba)),\r\n    background = styleColorBar(range(tab2 %>% dplyr::select(-cluster, -predicted_class_proba)), 'lightblue'),\r\n    backgroundSize = '98% 90%',\r\n    backgroundRepeat = 'no-repeat',\r\n    backgroundPosition = 'center'\r\n  ) \r\n\r\n\r\n\r\nMaybe you’ll find the method described here useful in one of your ML projects. Happy data sleuthing 🙂\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-09-personas-based-on-ml-local-interpretation-algos/./persona_lm_illustration.png",
    "last_modified": "2023-11-13T16:54:11+01:00",
    "input_file": "personas-based-on-ml-local-interpretation-algos.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-10-24-sentiment-analysis-validation/",
    "title": "Sentiment analysis of employee survey comments using zero-shot classification",
    "description": "An attempt to validate a zero-shot sentiment classification.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-10-24",
    "categories": [
      "zero-shot learning",
      "ai",
      "machine learning",
      "sentiment analysis",
      "employee survey",
      "python"
    ],
    "contents": "\r\nRecently, I experimented with zero-shot classification - a machine learning task where the model classifies data into categories it hasn’t encountered during training - to determine the sentiment of comments from an employee survey (positive, negative, mixed, neutral).\r\nI chose this approach as an alternative to the traditional lexicon and rule-based NLTK sentiment analyzer. Using the transformer architecture of the model (bart-large-mnli from Facebook), I aimed to capture context from entire comments when gauging their sentiment. I admit that I had some doubts because the model wasn’t specifically trained for sentiment analysis. Instead, it generalizes its understanding from the MNLI tasks to deduce sentiments.\r\nFortunately, each comment was tied to specific statements that also received ratings on a standard 0-10 scale. This allowed me to cross-reference the sentiment classification with the ratings people assigned to these statements.\r\nSo, how did the model do? As the attached chart shows, the average sentiment classification probability is pretty compellingly consistent with the ratings on the standard scale.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(readr)\r\n\r\nmydata <- readr::read_csv(\"./mydata.csv\")\r\n\r\nlabel_data <- mydata %>% \r\n  dplyr::group_by(sentiment_category) %>% \r\n  dplyr::slice(1)\r\n\r\nmydata %>% \r\n  ggplot2::ggplot(aes(x = Score, y = avg_sentiment_score, color = sentiment_category)) +\r\n  ggplot2::geom_line(linewidth = 1.5) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::scale_y_continuous(limits = c(0,NA), breaks = seq(0,1,0.1)) +\r\n  ggplot2::scale_x_continuous(limits = c(-1,10), breaks = seq(0,10,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Mixed\"=\"#7b00e7\", \"Negative\"=\"#db370e\", \"Neutral\"=\"grey\", \"Positive\"=\"#208600\")) +\r\n  ggplot2::geom_text(data=label_data, aes(label=sentiment_category), nudge_x=-0.5, hjust=0.75, vjust = 0, size = 5, fontface = \"bold\") +\r\n  ggplot2::labs(\r\n    x = \"SURVEY ITEM RATING (0-10)\",\r\n    y = \"AVERAGE PROBABILITY OF SENTIMENT CATEGORY\",\r\n    title = \"Validation of zero-shot sentiment classification\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"none\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nSeems like I can add another useful tool to my toolbox. If you are interested in trying it out, you can use a short code snippet below for it.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nfrom transformers import pipeline\r\n\r\n# sentiment analysis with zero-shot classification using the facebook/bart-large-mnli model\r\nsentiment_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\r\ncomments = df['comments'].to_list()\r\n# defining the candidate labels \r\ncandidate_labels = [\"positive\", \"negative\", \"neutral\", \"mixed\"]\r\n# setting the hypothesis template\r\nhypothesis_template = \"The sentiment of this employee feedback is {}.\"\r\n# estimating the sentiment labels\r\nprediction = sentiment_classifier(comments, candidate_labels, hypothesis_template=hypothesis_template)\r\nprediction = pd.DataFrame(prediction)\r\n# creating columns with predicted sentiment (label with the highest probability) and sentiment scores for individual labels  \r\ndf['sentiment_label'] = prediction['labels'].apply(lambda x: x[0])\r\ndf['sentiment_score_positive'] = prediction.apply(lambda x: x['scores'][x['labels'].index('positive')], axis=1)\r\ndf['sentiment_score_negative'] = prediction.apply(lambda x: x['scores'][x['labels'].index('negative')], axis=1)\r\ndf['sentiment_score_neutral'] = prediction.apply(lambda x: x['scores'][x['labels'].index('neutral')], axis=1)\r\ndf['sentiment_score_mixed'] = prediction.apply(lambda x: x['scores'][x['labels'].index('mixed')], axis=1)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-24-sentiment-analysis-validation/./cover_pic.png",
    "last_modified": "2023-10-24T12:56:56+02:00",
    "input_file": "sentiment-analysis-validation.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-10-23-nlp-llm-and-onboarding/",
    "title": "Using NLP & LLM to combat 'tip-of-the-tongue' moments during onboarding",
    "description": "How to make onboarding experience a little bit smoother with the help of NLP and LLM.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-10-10",
    "categories": [
      "onboarding",
      "nlp",
      "llm",
      "embeddings",
      "ai"
    ],
    "contents": "\r\nI believe that many of you have had a similar experience when you joined a new company: you have taken a ton of notes from various meetings and trainings, you know that the answer to your immediate question is in there somewhere, but it is too hard to find the right notes, so you capitulate and decide to try to find the answer in a different way, e.g. by asking a more tenured colleague.\r\nTo make better use of my notes from my current onboarding, I created a quick and dirty app that uses embeddings to find the relevant files with my notes and LLM to summarise answers to my questions.\r\nSo, for example, if I need to find out who owns a certain business process, I can just type my question into the app and quickly get an answer, including tips on what files to look at to verify the answer or find other related information.\r\n\r\nIf you want to check out the code behind the app, you can find it in this GitHub repo.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-23-nlp-llm-and-onboarding/./lost_info.jpg",
    "last_modified": "2023-10-23T19:47:23+02:00",
    "input_file": "nlp-llm-and-onboarding.knit.md"
  },
  {
    "path": "posts/2023-09-24-exploration-vs-exploitation-tradeoff/",
    "title": "Exploration vs. Exploitation trade-off in our calendars",
    "description": "As I was going through my calendar recently to check who I had already met during my onboarding at Sanofi, I realized that one way to look at the calendar is through the lens of the Exploration vs. Exploitation trade-off. What lessons can we take from this?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-25",
    "categories": [
      "calendar",
      "time management",
      "decision making",
      "exploration vs exploration trade-off"
    ],
    "contents": "\r\nAs a “maker”, I’m used to using my time in larger blocks of time to do focused work on various data products. Now, however, my time tends to be spread out between a series of 30-45 minute long meetings that help me navigate a complex organization like Sanofi, explore opportunities for collaboration, and make valuable contacts that can help me deliver on tasks in the future.\r\n\r\nIllustration of the Maker’s vs. Manager’s schedule.\r\nI wondered if I could gain any potentially useful insights from this particular “intuition pump”. I took the help of the book “Algorithms to Live By” by Brian Christian and Tom Griffiths and found at least five such insights:\r\nAlthough one has a primary mode of operation (e.g. exploitation for makers, exploration for managers), one should not completely ignore the other mode and should allocate a small, consistent portion of one’s time to ensure that one does not miss valuable insights or opportunities that lie outside one’s primary mode of operation (based on the Epsilon-Greedy strategy to solve the Multi-Armed Bandit problem).\r\nOver time, as one becomes more aware of the options available, the need for exploration may decrease, allowing for more targeted exploitation of known best opportunities (based on the Decaying Epsilon strategy to solve the Multi-Armed Bandit problem).\r\nAlthough our primary focus is on delivery, we should remain open and try new options that have the highest potential upside, despite the associated high uncertainty (based on the Upper Confidence Bound strategy to solution of the Multi-Armed Bandit problem).\r\nIf you plan to stay with the company for a while, you should be open to further exploration, as newly found valuable opportunities can be used later in the future, and vice versa (btw, this could be a useful signal of intentions to leave, as some research suggests that people actually tend to resolve this type of trade-offs this way).\r\nBe Bayesian, i.e., choose your paths based on your current beliefs, regularly check the balance between exploration and exploitation on your calendar, and update your beliefs and make new, more informed decisions as evidence of outcomes accumulates (based on the Thompson Sampling strategy to solution of the Multi-Armed Bandit problem).\r\nMaybe you’ll find some of these insights helpful in finding a better balance between exploring new paths and following familiar ones, leading to more effective use of your time. If nothing else, take this as a recommendation to read the book mentioned above - there’s plenty in there for inspiration.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-24-exploration-vs-exploitation-tradeoff/./multiArmedBandit.png",
    "last_modified": "2023-09-24T20:26:53+02:00",
    "input_file": "exploration-vs-exploitation-tradeoff.knit.md",
    "preview_width": 1400,
    "preview_height": 544
  },
  {
    "path": "posts/2023-09-17-bayesian-simulation/",
    "title": "Harnessing Bayesian analysis for business process simulation",
    "description": "A demonstration of how the outputs of Bayesian analysis can be used to simulate business processes while preserving inherent uncertainties.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-17",
    "categories": [
      "bayesian statistics",
      "business process simulation",
      "python",
      "pymc"
    ],
    "contents": "\r\nOne of the advantages of doing statistical analysis in a Bayesian framework is that its generative part makes it a natural fit for business process simulation, which incorporates all the uncertainties inherent in the statistical models we use to capture the patterns of interest.\r\nOnce the parameters of the models have been estimated, their corresponding posterior distributions can be easily sampled and used in combination with a range of input values to simulate the expected outcomes, including the associated uncertainty that needs to be taken into account when making decisions.\r\nTo illustrate, imagine, for example, that as a CHRO you want to estimate the number of potential new employees brought in by employees who choose to participate in a new referral program. Fortunately, you have data from a small, three-month pilot of this program in which you offered participation to a small random sample of employees. Using a combination of simple Binomial and Poisson models, you can easily arrive at a reasonable estimate of the outcome of interest when you introduce the program to a larger portion of the company.\r\nLets’ implement this simple illustrative example with PyMC, a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods.\r\nFirst, let’s upload the data from the pilot program. The first table includes 150 employees who were randomly selected and offered participation in the pilot program. The second table then shows 42 employees who chose to participate in the program and the number of potential new employees they brought in.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\n\r\n# table with all pilot nominees\r\nnominees=pd.read_excel(\"./dataBayesSim.xlsx\", sheet_name=\"nominees\")\r\n\r\n# table with all pilot participants\r\nparticipants=pd.read_excel(\"./dataBayesSim.xlsx\", sheet_name=\"participants\")\r\n\r\n# showing first few rows of the tables\r\nnominees.head(5)\r\n  employeeID  participation\r\n0         e1              0\r\n1         e2              0\r\n2         e3              0\r\n3         e4              0\r\n4         e5              0\r\n\r\nShow code\r\nparticipants.head(5)\r\n  employeeID  referrals\r\n0         e7          2\r\n1         e9          4\r\n2        e10          2\r\n3        e11          2\r\n4        e25          1\r\n\r\nIn order to estimate the expected number of new potential employees after the introduction of a new referral program to a larger part of the company, we want to model the probability that nominees actually participate in the program and the expected number of potential candidates that a participant brings in. To do this, we can fit Binomial and Poisson models to the data, respectively. As mentioned above, we will do this in a Bayesian framework that will make it easier to deal with uncertainty later in our simulation. A side note: for the sake of brevity, I omit the usual sanity checks that should be performed before drawing any conclusions from fitted models - e.g., checking for convergence of Markov chains or posterior predictive checks for how well the fitted models predict observed data.\r\nLet’s start with the first model. From the summary below, we see that the estimated probability of participating in the programme is between 0.21 and 0.35.\r\n\r\n\r\nShow code\r\nimport pymc as pm\r\n\r\nShow code\r\n# estimating the participation rate\r\nnominated = nominees.shape[0]\r\nparticipated = nominees['participation'].sum()\r\n\r\n# setting up the model\r\nwith pm.Model() as participationModel:\r\n  # assigning a flat Beta prior for p\r\n  p = pm.Beta(\"p\", alpha=1, beta=1)\r\n  \r\n  # defining likelihood\r\n  obs = pm.Binomial(\"obs\", p=p, n=nominated, observed=participated)\r\n  \r\n  # running mcmc\r\n  idata = pm.sample(3000, tune=500, chains=3, cores=1)\r\n  \r\n  # generating posterior predictive sample\r\n  participationModelPosterior = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\r\n\r\n\r\n\r\nShow code\r\nimport arviz as az\r\n\r\n# trace plot showing the evolution of parameter vector over the iterations of Markov chain(s)\r\n#az.plot_trace(idata, kind=\"trace\", divergences=\"bottom\", show=True)\r\n\r\n# posterior predictive check\r\n#az.plot_ppc(participationModelPosterior, num_pp_samples=500, random_seed=7, alpha=0.3, textsize=14, kind='kde', show=True)\r\n\r\n# tabular and visual summary of the posterior probability distribution of the p parameter value\r\naz.summary(idata).round(2)\r\n   mean    sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat\r\np  0.28  0.04    0.22     0.35        0.0      0.0    4338.0    6229.0    1.0\r\n\r\nShow code\r\naz.plot_posterior(idata, hdi_prob=.95, show=True)\r\n\r\n\r\nThe second model then suggests that program participants brought in an average of 1.7 to 2.5 referrals.\r\n\r\n\r\nShow code\r\n# setting up the Poisson model\r\nwith pm.Model() as referralModel:\r\n  # weakly informative exponential prior for lambda parameter with mean 3\r\n  lambda_ = pm.Exponential('lambda', 1/3)\r\n  # alternative flat prior for lambda parameter\r\n  #lambda_ = pm.Uniform('lambda', lower=0, upper=25)\r\n  \r\n  # Poisson likelihood\r\n  y_obs = pm.Poisson('y_obs', mu=lambda_, observed=participants['referrals'])\r\n  \r\n  # running mcmc\r\n  trace = pm.sample(3000, tune=500, chains=3, cores=1)\r\n  \r\n  # generating posterior predictive sample\r\n  referralModelPosterior = pm.sample_posterior_predictive(trace, extend_inferencedata=True)\r\n\r\n\r\n\r\nShow code\r\n# trace plot showing the evolution of parameter vector over the iterations of Markov chain(s)\r\n#az.plot_trace(trace, kind=\"trace\", divergences=\"bottom\", show=True)\r\n\r\n# posterior predictive check\r\n#az.plot_ppc(referralModelPosterior, num_pp_samples=500, random_seed=7, alpha=0.3, textsize=14, kind='kde', show=True)\r\n\r\n# tabular and visual summary of the posterior probability distribution of the p parameter value\r\naz.summary(trace).round(2)\r\n        mean    sd  hdi_3%  hdi_97%  ...  mcse_sd  ess_bulk  ess_tail  r_hat\r\nlambda   2.1  0.22    1.69     2.51  ...      0.0    3676.0    6056.0    1.0\r\n\r\n[1 rows x 9 columns]\r\n\r\nShow code\r\naz.plot_posterior(trace, hdi_prob=.95, show=True)\r\n\r\n\r\nWe can now sample the posterior distributions of the parameters p and lambda, insert them into the dataframe, and for each row/combination calculate the expected number of referrals brought in by participating employees when the program is rolled out to the entire population of 1500 employees. As you can see below, using the IQR, our CHRO can expect recruiters to reach 790 to 990 potential candidates once the new company-wide referral program is in place.\r\n\r\n\r\nShow code\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"white\")\r\n\r\n# sampling from the posterior distribution the parameters p and lambda\r\nposterior = pd.DataFrame({\r\n    'p': idata['posterior']['p'].values.flatten(),\r\n    'lambda': trace['posterior']['lambda'].values.flatten()\r\n})\r\n\r\n# computing expected number of referrals with 1500 nominees\r\nposterior['expectedReferrals'] = 1500*posterior['p']*posterior['lambda']\r\n\r\n# computing summary statistics\r\nm = posterior['expectedReferrals'].mean().round(1)\r\nQ1 = np.percentile(posterior['expectedReferrals'], 25).round(1)\r\nQ2 = np.percentile(posterior['expectedReferrals'], 50).round(1)\r\nQ3 = np.percentile(posterior['expectedReferrals'], 75).round(1)\r\n\r\n# visualizing results\r\nsns.histplot(posterior['expectedReferrals'], bins=30, kde=True, color='#5b7db6').set(xlabel =\"Number of new referrals\", ylabel = \"Count\")\r\nplt.gcf().suptitle('Expected referrals for program rollout to all 1500 employees', fontsize=13)\r\nplt.gca().set_title(f'Mean={m}, Q1={Q1}, Median={Q2}, Q3={Q3}', fontsize=10)\r\nplt.show()\r\n\r\n\r\nAnd it doesn’t have to end there. For example, this estimate can be combined with other inputs, e.g. the cost of a new referral program, the cost of an alternative solution, etc., to make a better informed decision, taking into account the existing uncertainty.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-17-bayesian-simulation/./plot.png",
    "last_modified": "2023-09-17T19:26:16+02:00",
    "input_file": "bayesian-simulation.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-09-03-dag-and-double-ml/",
    "title": "A plausible model of data-generating process eats ML algorithms for breakfast",
    "description": "An illustration of one of the lessons I took away from studying the use of meta-learners for causal inference.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-03",
    "categories": [
      "causal inference",
      "double machine learning",
      "dag",
      "python"
    ],
    "contents": "\r\nI’m just in the middle of studying the use of meta-learners for causal inference, i.e. how to repurpose conventional ML models to estimate treatment effect. One of the lessons I took away from this study so far is that no matter how fancy the ML algorithm we use, without a plausible model of the data-generating process, we cannot hope for an unbiased estimate of the treatment effect as without it, it is difficult to select relevant covariates and avoid those that introduce bias into our estimation (e.g. colliders, mediators and their descendants).\r\nTo illustrate and reinforce this lesson for myself, I coded a small example of estimating the average treatment effect of training performance on productivity using synthetic data with a known data-generating process and the Double ML method.\r\nFirst, we create a DAG of the causal relationships behind our data. We see that, according to this DAG, employee productivity is affected by years of experience, job fit, available resources, and training performance, which is our variable of interest. Employee satisfaction is also part of the DAG, which is affected by available resources, employee productivity, and training performance.\r\n\r\n\r\nShow code\r\n\r\nimport networkx as nx\r\nimport matplotlib.pyplot as plt\r\n\r\n# initializing DAG\r\nG = nx.DiGraph()\r\n\r\n# adding nodes\r\nnodes = ['Years of experience', 'Resources', 'Job_fit', 'Training', 'Productivity', 'Satisfaction', 'Cognitive ability']\r\nG.add_nodes_from(nodes)\r\n\r\n# adding edges\r\nedges = [('Years of experience', 'Productivity'),\r\n         ('Cognitive ability', 'Productivity'),\r\n         ('Cognitive ability', 'Training'),\r\n         ('Resources', 'Productivity'),\r\n         ('Resources', 'Satisfaction'),\r\n         ('Job_fit', 'Productivity'),\r\n         ('Training', 'Productivity'),\r\n         ('Training', 'Satisfaction'),\r\n         ('Productivity', 'Satisfaction')]\r\n\r\nG.add_edges_from(edges)\r\n\r\n# drawing DAG\r\npos = nx.fruchterman_reingold_layout(G, seed=5, iterations = 500, k = 2)\r\nlabels = {node: node for node in G.nodes()}\r\nnx.draw(G, pos, with_labels=True, labels=labels, node_color='lightblue', font_weight='bold', node_size=1500, font_size=10)\r\nplt.title(\"DAG\")\r\nplt.show()\r\n\r\n\r\nNow let’s generate synthetic data corresponding to this DAG.\r\n\r\n\r\nShow code\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# generating synthetic data\r\nnp.random.seed(42)\r\nn = 1000\r\n\r\n# generating features years of experience, resources, job fit, and cognitive ability\r\nX = np.random.normal(0, 1, (n, 4))\r\n\r\n# setting the true causal effect of training performance \r\ntrue_causal_effect = 3.5\r\n\r\n# training is influence by cognitive ability\r\ntraining = 1.8 * X[:, 3] + np.random.normal(0, 1, n)\r\n\r\n# productivity is influenced by all features \r\nproductivity = 2 * X[:, 0] + 1 * X[:, 1] + 0.5 * X[:, 2] + 2.2 * X[:, 3] + true_causal_effect * training + np.random.normal(0, 1, n)\r\n\r\n# defining an employee satisfaction variable that is influenced by resources, productivity, and training\r\nsatisfaction = 0.3 * X[:, 1] +  0.2 * productivity + 1.3 * training + np.random.normal(0, 1, n)\r\n\r\n# creating a final dataFrame\r\ndf = pd.DataFrame(X, columns=['years_of_experience', 'resources', 'job_fit', 'cognitive_ability'])\r\ndf['training'] = training\r\ndf['productivity'] = productivity\r\ndf['satisfaction'] = satisfaction \r\n\r\nWithout DAG, i.e., without understanding the data-generating process behind our data, we might be tempted to “throw” all available variables into our estimator. We can give it a try and use Double ML for that - a popular framework designed to provide unbiased and consistent estimates of treatment effects in the presence of high-dimensional controls while reducing the risk of overfitting. The common Double ML procedure looks as follows:\r\nRandomly splitting the data into two parts: one for estimating the control function and the other for estimating the treatment effect.\r\nUsing the first part of the data to train a machine learning model to predict the outcome variable based on covariates (without the treatment variable). Similarly, training another model to predict the treatment variable based on covariates.\r\nUsing the second part of the data to form the residuals for outcome and treatment variable and running a simple linear regression of outcome residuals on treatment residuals.\r\nRepeating steps 1-3 but switching the roles of the two data splits and averaging the estimates to get the final average treatment estimate.\r\nLet’s apply this approach to our data and use XGBoost models to estimate the control function and see how successful we will be in our efforts to estimate the known causal effect of training on productivity.\r\n\r\n\r\nShow code\r\n\r\nimport numpy as np\r\nfrom xgboost import XGBRegressor\r\nfrom sklearn.linear_model import LinearRegression\r\n\r\n# creating a variable for data splits\r\nnp.random.seed(42)\r\ndf['part'] = np.random.choice([0, 1], size=len(df))\r\n\r\nestimates = []\r\n\r\nfor p in [0,1]:\r\n    \r\n    # auxiliary variables for switching the roles of the two data splits\r\n    firstPart = 1-p\r\n    secondPart = p-0\r\n    \r\n    # used covariates\r\n    covariates = ['years_of_experience', 'resources', 'job_fit', 'cognitive_ability', 'satisfaction']\r\n  \r\n    # preparing datasets\r\n    X_First = df.loc[df['part'] == firstPart, covariates].values\r\n    y_Outcome_First = df.loc[df['part'] == firstPart,'productivity'].values\r\n    y_Treatment_First = df.loc[df['part'] == firstPart,'training'].values\r\n    \r\n    X_Second = df.loc[df['part'] == secondPart, covariates].values\r\n    y_Outcome_Second = df.loc[df['part'] == secondPart,'productivity'].values\r\n    y_Treatment_Second = df.loc[df['part'] == secondPart,'training'].values\r\n    \r\n    # controlling for covariates for productivity using XGBoost\r\n    model_Outcome = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Outcome.fit(X_First, y_Outcome_First)\r\n    residual_Outcome = y_Outcome_Second - model_Outcome.predict(X_Second)\r\n    \r\n    # controlling for covariates for training using XGBoost\r\n    model_Treatment = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Treatment.fit(X_First, y_Treatment_First)\r\n    residual_Treatment = y_Treatment_Second - model_Treatment.predict(X_Second)\r\n    \r\n    # Part 3: Estimate the causal effect using the residuals with Linear Regression\r\n    model_causal = LinearRegression()\r\n    model_causal.fit(residual_Treatment.reshape(-1, 1), residual_Outcome)\r\n    double_ml_effect = model_causal.coef_[0]\r\n    \r\n    estimates.append(double_ml_effect)\r\n  \r\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\r\n\r\nShow code\r\nprint(\"The estimated Average Treatment Effect is:\", sum(estimates)/len(estimates))\r\nThe estimated Average Treatment Effect is: 2.8913607224732614\r\n\r\nWe see that the estimated causal effect of training is approximately 2.9, quite far from the known value of 3.5. How so? Well, if you look at the DAG above, you can see that by using all the variables as covariates, not only have we correctly blocked the backdoor by controlling for employee cognitive ability, but we have also introduced bias into our estimation by controlling for satisfaction, which is a collider - a variable that is affected by both the outcome (productivity) and the treatment (training). And controlling for colliders leads to spurious correlations between variables or, as in our case, deflates the size of the estimated effect. We can easily check this using DAGitty, which is a wonderful browser-based environment for creating and analysing causal diagrams.\r\n\r\nSo let’s repeat the estimation, but now without satisfaction variable as covariate. As you can see below, we are now much closer to the actual causal effect.\r\n\r\n\r\nShow code\r\n\r\n# creating a variable for data splits\r\nnp.random.seed(42)\r\ndf['part'] = np.random.choice([0, 1], size=len(df))\r\n\r\nestimates = []\r\n\r\nfor p in [0,1]:\r\n    \r\n    # auxiliary variables for switching the roles of the two data splits\r\n    firstPart = 1-p\r\n    secondPart = p-0\r\n    \r\n    # used covariates\r\n    covariates = ['years_of_experience', 'resources', 'job_fit', 'cognitive_ability']\r\n  \r\n    # preparing datasets\r\n    X_First = df.loc[df['part'] == firstPart, covariates].values\r\n    y_Outcome_First = df.loc[df['part'] == firstPart,'productivity'].values\r\n    y_Treatment_First = df.loc[df['part'] == firstPart,'training'].values\r\n    \r\n    X_Second = df.loc[df['part'] == secondPart, covariates].values\r\n    y_Outcome_Second = df.loc[df['part'] == secondPart,'productivity'].values\r\n    y_Treatment_Second = df.loc[df['part'] == secondPart,'training'].values\r\n    \r\n    # controlling for covariates for productivity using XGBoost\r\n    model_Outcome = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Outcome.fit(X_First, y_Outcome_First)\r\n    residual_Outcome = y_Outcome_Second - model_Outcome.predict(X_Second)\r\n    \r\n    # controlling for covariates for training using XGBoost\r\n    model_Treatment = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Treatment.fit(X_First, y_Treatment_First)\r\n    residual_Treatment = y_Treatment_Second - model_Treatment.predict(X_Second)\r\n    \r\n    # Part 3: Estimate the causal effect using the residuals with Linear Regression\r\n    model_causal = LinearRegression()\r\n    model_causal.fit(residual_Treatment.reshape(-1, 1), residual_Outcome)\r\n    double_ml_effect = model_causal.coef_[0]\r\n    \r\n    estimates.append(double_ml_effect)\r\n  \r\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\r\n\r\nShow code\r\nprint(\"The estimated Average Treatment Effect is:\", sum(estimates)/len(estimates))\r\nThe estimated Average Treatment Effect is: 3.477088075986001\r\n\r\nDespite the simplicity of the example presented, I think it nicely demonstrates that by using fancy ML algorithms like XGBoost, we are not relieved of the need to have a plausible model of the data-generating process when estimating causal effects from observational data, and we can’t just blindly rely on some magical powers of ML to squeeze what we need out of whatever input we give it.\r\nJust a side note: If you want to make your life a little bit easier when using Double ML, you can use the DoubleML library for Python and R. The code below illustrates this library in action on our synthetic data.\r\n\r\n\r\nShow code\r\n\r\nfrom doubleml import DoubleMLData, DoubleMLPLR\r\n\r\nnp.random.seed(42)\r\n\r\n# specifying data and roles of individual variables\r\ndml_data = DoubleMLData(df, y_col='productivity', d_cols='training', x_cols=['years_of_experience', 'resources', 'job_fit', 'cognitive_ability'])\r\n\r\n# specifying ML model(s) used for estimation of the nuisance parts\r\nml_xgb = XGBRegressor(eta = 0.1, n_estimators =25)\r\n\r\n# initializing and parametrizing the model object which will be used to perform the estimation\r\ndml_plr_xgb = DoubleMLPLR(\r\n  dml_data,\r\n  ml_l = ml_xgb,\r\n  ml_m = ml_xgb,\r\n  n_folds = 5,\r\n  n_rep = 10,\r\n  score = 'partialling out',\r\n  dml_procedure = 'dml2')\r\n\r\n# estimation and inference\r\ndml_plr_xgb.fit()\r\n<doubleml.double_ml_plr.DoubleMLPLR object at 0x000001A9267BACE0>\r\n\r\nShow code\r\ndml_plr_xgb.summary\r\n              coef   std err          t  P>|t|     2.5 %    97.5 %\r\ntraining  3.426416  0.049597  69.085291    0.0  3.329208  3.523624\r\n\r\nShow code\r\ndml_plr_xgb.confint()\r\n             2.5 %    97.5 %\r\ntraining  3.329208  3.523624\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-03-dag-and-double-ml/dag-and-double-ml_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-09-03T22:52:43+02:00",
    "input_file": "dag-and-double-ml.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-08-22-r-and-power-bi/",
    "title": "Embedding R (or Python) ML models in Power BI dashboards",
    "description": "In my new job, we currently rely a lot on Power BI when presenting people-related insights to our stakeholders. Since I know Power BI quite superficially and we also want to share insights from more complex analyses with our stakeholders, I spent part of the weekend studying how to incorporate ML models created in R or Python into Power BI dashboards. I put my learnings in this blog post. It's definitely not rocket science, but it may still shorten the learning path for some of you who are in a similar situation.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-08-22",
    "categories": [
      "power bi",
      "r",
      "python",
      "machine learning"
    ],
    "contents": "\r\nNote to start: Although the example presented in this post is demonstrated in R, it could be analogously implemented in Python as well.\r\nFor demonstration, I will use the well-known IBM artificial attrition dataset. First, we need to train the model. I suppose you know the drill and know all the steps to go through to get a useful and reliable model. The following R script implements the whole process of data preparation, (XGBoost) model tuning, training, and validation. After we have the prediction model ready, we have to save it, as we will use it later in the Power BI (PBI) dashboard.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\nlibrary(recipes)\r\nlibrary(themis)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors()) %>%\r\n  themis::step_smote(Attrition, over_ratio = 1)\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n  )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n    tune::collect_predictions(parameters = xgb_best) %>% \r\n    yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  parsnip::fit(data_train)\r\n\r\n\r\n# variable importance\r\nxgb_fit %>%\r\n  tune::extract_fit_parsnip() %>%\r\n  vip::vip(num_features = 10, geom = \"col\")\r\n\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n    yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# saving the final model\r\nsaveRDS(xgb_fit, \"./final_xgb_model.RDS\")\r\n\r\n# saving testing data as a new dataset that will be used in Power BI dashboard\r\ndata_test %>%\r\n  dplyr::select(-Attrition) %>%\r\n  writexl::write_xlsx(\"./newData.xlsx\")\r\n\r\n\r\nNow we can move on to PBI. We use the testing data as a new dataset that we will score by the trained model and show stakeholders how the predicted flight risk varies by department, job role, and gender.\r\nFirst, we should check in the PBI settings that PBI has access to our R (or Python) instance (File -> Options and setting -> GLOBAL/R scripting/Python scripting). If so, we can upload new data using the Get data dialog and go to the Transform tab in the Query editor. Here, we should first check that the data types match those in R when preparing the model, and then we can load and run our scoring algorithm using the Run R script dialog. The following script will do the job.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(workflows)\r\n\r\n# uploading trained model\r\nmodel <- readRDS(\"PathTo/YourModel.RDS\")\r\n\r\n# generate predictions\r\nprediction <- predict(model, dataset) %>% \r\n  dplyr::bind_cols(predict(model, dataset, type = \"prob\")) %>%\r\n  dplyr::rename(\r\n    predictedCat = .pred_class,\r\n    probLeave = .pred_1,\r\n    probStay = .pred_0\r\n  )\r\n\r\n# enriching original dataset\r\nprediction <- cbind(dataset, prediction)\r\n\r\n\r\nAfter running the script, our original data will be enriched with the generated predictions. We can apply this transformation, save it and exit the Query editor using the Close & Apply button. We will then have flight risk predictions that we can visualize along with the original data, allowing us to see how predicted flight risk varies by department, job role, and gender, among other things. Again, we could use R or Python for this purpose, as PBI does not offer some types of data visualization by default, especially those related to visualizing data variability. For example, to create the raincloud plots below, I used the ggplot2 and ggdist packages in R. Below is a script used within the R visual that implements one of the plots. To fine-tune them, I recommend using RStudio (if you have it on your computer), which is accessible directly from the R visual editor in PBI.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n# dataset <- data.frame(probLeave, Department)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggdist)\r\n\r\ndataset %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(Department, probLeave, mean, .na_rm = TRUE), y = probLeave)) + \r\n  ggdist::stat_halfeye(\r\n    adjust = .5, \r\n    width = .6, \r\n    .width = 0, \r\n    justification = -.3, \r\n    point_colour = NA,\r\n    fill = \"#bca36b\",\r\n    color = \"#bca36b\"\r\n  ) + \r\n  ggplot2::geom_boxplot(\r\n    width = .25, \r\n    outlier.shape = NA\r\n  ) +\r\n  ggplot2::stat_summary(fun.y=mean, geom=\"point\", shape=20, size=5, color=\"#bca36b\", fill=\"#bca36b\") +\r\n  geom_point(\r\n    size = 1.3,\r\n    alpha = .3,\r\n    position = position_jitter(seed = 1, width = .09, height = .008\r\n    )\r\n  ) + \r\n  ggplot2::coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"PROBABILITY OF LEAVING\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 strip.text.x = element_text(size = 11, face = \"plain\"),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=c(.95,.88),\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(0,0,0,0),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nWe may also want to allow dashboard users to enter custom values for specific personas of interest and let the scoring algorithm predict the corresponding risk of leaving the company. To make the demonstration of this feature somewhat easier, I will use a different prediction model that uses only some of the strongest predictors.\r\nTo enable this feature, we need to create several parameters that the user can set. For numeric predictors we can use the New parameter dialog box on the Modeling tab - we set the name of the parameters, their data type, their minimum, maximum, and default values, and confirm we want to show corresponding sliders in the dashboard. For categorical parameters, we need to create and then upload a table with all possible combinations of values of the categorical parameters used. When using R, we can use for example the expand.grid function to do this. The fields from this table are then used and displayed in the dashboard as single-selection filters. And all these parameters then serve as input to the R visual, where we load the model, change the names of some parameters to match those expected by the model, generate a prediction, and create the resulting visualization. All these steps are implemented by the following R script.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n# dataset <- data.frame(JobLevelParameter Value, YearsWithCurrManagerParameter Value, OverTime, Department, JobSatisfaction)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(workflows)\r\n\r\ndata <- dataset %>%\r\n    dplyr::rename(\r\n        JobLevel = `JobLevelParameter Value`, \r\n        YearsWithCurrManager = `YearsWithCurrManagerParameter Value`\r\n    )\r\n\r\n# uploading trained model\r\nmodel <- readRDS(\"PathTo/YourModel2.RDS\")\r\n\r\n# generate predictions\r\nprediction <- predict(model, data) %>% \r\n  dplyr::bind_cols(predict(model, data, type = \"prob\")) %>%\r\n  dplyr::rename(\r\n    predictedCat = .pred_class,\r\n    probLeave = .pred_1,\r\n    probStay = .pred_0\r\n  )\r\n\r\n# enriching original dataset\r\nprediction <- cbind(data, prediction)\r\n\r\nprediction %>%\r\n  ggplot2::ggplot(aes(x = 1, y = probLeave)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#444492\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::geom_text(aes(label = round(probLeave, 2)), nudge_y = 0.04, color = \"black\", size = 7, fontface = \"bold\") +\r\n  scale_y_continuous(limits = c(0,1.08), breaks = seq(0,1,0.1)) +\r\n  labs(\r\n    x = \"\",\r\n    y = \"PROBABILITY OF LEAVING\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 strip.text.x = element_text(size = 11, face = \"plain\"),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=c(.95,.88),\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(0,0,0,0),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nThis completed the work on our local computer. You can download the final dashboard here. If you use it, be sure to update the paths to the trained model in both the Query Editor and the R visuals. The next step is to set up the Power BI Service and install and configure the on-premises data gateway so that the R scripts will work in dashboards shared with others. More on this in a future blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-08-22-r-and-power-bi/./r_with_powerbi.png",
    "last_modified": "2023-08-24T21:28:55+02:00",
    "input_file": "r-and-power-bi.knit.md",
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2023-07-20-vocational-interests/",
    "title": "Vocational interests don't seem so uninteresting after all",
    "description": "Quite surprising (at least to me) findings on the validity of vocational interests for predicting a range of important work outcomes.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-20",
    "categories": [
      "vocational interests",
      "meta-analysis",
      "predictive validity",
      "job performance"
    ],
    "contents": "\r\nTo be honest, until recently, I tended to underestimate the importance of vocational interests in job performance prediction and considered questions about them during a job interview as a formality. In my defense, this view has also been supported by the low estimates of their predictive validity reported by classics such as Schmidt & Hunter (1998).\r\nHowever, I adjusted my view after coming across the updated validity estimate in the Sackett et al. meta-analysis (2022) and the results of the Nye et al. meta-analysis (2017) on the validity of interests for predicting job performance.\r\nThe latter study reported the following interesting findings:\r\nCorrelation between interest scores and job performance (corrected for both indirect range restriction and unreliability in the criterion) is 0.16 (SE=0.03).\r\n\r\nInterest congruence/match between an individual’s interests and his or her work is a much stronger predictor of performance outcomes than interest scores alone, with baseline correlations of 0.32 and 0.16, respectively.\r\n\r\nInterests are significantly better predictors of organizational citizenship behavior than other criteria (job performance, task performance, OCB, persistence, CWB, and training performance) but are less valid for predicting CWB and task performance.\r\nIf you tend to think about vocational interests as I have until recently, perhaps these two studies will help you update your priors a little bit 😉\r\nNote: The attached schemes are taken from another excellent resource on this topic by Nye et al. (2012).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-20-vocational-interests/./scheme2.png",
    "last_modified": "2023-07-20T19:09:20+02:00",
    "input_file": "vocational-interests.knit.md",
    "preview_width": 491,
    "preview_height": 283
  },
  {
    "path": "posts/2023-07-14-induced-centrality/",
    "title": "Induced centralities",
    "description": "A post about useful complement to common ONA centrality measures.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-14",
    "categories": [
      "organizational network analysis",
      "centrality measures",
      "r"
    ],
    "contents": "\r\nWhile reading the excellent publication Social Networks at Work from SIOP’s Organizational Frontiers series (btw, highly recommended to all PA professionals), I came across the interesting and useful concept of induced centrality.\r\nIt sort of reverses the logic of the common ONA centrality measures, which focus primarily on what one gets from the surrounding network of connections, and instead shows how individual nodes contribute to some global network characteristic of interest, i.e. what one does for the network as a whole.\r\nIts calculation is quite simple and straightforward - you just need to first calculate the global characteristic of the network that you are interested in as a reference point, e.g. its coherence, and then calculate how this measure changes when you remove individual nodes from the network. From this, you can deduce that the nodes that cause the most change in a specific direction contribute the most to a given measure.\r\nIn addition to its versatility and the interesting angle it offers, it can also be very useful in making visible otherwise hidden and invisible “heroes” who contribute to the greater good under the radar of public recognition.\r\nWhat follows is a small demonstration of using induced centrality to estimate which people play the role of expressive leaders who shorten the lengths of paths in the network. It’s an implementation of the idea briefly described in the aforementioned publication Social Networks at Work:\r\n“For example, suppose one theorizes that there are certain individuals in groups (perhaps called expressive leaders) who provide a certain social glue such that they tend to shorten the lengths of paths in the network (see, for example, the Heidi Roizen case by McGinn and Tempest, 2010). This sounds like we should use closeness centrality, since it is concerned with path lengths. But there are two problems with this. First of all, closeness centrality only counts the shortest paths, and not the circuitous paths that things such as gossip often take. Second, closeness gets at how long it takes for information to reach a given node, who is then presumed to benefit from this information. But the concept we’ve just outlined is about individuals who enable others to have short paths so that the whole group benefits. Closeness was not designed to measure this, and doesn’t. However an induced centrality measure can be created to measure exactly this: to what degree paths lengthen when you remove each node from the network.”\r\nFirst, let’s upload the data used for the demonstration and create the network object. I will use a dataset that captures information-sharing links between 15 members of my friendship network.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# uploading data\r\ndf <- readxl::read_excel(\"./friendshipNetwork.xlsx\")\r\n\r\n# creating network object\r\ng <- igraph::graph_from_data_frame(df, directed=TRUE) \r\n\r\n\r\nWe can now iterate over all directed pairs of nodes and compute the average length of paths between nodes we will use as a reference point. We won’t use all the paths but only the three shortest paths between each pair of nodes that would enable us to capture some of the circuitous paths mentioned in the problem description above.\r\n\r\n\r\nShow code\r\n\r\nall_nodes <- V(g)\r\ntotal_length <- 0\r\ntotal_paths <- 0\r\n# setting the number of 3 shortest paths between pair of nodes to capture also some of the circuitous paths \r\ntop_shortest_paths <- 3\r\n\r\n# loop for directed network\r\nfor (i in 1:length(all_nodes)) {\r\n  for (j in 1:length(all_nodes)) {\r\n    if (i != j) {\r\n      lengths <- unlist(igraph::all_simple_paths(g, all_nodes[i], all_nodes[j], mode = \"out\"))\r\n      lengths <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n      total_length <- total_length + sum(lengths, na.rm = TRUE)\r\n      total_paths <- total_paths + length(lengths)\r\n    }\r\n  }\r\n}\r\n\r\n# loop for undirected network \r\n# for (i in 1:(length(all_nodes) - 1)) {\r\n#   for (j in (i + 1):length(all_nodes)) {\r\n#     lengths <- all_simple_paths(g, all_nodes[i], all_nodes[j])\r\n#     lengths <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n#     total_length <- total_length + sum(lengths, na.rm = TRUE)\r\n#     total_paths <- total_paths + length(lengths)\r\n#   }\r\n# }\r\n\r\n# computing the average length of paths\r\naverage_length_ref <- total_length / total_paths\r\n\r\n\r\nNow let’s remove each node one at a time from the network and calculate the average lengths of paths between pairs consisting of the remaining nodes. We also need to deal somehow with situations when node removal leads to the disconnection of previously connected nodes (in such situations, the distance between nodes is by default assumed to be infinite or undefined, which would bias our estimation). I have decided to take the three shortest paths from the complete network and add 1 (this is somewhat equivalent to the additional effort required to find a new bonding connection). After this step, we can subtract the reference point from the obtained values and get the information about the absence of which nodes lengthens the paths between other nodes and thus act as a kind of social glue that facilitates the spread of information between nodes.\r\n\r\n\r\nShow code\r\n\r\n# vector for saving average lengths of paths for individual nodes\r\naverage_lengths <- numeric(length(all_nodes))\r\n\r\nfor (k in 1:length(all_nodes)) {\r\n\r\n  g_new <- g\r\n  g_new <- igraph::delete_vertices(g_new, all_nodes[k])\r\n  all_nodes_new <- V(g_new)\r\n  \r\n  total_length_new <- 0\r\n  total_paths_new <- 0\r\n  \r\n  # for directed network\r\n  for (i in 1:length(all_nodes_new)) {\r\n    for (j in 1:length(all_nodes_new)) {\r\n      if (i != j) {\r\n        # lengths of paths in the network with removed node \r\n        lengths_new <- unlist(igraph::all_simple_paths(g_new, all_nodes_new[i], all_nodes_new[j], mode=\"out\"))\r\n        # lengths of paths in the complete network\r\n        lengths <- unlist(igraph::all_simple_paths(g, all_nodes_new[i]$name, all_nodes_new[j]$name, mode=\"out\"))\r\n        # dealing with situations when node removal leads to disconnection of previously connected nodes by taking 3 shortest paths from the full network and adding 1\r\n        if(is.null(lengths_new) & !is.null(lengths)){\r\n          lengths_new <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n          lengths_new <- lengths_new + 1\r\n        } else{\r\n          lengths_new <- sort(lengths_new, decreasing = FALSE)[1:top_shortest_paths]\r\n        }\r\n        total_length_new <- total_length_new + sum(lengths_new, na.rm = TRUE)\r\n        total_paths_new <- total_paths_new + length(lengths_new)\r\n      }\r\n    }\r\n  }\r\n  \r\n  # for undirected network \r\n  # for (i in 1:(length(all_nodes_new) - 1)) {\r\n  #   for (j in (i + 1):length(all_nodes_new)) {\r\n  #     lengths_new <- length_of_all_paths(g_new, all_nodes_new[i], all_nodes_new[j])\r\n  #     lengths <- length_of_all_paths(g, all_nodes_new[i]$name, all_nodes_new[j]$name)\r\n  #     if(is.null(lengths_new) & !is.null(lengths)){\r\n  #      lengths_new <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n  #      lengths_new <- lengths_new + 1\r\n  #     } else{\r\n  #      lengths_new <- sort(lengths_new, decreasing = FALSE)[1:top_shortest_paths]\r\n  #     }\r\n  #     total_length_new <- total_length_new + sum(lengths_new, na.rm = TRUE)\r\n  #     total_paths_new <- total_paths_new + length(lengths_new)\r\n  #   }\r\n  # }\r\n  \r\n  average_lengths[k] <- total_length_new / total_paths_new\r\n}\r\n\r\n# computing the difference between average and reference point\r\naverage_length_diff <- average_lengths - average_length_ref\r\n\r\n# assigning computed differences to individual nodes\r\nV(g)$avg_length_diff <- average_length_diff\r\n\r\n\r\nThe graph below shows that nodes P2, P8, and P4 are the most critical in this respect.\r\n\r\n\r\nShow code\r\n\r\nggraph::ggraph(g, layout = \"kk\") + # other available layouts: 'star', 'circle', 'gem', 'dh', 'graphopt', 'grid', 'mds', 'randomly', 'fr', 'kk', 'drl', 'lgl'\r\n  ggraph::geom_edge_link(arrow = arrow(length = unit(2.5, 'mm')), end_cap = circle(2, 'mm')) +\r\n  ggraph::geom_node_point(aes(size = avg_length_diff), alpha = 1, color = ifelse(V(g)$avg_length_diff>0, \"#e15759\", \"black\")) +\r\n  ggplot2::scale_size_continuous(range = c(0.1,8)) +\r\n  ggraph::geom_node_label(aes(label = name), repel = TRUE) +\r\n  ggplot2::labs(\r\n    title = \"Expressive leaders who shorten path lengths in the network\",\r\n    subtitle = \"Demonstration of the concept of induced centrality\",\r\n    size = \"Increase in average path length after node removal\",\r\n    caption = \"\\nNodes with an increase greater than 0 are highlighted in red.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,9,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title = element_blank(),\r\n    axis.text = element_blank(),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    axis.line = element_blank(),\r\n    legend.position=\"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2\r\n3.4.0.\r\nℹ Please use `linewidth` in the `default_aes` field and elsewhere\r\n  instead.\r\nThis warning is displayed once every 8 hours.\r\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning\r\nwas generated.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-14-induced-centrality/./ona.png",
    "last_modified": "2023-07-14T12:09:17+02:00",
    "input_file": "induced-centrality.knit.md",
    "preview_width": 1500,
    "preview_height": 1111
  },
  {
    "path": "posts/2023-07-11-searching-and-querying-aihr-blog-posts/",
    "title": "Searching & querying AIHR blog posts on People Analytics topics",
    "description": "I'm sharing a by-product of my learning about vector database search that may be useful to some of you who want to learn something new about People Analytics.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-11",
    "categories": [
      "aihr",
      "people analytics",
      "vector database",
      "learning by doing"
    ],
    "contents": "\r\nIt’s an app that allows you to quickly search for People Analytics topics in a database of almost 260 AIHR blog posts and get key ideas and insights from the posts that best match your search topic, with the possibility to go to the original post via a provided link.\r\n\r\nThe cluster analysis revealed that the posts cover more than 30 topics, so there’s a lot to choose from. Feel free to give it a try and let me know how it works for you. Here’s a link to the app.\r\nLast but not least, a big “thank you” goes to AIHR and all the contributing authors for putting together such an amazing resource on People Analytics 🙏\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-11-searching-and-querying-aihr-blog-posts/./search.jpg",
    "last_modified": "2023-07-11T21:08:22+02:00",
    "input_file": "searching-and-querying-aihr-blog-posts.knit.md"
  },
  {
    "path": "posts/2023-07-03-bayesian-shrinkage/",
    "title": "Using Bayesian shrinkage in reporting employee turnover",
    "description": "When you report turnover rates by team, do you take into account the size of individual teams, or do you take the turnover rate numbers as they are?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-03",
    "categories": [
      "employee turnover",
      "hr reporting",
      "hr metrics",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nIt’s a no-brainer that when comparing team performance on retention, the departure of one person in small teams with fewer members will have a more significant impact on turnover rates than it does in larger teams. However, this creates room for potential misinterpretation and inadequate actions.\r\nIt’s related to the well-known phenomenon of variation being more likely in smaller samples. Fans of Daniel Kahneman and Amos Tversky will probably recall the famous cognitive bias of insensitivity to sample size which occurs when people judge the probability of obtaining a statistic regardless of sample size.\r\nOne way to reduce this effect is Bayesian shrinkage. This approach, which serves as a kind of regularization, involves borrowing information from the overall company turnover rate to influence the turnover rate of smaller teams. It works by “shrinking” the turnover rate of smaller teams towards the company average, and thus creating a balance between the observed rate and the company average.\r\nIt’s not dissimilar to what one intuitively does when deciding what movie to watch or what restaurant to go to, when movies and restaurants vary widely in the number of ratings available.\r\nYou can see this approach in action on the chart below. The smaller the team and the further its turnover rate is from the company-wide turnover rate (the vertical dashed line), the more the turnover rate estimate for that team is shifted towards the company-wide value (the distance between the red cross and the black dot) - see, for example, teams 4 and 6. For comparison, check teams 12 and 9 that don’t show much of a shrinkage effect due to the big size and small distance from the company-wide turnover rate, respectively.\r\n\r\n\r\nShow code\r\n\r\n# uploading the necessary libraries\r\nlibrary(tidyverse) # data manipulation and dataviz\r\nlibrary(brms) # bayesian stats\r\nlibrary(cmdstanr) # bayesian stats\r\nlibrary(ggdist) # dataviz\r\n\r\n# creating artificial data\r\n# setting a seed for reproducibility\r\nset.seed(123)\r\n# number of teams\r\nnTeams <- 12\r\n# generating team sizes ranging from 10 to 100\r\nteamSizes <- sample(10:100, nTeams, replace = TRUE)\r\n# generating 'true' turnover rates from a beta distribution\r\ntrueRates <- rbeta(nTeams, 2, 10)\r\n# for each team, simulating the number of employees who left\r\nnumberLeft <- rbinom(nTeams, teamSizes, trueRates)\r\n# generating team IDs\r\nteamId <- as.character(1:nTeams)\r\n# creating the data frame\r\nteamsData <- data.frame(teamId, teamSizes, numberLeft)\r\n\r\n# fitting multilevel Bayesian logistic regression with wide, uninformative priors\r\nmodel <- brms::brm(\r\n  numberLeft | trials(teamSizes) ~ 1 + (1 | teamId),\r\n  data = teamsData,\r\n  family = binomial(link = \"logit\"),\r\n  prior = prior(normal(0, 10), class = \"Intercept\"),\r\n  chains = 4,\r\n  iter = 5000,\r\n  control = list(adapt_delta = 0.95),\r\n  seed = 123,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n# model summary\r\n#summary(model)\r\n#fixef(model)\r\n#ranef(model)\r\n\r\n# extracting the posterior samples\r\n#parnames(model)\r\nposteriorDraws <- as_draws_df(model, variable = \"Intercept\", regex = TRUE) %>%\r\n  dplyr::select(-.draw, -.chain, -.iteration, -sd_teamId__Intercept) %>%\r\n  tidyr::pivot_longer(cols = -b_Intercept, names_to = \"teamId\", values_to = \"paramValues\") %>%\r\n  dplyr::mutate(\r\n    teamId = stringr::str_extract(teamId, \"\\\\d+\"),\r\n    team = stringr::str_glue(\"Team {teamId}\"),\r\n    logOdds = paramValues + b_Intercept,\r\n    estimatedTR = exp(logOdds) / (1 + exp(logOdds))\r\n    ) %>%\r\n  dplyr::left_join(teamsData %>% dplyr::select(teamId, teamSizes), by = \"teamId\") %>%\r\n  dplyr::mutate(\r\n    team = stringr::str_glue(\"{team} (n={teamSizes})\"),\r\n    team = forcats::fct_reorder(factor(team), as.numeric(teamId))\r\n    )\r\n\r\n# computing fixed, population-level effect estimate\r\nfixedEffect <- 1 / (1 + exp(-1*fixef(model)[1]))\r\n\r\n# computing observed turnover rate by team\r\nobservedRT <- teamsData %>% \r\n  dplyr::mutate(team = stringr::str_glue(\"Team {teamId}\")) %>%\r\n  dplyr::mutate(\r\n    observedRT = numberLeft/teamSizes,\r\n    team = stringr::str_glue(\"{team} (n={teamSizes})\"),\r\n    team = forcats::fct_reorder(factor(team), as.numeric(teamId))\r\n  )\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# dataviz\r\nggplot2::ggplot(data = posteriorDraws, aes(x=estimatedTR, group = team)) + \r\n  ggdist::stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\", fill = \"skyblue\", normalize = \"groups\") +\r\n  ggplot2::geom_point(data = observedRT, aes(x = observedRT, y = 0, group = team), color = \"red\", inherit.aes = F, size = 2.5, shape=3, stroke = 1.5) +\r\n  ggplot2::geom_vline(xintercept = fixedEffect, linetype = \"dashed\", color = \"#2C2F46\") +\r\n  ggplot2::facet_wrap(~team, ncol = 2) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,1,0.1)) +\r\n  ggplot2::labs(\r\n    y = \"NORMALIZED DENSITY\",\r\n    x = \"ESTIMATED TURNOVER RATE\",\r\n    title = \"Using Bayesian shrinkage in reporting employee turnover\",\r\n    caption = \"\\nThe black solid lines represent the 80% and 95% credibility intervals, respectively. The black dot represents the median of the Highest Confidence Interval.\\nThe vertical dashed line represents the fixed, population-level effect estimate. The red cross represents the observed turnover rate for a given team.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 1),\r\n    axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 9),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nIf you are trying to deal with this effect in your reporting practice, can you share the approach you use and serves you well?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-03-bayesian-shrinkage/./shrinkage.png",
    "last_modified": "2023-07-04T08:15:40+02:00",
    "input_file": "bayesian-shrinkage.knit.md",
    "preview_width": 421,
    "preview_height": 349
  },
  {
    "path": "posts/2023-06-28-job-comparator/",
    "title": "A bet on a new job",
    "description": "Sharing a by-product of my search for a new full-time job.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-29",
    "categories": [
      "job selection",
      "bayesian statistics",
      "r",
      "shiny"
    ],
    "contents": "\r\nBeing in the final phase of my new job search, I wanted to be able to aggregate the information and impressions I gathered during the hiring process, with all the uncertainties, to make the best decision possible.\r\nTo do this, I put together a “back-of-the-envelope” calculation that combines, in a Bayesian way, the impressions one has of various aspects of the jobs one is applying for.\r\n\r\nIt works with several factors that research suggests are related to job satisfaction and that a person has the chance to estimate subjectively to some degree during the hiring process from job ads, interviews, sample tasks, company reviews from current or former employees on Glassdoor, etc. Specifically, it takes into account the following factors:\r\nSalary\r\nJob security\r\nWork-life balance\r\nCareer progression\r\nOrganizational culture\r\nJob content\r\nBenefits\r\nRelationships with supervisors\r\nRelationships with colleagues\r\nOne’s task is to simply determine, based on the available information, the range of how much he or she can expect to be satisfied with these factors in a given job. The app then aggregates the evidence and estimates the expected overall level of job satisfaction, including a level of uncertainty that can provide a guide as to where the person should try to obtain some additional information to reduce this uncertainty. One can also adjust the weights of each factor based on one’s personal preferences.\r\nIf you have at least two job offers to choose from, you may find the app as useful as I did. Here’s a link to the app.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-28-job-comparator/./decisionMaking.jpg",
    "last_modified": "2023-06-30T06:58:24+02:00",
    "input_file": "job-comparator.knit.md"
  },
  {
    "path": "posts/2023-06-27-deloitte-hc-trends-themes/",
    "title": "Themes in Deloitte's Global Human Capital Trends between 2011 and 2023",
    "description": "Reading the latest release of Deloitte Global HC Trends made me wonder what common themes this regular series has been covering throughout its 12 years long history.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-27",
    "categories": [
      "global hc trends",
      "deloitte",
      "openai",
      "r",
      "python"
    ],
    "contents": "\r\nAside from satisfying a simple curiosity, it was also a good opportunity to try out a nerdy combination of various cool DS tools: openAI’s embeddings for determining trends similarity, UMAP for dimensionality reduction, DBSCAN for cluster analysis, openAI’s chat completion for cluster summarization and naming, Plotly for interactive dataviz, Shiny for dashboarding, and Python and R for orchestrating it all.\r\nThe result? The analysis revealed 13 distinct themes among the 118 specific trends:\r\nGlobal Talent Management Strategies (23)\r\nLeadership Development and Talent Management (16)\r\nHR Transformation and Innovation (15)\r\nHuman Capital and Workforce Strategies (12)\r\nWorkforce Data and Analytics (10)\r\nCognitive Technologies and Workforce (8)\r\nEmployee-Centric Learning and Development (7)\r\nPerformance Management and Compensation (7)\r\nImproving Employee Experience and Well-being (6)\r\nCloud Computing and HR Transformation (4)\r\nEmployee Engagement and Retention (4)\r\nDiversity in Business Strategy (3)\r\nWorkplace Flexibility Strategies (3)\r\n\r\nIt’s no wonder I’ve had dejavu feelings about some trends over the years, but that’s why they are called trends, because they persist over time, right? 😉\r\nIf you would like to check the analysis output interactively and in greater detail, you can use this simple dashboard.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-27-deloitte-hc-trends-themes/./dttHCTrends.png",
    "last_modified": "2023-06-27T11:10:07+02:00",
    "input_file": "deloitte-hc-trends-themes.knit.md",
    "preview_width": 1000,
    "preview_height": 1000
  },
  {
    "path": "posts/2023-06-22-team-level-predictors-of-innovation/",
    "title": "Team-level predictors of innovation at work",
    "description": "Team processes seem to beat team composition and structure when it comes to innovation at work.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-22",
    "categories": [
      "innovation",
      "team",
      "meta-analysis",
      "people analytics"
    ],
    "contents": "\r\nAt least this is suggested by an interesting meta-analysis of team-level predictors of innovation at work by Hülsheger, Anderson, & Salgado (2009).\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data with the results of the meta-analysis \r\ndata <- readxl::read_xlsx(\"./metaAnalysisResults.xlsx\")\r\n#dplyr::glimpse(data)\r\n\r\ndata %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(variable, rho), y = rho, group = area, color = area)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_errorbar(aes(ymin=l95, ymax=h95), width=.2, position=position_dodge(0.05), linewidth = 1) +\r\n  ggplot2::geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_color_manual(values = c(\"Team composition and structure\"=\"#4e79a7\", \"Team process\" = \"#f28e2b\")) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"AVERAGE CORRECTED CORRELATION\",\r\n    title = \"Team-Level Predictors of Innovation at Work\",\r\n    caption = \"\\nThe bars around the point estimates represent the 2.5% lower and 97.5% upper limits of the 95% confidence interval.\\nSource: Hülsheger, U. R., Anderson, N., & Salgado, J. F. (2009). Team-level predictors of innovation at work: A comprehensive meta-analysis spanning three decades\\nof research. Journal of Applied Psychology, 94(5), 1128–1145. https://doi.org/10.1037/a0015978\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,10,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 10, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"top\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.text = element_text(size = 12),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nIf that’s true, not sure whether it’s good news or bad news for companies’ innovation initiatives. Is it easier to change processes or team composition? I expect there will be a lot of “it depends” 😉 What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-22-team-level-predictors-of-innovation/./innovation.jpg",
    "last_modified": "2023-06-22T13:09:06+02:00",
    "input_file": "team-level-predictors-of-innovation.knit.md"
  },
  {
    "path": "posts/2023-06-19-latent-class-analysis/",
    "title": "Latent Class Analysis of responses from employee surveys",
    "description": "How listening to a podcast about conspiracies and disinformation inspired me to try out a \"new\" statistical tool popular among sociologists.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-19",
    "categories": [
      "latent class analysis",
      "employee survey",
      "people analytics",
      "r"
    ],
    "contents": "\r\nI recently listened to a podcast about a very interesting study about conspiracies and disinformation in Czech society, and one of the authors of the study, Matous Pilnacek, a sociologist, spoke very enthusiastically and positively during the interview about the analytical possibilities offered by Latent Class Analysis (LCA).\r\nNot being a sociologist, among whom this tool is well-known, I was quite easily impressed and hooked 🙂 LCA allows probabilistic modeling of multivariate categorical data with the assumption that there are latent classes of people who are characterized by a common pattern of probabilities of responses to a set of questions on some categorical, e.g. Likert scale.\r\nLCA is thus a natural fit for identifying subgroups of people with similar work views. Compared to other methods used in this context, such as Factor Analysis, k-means, or hierarchical clustering, it has several advantages:\r\nLCA can handle well categorical observed variables.\r\nLCA estimates probabilities of class membership, so it inherently incorporates uncertainty about which class each individual belongs to.\r\nLCA allows for meaningful analysis of missing answers or non-answers together with proper answers on a Likert scale.\r\nLCA provides a framework (via information criteria like BIC, AIC, or likelihood ratio tests) for comparing models with different numbers of classes.\r\nLCA’s output is intuitive and easy to interpret.\r\nWhat follows, is a a small demonstration of this tool on artificial employee survey data accompanying the book Predictive HR Analytics: Mastering the HR Metric by Edwards & Edwards (2019). It contains the survey responses of 832 employees on a 1 ‘strongly disagree’ to 5 ‘strongly agree’ response scale for a following set of statements.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\n\r\n# uploading legend to the data\r\nlegend <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Legend\") \r\n\r\n# user-friendly table with individual survey items\r\nDT::datatable(\r\n  legend %>% dplyr::mutate(Scale = as.factor(Scale)),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nBefore moving on to modeling, we first need to wrangle the data a bit - select the relevant variables, change their data type, and replace missing values with “Prefer Not to Say” reply category.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ndata <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Data\")\r\n\r\n# preparing the data for modeling\r\nmydata <- data %>%\r\n  dplyr::select(-sex:-ethnicity) %>%\r\n  dplyr::mutate_all(as.character) %>%\r\n  replace(is.na(.), \"Prefer Not to Say\") %>%\r\n  dplyr::mutate_all(factor, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"Prefer Not to Say\"))\r\n\r\n\r\nNow we can proceed with the modeling. For this we will use poLCA R package. One of the parameters to be set is the expected number of classes. To choose the right number, we need to fit several LCA models with different numbers of classes and, based on information criteria such as BIC or AIC, choose the model with the best balance between model complexity and good fit to the data. Here, I set the parameter to the best value I determined earlier.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(poLCA)\r\n\r\n# specifying and running the model\r\nset.seed(1234)\r\nlca_model <- poLCA::poLCA(\r\n  cbind(ManMot1, ManMot2, ManMot3, ManMot4, ocb1, ocb2, ocb3, ocb4, aut1, aut2, aut3, Justice1, Justice2, Justice3, JobSat1, JobSat2, Quit1, Quit2, Quit3, Man1, Man2, Man3, Eng1, Eng2, Eng3, Eng4, pos1, pos2, pos3)~1, \r\n  data = mydata, \r\n  nclass = 4, \r\n  nrep = 3,\r\n  verbose = FALSE,\r\n  graphs = FALSE\r\n)\r\n\r\n\r\nLet’s check some of the outputs of the analysis: 1) probability of responses to individual items by people from different classes,\r\n\r\n\r\nShow code\r\n\r\n# looking at the model output\r\nlca_model\r\n\r\nConditional item response (column) probabilities,\r\n by outcome variable, for each class (row) \r\n \r\n$ManMot1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1350 0.4245 0.3092 0.0875 0.0227 0.0211\r\nclass 2:  0.0732 0.1220 0.1463 0.0244 0.0000 0.6341\r\nclass 3:  0.0906 0.1342 0.2384 0.2976 0.2021 0.0371\r\nclass 4:  0.4765 0.3591 0.1274 0.0121 0.0132 0.0118\r\n\r\n$ManMot2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.3345 0.4531 0.1377 0.0534 0.0081 0.0132\r\nclass 2:  0.1220 0.1463 0.0976 0.0244 0.0000 0.6098\r\nclass 3:  0.2274 0.2580 0.2239 0.1508 0.1033 0.0366\r\nclass 4:  0.7507 0.1967 0.0325 0.0080 0.0121 0.0000\r\n\r\n$ManMot3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1180 0.3715 0.3610 0.1077 0.0209 0.0209\r\nclass 2:  0.0732 0.0732 0.2195 0.0000 0.0000 0.6341\r\nclass 3:  0.1005 0.1045 0.2517 0.3043 0.2019 0.0371\r\nclass 4:  0.4354 0.3590 0.1482 0.0252 0.0201 0.0121\r\n\r\n$ManMot4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1965 0.4915 0.1830 0.0853 0.0225 0.0212\r\nclass 2:  0.0732 0.1463 0.0976 0.0488 0.0000 0.6341\r\nclass 3:  0.1437 0.1539 0.2503 0.2418 0.1798 0.0305\r\nclass 4:  0.5329 0.3401 0.1070 0.0000 0.0121 0.0078\r\n\r\n$ocb1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.3092 0.4552 0.1880 0.0344 0.0026 0.0105\r\nclass 2:  0.1463 0.1707 0.0732 0.0000 0.0000 0.6098\r\nclass 3:  0.4200 0.3895 0.1463 0.0135 0.0122 0.0185\r\nclass 4:  0.6336 0.2680 0.0592 0.0272 0.0000 0.0120\r\n\r\n$ocb2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1645 0.4304 0.3103 0.0577 0.0131 0.0240\r\nclass 2:  0.0732 0.1707 0.1463 0.0000 0.0000 0.6098\r\nclass 3:  0.2486 0.4233 0.2233 0.0617 0.0246 0.0185\r\nclass 4:  0.3427 0.3929 0.1963 0.0403 0.0121 0.0156\r\n\r\n$ocb3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1373 0.3711 0.3566 0.0869 0.0293 0.0188\r\nclass 2:  0.0732 0.1707 0.0732 0.0488 0.0244 0.6098\r\nclass 3:  0.2291 0.3319 0.2798 0.0853 0.0493 0.0246\r\nclass 4:  0.3206 0.3222 0.2333 0.0647 0.0396 0.0196\r\n\r\n$ocb4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0971 0.2143 0.2582 0.2185 0.1988 0.0131\r\nclass 2:  0.0732 0.1220 0.0000 0.0976 0.0732 0.6341\r\nclass 3:  0.2431 0.1670 0.1081 0.1557 0.2955 0.0307\r\nclass 4:  0.3205 0.1708 0.1708 0.1759 0.1459 0.0162\r\n\r\n$aut1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5) Pr(6)\r\nclass 1:  0.1914 0.6010 0.1600 0.0476 0.0000     0\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000     1\r\nclass 3:  0.3128 0.3342 0.1095 0.1765 0.0671     0\r\nclass 4:  0.5015 0.4326 0.0659 0.0000 0.0000     0\r\n\r\n$aut2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5) Pr(6)\r\nclass 1:  0.0892 0.5412 0.2574 0.1121 0.0000     0\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000     1\r\nclass 3:  0.1521 0.3402 0.1754 0.2164 0.1159     0\r\nclass 4:  0.4131 0.4234 0.1476 0.0160 0.0000     0\r\n\r\n$aut3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1199 0.5929 0.2172 0.0674 0.0025 0.0000\r\nclass 2:  0.0000 0.0000 0.0244 0.0000 0.0000 0.9756\r\nclass 3:  0.2286 0.3560 0.1749 0.1426 0.0918 0.0061\r\nclass 4:  0.4325 0.4548 0.0964 0.0082 0.0000 0.0081\r\n\r\n$Justice1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0782 0.7351 0.1756 0.0110 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0061 0.0190 0.2754 0.3353 0.3398 0.0245\r\nclass 4:  0.0526 0.3241 0.5002 0.0742 0.0247 0.0242\r\n\r\n$Justice2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0759 0.7355 0.1788 0.0099 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0061 0.0190 0.2458 0.3396 0.3528 0.0367\r\nclass 4:  0.0445 0.2954 0.5556 0.0463 0.0380 0.0202\r\n\r\n$Justice3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0332 0.6473 0.2877 0.0318 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0000 0.0305 0.2225 0.3209 0.3956 0.0305\r\nclass 4:  0.0404 0.2239 0.5407 0.1179 0.0569 0.0202\r\n\r\n$JobSat1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0749 0.7008 0.1795 0.0298 0.0053 0.0097\r\nclass 2:  0.0244 0.0000 0.0000 0.0000 0.0000 0.9756\r\nclass 3:  0.1355 0.3993 0.1967 0.1263 0.1220 0.0202\r\nclass 4:  0.4703 0.4749 0.0267 0.0242 0.0000 0.0041\r\n\r\n$JobSat2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0527 0.6334 0.2540 0.0546 0.0000 0.0053\r\nclass 2:  0.0244 0.0000 0.0000 0.0000 0.0000 0.9756\r\nclass 3:  0.1100 0.2836 0.2498 0.2345 0.1037 0.0183\r\nclass 4:  0.3717 0.5498 0.0388 0.0356 0.0000 0.0040\r\n\r\n$Quit1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0086 0.0503 0.3622 0.5410 0.0193 0.0185\r\nclass 2:  0.0488 0.0000 0.1463 0.1707 0.0244 0.6098\r\nclass 3:  0.1476 0.2228 0.1411 0.2832 0.1870 0.0182\r\nclass 4:  0.0103 0.0176 0.0660 0.3442 0.5619 0.0000\r\n\r\n$Quit2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0076 0.1285 0.2897 0.5154 0.0378 0.0210\r\nclass 2:  0.0732 0.0732 0.0732 0.1463 0.0488 0.5854\r\nclass 3:  0.1804 0.2770 0.1514 0.2373 0.1476 0.0063\r\nclass 4:  0.0103 0.0356 0.0533 0.3251 0.5758 0.0000\r\n\r\n$Quit3\r\n           Pr(1)  Pr(2) Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0065 0.0874 0.365 0.5037 0.0137 0.0237\r\nclass 2:  0.0244 0.0488 0.122 0.1463 0.0488 0.6098\r\nclass 3:  0.1862 0.2099 0.159 0.2557 0.1649 0.0243\r\nclass 4:  0.0081 0.0179 0.058 0.3631 0.5529 0.0000\r\n\r\n$Man1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0161 0.4123 0.4782 0.0434 0.0026 0.0474\r\nclass 2:  0.0244 0.2683 0.0732 0.0244 0.0000 0.6098\r\nclass 3:  0.0183 0.1371 0.4832 0.2147 0.1404 0.0062\r\nclass 4:  0.2745 0.5092 0.1949 0.0094 0.0000 0.0121\r\n\r\n$Man2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0187 0.3700 0.4954 0.0661 0.0051 0.0448\r\nclass 2:  0.0244 0.2683 0.0244 0.0732 0.0000 0.6098\r\nclass 3:  0.0244 0.1232 0.4177 0.2329 0.1956 0.0062\r\nclass 4:  0.2381 0.5147 0.1917 0.0354 0.0081 0.0121\r\n\r\n$Man3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0213 0.3718 0.5068 0.0536 0.0044 0.0422\r\nclass 2:  0.0244 0.2439 0.0732 0.0488 0.0000 0.6098\r\nclass 3:  0.0244 0.0998 0.4262 0.2522 0.1913 0.0062\r\nclass 4:  0.2341 0.5234 0.1969 0.0255 0.0040 0.0160\r\n\r\n$Eng1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0100 0.3362 0.4682 0.1654 0.0158 0.0044\r\nclass 2:  0.0000 0.1463 0.1463 0.0732 0.0488 0.5854\r\nclass 3:  0.1709 0.2392 0.1777 0.2413 0.1526 0.0183\r\nclass 4:  0.2432 0.4776 0.2106 0.0551 0.0040 0.0094\r\n\r\n$Eng2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0406 0.4844 0.4136 0.0581 0.0000 0.0032\r\nclass 2:  0.0732 0.1220 0.1463 0.0244 0.0488 0.5854\r\nclass 3:  0.2601 0.2988 0.1498 0.1815 0.1098 0.0000\r\nclass 4:  0.3312 0.5015 0.1391 0.0088 0.0000 0.0193\r\n\r\n$Eng3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0043 0.4245 0.5074 0.0572 0.0000 0.0065\r\nclass 2:  0.0244 0.0976 0.1463 0.0732 0.0732 0.5854\r\nclass 3:  0.1887 0.2142 0.2704 0.1802 0.1403 0.0061\r\nclass 4:  0.2442 0.5647 0.1576 0.0151 0.0000 0.0183\r\n\r\n$Eng4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0026 0.3724 0.5065 0.1117 0.0026 0.0044\r\nclass 2:  0.0244 0.1463 0.1220 0.0732 0.0488 0.5854\r\nclass 3:  0.2015 0.1987 0.2597 0.2117 0.1283 0.0000\r\nclass 4:  0.2870 0.5095 0.1824 0.0117 0.0000 0.0095\r\n\r\n$pos1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.1046 0.6481 0.2241 0.0073 0.0160\r\nclass 2:  0.0000 0.0488 0.2195 0.0488 0.0732 0.6098\r\nclass 3:  0.0305 0.0479 0.1663 0.4436 0.3117 0.0000\r\nclass 4:  0.0728 0.2523 0.4393 0.1949 0.0127 0.0280\r\n\r\n$pos2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0023 0.1124 0.6717 0.1952 0.0024 0.0159\r\nclass 2:  0.0244 0.0488 0.2439 0.0488 0.0244 0.6098\r\nclass 3:  0.0549 0.0313 0.2471 0.3795 0.2812 0.0061\r\nclass 4:  0.0289 0.2999 0.4990 0.1482 0.0000 0.0240\r\n\r\n$pos3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0608 0.6749 0.2278 0.0205 0.0159\r\nclass 2:  0.0244 0.0000 0.2927 0.0244 0.0488 0.6098\r\nclass 3:  0.0488 0.0180 0.2200 0.3711 0.3421 0.0000\r\nclass 4:  0.0202 0.2505 0.5242 0.1766 0.0086 0.0200\r\n\r\nEstimated class population shares \r\n 0.4564 0.0493 0.197 0.2973 \r\n \r\nPredicted class memberships (by modal posterior prob.) \r\n 0.4639 0.0493 0.1947 0.2921 \r\n \r\n========================================================= \r\nFit for 4 latent classes: \r\n========================================================= \r\nnumber of observations: 832 \r\nnumber of estimated parameters: 583 \r\nresidual degrees of freedom: 249 \r\nmaximum log-likelihood: -29354.91 \r\n \r\nAIC(4): 59875.81\r\nBIC(4): 62629.81\r\nG^2(4): 47676.67 (Likelihood ratio/deviance statistic) \r\nX^2(4): 1.120338e+25 (Chi-square goodness of fit) \r\n \r\n\r\nprobabilities of people belonging to individual classes,\r\n\r\n\r\nShow code\r\n\r\n# probabilities of belonging to individual classes (first 10 rows)\r\nhead(lca_model$posterior, n = 10)\r\n\r\n              [,1] [,2]         [,3]          [,4]\r\n [1,] 1.705863e-01    0 1.088439e-08  8.294137e-01\r\n [2,] 1.208337e-02    0 9.879166e-01  0.000000e+00\r\n [3,] 4.990889e-13    0 1.000000e+00  0.000000e+00\r\n [4,] 9.999903e-01    0 9.649316e-06  6.879833e-09\r\n [5,] 0.000000e+00    1 0.000000e+00  0.000000e+00\r\n [6,] 9.988548e-01    0 3.805196e-06  1.141380e-03\r\n [7,] 1.782575e-13    0 1.000000e+00  0.000000e+00\r\n [8,] 0.000000e+00    0 1.000000e+00  0.000000e+00\r\n [9,] 1.272725e-04    0 1.104259e-11  9.998727e-01\r\n[10,] 1.064019e-09    0 1.000000e+00 1.215934e-244\r\n\r\nand 3) predicted belongings of people to the classes.\r\n\r\n\r\nShow code\r\n\r\n# predicted belongings to the classes\r\nlca_model$predclass\r\n\r\n  [1] 4 3 3 1 2 1 3 3 4 3 1 1 1 4 4 1 1 1 4 1 1 1 3 4 2 3 4 1 4 3 1 4\r\n [33] 3 1 4 3 3 1 3 3 4 4 1 1 3 1 4 1 1 3 1 3 4 1 1 2 1 4 3 1 3 1 4 3\r\n [65] 4 1 4 3 4 1 1 1 4 3 2 4 1 1 4 1 3 3 1 4 1 4 1 3 4 4 4 4 1 1 1 4\r\n [97] 4 1 3 1 3 3 1 4 1 4 3 3 1 2 1 1 4 1 4 1 4 1 4 4 1 4 3 1 4 1 1 3\r\n[129] 1 1 4 4 4 4 4 1 4 3 1 1 3 1 1 1 1 1 4 3 1 3 4 1 4 3 1 1 1 1 3 3\r\n[161] 1 1 1 4 1 1 3 1 1 1 4 3 3 1 1 1 1 1 1 3 1 3 1 1 4 4 3 4 1 4 3 3\r\n[193] 3 3 4 1 4 2 3 4 4 3 1 4 1 4 4 4 4 3 3 1 4 1 4 3 1 3 4 1 1 4 4 1\r\n[225] 4 1 4 4 1 4 4 4 3 4 1 1 4 1 3 1 4 1 4 1 3 4 3 1 1 1 4 1 1 1 4 1\r\n[257] 4 1 1 1 1 3 1 1 4 1 1 1 4 1 1 1 1 1 3 4 4 1 1 4 4 3 4 3 1 3 1 3\r\n[289] 1 2 4 4 3 4 1 4 4 1 1 1 1 1 4 1 4 3 1 1 4 1 1 3 1 4 1 4 1 4 1 4\r\n[321] 4 4 1 4 1 4 3 4 4 1 1 4 4 2 4 1 1 1 1 4 1 4 3 1 4 4 4 1 1 4 3 2\r\n[353] 4 1 4 3 3 1 4 4 3 2 4 1 1 1 4 1 2 1 1 4 1 1 1 1 4 1 1 1 1 2 4 1\r\n[385] 1 1 2 1 1 3 4 1 1 3 1 4 1 1 1 2 1 1 4 1 3 2 1 1 3 1 4 4 1 1 1 1\r\n[417] 1 1 4 4 4 1 1 3 1 1 4 3 1 1 1 1 1 4 1 2 1 4 4 1 1 1 1 1 2 1 4 2\r\n[449] 1 4 1 3 1 1 3 4 3 1 1 3 4 4 4 4 4 3 1 4 1 1 3 3 3 1 1 3 1 4 3 1\r\n[481] 1 1 1 4 4 2 1 3 1 1 1 4 4 1 3 1 1 3 1 1 1 1 3 4 4 4 3 1 1 1 4 3\r\n[513] 4 1 1 4 1 1 1 4 2 1 4 1 4 1 3 1 3 2 4 2 3 4 1 1 4 4 4 1 3 1 4 1\r\n[545] 1 1 4 3 1 4 4 4 1 3 1 2 1 1 3 4 2 3 2 1 1 1 1 4 4 2 1 3 1 4 1 3\r\n[577] 4 3 3 4 1 4 1 2 4 3 1 1 1 1 1 1 3 4 4 1 4 1 1 4 1 1 1 4 1 4 1 1\r\n[609] 3 1 3 1 1 3 4 4 3 1 4 1 3 4 1 1 4 1 4 4 4 1 3 4 2 4 1 1 1 4 1 4\r\n[641] 1 1 1 1 1 3 1 3 1 1 3 1 1 4 1 4 3 1 4 1 2 2 1 3 4 3 1 4 2 3 4 1\r\n[673] 3 3 4 3 1 1 1 1 4 1 1 3 1 3 1 1 3 1 1 1 1 1 1 3 2 1 1 1 4 1 4 4\r\n[705] 4 1 1 1 1 1 1 3 4 3 4 1 3 1 1 1 2 3 3 3 3 3 4 3 1 1 1 3 3 1 3 2\r\n[737] 1 2 3 1 4 1 4 2 3 1 1 4 3 1 3 4 3 4 1 4 1 1 3 4 4 4 3 1 1 2 1 1\r\n[769] 1 1 4 3 3 4 4 1 3 4 2 4 4 3 3 1 3 1 2 4 1 1 3 1 4 3 4 1 1 1 4 1\r\n[801] 2 1 4 1 4 4 4 1 1 4 1 4 4 4 2 4 4 3 4 1 1 3 1 3 3 1 1 4 4 3 3 1\r\n\r\nShow code\r\n\r\n# checking the size of the classes\r\ntable(lca_model$predclass)\r\n\r\n\r\n  1   2   3   4 \r\n386  41 162 243 \r\n\r\nIf we wanted to visualize the results using our own charts, for example, with full-stacked bar charts showing average probability of responses per scale and class, we need to do some data wrangling of information extracted from the fitted model.\r\n\r\n\r\nShow code\r\n\r\n# extracting information from the model for dataviz\r\nscaleNames <- mydata %>% names()\r\nclasses <- 4\r\nresponsesDf <- data.frame()\r\n\r\nfor(s in scaleNames){\r\n  \r\n  df <- data.frame()\r\n  \r\n  counter <- 1\r\n  \r\n  for(v in 1:length(lca_model$probs[[s]])){\r\n    \r\n    p <- lca_model$probs[[s]][v]\r\n    cl <- paste0(\"Class \", counter)\r\n    \r\n    supp <- data.frame(item = s, class = cl, p = p)\r\n    \r\n    df <- rbind(df, supp)\r\n    \r\n    if(counter < classes){\r\n      \r\n      counter <- counter + 1\r\n      \r\n    } else{\r\n      \r\n      counter <- 1\r\n      \r\n    }\r\n    \r\n  }\r\n  \r\n  df <- df %>%\r\n    dplyr::mutate(choice = rep(c(\"Strongly Disagree\",\"Disagree\",\"Neither Agree nor Disagree\",\"Agree\",\"Strongly Agree\",\"Prefer Not to Say\"), each=classes))\r\n  \r\n  responsesDf <- rbind(responsesDf, df)\r\n  \r\n}\r\n\r\n# computing size of individual classes\r\nclassCnts <- table(lca_model$predclass) %>% \r\n  as.data.frame() %>%\r\n  dplyr::mutate(\r\n    Var1 = as.character(Var1),\r\n    Var1 =  paste0(\"Class \", Var1)\r\n  ) %>%\r\n  dplyr::rename(\r\n    class = Var1,\r\n    freq = Freq\r\n    )\r\n\r\n# final df for dataviz\r\ndatavizDf <- responsesDf %>%\r\n  dplyr::mutate(\r\n    scale = stringr::str_remove_all(item, \"\\\\d\"),\r\n    choice = factor(choice, levels = c(\"Strongly Disagree\",\"Disagree\",\"Neither Agree nor Disagree\",\"Agree\",\"Strongly Agree\",\"Prefer Not to Say\"), ordered = TRUE),\r\n    scale = case_when(\r\n      scale == \"aut\" ~ \"Autonomy\",\r\n      scale == \"Eng\" ~ \"Engagement\",\r\n      scale == \"JobSat\" ~ \"Job Satisfaction\",\r\n      scale == \"Justice\" ~ \"Proc.Justice\",\r\n      scale == \"Man\" ~ \"Management\",\r\n      scale == \"ManMot\" ~ \"Mng.Motivation\",\r\n      scale == \"ocb\" ~ \"OCB\",\r\n      scale == \"pos\" ~ \"Org.Support\",\r\n      scale == \"Quit\" ~ \"Quitting Intentions\"\r\n    )\r\n    ) %>%\r\n  dplyr::group_by(scale, class, choice) %>%\r\n  dplyr::summarise(p = mean(p)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::left_join(classCnts, by = \"class\") %>%\r\n  dplyr::mutate(class = stringr::str_glue(\"{class} (n = {freq})\"))\r\n\r\n\r\nIn the resulting graphs, we quickly see that there is one class of people who often prefer not to give their opinion in an employee survey (Class 2), one class of people with a strongly negative view of their employment experience (Class 4), one class of people with a neutral to slightly negative view of work (Class 1), and one class of people with a neutral to positive view of work (Class 3).\r\n\r\n\r\nShow code\r\n\r\n# dataviz\r\ndatavizDf %>%\r\n  ggplot2::ggplot(aes(x = scale, y = p, fill = choice)) +\r\n  ggplot2::scale_x_discrete(limits = rev) +\r\n  ggplot2::geom_bar(stat = \"identity\", position = position_fill(reverse = TRUE)) +\r\n  ggplot2::scale_fill_manual(values = c(\"Strongly Disagree\"=\"#c00000\",\"Disagree\"=\"#ed7d31\",\"Neither Agree nor Disagree\"=\"#ffc000\",\"Agree\"=\"#00b050\",\"Strongly Agree\"=\"#4472c4\",\"Prefer Not to Say\"=\"#d9d4d4\")) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::facet_wrap(~class,nrow = 2) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"AVERAGE PROBABILITY OF RESPONSE\",\r\n    fill = \"\",\r\n    title = \"Latent Class Analysis of responses from the employee survey\"\r\n    ) +\r\n  ggplot2::theme_bw() +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text = element_text(size = 12, face = \"bold\"),\r\n    strip.background = element_rect(fill = \"white\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    legend.box.margin=margin(0,0,0,0),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\",\r\n  ) +\r\n  ggplot2::guides(fill = guide_legend(nrow = 1))\r\n\r\n\r\n\r\nIt would certainly be possible to delve deeper into the results, but for a basic overview of the process of LCA and its outputs, this might be sufficient. If interested, a more detailed introduction to LCA can be found, for example, in this practitioner’s guide.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-19-latent-class-analysis/./scheme.png",
    "last_modified": "2023-06-20T07:27:31+02:00",
    "input_file": "latent-class-analysis.knit.md",
    "preview_width": 930,
    "preview_height": 525
  },
  {
    "path": "posts/2023-06-05-mindfulness-and-objectivity/",
    "title": "Another positive effect of mindfulness meditation on the horizon?",
    "description": "Is it possibile to improve the objectivity of decision making through mindfulness meditation?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-05",
    "categories": [
      "decision-making",
      "mindfulness",
      "field experiment"
    ],
    "contents": "\r\nMaybe, at least this is suggested by a pre-registered field experiment by Ash et al. (2023), where people practicing mindfulness meditation for 15 minutes a day for 2 weeks compared to an active control (listening to relaxing music) showed a reduced tendency to avoid painful information that may trigger worry or regret, with a likely mechanism for this effect being improved emotion regulation.\r\nIf this were the case, this would be great news for our decision making, as the ability to impartially evaluate all relevant information is one of the keys to good decision making.\r\nIt’s true that the observed effect was rather small and barely distinguishable from noise and we may be concerned about its reproducibility, but as a Trekkie and a big fan of Mr. Spock, I like the idea that there might be a method that could make us a little more like this cool half-Vulcan and half-human 😉🖖\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-05-mindfulness-and-objectivity/./spock.jpg",
    "last_modified": "2023-06-05T10:57:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-05-bayesian-networks-in-people-analytics/",
    "title": "Use of Bayesian networks in people analytics?",
    "description": "Bayesian networks seem to have some interesting properties that could make them useful for various people analytics use cases, but for some reason this is not the case.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-01",
    "categories": [
      "bayesian networks",
      "probabilistic graphical models",
      "people analytics"
    ],
    "contents": "\r\nIn a people analytics project I’m involved in, we were asked to come up with a prediction model that would perform reasonably well while being very easy to understand and interpret for non-technical users.\r\nIn considering various options, we also came across Bayesian networks (BNs), probabilistic graphical models consisting of nodes and directed edges that represent conditional (and under certain assumptions, causal) relationships between random variables. They seem to have several interesting properties that would suit our needs, namely:\r\nBN models are not “black boxes”, but all their parameters have a comprehensible interpretation, which may reduce users’ algorithm aversion and increase their willingness to take algorithm outputs into account in their decision-making.\r\nExpert knowledge can easily be used in the development of BN models by building the network structure using expert knowledge of im/plausible (causal) relationships between the variables under study.\r\nThe model lends itself to a user-friendly visualization of its structure, helping to design new models that take into account both data-based constraints and the knowledge of experts in the field.\r\nIt is possible to use the data itself to estimate the possible network topology and thus gain preliminary insight into the structure of the problem domain defined by the available variables.\r\nAfter fitting the model, it can be used for reasoning, i.e., calculating probabilities of interest conditional on the available evidence. This reasoning can be done even with incomplete data, based only on the known values of any combination of the available variables.\r\nThe attached graph is for illustrative purposes only - the output from early experiments with BNs on artificial IBM employee attrition data.\r\nHowever, when searching for information about this method, we found that it is not particularly popular among people analytics and I/O psychology practitioners. Would anyone of the readers happen to have a good or bad experience using this tool and would also be willing to share it? 🙏\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-05-bayesian-networks-in-people-analytics/./plot.png",
    "last_modified": "2023-06-05T10:51:22+02:00",
    "input_file": {},
    "preview_width": 1121,
    "preview_height": 746
  },
  {
    "path": "posts/2023-05-26-selection-procedures-validity-update/",
    "title": "Visualizing shifts in validity estimates for selection procedures",
    "description": "Let's take a slopegraph perspective to assess changes in estimates of the validity of selection procedures.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-26",
    "categories": [
      "i/o psychology",
      "validity",
      "employee selection",
      "meta-analysis",
      "data visualization"
    ],
    "contents": "\r\nI suppose that many, if not the majority, of I/O psychology and people analytics folks have already heard about a new meta-analytic estimation of validity for selection procedures that is based on a more realistic range-restriction correction performed by Sackett et al. (2022).\r\nNumerous articles have explored the results of this meta-analysis and its presumed implications for the hiring process. However, what I found lacking was a clear visual representation of the changes in the validity estimates, including the original article.\r\nBecause I needed one for a training I was conducting on people analytics and EB-HRM, I created one. Given the nature of the change to be shown, I chose a slopegraph that nicely and intuitively illustrates a two-point change.\r\n\r\n\r\nShow code\r\n\r\n# uploading the necessary libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(ggrepel)\r\n\r\n# uploading data\r\ndata <- readxl::read_xlsx(\"./data.xlsx\")\r\n#glimpse(data)\r\n\r\n# transforming data\r\ndataLong <- data %>%\r\n  tidyr::drop_na() %>%\r\n  tidyr::pivot_longer(Hunter:Sackett, names_to = \"analysis\", values_to = \"validity\") %>%\r\n  dplyr::mutate(analysis = case_when(\r\n    analysis == \"Hunter\" ~ \"Schmidt & Hunter (1998)\",\r\n    analysis == \"Sackett\" ~ \"Sackett et al. (2022)\",\r\n    TRUE ~ \"unknown\"\r\n    ),\r\n    analysis = factor(analysis, levels = c(\"Schmidt & Hunter (1998)\", \"Sackett et al. (2022)\"))\r\n  )\r\n\r\n# creating custom color palette based on Tableau colors\r\nmy_palette <- c(\r\n  \"#4E79A7\", \"#F28E2C\", \"#E15759\", \"#76B7B2\", \"#59A14F\",\r\n  \"#EDC949\", \"#B07AA2\", \"#FF9DA7\", \"#9C755F\", \"#BAB0AB\",\r\n  \"#2F8AC4\"  # an additional distinct color\r\n)\r\n\r\n# creating the slopegraph\r\ndataLong %>%\r\n  ggplot2::ggplot(aes(x = analysis, y = validity, group = SelectionProcedure)) +\r\n  ggplot2::geom_line(aes(color = SelectionProcedure), linewidth = 1) +\r\n  ggplot2::geom_point(aes(color = SelectionProcedure), size = 3) +\r\n  ggrepel::geom_text_repel(data = dataLong %>% filter(analysis == \"Schmidt & Hunter (1998)\"), aes(label = SelectionProcedure, color = SelectionProcedure), size = 4.5, hjust = 1.2, vjust = 0.5, direction = \"y\", force = 1) +\r\n  ggrepel::geom_text_repel(data = dataLong %>% filter(analysis == \"Sackett et al. (2022)\"), aes(label = SelectionProcedure, color = SelectionProcedure), size = 4.5, hjust = -0.2, vjust = 0.5, direction = \"y\", force = 1) +\r\n  ggplot2::scale_color_manual(values = my_palette) +\r\n  ggplot2::labs(\r\n    title = \"Comparison of employee selection procedures validity estimates\",\r\n    y = \"VALIDITY ESTIMATES\", \r\n    x = \"\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 22, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_blank(),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"none\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nMaybe the visualization will come in handy for you as well when trying to “rewire” your long-held beliefs and assumptions. It should make clearer in what direction and to what extent to do so 😉\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-26-selection-procedures-validity-update/selection-procedures-validity-update_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-05-26T12:52:41+02:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1440
  },
  {
    "path": "posts/2023-05-16-psychometric-network-analysis/",
    "title": "Psychometric network analysis & employee survey data",
    "description": "A demonstration of how psychometric network analysis can be used to gain insights into employee survey data.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-16",
    "categories": [
      "network analysis",
      "psychometrics",
      "employee survey",
      "employee engagement",
      "employee satisfaction",
      "r"
    ],
    "contents": "\r\nIf you’re looking for an alternative to factor analysis for processing employee survey data, consider using psychometric network analysis (hereafter PNA).\r\nIt provides insight into the interdependencies between different topics related to employee experience, and the resulting network diagrams make these insights easily accessible even to non-experts.\r\nIt helps to identify key topics (nodes) that have the most connections or influence within the network, which can be valuable for identifying areas of focus.\r\nAlthough PNA doesn’t work with latent factors, common community detection algorithms (e.g., Louvain’s method) can be used to identify clusters of more densely interconnected topics.\r\nIn interpreting the PNA outputs, one can rely on the apparatus of bootstrapping statistics to distinguish signal from noise.\r\nWhat follows is a small demonstration of the use of PNA on employee survey data. For this purpose, I used a sample dataset that accompanies the book Predictive HR Analytics: Mastering the HR Metric by Edwards & Edwards (2019). It contains the survey responses of 832 employees on a 1 ‘strongly disagree’ to 5 ‘strongly agree’ response scale for a following set of statements.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\n\r\n# uploading legend to the data\r\nlegend <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Legend\") \r\n\r\n# user-friendly table with individual survey items\r\nDT::datatable(\r\n  legend %>% dplyr::mutate(Scale = as.factor(Scale)),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nAnd here is a table with the survey responses we will analyse.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ndata <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Data\")\r\n\r\n# selecting relevant data\r\nmydata <- data %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\n# user-friendly table with the data used in the analysis\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nWe will use the bootnet R package to estimate the regularized partial correlation network using the Spearman correlation matrix and LASSO regularization, and then select the final network using the Extended Bayesian Information Criterion (with γ parameter set to 0.5).\r\n\r\n\r\nShow code\r\n\r\n# estimating a regularized partial correlation network\r\nnetwork <- bootnet::estimateNetwork(\r\n  mydata,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE # when TRUE, enforces higher specificity, at the cost of sensitivity\r\n)\r\n\r\nprint(network)\r\n\r\n\r\n=== Estimated network ===\r\nNumber of nodes: 29 \r\nNumber of non-zero edges: 158 / 406 \r\nMean weight: 0.02974358 \r\nNetwork stored in network$graph \r\n \r\nDefault set used: EBICglasso \r\n \r\nUse plot(network) to plot estimated network \r\nUse bootnet(network) to bootstrap edge weights and centrality indices \r\n\r\nRelevant references:\r\n\r\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9 (3), 432-441.\r\n    Foygel, R., & Drton, M. (2010). Extended Bayesian information criteria for Gaussian graphical models. \r\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2014). glasso: Graphical lasso estimation of gaussian graphical models. Retrieved from https://CRAN.R-project.org/package=glasso\r\n    Epskamp, S., Cramer, A., Waldorp, L., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48 (1), 1-18.\r\n    Epskamp, S., Borsboom, D., & Fried, E. I. (2016). Estimating psychological networks and their accuracy: a tutorial paper. arXiv preprint, arXiv:1604.08462.\r\n\r\nThe above brief summary of the estimated network shows that it is a relatively dense network with 158 out of 406 possible connections (39%). Now let’s plot the network.\r\n\r\n\r\nShow code\r\n\r\n# plotting the estimated network \r\nplot(\r\n  network, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydata),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\"\r\n)\r\n\r\n\r\n\r\nWe can also plot the network in ggplot style, which can be useful when we need more control over the chart, e.g. when we want to plot identified communities/clusters instead of theoretical scales. To do this, we just need to get the necessary information from the network object and do a few data manipulations.\r\n\r\n\r\nShow code\r\n\r\n# uploading \r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# extracting information about the connections form the network object\r\nngMatrix <- network$graph\r\n# inputting to upper part of the matrix zeroes (the edges between items are symmetrical, so we need just a half of the matrix)  \r\nngMatrix[upper.tri(ngMatrix)] <- 0\r\n\r\n# transforming matrix into dataframe\r\nngDf <- ngMatrix %>%\r\n  as.data.frame() %>%\r\n  tibble::rownames_to_column() %>%\r\n  dplyr::rename(itemID = rowname)\r\n\r\n# creating a dataframe with information about connections between items\r\nfromToList <- data.frame()\r\n\r\nfor(i in unique(ngDf$itemID)){\r\n  \r\n  suppDf <- ngDf %>%\r\n    dplyr::filter(itemID == i) %>%\r\n    tidyr::pivot_longer(-itemID, names_to = \"to\", values_to = \"weight\") %>%\r\n    dplyr::filter(weight != 0) %>%\r\n    dplyr::rename(from = itemID) %>%\r\n    dplyr::mutate(\r\n      sign = case_when(\r\n        weight < 0 ~ \"negative\",\r\n        weight > 0 ~ \"positive\",\r\n        TRUE ~ \"zero\"\r\n      ),\r\n      weight = abs(weight)\r\n    )\r\n  \r\n  fromToList <- dplyr::bind_rows(fromToList, suppDf)\r\n  \r\n}\r\n\r\n\r\n# creating a dataframe with information about individual items \r\nitems <- data.frame(\r\n  item = names(mydata),\r\n  scale = legend %>% pull(Scale)\r\n)\r\n\r\n# creating igraph object\r\nigraph_graph <- igraph::graph_from_data_frame(fromToList, directed=FALSE, vertices = items)\r\n\r\n# visualizing the network\r\nset.seed(123)\r\nggraph::ggraph(igraph_graph, layout = \"fr\", maxiter = 500) +  # fr, kk, drl, mds, maxiter = 500 is default\r\n  ggraph::geom_edge_link(aes(edge_width = weight, color = sign), alpha = 0.05) + \r\n  ggraph::geom_node_point(aes(color = scale), size = 5) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  ggraph::theme_graph(background = \"white\") +\r\n  ggraph::scale_edge_color_manual(values = c(\"negative\" = \"red\", \"positive\" = \"blue\")) +\r\n  ggplot2::scale_color_brewer(palette=\"Set1\")\r\n\r\n\r\n\r\nFrom the charts we can quickly gain a basic idea of the internal structure of the data. For example, we can notice that:\r\nitems from the same scales tend to cluster close to each other;\r\nthere are items from different scales that appear to measure motivational aspects of employee attitudes (engagement, organizational citizenship behavior, and quitting intentions), and satisfaction with the “external forces” that affect employee experience (management, justice, and perceived organizational support);\r\nthe topic of autonomy is closely related to the topic of managerial motivation;\r\nmost of the negative partial correlations are between items measuring quitting intentions and other survey items.\r\nTo identify most influential topics (nodes) within the network, we can use centrality measures that quantify the relative importance of a node within a network based on different aspects of a node’s role in the network.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(qgraph)\r\n\r\n# computing centrality measures for individual survey items\r\nqgraph::centralityPlot(network, include = \"all\", orderBy = \"ExpectedInfluence\")\r\n\r\n\r\n\r\nBased on the expected influence centrality measure, which takes into account the presence of both negative and positive edges, the following items appear to be among the most influential ones:\r\nMan3: Management acts decisively.\r\nManMot3: My manager encourages me to do my best.\r\nJustice2: Procedures are applied fairly.\r\naut3: I am given enough leeway to get the job done.\r\nManMot1: My manager motivates me.\r\nBut to know how much we can rely on these measures, we must first test their stability using correlation stability analysis that repeatedly computes centrality indices of subsets of the data and correlates these with the centrality indices of the full data. This process generates a correlation stability coefficient (CS-Coefficient) for each centrality measure. The CS-Coefficient corresponds to the proportion of cases that can be dropped while retaining with 95% certainty a certain level of correlation (e.g., 0.7) between the original centrality measure and the centrality measure of the subset. The higher the CS-Coefficient, the more robust the centrality measure is to the reduction of cases. According to Epskamp & Fried (2018), the CS-coefficient should be above 0.5, and should be at least above 0.25. In our case, we can see the that we can make solid interpretations based on centrality measures of strength (CS(cor = 0.7) = 0.75) and expected influence (CS(cor = 0.7) = 0.75).\r\n\r\n\r\nShow code\r\n\r\n# estimating the stability of centrality measures\r\nbootnet_case_dropping <- bootnet::bootnet(\r\n  network, \r\n  nBoots = 2500,\r\n  type = \"case\",\r\n  nCores = 6,\r\n  statistics = c('strength', 'expectedInfluence', 'betweenness', 'closeness')\r\n)\r\n\r\n# plotting the results\r\nplot(bootnet_case_dropping, 'all')\r\n\r\n\r\nShow code\r\n\r\n# listing the results\r\nbootnet::corStability(bootnet_case_dropping)\r\n\r\n=== Correlation Stability Analysis === \r\n\r\nSampling levels tested:\r\n   nPerson Drop%   n\r\n1      208  75.0 232\r\n2      273  67.2 242\r\n3      337  59.5 254\r\n4      402  51.7 246\r\n5      467  43.9 207\r\n6      532  36.1 259\r\n7      596  28.4 279\r\n8      661  20.6 267\r\n9      726  12.7 251\r\n10     790   5.0 263\r\n\r\nMaximum drop proportions to retain correlation of 0.7 in at least 95% of the samples:\r\n\r\nbetweenness: 0.05 (CS-coefficient is lowest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0, caseMax = 0.127) \r\n\r\ncloseness: 0.05 (CS-coefficient is lowest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0, caseMax = 0.127) \r\n\r\nexpectedInfluence: 0.75 (CS-coefficient is highest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0.672, caseMax = 1) \r\n\r\nstrength: 0.75 (CS-coefficient is highest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0.672, caseMax = 1) \r\n\r\nAccuracy can also be increased by increasing both 'nBoots' and 'caseN'.\r\n\r\nIn a similar way, i.e. using a bootstrapping, we can estimate the stability of the edge weights. This gives us information on how much the edge weights for individual connections vary with 95% confidence intervals. From the chart below, it’s apparent that some edge weights are more accurate than others, as they show a narrower band. Simultaneously, we observe that the majority of edges closer to zero appear to be non-significant, as they intersect with zero in the bootstrapped samples.\r\n\r\n\r\nShow code\r\n\r\n# estimating the stability of edge weights \r\nbootnet_nonpar <- bootnet::bootnet(\r\n  network, \r\n  nBoots = 1000,\r\n  nCores = 6\r\n  )\r\n\r\n# plotting the results\r\nplot(bootnet_nonpar, labels = FALSE, order = \"sample\")\r\n\r\n\r\n\r\nTo find internal structure in the estimated network, we need not rely solely on the visual inspection of the network diagram, but we can use some of the common community detection algorithms that can be used to identify clusters of more densely connected topics. In the example below, we have merely ” replicated” an existing clustering by scale, which is an indication that the authors have managed to construct a survey that measures distinct constructs that they originally intended to measure, but in practice you are likely to observe less distinct patterns more often, which can give you clues about how to improve the construction of the employee survey.\r\n\r\n\r\nShow code\r\n\r\n# identifying communities/clusters\r\nclu <- igraph::cluster_louvain(igraph_graph, weights = fromToList$weight) # cluster_optimal, cluster_louvain, cluster_leading_eigen, cluster_fast_greedy, cluster_walktrap,   cluster_edge_betweenness, cluster_spinglass\r\n\r\n# assigning communities/clusters to nodes\r\nmember <- membership(clu)\r\nV(igraph_graph)$cluster <- as.character(member)\r\n\r\n# visualizing the network with identified communities/clusters\r\nset.seed(123)\r\nggraph::ggraph(igraph_graph, layout = \"fr\", maxiter = 500) +  # fr, kk, drl, mds, maxiter = 500 is default\r\n  ggraph::geom_edge_link(aes(edge_width = weight, color = sign), alpha = 0.05) + \r\n  ggraph::geom_node_point(aes(color = cluster), size = 5) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  ggraph::theme_graph(background = \"white\") +\r\n  ggraph::scale_edge_color_manual(values = c(\"negative\" = \"red\", \"positive\" = \"blue\")) +\r\n  ggplot2::scale_color_brewer(palette=\"Set1\")\r\n\r\n\r\n\r\nAnother use case would be to test for differences in job attitudes’ connectivity and centrality across different groups of employees. To do this, we can use the NetworkComparisonTest R package that implements permutation based hypothesis testing of differences between two networks. Let’s illustrate it with differences between networks estimated on data coming from females and males. We first filter data for both genders and than estimate and visualize their respective attitudinal networks.\r\n\r\n\r\nShow code\r\n\r\n# filtering data for females and males \r\nmydataFemale <- data %>%\r\n  dplyr::filter(sex == 2) %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\nmydataMale <- data %>%\r\n  dplyr::filter(sex == 1) %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\n# estimating a regularized partial correlation network for females and males\r\nnetworkFemale <- bootnet::estimateNetwork(\r\n  mydataFemale,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE \r\n)\r\n\r\nnetworkMale <- bootnet::estimateNetwork(\r\n  mydataMale,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE \r\n)\r\n\r\n# plotting the estimated networks\r\nplot(\r\n  networkFemale, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydataFemale),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\",\r\n  title = \"Females\"\r\n)\r\n\r\n\r\nShow code\r\n\r\nplot(\r\n  networkMale, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydataMale),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\",\r\n  title = \"Males\"\r\n)\r\n\r\n\r\n\r\nWe can check four basic types of differences between the networks:\r\nThe maximum difference in edge weights (M statistic) that tells us whether the structure of the network is identical across two compared groups, and thus whether the groups differ in the overall “shape” or “architecture” of their attitudes.\r\nThe difference in global strength (S statistic) that tells us whether the density of the network is identical across the groups, and thus whether they differ in their openness to change of their attitudes and behaviors (see, for example, Zwicker et al. (2020)).\r\nThe difference in the centrality measures across the groups that tell us whether the groups differ in topics that are most important/influential in their attitudinal network.\r\nThe difference between groups in specific edges. According to van Borkulo et al. (2017), when the M statistic is not statistically significant, it is recommended not to test group-level differences for specific edges as it increases the likelihood of Type 1 error.\r\nAs you can see below, (almost) all test results are statistically non-significant, so we don’t have sufficiently strong evidence for claiming that there are substantive differences between females and males in their respective attitudinal networks. Only in the case of the Eng1 item (I share the values of this organization.), females show a statistically significantly smaller strength centrality measure compared to males. Given that M statistic is statistically non-significant, we don’t test statistical significance of differences for specific edges.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(NetworkComparisonTest)\r\n\r\n# testing the differences\r\nset.seed(123)\r\ntestGenderDiff <- NetworkComparisonTest::NCT(\r\n  networkFemale, \r\n  networkMale, \r\n  binary.data=FALSE, \r\n  paired=FALSE,\r\n  weighted=TRUE,\r\n  test.edges=FALSE, \r\n  edges=\"all\",\r\n  p.adjust.methods=\"holm\",\r\n  test.centrality=TRUE, \r\n  centrality=c(\"strength\",\"expectedInfluence\"),\r\n  nodes=\"all\",\r\n  progressbar=FALSE\r\n  )\r\n\r\n# printing test results\r\nprint(testGenderDiff)\r\n\r\n\r\n NETWORK INVARIANCE TEST \r\n Test statistic M:  0.1747811 \r\n p-value 0.63 \r\n\r\n GLOBAL STRENGTH INVARIANCE TEST \r\n Global strength per group:  12.8295 13.39245 \r\n Test statistic S:  0.5629521 \r\n p-value 0.18 \r\n\r\n EDGE INVARIANCE TEST \r\n\r\nNULL\r\n\r\n CENTRALITY INVARIANCE TEST \r\n \r\n         strength expectedInfluence\r\nManMot1         1              1.00\r\nManMot2         1              1.00\r\nManMot3         1              1.00\r\nManMot4         1              1.00\r\nocb1            1              1.00\r\nocb2            1              1.00\r\nocb3            1              1.00\r\nocb4            1              1.00\r\naut1            1              1.00\r\naut2            1              1.00\r\naut3            1              1.00\r\nJustice1        1              1.00\r\nJustice2        1              1.00\r\nJustice3        1              1.00\r\nJobSat1         1              1.00\r\nJobSat2         1              1.00\r\nQuit1           1              1.00\r\nQuit2           1              1.00\r\nQuit3           1              1.00\r\nMan1            1              1.00\r\nMan2            1              1.00\r\nMan3            1              1.00\r\nEng1            0              0.57\r\nEng2            1              1.00\r\nEng3            1              1.00\r\nEng4            1              1.00\r\npos1            1              1.00\r\npos2            1              1.00\r\npos3            1              1.00\r\n\r\nShow code\r\n\r\n# checking observed differences in centrality measures\r\n# testGenderDiff$diffcen.real \r\n\r\n# plotting results of the network structure invariance test\r\n# plot(testGenderDiff,what=\"network\")\r\n# plotting results of global strength invariance test\r\n# plot(testGenderDiff,what=\"strength\")\r\n# plotting results of the edge invariance test\r\n# plot(testGenderDiff,what=\"edge\")\r\n\r\n\r\nI hope you find this post useful and that it inspires you to try PNA on your own data. If you were looking for more authoritative sources on PNA, Sacha Epskamp’s site is a good place to start. An excellent introduction to the topic can also be found in Letouche & Wille (2022) and Dalege et al. (2017), respectively.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-16-psychometric-network-analysis/psychometric-network-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-05-26T12:58:02+02:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2023-05-10-regression-to-the-mean/",
    "title": "Employee commitment over time & regression to the mean",
    "description": "A nice illustration of the regression to the mean phenomenon in the space of people analytics.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-10",
    "categories": [
      "people analytics",
      "critical thinking",
      "regression to the mean",
      "r"
    ],
    "contents": "\r\nA vendor specializing in employee engagement measurements recently presented their findings that companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced the largest decreases and largest increases in commitment post-layoff, respectively. They post-hoc-hypothesized that highly committed employees feel especially hurt and betrayed when layoffs occur.\r\nHowever, when a similar pattern emerges, a warning light should always flash that regression to the mean (RTM) might actually be behind it. When a variable is imperfectly correlated with another variable, extreme values tend to gravitate towards the mean in subsequent measurements, which can make natural variations in repeated data appear like genuine change.\r\nThis doesn’t mean that RTM was the sole factor in the aforementioned case. However, to know better, it’s necessary to control for it.\r\nWhat follows is a simple simulation to illustrate how the reported finding could occur purely or partially due to RTM, and how one might control for it.\r\nFirst, let’s create correlated employee commitment observations per company from time 1 (T1) and time 2 (T2).\r\n\r\n\r\nShow code\r\n\r\n# uploading necessary libraries\r\nlibrary(tidyverse)\r\n\r\nset.seed(1) # seed for reproducibility\r\nn <- 2000 # number of observations\r\nT1 <- rnorm(n) # generating observations at time 1\r\nT2 <- 0.7*T1 + rnorm(n)*sqrt(1-0.7^2) # generating correlated observations at time 2\r\n\r\n# cor(T1, T2) # checking the correlation\r\n\r\ndf <- data.frame(T1=T1, T2=T2) # putting created variables into dataframe\r\n\r\n\r\nWe also need a benchmark for T1 so that we can calculate the difference between T1 values and T1 benchmark. In addition, we also need to calculate the difference between T2 and T1.\r\n\r\n\r\nShow code\r\n\r\n# computing benchmark for pre-layoff period (T1)\r\nbenchmark <- df %>%\r\n  dplyr::summarise(\r\n    benchmark = mean(T1)\r\n  ) %>%\r\n  dplyr::pull(benchmark)\r\n\r\n# computing differences between T2 and T1 and between T1 and T1 benchmark (average)\r\ndf <- df %>%\r\n  dplyr::mutate(\r\n    timeDiff = T2-T1,\r\n    benchmarkDiff = T1-benchmark\r\n  )\r\n\r\n\r\nThen all we have to do is randomly assign individual observations to the group of companies that made layoffs and those that did not.\r\n\r\n\r\nShow code\r\n\r\n# assigning each observation randomly one of two labels - Layoffs/NoLayoffs \r\ndf1 <- df %>%\r\n  dplyr::mutate(\r\n    Layoffs = sample(c(\"Layoffs\", \"NoLayoffs\"), size = n(), replace = TRUE, prob = c(0.15, 0.85))\r\n  )\r\n\r\n\r\nNow we can contrast the differences between T1 values and T1 benchmark on the one hand and the differences between T2 and T1 on the other. As we can see in the chart below, the pattern matches well with the originally reported finding\r\n- companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced the largest decreases and largest increases in commitment post-layoff, respectively, but now purely as a result of RTM.\r\n\r\n\r\nShow code\r\n\r\n# visualizing relationship between the \r\ndf1 %>%\r\n  dplyr::filter(Layoffs == \"Layoffs\") %>%\r\n  ggplot2::ggplot(aes(x = benchmarkDiff, y = timeDiff)) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = F) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-2,4,1)) +\r\n  labs(\r\n    x = \"DIFFERENCE BETWEEN T1 AND T1 BENCHMARK\",\r\n    y = \"DIFFERENCE BETWEEN T2 AND T1\",\r\n    title = \"Changes in employee commitment purely due to regression to the mean\",\r\n    caption = \"\\nA replication of the original finding that companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced\\nthe largest decreases and largest increases in commitment post-layoff, respectively, relying only on a regression to the mean phenomenon.\\nT1 and T2 refer to commitment measurements at time 1 (prior to a layoff) and time 2 (after a layoff), respectively.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\nIn general, in order to make a valid estimate of the effect of layoffs on employee commitment when RTM is at play, we need to control for its effect. One way to do this is to include the difference between T1 values and T1 benchmark in the linear regression model as illustrated below.\r\n\r\n\r\nShow code\r\n\r\n# modeling T2 while controlling for the effect of regression to the mean\r\nmodel1 <- glm(T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), data = df1)\r\nsummary(model1)\r\n\r\n\r\nCall:\r\nglm(formula = T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), \r\n    data = df1)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-2.54938  -0.47760  -0.01441   0.50541   2.19243  \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       0.01538    0.04210   0.365    0.715    \r\nbenchmarkDiff     0.72305    0.01593  45.382   <2e-16 ***\r\nLayoffsNoLayoffs -0.01620    0.04577  -0.354    0.723    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 0.5458733)\r\n\r\n    Null deviance: 2214.4  on 1999  degrees of freedom\r\nResidual deviance: 1090.1  on 1997  degrees of freedom\r\nAIC: 4470\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nIt is clear from the estimated model that there is not much evidence in favor of the existence of a layoff effect, which should not be surprising since individual observations were purely randomly assigned to groups of companies with and without layoffs. However, when we adjusted the data to better reflect the hypothesized causal mechanism behind the observed pattern, the effect of layoffs was detected as statistically significant.\r\n\r\n\r\nShow code\r\n\r\n# creating a new dataset that better reflects the hypothesized causal mechanism behind the observed pattern\r\ndf2 <- df %>%\r\n  dplyr::rowwise() %>%\r\n  dplyr::mutate(\r\n    Layoffs = sample(c(\"Layoffs\", \"NoLayoffs\"), size = 1, replace = TRUE, prob = c(0.15, 0.85)),\r\n    T2 = ifelse(Layoffs == \"Layoffs\" & T1 >= 0.5, T2 - runif(min = 0.3, max = 1.5, n = 1), T2),\r\n    T2 = ifelse(Layoffs == \"Layoffs\" & T1 <= -0.5 , T2 - runif(min = -0.75, max = 0.2, n = 1), T2)\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(timeDiff = T2-T1) # recomputing timeDiff\r\n\r\n# modeling T2 while controlling for the effect of regression to the mean\r\nmodel2 <- glm(T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), data = df2)\r\nsummary(model2)\r\n\r\n\r\nCall:\r\nglm(formula = T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), \r\n    data = df2)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-2.99200  -0.49173  -0.00417   0.52662   2.52414  \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)      -0.19596    0.04404  -4.449  9.1e-06 ***\r\nbenchmarkDiff     0.66308    0.01662  39.908  < 2e-16 ***\r\nLayoffsNoLayoffs  0.19571    0.04786   4.089  4.5e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 0.5933289)\r\n\r\n    Null deviance: 2135.6  on 1999  degrees of freedom\r\nResidual deviance: 1184.9  on 1997  degrees of freedom\r\nAIC: 4636.7\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nFeel free to share your own experiences and encounters with the phenomena of regression to the mean in your people analytics practice.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-10-regression-to-the-mean/./rtm.gif",
    "last_modified": "2023-05-10T12:50:20+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-04-cv-job-match-career-site/",
    "title": "Improving a company career site with tools from OpenAI",
    "description": "How my own experience of exploring new job opportunities gave me the idea of how the company's career site could be easily improved using OpenAI's tools.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-04",
    "categories": [
      "recruitment",
      "candidate experience",
      "career site",
      "openai",
      "embeddings",
      "gpt",
      "python",
      "shiny"
    ],
    "contents": "\r\nMy recent exploration of new job opportunities has inspired me to consider what I would appreciate as a job candidate when visiting the career site of a company I’d like to work for. Specifically, I would appreciate the following flow:\r\nUploading my CV on the company’s career site.\r\nReceiving a list of jobs sorted by the degree of match to my CV.\r\nObtaining an overview of my major mis/matches for the selected job.\r\nHaving the option to ask specific questions about the job (e.g. What could be the biggest challenge for me?)\r\nNot/Applying for the job after considering the provided information.\r\nIt’s clear that a process like this would make life easier not only for job candidates but also for companies, as more relevant candidates would likely apply for posted vacancies on average (as far as people are looking for similar jobs they have done in the past).\r\nI tested the feasibility of this idea by creating a functional POC career site, with OpenAI tools working behind the scenes, that supports this exact flow for 20 people-analytics-related job ads taken from One Model’s website with open roles in the people analytics space (by the way, kudos for that, One Model team 👏). You can try it for yourself with your own or sample CV on this webpage.\r\n\r\nLet me know in the comments what you think about the flow and/or how you would improve it to make it even more useful for job candidates and companies.\r\nP.S. It was also a good opportunity to try out Shiny for Python by Posit. Being a regular user of the R version of Shiny, I may be a little bit biased, but IMO it’s more user-friendly compared to Dash, so if you need to present the results of your Python code in an interactive web application, definitely give it a try.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-04-cv-job-match-career-site/./search.jpg",
    "last_modified": "2023-05-26T12:53:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/",
    "title": "GPT-4's performance in the knowledge test of evidence-based HRM practices",
    "description": "How did GPT-4 perform in the knowledge test of evidence-based HRM practices? Let's check it out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-02",
    "categories": [
      "gpt",
      "ai",
      "evidence-based management",
      "hr management",
      "people management",
      "hr practices"
    ],
    "contents": "\r\nSome time ago, I “replicated” Rynes, Colbert, and Brown’s 2002 study on HR practitioners’ beliefs about effective HR practices on a convenience sample of more than 140 LinkedIn users. The results of this “replication” closely resembled the results of the original study. On average, respondents correctly answered 19.4 out of 35 items, achieving a 55% success rate, which was very close to the 57% average success rate in the original study (and also quite close to the 50% success rate that corresponds to random choice, given the TRUE/FALSE response format).\r\nI was curious to see how GPT-4 would perform in this test, as it had been evaluated on various standardized tests such as the SAT, GRE, Bar Exam, and AP. The prompts used had the following form: Read the following statement and indicate whether it is true or false. Keep in mind that the statement refers to general trends and patterns that apply on average but not necessarily to all cases. When evaluating the statement, ensure that you correctly interpret the words used in the statement and take into account existing scientific evidence. Give me the answer either true or false, without intermediate values, in a boolean way. Finally, briefly explain your reasoning behind your answer. The statement is as follows:…\r\nSo, what were the results? GPT-4 answered 29 out of 35 items right, i.e., it achieved an 83% success rate, which corresponds to the 99th and 97th percentiles in the original and “replicated” studies, respectively. GPT-4’s results were thus superior to majority of people who took the test.\r\n\r\nHowever, even when it gave a correct answer, it did not always rely on correct facts and/or valid reasoning, which could be a problem if management decided to act on the answers provided. See the table below to check the details of its responses.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(DT)\r\n\r\n# uploading data\r\nmydata <- readxl::read_xlsx(\"./gpt4Responses.xlsx\")\r\n\r\n# creating user-friendly table\r\nDT::datatable(\r\n  mydata %>% \r\n    dplyr::select(itemId, item, gpt4Response, correctAnswer, gpt4Reasoning, researchEvidence, possibleContingencies) %>%\r\n    dplyr::rename(\"Item ID\"=itemId, Item=item, \"GPT-4 response\"=gpt4Response, \"Correct answer\"=correctAnswer, \"GPT-4 reasoning\"=gpt4Reasoning, \"Research evidence\"=researchEvidence, \"Possible contingencies\"=possibleContingencies),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 3,\r\n    autoWidth = TRUE,\r\n    columnDefs = list(list(width = '500px', targets = c(\"Item\", \"GPT-4 reasoning\", \"Research evidence\", \"Possible contingencies\"))),\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  DT::formatStyle(1:7, 'vertical-align'='top')\r\n\r\n\r\n\r\nThe possible takeaway from this finding? Although GPT-4 can be a handy tool for exploring possible solutions to specific HR-related problems, on its own and in its current form it cannot replace the good old systematic search for and retrieval of evidence, critical evaluation of its reliability and relevance, and its weighing and synthesis as conducted and/or supervised by human experts.\r\nP.S. I didn’t test the reliability of GPT-4’s responses, nor did I set its temperature to 0, so it’s possible that you might obtain somewhat different results if you decide to replicate the test. In addition, please keep in mind that the comparison presented here is not entirely an apples-to-apples comparison, mainly due to the fact that new evidence may have emerged that does not match the correct answers in the original study conducted more than 20 years ago.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/./gpt4.jpg",
    "last_modified": "2023-05-01T22:45:00+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-28-employee-feedback-analysis-using-openai/",
    "title": "Employee feedback analysis using tools from OpenAI",
    "description": "How to use GPT and embeddings from OpenAI for identifying topics and related sentiments in employee feedback.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-28",
    "categories": [
      "openai",
      "gpt",
      "embeddings",
      "employee feedback",
      "employee survey",
      "topic analysis",
      "python",
      "r",
      "shiny app"
    ],
    "contents": "\r\nA while ago, I posted about the potential of using GPT for processing open-ended feedback from employees. I simply inputted a block of text into GPT and asked for a summary of the major topics found in the feedback. Although the output was quite accurate and the information compression achieved was very useful, this approach was somewhat limited in terms of scalability and granularity of the information provided.\r\nTo address these limitations, I experimented with another approach that includes the following steps:\r\nLooping over feedback from individual employees and sending them one by one to GPT.\r\nPrompting GPT to identify all present topics in each feedback, determining their respective sentiments (positive, negative, mixed, or neutral), and extracting the relevant parts of the feedback based on which the topic was identified.\r\nPrompting GPT to categorize identified topics using a provided list of topic categories (e.g., compensation and benefits, work-life balance, collaboration and teamwork, etc.), while taking into account contextual information in the relevant parts of the feedback. Alternatively, categorizing by matching embeddings of identified topics, contextual information, and topic categories.\r\nPlotting the topic categories by the number of their occurrences and type of associated sentiment.\r\nInteractive exploration of specific topics clustered by their semantic similarity based on their respective embeddings and visualized with the help of t-SNE dimensionality reduction technique.\r\nCreating a filterable table with identified topics and all original and extracted information that may be useful for further exploration of the feedback and for checking the precision of the topic identification.\r\nI had to experiment a bit with the prompts and include some data-munging inter-steps to get useful outputs, however, it now works relatively smoothly and provides pretty good results. To test the plausibility of this approach, I tried it on publicly available feedback from more than 300 current and former employees of an unnamed company published on Glassdoor and shared on Kaggle. You can check the results of the analysis yourself in this simple dashboard.\r\n\r\nIn my opinion, it works quite well and could represent a very time- and cost-effective way to gain useful insights from employee open-ended feedback at scale, with the caveat that one has to ensure the security of the processed data, for example, by using a local LLM. Let me know what you think about this approach. And if you are interested in the Python code behind the analysis so you can play with it on your own data, here’s the link to the GitHub page with the Python code.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-28-employee-feedback-analysis-using-openai/./employeeListening.avif",
    "last_modified": "2023-06-27T17:03:08+02:00",
    "input_file": "employee-feedback-analysis-using-openai.knit.md"
  },
  {
    "path": "posts/2023-04-24-glassdoor/",
    "title": "When flawed statistical & causal reasoning leads to a valid conclusion anyway",
    "description": "Comparison of Glassdoor ratings from current and former employees.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-24",
    "categories": [
      "glassdoor",
      "employee experience",
      "employee satisfaction",
      "employee turnover"
    ],
    "contents": "\r\nOne simple lesson from the observation that former employees tend to rate their employers more harshly on Glassdoor compared to current employees: Strive to retain your employees, and you’ll likely have a more satisfied workforce and better Glassdoor ratings 😁\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data (link to the original dataset: https://www.kaggle.com/datasets/davidgauthier/glassdoor-job-reviews/code)\r\n# data <- readr::read_csv(\"./glassdoor_reviews.csv\")\r\n# \r\n# # preparing data\r\n# mydata <- data %>%\r\n#   # selecting relevant vars\r\n#   dplyr::select(firm, current, overall_rating, work_life_balance, culture_values, career_opp, comp_benefits, senior_mgmt) %>%\r\n#   # keeping companies with at least 300 records\r\n#   dplyr::group_by(firm) %>%\r\n#   dplyr::mutate(n = n()) %>%\r\n#   dplyr::ungroup() %>%\r\n#   dplyr::filter(n >= 500) %>%\r\n#   dplyr::select(-n) %>%\r\n#   # renaming employee status and keeping only current and former employees\r\n#   dplyr::mutate(\r\n#     status = tolower(current),\r\n#     status = case_when(\r\n#       stringr::str_detect(status, \"\\\\bcurrent\\\\b\") ~ \"Current employee\",\r\n#       stringr::str_detect(status, \"\\\\bformer\\\\b\") ~ \"Former employee\",\r\n#       TRUE ~ \"Unknown\"\r\n#     )\r\n#     ) %>%\r\n#   dplyr::filter(status != \"Unknown\") %>%\r\n#   dplyr::select(-current) %>%\r\n#   # changing wide format to long one\r\n#   tidyr::pivot_longer(overall_rating:senior_mgmt, names_to = \"rating_dimension\", values_to = \"value\") %>%\r\n#   # removing missing values\r\n#   dplyr::filter(!is.na(value)) %>%\r\n#   # renaming rating dimensions\r\n#   dplyr::mutate(rating_dimension = case_when(\r\n#     rating_dimension == \"overall_rating\" ~ \"Overall rating\",\r\n#     rating_dimension == \"work_life_balance\" ~ \"Work-life balance\",\r\n#     rating_dimension == \"culture_values\" ~ \"Culture values\",\r\n#     rating_dimension == \"career_opp\" ~ \"Career opportunities\",\r\n#     rating_dimension == \"comp_benefits\" ~ \"Compensation & benefits\",\r\n#     rating_dimension == \"senior_mgmt\" ~ \"Senior management\",\r\n#     TRUE ~ \"Unknown\"\r\n#   )\r\n#   ) %>%\r\n#   # removing unknown rating dimensions\r\n#   dplyr::filter(rating_dimension != \"Unknown\")\r\n\r\n# to save space in my GitHub repo, I will upload already filtered dataset saved as .RDS file\r\nmydata <- readRDS(\"./glassdoor_reviews_filtered.RDS\")\r\n\r\n\r\n# dataviz \r\n# computing weighted average probability of a given rating for companies in the sample\r\nvizData <- mydata %>%\r\n  dplyr::group_by(firm, status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    n = n()\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::group_by(firm, status, rating_dimension) %>%\r\n  dplyr::mutate(nAll = sum(n)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    prop = n/nAll,\r\n    wprop = prop*nAll\r\n    ) %>%\r\n  dplyr::group_by(status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    wpropsum = sum(wprop),\r\n    w = sum(nAll)\r\n    ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(fprop = wpropsum/w)\r\n\r\n# chart\r\nvizData %>%\r\n  dplyr::mutate(rating_dimension = factor(rating_dimension, levels = c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\"))) %>%\r\n  ggplot2::ggplot(aes(x = value, y = fprop, fill = forcats::fct_rev(status))) +\r\n  ggplot2::geom_bar(stat = \"identity\", position=\"dodge\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_fill_manual(values = c(\"Current employee\" = \"#4E79A7\", \"Former employee\" = \"gray\")) +\r\n  ggplot2::facet_wrap(~rating_dimension, ncol = 3, scales = \"fixed\") +\r\n  ggplot2::labs(\r\n    title = \"<span style='font-size:22pt;font-weight:bold;'>**Comparison of Glassdoor ratings from** \r\n    <span style='color:#4E79A7;'>**current**<\/span> **and**\r\n    <span style='color:#999696;'>**former employees**<\/span>\r\n    <\/span>\",\r\n    caption = \"\\nBased on a sample of ratings from 792,390 individuals across 165 companies with more than 500 records each.\\nThe values represent the weighted average probability of a given rating for companies in the sample.\",\r\n    x = \"RATING\",\r\n    y = \"PROBABILITY OF RATING\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nOn a more serious note, it may be quite interesting and potentially useful to examine the order of estimated differences in specific areas between current and former employees as it may provide some insights on which areas to focus on when trying to retain employees within the company. We can use a multilevel ordered regression analysis on a random sample of 300 ratings per company for this purpose.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(ordinal)\r\nlibrary(broom.mixed)\r\nlibrary(parameters)\r\n\r\n# modeling responses using multilevel ordered regression analysis\r\nfits <- data.frame()\r\n# looping over individual dimensions\r\nfor(scale in c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\")){\r\n  #print(scale)\r\n  set.seed(1234)\r\n  model <- ordinal::clmm(\"value ~ status + (1 | firm)\", data = mydata %>% dplyr::filter(rating_dimension == scale) %>% dplyr::mutate(value = factor(value,ordered = TRUE)) %>% dplyr::group_by(firm) %>% dplyr::sample_n(300) %>% dplyr::ungroup())\r\n  #summary(model)\r\n  \r\n  # extracting information about fitted models\r\n  supp <- broom.mixed::tidy(model) %>%\r\n    dplyr::filter(term == \"statusFormer employee\") %>%\r\n    dplyr::bind_cols(parameters::ci(model) %>% filter(Parameter == \"statusFormer employee\") %>% select(CI_low, CI_high)) %>%\r\n    dplyr::select(-coef.type) %>%\r\n    dplyr::mutate(scale = scale) %>%\r\n    dplyr::select(scale, everything())\r\n  \r\n  fits <- dplyr::bind_rows(fits, supp)\r\n  \r\n}\r\n\r\nfits %>%\r\n  arrange(estimate)\r\n\r\n                    scale                  term   estimate  std.error\r\n1          Culture values statusFormer employee -0.6750302 0.01712937\r\n2          Overall rating statusFormer employee -0.6373467 0.01707703\r\n3       Senior management statusFormer employee -0.6314480 0.01693096\r\n4    Career opportunities statusFormer employee -0.5943039 0.01693091\r\n5       Work-life balance statusFormer employee -0.5263801 0.01689702\r\n6 Compensation & benefits statusFormer employee -0.3574820 0.01691018\r\n  statistic       p.value     CI_low    CI_high\r\n1 -39.40777  0.000000e+00 -0.7086032 -0.6414573\r\n2 -37.32187 7.251441e-305 -0.6708171 -0.6038763\r\n3 -37.29547 1.943268e-304 -0.6646320 -0.5982639\r\n4 -35.10171 6.347105e-270 -0.6274878 -0.5611199\r\n5 -31.15224 4.729748e-213 -0.5594977 -0.4932626\r\n6 -21.14005  3.407214e-99 -0.3906253 -0.3243386\r\n\r\nCaveat: As the title of this post implies, readers should be aware that numerous biases can distort the portrayal of employee experiences reflected in Glassdoor ratings. Some of the most significant biases include survivorship bias, social desirability, non-response bias, self-selection, and motivated reasoning.\r\nDr. Paul De Young’s personal experience in this regard is quite telling: “There is often a high preponderance of phony ratings among so-called current employees on Glassdoor. Beware of bogus “part time” current employees giving high ratings, especially if the company does not employ a lot of part-time employees.Also, I learned from an HR executive that if you want to get ratings up on Glassdoor, encourage ALL your employees to rate the company. Most often it is the mistreated employees who post because this is an outlet for their misfortune. By getting more employees to rate, chances are your ratings will increase. Watch for actively monitored employers on Glassdoor. You can usually tell a bogus rating because there is a high rating with very few comments in jobs that do not exist. The first thing I do when looking at a company is to filter out the part time employees and look at the impact on the overall scores. If they jump down, you have to wonder about the validity of the ratings. Read the comments, they are more telling. There are all kinds of ways to game the system. Glassdoor is helpful, but doesn’t always give you a valid picture without looking at the details, which is where the devil lives.”\r\nHowever, it doesn’t mean that there is no signal in Glassdoor ratings. For example, behavioral scientists at Culture Amp investigated the relationship between Glassdoor ratings and employee engagement data collected by Culture Amp. The findings suggested a strong correlation between employee engagement and Glassdoor ratings, particularly as the number of reviews increases (r = 0.69 for 100+ reviews). Companies with higher engagement scores tended to have better Glassdoor ratings, including higher CEO approval percentages and a greater likelihood of being recommended as a workplace. The study also identified the five factors with the largest relationship to Glassdoor scores, which included Learning and Development, Service and Quality Focus, Decision-making, Leadership, and Collaboration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-24-glassdoor/./glassdoor.png",
    "last_modified": "2023-04-24T23:58:31+02:00",
    "input_file": {},
    "preview_width": 768,
    "preview_height": 595
  },
  {
    "path": "posts/2023-04-18-multilevel-correlation/",
    "title": "In need of multilevel correlations?",
    "description": "A post about a great R package to reach for when you need to calculate correlations on nested data.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-18",
    "categories": [
      "correlation",
      "multilevel analysis",
      "hierarchical analysis",
      "r"
    ],
    "contents": "\r\nI am currently in the middle of a project where I am working with nested data and need to report multilevel correlations.\r\nTo my surprise, for quite a long time I couldn’t find any libraries in the R or Python ecosystems that provided an easy-to-use implementation of this type of analysis. I thought I would have to code it from scratch using Stan or JAGS.\r\nFortunately, I discovered a fantastic correlation package (part of the easystats universe) that can compute various types of correlations, including multilevel correlations, partial correlations, Bayesian correlations, polychoric correlations, biweight correlations, distance correlations, and more.\r\nSince nested data is (almost) everywhere, consider trying this package as it can make your life as an analyst a little bit easier 😉 Check out the code below to see it in action.\r\n\r\n\r\nShow code\r\n\r\n# Uploading libraries and creating custom functions\r\nlibrary(tidyverse)\r\nlibrary(correlation)\r\nlibrary(ggsci)\r\nlibrary(MASS) \r\n\r\n# Creating simulated dataset with nested data\r\n\r\n# Setting some basic parameters of the dataset\r\nnum_teams <- 7\r\nteam_ids <- LETTERS[1:num_teams]\r\nmin_rows <- 35\r\n\r\n# Defining function to generate data for a team with specified correlation\r\ngenerate_team_data <- function(team_id, correlation, job_sat_mean, agility_maturity_mean) {\r\n  \r\n  # Creating covariance matrix\r\n  covariance <- correlation * (20 * 50)\r\n  means <- c(job_sat_mean, agility_maturity_mean)\r\n  cov_matrix <- matrix(c(100, covariance, covariance, 2500), nrow = 2)\r\n  \r\n  # Generating correlated data\r\n  data <- MASS::mvrnorm(n = min_rows, mu = means, Sigma = cov_matrix)\r\n  \r\n  # Scaling the data\r\n  data[, 1] <- scale(data[, 1],center = FALSE,scale = TRUE)\r\n  data[, 2] <- scale(data[, 2],center = FALSE,scale = TRUE)\r\n  \r\n  # Putting data into dataframe\r\n  df <- data.frame(\r\n    TeamID = team_id,\r\n    JobSatisfaction = data[, 1],\r\n    AgilityMaturity = data[, 2])\r\n  \r\n  return(df)\r\n  \r\n}\r\n\r\n# Generating random means for job satisfaction and agility maturity for each of the teams within some range\r\nset.seed(42)\r\njob_sat_means <- runif(num_teams, min = -5, max = 5)\r\nagility_maturity_means <- runif(num_teams, min = 40, max = 60)\r\n\r\n# Generating random correlations for each of the teams team within some range\r\nset.seed(421)\r\ncorrelations <- runif(num_teams, min = -0.3, max = 0.4)\r\n\r\n# Generating data for each team and store in a list\r\nset.seed(123)\r\nteam_data <- mapply(generate_team_data, team_id = team_ids, correlation = correlations, job_sat_mean = job_sat_means, agility_maturity_mean = agility_maturity_means, SIMPLIFY = FALSE)\r\n\r\n# Combining team data into a single data frame\r\nsimulated_data <- do.call(rbind, team_data)\r\n\r\n# Computing multilevel Bayesian Pearson  correlation analysis\r\nc <- correlation::correlation(\r\n  simulated_data,\r\n  method = \"pearson\", \r\n  multilevel = TRUE, \r\n  bayesian = TRUE\r\n)\r\n\r\n# Extracting results of the analysis to be included in the chart defined below\r\nPearson_r = c[1,3]\r\nCI95L = c[1,4]\r\nCI95H = c[1,5]\r\n\r\n# Plotting the chart\r\nggplot2::ggplot(simulated_data, aes(y = JobSatisfaction, x = AgilityMaturity, color = TeamID)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE, size = 1.5) + \r\n  ggsci::scale_color_tron() +\r\n  ggplot2::labs(\r\n    y = \"JOB SATISFACTION\",\r\n    x = \"PERCEIVED ORGANIZATIONAL AGILITY MATURITY\",\r\n    title = \"Is organizational agility related to job satisfaction?\",\r\n    subtitle = stringr::str_glue(\"Bayesian Pearson r = {round(Pearson_r,2)}; 95% CrI: [{round(CI95L,2)}, {round(CI95H,2)}]\")\r\n  ) +\r\n \r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-18-multilevel-correlation/./plot.png",
    "last_modified": "2023-04-25T09:23:35+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2023-04-17-time-management/",
    "title": "Consequences of time management in the workplace",
    "description": "Some interesting insights from a meta-analytic review of the consequences of time management behaviors in the workplace.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-17",
    "categories": [
      "time management",
      "meta-analysis",
      "job satisfaction",
      "job performance",
      "stress",
      "burnout"
    ],
    "contents": "\r\nBedi & Sass (2022) conducted a meta-analytic review of the consequences of employee time management behaviors on several employee outcomes. What are the main insights?\r\nIt may not come as a big surprise, but it is encouraging that data support the association between time management and various beneficial employee outcomes, such as increased job satisfaction, job performance, and lower levels of stress and burnout. Unfortunately, the “proven” association is not causal, as the majority of studies were cross-sectional. In fact, there are not many studies on the causal effects of time management. The exception to this is procrastination, for which there is evidence that time management can help - see, for example, the meta-analysis by Van Eerde & Klingsieck (2018) on this topic.\r\nThe relationship between time management and employee outcomes is not only direct but also partially mediated by work-family conflict. This finding underscores the importance of work-life balance and highlights the need for organizations to help employees better address this specific issue, as it may positively affect a variety of employee outcomes.\r\n\r\nPerceived control over time, achieved through the use of time management, shows incremental validity in predicting job satisfaction, job performance, and stress with respect to the personality trait of conscientiousness. This suggests that regardless of an individual’s innate level of prudence, they may benefit from adopting time management in their professional lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-17-time-management/./tm.jpg",
    "last_modified": "2023-04-17T20:06:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-13-interpretable-ml/",
    "title": "Interpretable machine learning with modelStudio",
    "description": "There's a new kid on the block in the R ecosystem that can help analysts understand the behavior of their ML models.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-13",
    "categories": [
      "interpretability",
      "explainability",
      "machine learning",
      "predictive models",
      "r"
    ],
    "contents": "\r\nThere’s a great new R package, modelStudio, that makes it much easier for analysts to create both global and local interpretations of predictive models using an interactive interface.\r\nOnce you’ve trained the model, you just get the DALEX explainer object ready and start up modelStudio that will run the following analyses (among others) and show the corresponding plots:\r\nFeature Importance: A visual representation that ranks and displays the significance of each input variable in a predictive model, helping to identify the most influential features for model predictions.\r\nPartial Dependence: A visualization that shows the relationship between a feature and the predicted outcome while averaging out the effects of all other features, to understand the marginal impact of a specific feature on the model’s predictions.\r\nAccumulated Dependence: Similar to the previous method, but reducing the influence of the assumption of uncorrelated features, providing a more robust and reliable representation of the feature’s impact on the model’s predictions.\r\nBreak Down Plot: A graphical explanation tool that demonstrates the contribution of each feature to a specific instance’s prediction, allowing for individual-level interpretation of model outcomes.\r\nShapley Values: A cooperative game theory-based approach for fairly attributing each feature’s contribution to a specific prediction, providing interpretable and consistent explanations for machine learning models.\r\nCeteris Paribus: A method that helps with understanding the influence of individual features on specific predictions by isolating the effect of a single variable while holding all other variables constant.\r\nIf you use ML in HR or any other field where it’s crucial to explain why you make specific predictions, classifications, and the resulting recommendations or decisions, definitely give it a try.\r\nWhat follows is a short demonstration of the tool using the well-known artificial IBM attrition dataset. First, let’s import the attrition dataset from the modeldata library and change the coding of the criterion variable, which will later make it easier to set up the DALEX explainer, which requires numerical data type for criterion variable even in classification tasks.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n\r\nWe now split the data into training, test, and validation datasets so that we can tune the prediction model, fit the model, and test its performance on new, previously unseen data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\n\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\nNow let’s define the whole model training workflow, which includes the data adjustment pipeline and the specification of the model used. We will use XGBoost, presumably the best ML algorithm for tabular type of data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(recipes)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\n\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors())\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\nAlthough the XGBoost algorithm works quite well with the default hyper-parameters, we will use the validation dataset to tune some of its hyper-parameters to get the best performance out of it. As can be seen below, after tuning the best model performs quite well in terms of AUC, which has a value of around 0.8.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n    )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n# A tibble: 1 × 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.802\r\n\r\nShow code\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n\r\nNow we can set up the final model training workflow and fit the model to the entire training dataset and check its performance on out-of-sample data using k-fold cross-validation and testing dataset. As we see below, in both cases the model performance as measured by AUC is around the value of 0.8.\r\n\r\n\r\nShow code\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  fit(data_train)\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# A tibble: 1 × 6\r\n  .metric .estimator  mean     n std_err .config             \r\n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 roc_auc binary     0.799    10  0.0166 Preprocessor1_Model1\r\n\r\nShow code\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n  yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# A tibble: 1 × 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.871\r\n\r\nNow we’re ready to explore the inner workings of our trained model. After setting up the explainer from the DALEX package, we simply insert this object into the modelStudio function and run it. We then get an interactive interface in our browser that we can use to easily check what our model is doing to make its predictions. You can try it out for yourself using the interactive interface below.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(DALEX)\r\nlibrary(modelStudio)\r\n\r\n# setting up the DALEX explainer object\r\n# creating predict function\r\npred <- function(model, newdata)  {\r\n  results <- (predict(model, newdata, type = \"prob\")[[1]])\r\n  return(results)\r\n}\r\n\r\nexplainer <- DALEX::explain(\r\n  model = xgb_fit,\r\n  data = data_test %>% mutate(Attrition = as.integer(Attrition)),\r\n  y = data_test %>% mutate(Attrition = as.integer(Attrition)) %>% pull(Attrition),\r\n  predict_function = pred,\r\n  type = \"classification\",\r\n  verbose = FALSE \r\n)\r\n\r\n# running ModelStudio\r\nmodelStudio::modelStudio(\r\n  explainer,\r\n  max_features = 100, # Maximum number of features to be included in BD, SV, and FI plots\r\n  N = 300, # Number of observations used for the calculation of PD and AD\r\n  new_observation_n = 3, #Number of observations to be taken from the data\r\n  show_info = TRUE,\r\n  viewer = \"browser\",\r\n  facet_dim = c(2,2) # layout of the resultiing charts\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-13-interpretable-ml/./miracle.jpg",
    "last_modified": "2023-04-25T09:27:11+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-openai-personality-interpretation/",
    "title": "Ask your personality using GPT",
    "description": "Can Generative AI like GPT meaningfully interpret personality profiles?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-11",
    "categories": [
      "gpt",
      "ai",
      "personality",
      "self-awareness",
      "hogan personality inventory",
      "shiny app",
      "r"
    ],
    "contents": "\r\nOne of my friends recently asked me if I could provide him with an interpretation of a free personality test he took on the internet. As a joke, I asked him if he had already tried using GPT for this.\r\nThis sparked my interest in how GPT would actually handle this kind of task. So, I provided it with my Hogan Personality Inventory (HPI) profile (in percentile scores, as requested), and to my surprise, it performed quite well - even when asked about more complex questions like interactions between my scores on selected scales or my strengths and weaknesses for specific jobs and tasks.\r\nBased on this experience, I created a simple POC app where users can input their HPI profile and some contextual information, such as their current or desired job, and ask GPT predefined or their own questions about their personality.\r\nLink to the app: https://aanalytics.shinyapps.io/ask_your_personality/\r\n\r\nI’ve only tested it on a few profiles, so if you know your HPI profile (or your Big Five traits that are behind HPI), I would be happy to hear how you perceive the face validity and potential usefulness of the generated interpretations. Perhaps I have just become a victim of the well-known Barnum effect 😉\r\nI am well aware that there are clear risks associated with using a generic GPT for such a task. However, I believe that with proper fine-tuning, an explicit disclaimer, and access to a qualified professional for possible consultation, it could be a useful tool for helping people gain self-awareness more easily - in many situations a crucial prerequisite for high-quality decisions. What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-openai-personality-interpretation/./introPic2.jfif",
    "last_modified": "2023-04-11T20:05:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-engagement-interventions/",
    "title": "Effectiveness of interventions for encreasing employee engagement",
    "description": "What evidence do we have for the effectiveness of interventions for increasing employee engagement? Let's check it out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-27",
    "categories": [
      "employee engagement",
      "work engagement",
      "interventions",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is some evidence that engagement of employees has positive causal impact on the bottom line of organizations (see, for example, the meta-analysis by Harter et al. (2010), however, be aware of the specific definition of engagement used there, which focuses more on the contextual factors and conditions enabling engagement and less on the psychological states of engagement). Consequently, we might be naturally interested in whether we can positively influence engagement of employees through the use of certain interventions.\r\nBased on a systematic review and meta-analysis of studies with controlled workplace interventions by Knight, Patterson, and Dawson (2017), it seems the answer might be yes. The authors found a small positive effect on work engagement and each of its three sub-components: vigor, dedication, and absorption, as measured by the Utrecht Work Engagement Scale (UWES) from Bakker and Schaufeli.\r\nWhen it comes to the types of intervention (personal resource building, job resource building, leadership training, and health promotion), a moderator analysis did not find evidence for their differing effectiveness. However, there was evidence for a medium to strong effect of intervention style in favor of group interventions (vs. individual), with the possible explanation being that group interventions effectively influence certain work engagement antecedents, such as social support and influence in decision-making.\r\n\r\nRegarding the sustainability of effects, there was no significant effect of time in the case of overall work engagement. However, for the vigor sub-component, there were stronger effects immediately post-intervention than at follow-up, with the opposite being true for dedication and absorption sub-components.\r\nHave you implemented any interventions to boost engagement of employees in your organization? Did you measure their effectiveness? What were your experiences and results? Feel free to share your thoughts in the comments below.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-engagement-interventions/./training.jpg",
    "last_modified": "2023-04-11T19:33:41+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-managers-overconfidence/",
    "title": "Where do managers put on their rose-tinted glasses the most?",
    "description": "In which areas are managers and leaders prone to overconfidence, and how can this overconfidence potentially impact team functioning? Let's check some data to address this question.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-23",
    "categories": [
      "team management",
      "blind spots",
      "rocket model",
      "team assessment survey"
    ],
    "contents": "\r\nThe “illusory superiority” or “better-than-average effect” causes individuals to overestimate their abilities compared to others. Unsurprisingly, even managers and leaders fall for this trap. After all, they’re human too, right? 😉 For a review of studies on this topic, see, for example, the systematic review by Heavey et al. (2022).\r\nTo pinpoint particular areas where managers and leaders are prone to overconfidence, data from the Team Assessment Survey (TAS), a team assessment tool rooted in the Rocket Model of team performance, can be valuable as it enables comparisons between how team leaders and team members perceive their team’s functioning. For example, based on data from a Slovak sample of 85 teams with 835 members, it seems that team leaders rate their team’s effectiveness way better than team members in the following five areas in descending order:\r\nResources: Does the team have the budget, equipment, authority, and political capital it needs to accomplish its goals?\r\nTalent: Is the team sized correctly? Are team members’ roles clear? Does the team have the right skills to succeed? Are people developing new skills? Do rewards encourage or discourage teamwork?\r\nContext: Are team members in agreement about the team’s political and economic realities, customers, competitors, suppliers, and key assumptions and challenges?\r\nMission: What is the team’s purpose? What are its goals? How does the team define winning? What are its strategies and plans for accomplishing its goals?\r\nCourage: Do team members trust each other? Is there an optimal level of tension and collaboration on this team? Do team members challenge each other in a constructive manner?\r\n\r\nWhen managers get overconfident in these areas, it can lead to all sorts of issues like underinvestment in critical resources, inefficient resource allocation, ignoring skill gaps, insufficient role clarity, making bad decisions, disjointed strategic planning, confusion, misaligned priorities, uncoordinated efforts, blocking innovation, etc.\r\nWhat’s the fix? Among other things, managers need to be aware of their own biases and work on open communication, feedback, and collaboration with team members. Easier said than done, but it’s crucial to avoid hurting the performance of the team.\r\nObviously, the size and demographics of the sample used are limited and conclusions are therefore difficult to generalize, but I may try to check with Dr. Gordon Curphy, the author of the Rocket Model of team performance and TAS, to see if this pattern is also replicated in a larger, international sample.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-managers-overconfidence/./pinkGlasses.jpg",
    "last_modified": "2023-04-11T15:58:18+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-app-piloting-and-dif/",
    "title": "Estimating the impact of a new business app by piloting & method of difference-in-differences",
    "description": "What is the benefit of using the difference-in-differences method in combination with piloting a new business app, and how can this help estimate the app's effectiveness on key outcomes like time spent with prospects or closed deals?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-20",
    "categories": [
      "pilot program",
      "difference in differences",
      "data-driven decision-making"
    ],
    "contents": "\r\nWhen considering the introduction of a new business app, one of the benefits of piloting it is that it provides a great opportunity to test its causal impact on business processes or outcomes of interest. For example, in our pilots, apps are typically implemented for 3 or 6 months only in some teams and not in others. Such a situation creates ideal conditions for applying the difference-in-differences (DiD) method, which is used to approximate an experimental research design with observational data only.\r\nTo use one specific example, one of the problems addressed by our Sales Analytics app is that sales reps spend more time collaborating internally instead of communicating with prospects. The premise may be that the better visibility into time spent that the app enables will help sales reps and their managers better plan activities during their regular weekly 1-on-1 meetings, all with (hopefully) a positive impact on time spent with prospects.\r\nBy piloting the app in just one team and finding another team with a similar, parallel trend in the selected criterion, we can try to estimate its effectiveness. As shown in the attached chart, the fitted DiD model in this particular case slightly supports the effectiveness of the app, at least in terms of the amount of time spent with prospects, but can easily be switched to another criterion that is closer to the company’s bottom line, e.g., the number of closed deals. Moreover, if we are aware of certain systematic differences between the teams, such as the experience level of the sales reps, we can include relevant control variables in the model to achieve a more accurate estimation of the app’s effectiveness.\r\n\r\nSo, next time you consider introducing a new business app, consider piloting it in combination with the DiD method to better understand its impact on your organization’s goals. Happy piloting! ✌️\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-app-piloting-and-dif/./impact.jpg",
    "last_modified": "2023-04-11T15:04:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-meeting-yes-men/",
    "title": "Are there meeting “yes-men”?",
    "description": "One of our clients was struggling with meeting overload and wanted to know if the people who attend too many meetings are the kind of \"yes-men\" who just can't say no to meeting invites. You know the type - always saying \"yes\" and never protecting their precious time. What did they find?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-17",
    "categories": [
      "meeting culture",
      "meeting overload"
    ],
    "contents": "\r\nTo test this hypothesis, they looked at the monthly number of meetings people attended and the relative frequency of their responses to meeting invitations. Here’s what they found:\r\nPeople seemed to be similarly explicit in signaling their intentions about the meetings they were invited to, regardless of how many meetings they had on their plate.\r\nPeople accepted fewer meeting invites the more meetings they attended.\r\nThose who went to a bunch of meetings were more likely to say they’re not sure if they can make it or not.\r\nPeople who were busy with meetings declined more meeting invites than those who had fewer meetings.\r\n\r\nThus, contrary to initial expectations, the data showed that people who attended more meetings, on average, tended to accept fewer invites, were more likely to be unsure about their availability, and actually declined more invites than those with fewer meetings.\r\nWhile the client couldn’t rule out that there might be some individuals fitting “yes-men” description in their company (in fact, one can easily spot a few people in the corresponding chart who attended many meetings and at the same time underutilized the option of declining the meeting invites), these results suggested that there isn’t a systematic problem in this specific area. Time for our client to explore other avenues through which the problem with meeting overload could be addressed. More on that in some of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-meeting-yes-men/./yes_man_yes.gif",
    "last_modified": "2023-04-11T14:44:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/",
    "title": "Impact of employee satisfaction at work on a company's bottom line",
    "description": "While there is evidence supporting the connection between employee satisfaction and a company's bottom line, it's essential to determine whether higher satisfaction directly causes better performance. Is there some evidence for that? Let's check it out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-14",
    "categories": [
      "performance",
      "employee satisfaction",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is now little doubt that employee attitudes, such as employee satisfaction or employee engagement, are to some extent related to performance - for some evidence in support of this claim, see, for example, the meta-analysis of the relationships between employee satisfaction, employee engagement and business unit-level performance conducted by Harter, Schmidt, & Hayes (2002).\r\nBut does it mean that higher employee satisfaction causes higher performance? As any statistically savvy person knows, not necessarily.\r\nFortunately, there are research designs that can help us untangle this conundrum a bit. One of these designs is path analysis of longitudinal/time-series data. This approach was also taken by Harter et al. (2010) in their meta-analysis “Causal Impact of Employee Work Perceptions on the Bottom Line of Organizations” with the following results:\r\n“Using a massive longitudinal database that included 2,178 business units in 10 large organizations, we found evidence supporting the causal impact of employee perceptions on […] bottom-line measures [customer loyalty, employee retention, revenue, and profit]; reverse causality of bottom-line measures on employee perceptions existed but was weaker.”\r\nThe attached figure with the two alternative path models clearly shows that the causal path from employee perceptions to outcomes is stronger than the other way around, especially when it comes to theoretically more proximal outcomes such as employee retention and customer loyalty.\r\n\r\nDespite not being free of potential biases, these results add more weight to the argument that investing in improving employee job satisfaction also makes sense for improving a company’s bottom line, rather than “only” improving employee well-being. The question now is what interventions to improve employee satisfaction might work. Let’s review the available evidence on this issue in one of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/./successfulEmployees.jpg",
    "last_modified": "2023-04-11T13:39:56+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-excel-and-python/",
    "title": "Excel + Python = Word Document",
    "description": "Using combination of Excel and Python for semi-automatic Word document generation.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-10",
    "categories": [
      "excel",
      "python",
      "document",
      "automation"
    ],
    "contents": "\r\nIn the spirit of “Excel isn’t dead and is actually doing well” posts, I’m sharing one practical example of combining Excel and Python to do one specific job. It’s not as exciting and sexy as ChatGPT, but it may still come in handy for someone, as it did for one of my friends.\r\nA friend of mine who works in psychological counselling has to write a large number of reports which, among other things, contain many recommendations for various compensations and interventions depending on established diagnosis.\r\nTo make his job easier, he created a simple Excel spreadsheet with a list of diagnoses and corresponding recommendations. He needed to generate a simple Word document listing and describing the appropriate compensations and interventions after he had marked the appropriate diagnoses for the client in Excel.\r\nI originally wanted to do it all in Excel, but since I’m not the best friend with VBA, I couldn’t get rid of the various text formatting issues. So I reached for Python and one of its document-related libraries and linked it via macro to Excel, which acts only as a source of input data, based on which Python generates a simple report when a button is pressed in Excel. Once generated, the document is ready for further editing and tuning.\r\nIf you are interested in technical details, you can check my GitHub page.\r\nMay the Excel be with you 🙂\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-excel-and-python/./comboPic.png",
    "last_modified": "2023-04-11T13:06:43+02:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2023-04-11-job-attitudes-and-employee-outcomes/",
    "title": "Employee outcomes & employees' job attitudes",
    "description": "What employee outcomes are predicted by what employees' job attitudes? Let's check it out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-06",
    "categories": [
      "job attitudes",
      "job performance",
      "employee turnover",
      "organizational citizenship behavior"
    ],
    "contents": "\r\nIf you’ve ever measured employee job attitudes (i.e. constructs such as organizational commitment, procedural justice, distributive justice, job involvement, job engagement, job satisfaction, etc.), you probably won’t be surprised to learn that job attitudes are usually quite strongly correlated.\r\nThis is also the conclusion of a meta-analytic review of job attitudes by Woznyj et al. (2022), which showed that job attitudes are moderately to strongly correlated with each other, with most relations falling between ρ = .50 and .69.\r\nDespite this, relative weights and incremental validity analyses revealed that some attitudes have greater validity in predicting key employee outcomes. As shown in the table attached,\r\nperformance is most strongly predicted by job satisfaction, job engagement, and distributive justice (an employee’s perceived fairness of outcomes),\r\nturnover intentions is most strongly predicted by job satisfaction, perceived organizational support (a general evaluation regarding the extent to which employees feel their organization values their contribution and cares about their well-being), and distributive justice, and\r\norganizational citizenship behaviors is most strongly predicted by job engagement, procedural justice (perceived fairness of the means, or procedures, used to determine outcomes), and job involvement (the degree to which a person identifies psychologically with his or her work, or the importance of work in his or her total self-image).\r\n\r\nIMO, knowing this can be quite useful in planning what candidate constructs to measure in your company in an effort to support specific employee outcomes.\r\nWhat job attitudes do you regularly measure in your company? And does it pay off in any way, i.e. does it have any tangible impact?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-job-attitudes-and-employee-outcomes/./employees.jpg",
    "last_modified": "2023-04-11T12:51:00+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-05-micro-breaks/",
    "title": "Effectiveness of micro-breaks at work",
    "description": "Quite satisfying news from a meta-analysis on the efficacy of micro-breaks for increasing well-being and performance in the workplace.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "micro-break",
      "workplace",
      "well-being",
      "performance",
      "meta-analysis"
    ],
    "contents": "\r\nIf you feel you need a short break, take one - even if it is no longer than 10 minutes, it can increase your perceived well-being and some types of performance.\r\nAs the authors of the study, Albulescu et al. (2022), conclude: “Our results revealed that micro-breaks are efficient in preserving high levels of vigor and alleviating fatigue. It seems that the effects are univocal and generalizable for the well-being outcomes. These were relatively homogeneous, and none of the included moderators were significant. Hence, the data suggest that micro-breaks may be a panacea for fostering well-being during worktime.\r\nWhen it comes to performance, the data revealed some nuances. The break duration was a significant covariate of the effect of micro-breaks: the longer the break, the better the performance. Moreover, the type of task from which participants were taking the break also emerged as a significant moderator. Micro-breaks could significantly increase performance for clerical work or creative exercises and not for a cognitively demanding task.”\r\nThere is nothing like having a meta-analysis to back up your habits, or better yet, initially bad habits 😉\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-05-micro-breaks/./break.jpg",
    "last_modified": "2023-03-05T19:50:59+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-01-distracted-time/",
    "title": "Not-so-hidden cost of working in an office",
    "description": "Just a few data-backed thoughts on why many of us may often feel more distracted when working in an office.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-01",
    "categories": [
      "remote work",
      "in-office work",
      "collaboration",
      "focus time",
      "distraction"
    ],
    "contents": "\r\nWe have only recently started to measure the allocation of work time to collaborative and non-collaborative activities at Time is Ltd..\r\n\r\nWhen checking the resulting numbers in the context of where people work from, it wasn’t that much of a surprise that people spend less time (on average ~37 minutes less per person per day) on collaborative activities when working remotely vs. in the office. Such a pattern is probably a good thing in many cases as it means that people have more time for focused work when working remotely, and use their time for intensive collaboration when in the office.\r\nWhat was quite surprising to me, however, was the rather large difference in the amount of distracted time. When working in the office there was much more distracted time, i.e. time when people were working on their tasks while being distracted by various collaborative activities. Therefore, they didn’t have enough time to get into the flow. On average, the difference was a staggering ~69 minutes per person per day.\r\nHowever, when I thought about it a bit more (and also after I realized how we actually calculate distraction time️), it started to make more sense to me. After all, collaborative activities don’t just “rob” us of time per se, they also fragment our available time into smaller chunks, which carries a cost in the form of cognitive overhead associated with task switching.\r\nI suppose it’s a trade-off that can’t be completely solved in principle, but can only be mitigated with tools like batching or timeboxing. What is your own experience with this phenomenon and what tools do you use to deal with it?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-01-distracted-time/./distraction.jpg",
    "last_modified": "2023-03-01T19:41:32+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-chatgpt-and-employee-feedback/",
    "title": "Using ChatGPT to summarize and explore employee feedback?",
    "description": "What's the potential use of tools like ChatGPT in analyzing open-ended feedback from employee engagement and satisfaction surveys? Let's take a look at the result of my little experiment in this area.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "chatgpt",
      "ai",
      "hrm",
      "people analytics",
      "employee engagement",
      "surveys"
    ],
    "contents": "\r\nIMHO, ChatGPT and similar tools may be quite useful in the near future (and probably to some extent even today) to quickly summarize and explore the feedback provided by employees through open-ended questions in employee engagement and satisfaction surveys.\r\nI just experimented with a small sample of anonymized employee feedback (just dozens of lines of text from a question about what employees would change in the company) and asked ChatGPT to summarize the feedback, identify the main topics covered in the feedback, and provide me with details about the selected topics - see the attached image of the conversation for illustration.\r\n\r\nJust based on my very limited sample, I found that there was a pretty good balance between information compression and accuracy, while the interaction was very natural, similar to asking HRBP what the results of the last ESS were for my department/team. Certainly, there are still too many known and unknown risks associated with these tools to rely blindly on them alone, but I can imagine that in the foreseeable future, when many of these risks are successfully mitigated, this will be one of the ways managers will listen to the voice of their people.\r\nHas anyone experimented with ChatGPT on similar kinds of HR data?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-chatgpt-and-employee-feedback/./chatgpt_listening.jpg",
    "last_modified": "2023-02-26T19:30:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-large-and-recurring-meetings/",
    "title": "Where to look first when considering meeting reset?",
    "description": "Let's briefly discuss the potential benefits of focusing on optimizing large recurring meetings to save time in the workplace.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-19",
    "categories": [
      "meeting overload",
      "meeting culture",
      "meeting reset"
    ],
    "contents": "\r\nWhen thinking about the meeting reset like they did in Asana, IMHO, it’s not a bad idea to focus on one specific category of meetings first.\r\nI am thinking of large recurring meetings that combine two big time wasters, and thus hide a greater opportunity for time savings:\r\nLarge meetings are quite often a waste of people’s time as they don’t allow everyone to meaningfully contribute. These meetings often serve only to disseminate information and can therefore be safely replaced by less intrusive asynchronous collaboration tools such as email, instant messaging or some kind of knowledge management tool.\r\nRecurring meetings have their place, but often tend to outlive their purpose over time and waste the time of everyone involved. One should therefore regularly ask oneself the following questions in this context and act accordingly: Are meetings being planned automatically, rather than out of necessity? Are attendees invited as a formality, or will they bring value? Is there still clear agenda for these meetings? Are we already sufficiently aligned or do we need some additional platform for doing so? Does the current frequency of our meetings meet our needs? etc.\r\nAs illustrated in the attached chart, large and recurring meetings can represent a relatively large chunk of time in an employee’s work month. Large meetings here account for approximately 37% of all meeting time and more than 80% of large meetings are recurring. This represents a relatively large room for optimizing the time spent in meetings.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-large-and-recurring-meetings/./fatigue.jpg",
    "last_modified": "2023-02-26T19:09:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-hrm-value-chain-and-sem/",
    "title": "HRM value chain and structural equation modeling - Moneyball case",
    "description": "What's the link between the HRM value chain and structural equation modeling? Let’s check it out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "hrm value chain",
      "people analytics",
      "path analysis",
      "structural equation modeling",
      "bayesian statistics",
      "r"
    ],
    "contents": "\r\nAs the economy cooled, more posts started appearing in my social\r\nbubble about how people analytics can prove its value, justify its\r\nexistence, and increase its resilience to layoffs.\r\nOne of the often-mentioned conceptual tools in this context is the\r\nHRM value chain which shows how people processes lead\r\nto achieving companies’ desired business outcomes. There are more\r\nversions of this tool, e.g. one from Jaap\r\nPaauwe and Ray Richardson or another one from Max\r\nBlumberg, but what they have in common is a kind of causal flow from\r\nHRM activities to HRM outcomes to business outcomes.\r\nExample of the HRM value chain\r\nmodel from Paauwe\r\n& Richardson (1997).\r\n\r\nExample of the HRM value chain\r\nmodel from Blumberg\r\n(2018).\r\nHowever, as potentially useful as this metaphor of the organization\r\nas a kind of “machine” with certain inputs, processes, and outputs is,\r\nit is still only a conceptual tool that may or may not correspond to the\r\nreality of a particular organization.\r\nIn this regard, it may help if we try to operationalize this\r\nmetaphor. In this effort, structural\r\nequation modeling can be very handy, as it allows us to\r\nformalize our ideas about the relationships between several different\r\nvariables and to assess the extent to which these ideas are consistent\r\nwith the available data. After all, no one wants to make decisions based\r\non false assumptions.\r\nTo illustrate this with a more tangible example, let’s use sabermetric data\r\nfrom the famous Moneyball\r\ncase and let’s try to formally model the Oakland A’s\r\n(OAK) as a “machine” that produces playoffs by trying to win more games\r\nor score more points than opposing teams, using inputs in the form of\r\nplayers’ ability to play well at bat and in the field.\r\nBased on our expert knowledge of the game of baseball, our working\r\nhypotheses, and the results of some previous analyses, we can construct\r\na conceptual model of how a baseball team functions, as outlined\r\nbelow.\r\nConceptual model of how a\r\nbaseball team works.\r\nThe diagram shows clearly how this “machine” works: Its outputs are\r\nqualifications for the playoffs, which it achieves by trying to win more\r\ngames or score more points than the opposing teams; to do this, it uses\r\ninputs in the form of the players’ ability to play well at bat and in\r\nthe field; the inputs affecting the “machine’s” operation are also the\r\nsimilar abilities of the opposing teams’ players. This is, of course, a\r\nvery simplistic causal model of how the OAK team operates, but as the\r\nfamous statistical aphorism states, all models are\r\nwrong, but some are useful.\r\nHowever incomplete our models of how an organization works will\r\nalways be, it is essential to check that these models sufficiently\r\nreflect reality as conveyed by the available data. Only after such an\r\nassessment of the plausibility of the model it is reasonable to base\r\nfurther, e.g. hiring or L&D decisions on it. And, as hinted at the\r\nbeginning of this post, we will use the statistical method of structural\r\nequation modeling to do this. So let’s apply this method to our model of\r\nthe OAK and test its plausibility.\r\nWe will use data from a publicly available database\r\nof MLB historical statistics. Specifically, we will use the\r\nfollowing variables:\r\nqualification for the playoffs in a given season\r\n(Playoffs),\r\nnumber of wins in a given season (W),\r\nnumber of points won in a given season (RS),\r\nnumber of points lost in a given season (RA),\r\naverage frequency with which a player reaches base per plate\r\nappearance in a given season (OBP - On-Base Percentage) and\r\nanalogous statistics for opponent teams (OOBP),\r\naverage number of bases players earn per at bat in a given season\r\n(SLG - Slugging Percentage) and analogous statistics for\r\nopponent teams (OSLG).\r\nIn terms of time, we will work with data from the years 1996-2001,\r\nwhich precede 2002, when the story of Moneyball mostly takes place.\r\n\r\n\r\nShow code\r\n\r\n# uploading set of libraries for data manipulation and visualization\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\nbaseball <- readr::read_csv(\"./baseball.csv\")\r\n\r\n# filtering data used for the analysis\r\nmyData <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995\r\n    ) %>%\r\n  dplyr::select(Team, OBP, SLG, OOBP, OSLG, RS, RA, W, Playoffs)\r\n\r\n# user-friendly table with data used for the analysis\r\nDT::datatable(\r\n  myData,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\nNow let’s define a formal model of the OAK and fit it to the data\r\nusing the brms R\r\npackage which allows us to make inferences about the model\r\nparameters within a Bayesian\r\ninferential framework. Specifically, we will build and fit a\r\nso-called path\r\nanalysis model, which is a special type of structural equation\r\nmodeling used to describe directed dependencies among a set of\r\nvariables. Given that we have data for a group of the same teams over\r\nseveral seasons, we must also incorporate the hierarchical nature of the\r\ndata into the model.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for fitting Bayesian generalized (non-)linear multivariate multilevel models using Stan for full Bayesian inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# specifying individual parts of the SEM (without modeling the correlation between response variables and using Student's t distribution instead of the Gaussian distribution to make the model more robust) \r\na <- brms::bf(W ~ 1 + RS + RA + (1 + RS + RA | Team), family = student())\r\nb <- brms::bf(RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team), family = student())\r\nc <- brms::bf(RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team), family = student())\r\nd <- brms::bf(Playoffs ~ 1 + W + (1 + W | Team), family = bernoulli())\r\n\r\n# fitting the model\r\nfit <- brms::brm(\r\n  a + b + c + d + set_rescor(FALSE), \r\n  data = myData,\r\n  iter = 3000,\r\n  chains = 3,\r\n  warmup = 500,\r\n  thin = 1,\r\n  seed = 123,\r\n  cores = 5,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n  )\r\n\r\n# checking MCMC convergence\r\n# plot(fit)\r\n# summary(fit)\r\n\r\n\r\n\r\nAfter verifying that the mechanics of the MCMC algorithm work well\r\n(not shown here for brevity reasons), we should also verify how well the\r\nmodel fits the data by using the posterior predictive check for each of\r\nthe observed response variables in our model.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for arbitrarily complex composition of ggplot plots\r\nlibrary(patchwork)\r\n\r\n# posterior predictive check for all predicted observed variables \r\nplayoffsPpc <- brms::pp_check(fit, resp = \"Playoffs\", ndraws = 100) + ggplot2::labs(title = \"Playoffs variable\")\r\nwPpc <- brms::pp_check(fit, resp = \"W\", ndraws = 100) + ggplot2::labs(title = \"W variable\")\r\nrsPpc <- brms::pp_check(fit, resp = \"RS\", ndraws = 100) + ggplot2::labs(title = \"RS variable\")\r\nraPpc <- brms::pp_check(fit, resp = \"RA\", ndraws = 100) + ggplot2::labs(title = \"RA variable\")\r\n\r\nppc <- (playoffsPpc + wPpc) / (rsPpc + raPpc)\r\n\r\nprint(ppc)\r\n\r\n\r\n\r\n\r\nAs you can see, the model fits the data pretty well. Now we can look\r\nat the coefficients in the individual parts of the model. All of them\r\nshow non-zero values and are in directions that are consistent with our\r\nexpectations embodied in our conceptual model, i.e., all are positive\r\nexcept for the coefficient of the RA variable (number of points\r\nlost) as a predictor of the number of games won (W). These\r\nresults thus give us greater confidence in following our conceptual\r\nmodel of team functioning when making decisions about allocating our\r\nlimited resources.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting functions for posterior analysis, MCMC diagnostics, prior and posterior predictive checks, and other visualizations to support the applied Bayesian workflow\r\nlibrary(bayesplot)\r\n\r\n# getting overview of all parameters \r\n# get_variables(fit)\r\n# relevant parameters: \"b_RS_OBP\", \"b_RS_SLG\", \"b_RA_OOBP\", \"b_RA_OSLG\", \"b_W_RS\", \"b_W_RA\", \"b_Playoffs_W\"\r\n\r\nb_RS_OBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_OBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OBP variable in the RS ~ OBP part\"\r\n    ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n    )\r\n\r\n\r\nb_RS_SLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_SLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"SLG variable in the RS ~ SLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OOBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OOBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OOBP variable in the RA ~ OOBP part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OSLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OSLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OSLG variable in the RA ~ OSLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RS <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RS\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RS variable in the W ~ RS part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RA <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RA\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RA variable in the W ~ RA part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_Playoffs_W <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_Playoffs_W\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"W variable in the Playoffs ~ W part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\ncoefCharts <- (b_RS_OBP + b_RS_SLG + b_RA_OOBP) / (b_RA_OSLG + b_W_RS + b_W_RA) / (b_Playoffs_W + patchwork::plot_spacer() + patchwork::plot_spacer()) +\r\n  plot_annotation(\r\n    title = 'Posterior interval estimations',\r\n    caption = \"The solid vertical lines represent the point (median) estimates and the shaded areas represent the 95% credible interval.\",\r\n    theme = theme(\r\n      plot.title = element_text(size = 18),\r\n      plot.caption = element_text(size = 11)\r\n      )\r\n    )\r\n\r\nprint(coefCharts)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model  \r\nsummary(fit)\r\n\r\n\r\n Family: MV(student, student, student, bernoulli) \r\n  Links: mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = logit \r\nFormula: W ~ 1 + RS + RA + (1 + RS + RA | Team) \r\n         RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team) \r\n         RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team) \r\n         Playoffs ~ 1 + W + (1 + W | Team) \r\n   Data: myData (Number of observations: 90) \r\n  Draws: 3 chains, each with iter = 2500; warmup = 0; thin = 1;\r\n         total post-warmup draws = 7500\r\n\r\nGroup-Level Effects: \r\n~Team (Number of levels: 30) \r\n                                   Estimate Est.Error l-95% CI\r\nsd(W_Intercept)                        1.63      1.95     0.04\r\nsd(W_RS)                               0.00      0.00     0.00\r\nsd(W_RA)                               0.00      0.00     0.00\r\nsd(RA_Intercept)                      12.21     10.88     0.57\r\nsd(RA_OOBP)                           32.60     30.04     1.38\r\nsd(RA_OSLG)                           26.88     23.83     1.07\r\nsd(RS_Intercept)                      11.86     10.01     0.45\r\nsd(RS_OBP)                            33.58     27.10     1.55\r\nsd(RS_SLG)                            28.59     22.99     1.18\r\nsd(Playoffs_Intercept)                 2.20      2.16     0.09\r\nsd(Playoffs_W)                         0.22      0.47     0.00\r\ncor(W_Intercept,W_RS)                 -0.24      0.50    -0.96\r\ncor(W_Intercept,W_RA)                 -0.23      0.52    -0.97\r\ncor(W_RS,W_RA)                        -0.22      0.51    -0.95\r\ncor(RA_Intercept,RA_OOBP)             -0.20      0.51    -0.94\r\ncor(RA_Intercept,RA_OSLG)             -0.21      0.51    -0.95\r\ncor(RA_OOBP,RA_OSLG)                  -0.19      0.51    -0.95\r\ncor(RS_Intercept,RS_OBP)              -0.16      0.52    -0.94\r\ncor(RS_Intercept,RS_SLG)              -0.18      0.52    -0.95\r\ncor(RS_OBP,RS_SLG)                    -0.20      0.51    -0.94\r\ncor(Playoffs_Intercept,Playoffs_W)    -0.18      0.58    -0.98\r\n                                   u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsd(W_Intercept)                        7.45 1.01     1084     1547\r\nsd(W_RS)                               0.01 1.01      934     1938\r\nsd(W_RA)                               0.01 1.01     1012     3299\r\nsd(RA_Intercept)                      40.57 1.00     2271     3697\r\nsd(RA_OOBP)                          111.19 1.00     1464     2342\r\nsd(RA_OSLG)                           85.24 1.00      894     2025\r\nsd(RS_Intercept)                      38.03 1.00     1569     1794\r\nsd(RS_OBP)                           101.92 1.00      507      468\r\nsd(RS_SLG)                            86.47 1.00      654     2468\r\nsd(Playoffs_Intercept)                 7.42 1.00     2058     2623\r\nsd(Playoffs_W)                         1.80 1.07       33       16\r\ncor(W_Intercept,W_RS)                  0.79 1.00     4156     4013\r\ncor(W_Intercept,W_RA)                  0.82 1.00     1522     1654\r\ncor(W_RS,W_RA)                         0.80 1.00     3192     4270\r\ncor(RA_Intercept,RA_OOBP)              0.81 1.00     3376     3918\r\ncor(RA_Intercept,RA_OSLG)              0.80 1.00     3097     4358\r\ncor(RA_OOBP,RA_OSLG)                   0.84 1.01     1295     1547\r\ncor(RS_Intercept,RS_OBP)               0.85 1.00     1162     2565\r\ncor(RS_Intercept,RS_SLG)               0.84 1.00      983     1246\r\ncor(RS_OBP,RS_SLG)                     0.82 1.01      503     1922\r\ncor(Playoffs_Intercept,Playoffs_W)     0.93 1.01      187      219\r\n\r\nPopulation-Level Effects: \r\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nW_Intercept           85.97      5.78    74.51    97.16 1.00     4765\r\nRA_Intercept        -852.33     64.99  -982.24  -724.71 1.00     2989\r\nRS_Intercept        -949.77     67.88 -1084.15  -820.70 1.00     4559\r\nPlayoffs_Intercept  -471.68    923.11 -3529.81   -41.17 1.07       30\r\nW_RS                   0.10      0.01     0.09     0.11 1.00     3938\r\nW_RA                  -0.10      0.01    -0.11    -0.09 1.00     4087\r\nRA_OOBP             2850.08    297.29  2262.09  3420.05 1.00     2622\r\nRA_OSLG             1600.06    187.41  1241.97  1976.70 1.00     4134\r\nRS_OBP              3475.86    272.82  2955.81  4015.43 1.00     4594\r\nRS_SLG              1330.99    160.56  1014.09  1637.60 1.00     3936\r\nPlayoffs_W             5.30     10.37     0.46    39.82 1.07       30\r\n                   Tail_ESS\r\nW_Intercept            4672\r\nRA_Intercept           3771\r\nRS_Intercept           5211\r\nPlayoffs_Intercept       16\r\nW_RS                   5662\r\nW_RA                   4003\r\nRA_OOBP                2861\r\nRA_OSLG                4506\r\nRS_OBP                 3139\r\nRS_SLG                 4062\r\nPlayoffs_W               16\r\n\r\nFamily Specific Parameters: \r\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma_W      3.30      0.31     2.70     3.91 1.00     1995     1334\r\nsigma_RA    22.29      2.41    17.83    27.36 1.00     1130     1360\r\nsigma_RS    18.97      2.15    14.93    23.35 1.00     2979     3893\r\nnu_W        23.65     14.23     5.60    59.37 1.00     2419     1912\r\nnu_RA       21.98     14.02     4.93    57.35 1.00     1839     2379\r\nnu_RS       20.98     13.86     4.49    56.41 1.00     1853     2595\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nHowever, the main advantage of such a formalized model is that we can\r\nnow make specific predictions or simulations that will help us make more\r\ngranular decisions about where and how much to invest our limited\r\nresources, e.g. in terms of hiring and/or L&D activities. For\r\nexample, we know from the available data that teams need to win\r\napproximately 95 games to have a high chance of making the playoffs -\r\nsee the two graphs below.\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 1\r\nset.seed(1234)\r\nmyData %>%\r\n  # creating random numbers for dispersing points across the y axis of the graph\r\n  dplyr::mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot2::ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs))) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5)) +\r\n  ggplot2::scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"The team has not made it to the playoffs\", \"The team has made it to the playoffs\")) +\r\n  ggplot2::labs(\r\n    title = \"Teams that didn't/made the playoffs from 1996-2001\",\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 2\r\n\r\nmyData %>%\r\n  # dividing W variable into intervals \r\n  dplyr::mutate(\r\n    WCat = cut(W, breaks = 21, right = TRUE, include.lowest = TRUE, ordered_result = TRUE)\r\n  ) %>% \r\n  dplyr::group_by(WCat) %>%\r\n  dplyr::summarise(\r\n    PlayoffsProb = mean(Playoffs)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = WCat, y = PlayoffsProb)) +\r\n  ggplot2::geom_bar(stat = \"identity\", color = NA, fill = \"lightblue\") +\r\n  ggplot2::scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1), labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"PROBABILITY OF MAKING IT TO THE PLAYOFFS\",\r\n    title = \"Relation between the number of wins in the regular season and the probability\\nof advancing to the playoffs (1996-2001)\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nConsidering the last known OAK performance statistics from 2001, what\r\nnumber of games can OAK expect to win and what is the likelihood of\r\nmaking the playoffs next year? Let’s plug the 2001 OAK numbers into the\r\nmodel and check the prediction including all uncertainties.\r\n\r\n\r\nShow code\r\n\r\n# summary prediction of the number of matches won by OAK in 2002 using OAK's statistics from 2001 as input\r\n# set.seed(123)\r\n# predict(\r\n#   fit, \r\n#   resp = \"W\", \r\n#   newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n# )\r\n\r\n# generating predictions using OAK's statistics from 2001 as input\r\nset.seed(123)\r\nwp <- brms::posterior_predict(\r\n  fit, \r\n  resp = \"W\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n  )\r\n\r\n# actual number of matches won by OAK in 2002\r\nrealNWins <- baseball %>%\r\n  filter(Year == 2002, Team == \"OAK\") %>%\r\n  pull(W)\r\n\r\n# uploading library for visualizations of distributions and uncertainty\r\nlibrary(ggdist)\r\n\r\n# creating the graph\r\nwp %>%\r\n  as.data.frame() %>% \r\n  ggplot2::ggplot(aes(x = V1)) +\r\n  ggdist::stat_halfeye(\r\n    fill = \"lightblue\",\r\n    .width = c(0.80, 0.95),\r\n      ) +\r\n  ggplot2::geom_vline(xintercept = realNWins, linetype = \"dashed\") +\r\n  ggplot2::geom_text(x = realNWins+8.3, y = 1, label = \"Actual number of matches won by OAK in 2002\") +\r\n  ggplot2::scale_x_continuous(breaks = seq(80,120,5)) +\r\n  ggplot2::labs(\r\n    x = \"PREDICTED NUMBER OF MATCHES WON (W)\",\r\n    y = \"DENSITY\",\r\n    linetype = \"\",\r\n    title = \"Predicted number of matches won by OAK in 2002\",\r\n    caption = \"\\nThe black horizontal lines at the bottom of the graph represent the 80% and 95% probability intervals, respectively.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"none\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see that the 95% probability interval of the predicted number\r\nof games won by the OAK in 2002 is safely above the 95-point threshold.\r\nAnd when we compare the prediction to the 2002 reality, we see that they\r\nare pretty close. Also, the projected probability of OAK advancing to\r\nthe playoffs is very high, which is consistent with the fact that OAK\r\nsuccessfully qualified for the playoffs in 2002.\r\n\r\n\r\nShow code\r\n\r\n# prediction of OAK making the playoffs in 2002 using OAK's statistics from 2001 as input \r\nset.seed(123)\r\npredict(\r\n  fit, \r\n  resp = \"Playoffs\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG),\r\n  probs = c(0.025, 0.5, 0.975)\r\n)\r\n\r\n\r\n      Estimate Est.Error Q2.5 Q50 Q97.5\r\n[1,] 0.9993333  0.025813    1   1     1\r\n\r\nBased on this information, we may conclude that there is no urgent\r\nneed to invest heavily now in increasing the current quality of the\r\nteam’s play at bat and in the field (variables OBP and\r\nSLG), assuming that the opposing teams do not significantly\r\nincrease the quality of their play next year (variables OOBP\r\nand OSLG). Using the model, we can also obtain a specific range\r\nof the quality of the OAK’s play at bat and in the field that would\r\nstill allow it to reach at least 95 games won. Such information could be\r\nuseful, for example, in deciding which players can be safely traded\r\nwithout having a detrimental effect on the likelihood of making the\r\nplayoffs.\r\n\r\n\r\nShow code\r\n\r\n# simulation of the effect of SLG and OBP on the number of OAK games won in 2002\r\n\r\n# getting plausible range of values of OBP and SLG variables\r\nstats <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995, \r\n    Team == \"OAK\"\r\n    ) %>%\r\n  dplyr::summarise(\r\n    dplyr::across(c(OBP, SLG), list(mean = mean, sd = sd))\r\n  )\r\n\r\n# creating a sequence of plausible values for OBP and SLG variables (3 SDs around the mean value)   \r\nOBPSeq <- seq(from = stats$OBP_mean + (3*stats$OBP_sd), to = stats$OBP_mean - (3*stats$OBP_sd), length.out = 20)\r\nSLGSeq <- seq(from = stats$SLG_mean + (3*stats$SLG_sd), to = stats$SLG_mean - (3*stats$SLG_sd), length.out = 20)\r\n\r\n# creating df with all possible combinations of plausible values of OBP and SLG variables\r\nsimDf <- expand.grid(OBP = OBPSeq, SLG = SLGSeq)\r\n\r\n# getting variables necessary for prediction (statistics of OAK's opponent teams from 2001)\r\nstatsO <- baseball %>%\r\n  dplyr::filter(Year == 2001, Team == \"OAK\") \r\n\r\n# adding Team and W variables\r\nsimDf <- simDf %>%\r\n  dplyr::mutate(\r\n    Team = \"OAK\",\r\n    W = NaN\r\n  )\r\n\r\n# running the simulation\r\nfor(i in 1:nrow(simDf)){\r\n  \r\n  #print(i)\r\n  \r\n  # prediction of the RS response variable\r\n  RSPred <- predict(\r\n    fit, \r\n    resp = \"RS\", \r\n    newdata = simDf[i,]\r\n  )[1]\r\n  \r\n  # prediction of the W response variable\r\n  WPred <- predict(\r\n    fit, \r\n    resp = \"W\", \r\n    newdata = data.frame(Team = \"OAK\", RS = RSPred, RA = statsO$RA)\r\n  )[1]\r\n  \r\n  simDf[i,\"W\"] <- WPred  \r\n  \r\n}\r\n\r\n# creating the graph\r\nsimDf %>%\r\n  ggplot2::ggplot(aes(x = OBP, y = SLG, color = W)) +\r\n  ggplot2::geom_point(alpha = 18, size = 3) +\r\n  ggplot2::scale_y_continuous(breaks = seq(0.36, 0.52, 0.02)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0.32, 0.38, 0.01)) +\r\n  ggplot2::scale_colour_gradient2(\r\n    low = \"blue\",\r\n    mid = \"white\",\r\n    high = \"red\",\r\n    midpoint = 95,\r\n    space = \"Lab\",\r\n    na.value = \"grey50\",\r\n    guide = \"colourbar\",\r\n    aesthetics = \"colour\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Simulation of the effect of SLG and OBP on the number of OAK games won in 2002\",\r\n    caption = \"The white color corresponds to the 95-point threshold to qualify for the playoffs.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nAs many of you know, thanks to similar analyses, the OAK management\r\nbegan to select players for their team who, although they did not meet\r\nthe traditional criteria by which scouts judged the quality of baseball\r\nplayers, exhibited exactly those characteristics that, according to\r\nconducted analyses, predicted the number of points won and lost, and\r\nthus the likelihood of advancing to the playoffs, which was the\r\nmanagement’s main goal. Because competing teams underestimated the\r\nimportance of these player statistics and overestimated other, less\r\nimportant variables (e.g., batting average), OAK management was able to\r\npurchase players relatively inexpensively who enabled them to achieve\r\ntheir goals. As a result, the OAK won 20 more games per season than\r\nsimilarly “poor” teams and about the same number of games as 2 to 3\r\ntimes richer competition. The power of data in practice!\r\n\r\n\r\nShow code\r\n\r\n# salaries vs number of matches won \r\n# uploading data from the Lahman's Baseball Database, which is publicly available at https://www.seanlahman.com/baseball-archive/statistics/\r\n\r\nplayersSalaries <- readr::read_csv(\"./salaries.csv\")\r\nteamsWins <- readr::read_csv(\"./teams.csv\")\r\n\r\n# computing average sum of salaries paid by each team to its players in 1998-2001\r\nplayersSalariesAvg <- playersSalaries %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n    ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(playersSalariesAvg = sum(salary)/length(unique(yearID)))\r\n\r\n# calculating the average number of wins per season for each team from 1998-2001\r\nteamsWinsAvg <- teamsWins %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n  ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(teamsWinsAvg = sum(W)/length(unique(yearID)))\r\n\r\n\r\n\r\nplayersSalariesAvg %>%\r\n  dplyr::left_join(teamsWinsAvg , \"teamID\") %>%\r\n  dplyr::mutate(OAK = ifelse(teamID == \"OAK\", \"yes\", \"no\")) %>%\r\n  ggplot2::ggplot(aes(x= playersSalariesAvg, y = teamsWinsAvg, fill = OAK)) +\r\n  ggplot2::geom_point()+\r\n  ggplot2::labs(\r\n    title = \"Player salaries and number of wins in 1998-2001\",\r\n    x = \"AVERAGE SUM OF PLAYERS' SALARIES (USD)\",\r\n    y = \"AVERAGE NUMBER OF WINS PER SEASON\"\r\n    ) +\r\n  ggrepel::geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50') +\r\n  ggplot2::scale_fill_manual(\r\n    values = c(\"#ffffff\", \"#ffd400\"), \r\n    labels = c(\"no\",\"yes\")\r\n    ) +\r\n  ggplot2::scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  ggplot2::scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07), labels = scales::number_format(scale = 0.000001, suffix = \"M\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"node\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nFinally, let’s breathe some life into the dry numbers by watching a\r\nshort clip from the Moneyball film, which\r\nnicely summarizes some of the ideas presented in this blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-hrm-value-chain-and-sem/./baseball.jpg",
    "last_modified": "2023-02-09T10:09:16+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-org-chart-and-collaboration/",
    "title": "Org chart and collaboration",
    "description": "How to effectively combine information about the formal organizational structure of a company and the actual collaborative activities of its employees?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-26",
    "categories": [
      "organizational structure",
      "org chart",
      "collaboration",
      "data visualization"
    ],
    "contents": "\r\nHow to effectively combine information about the formal\r\norganizational structure of a company and the actual collaborative\r\nactivities of its employees?\r\nAt Time is Ltd. we are\r\nprimarily focused on collaboration data analytics, whether it’s\r\nmeetings, emails, chat, CRM, project management or version control\r\nsystems, but besides that we also have a product that helps companies\r\nmap their formal organisational structure. Btw, you can give it a try\r\nbecause many of its features are available for free on the Google Workspace\r\nMarketplace.\r\nWe are currently trying to connect these two “worlds” because in\r\nsituations of organizational transformation it can be very useful to\r\nhave information about the relationship between the current and/or\r\nintended formal structure of the organization on the one hand and the\r\nactual patterns of collaboration on the other.\r\nOne option we’re considering is using a kind of heatmap, which you\r\nmay be familiar with from eye-tracking studies used in marketing, to see\r\nwhere people focus their attention when interacting with products and\r\nmaking purchasing decisions. Now imagine if we overlayed a similar\r\nheatmap showing the intensity of collaboration between a selected unit\r\nand the rest of the organization over an org chart - see the figure\r\nbelow for illustration.\r\n\r\nWould you consider such a visualization useful if you were engaged in\r\norganizational transformation? Would you suggest any other kind of\r\nvisualization or specific metric related to collaboration? Thank you in\r\nadvance for any input you may have - it will help us broaden the range\r\nof perspectives we are currently considering.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-org-chart-and-collaboration/./collaborationOrgChartOverlay.png",
    "last_modified": "2023-02-08T22:12:38+01:00",
    "input_file": {},
    "preview_width": 1661,
    "preview_height": 970
  },
  {
    "path": "posts/2023-02-08-span-of-control/",
    "title": "Span of control and collaboration data",
    "description": "How can collaboration data be used to determine the \"optimal\" scope of control?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-24",
    "categories": [
      "span of control",
      "collaboration",
      "meetings"
    ],
    "contents": "\r\nIn determining the “optimal” span of control (SOC) for the company\r\nand specific departments and teams, it is always advisable to consider\r\nthe context and strategy of the company, the way in which individual\r\ndepartments and teams should perform their work, and the level of\r\ncompetence of individual managers.\r\nFor example, McKinsey\r\nsuggested the following four specific aspects of managerial complexity\r\nthat should be considered in this endeavor:\r\nThe time a manager spends doing her or his own work vs. managing\r\nothers.\r\nThe extent to which the work process is not standardized and\r\nformally structured.\r\nThe variety of work of the manager’s direct reports.\r\nThe amount of experience and training that team members need to do\r\ntheir jobs.\r\nThe more of the above, the smaller the SOC should be.\r\nIn addition to these factors, we can use as other useful inputs some\r\nmeasures of collaboration that can reasonably be expected to be related\r\nboth to SOC and to the obligations that managers have to their direct\r\nreports. A good example is the metric of the number of 1-on-1 meetings\r\nthat managers have with their direct reports. As the attached chart\r\nillustrates, when pitted against each other, we can look for points on\r\nthe SOC scale where managers start to fall short of the goal of a\r\ncertain minimum number of 1:1s they have with their people, e.g. 2\r\nmeetings per month.\r\n\r\nWhat factors do you typically consider when determining the optimal\r\nspan of control in your company? And do you regularly reassess the\r\nadequacy of the current SOC in the context of your current\r\nsituation?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-span-of-control/./scheme.jpg",
    "last_modified": "2023-02-08T18:32:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-09-slack-best-practices/",
    "title": "Attaching numbers to best practices for instant messaging",
    "description": "Slack and other instant messaging platforms can be both a blessing and a curse. Can we attach numbers to some of the recommendations on how to use them effectively? Let's take a look.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-20",
    "categories": [
      "slack",
      "instant messaging",
      "best practices"
    ],
    "contents": "\r\nAs many of you can probably confess, Slack and other instant\r\nmessaging (IM) platforms can be both a blessing and a curse. To support\r\nthe former and suppress the latter, authors of these tools and their\r\nusers themselves have come up with several recommendations on how to use\r\nthem effectively.\r\nSpecific numbers can be attached to some of these best practices to\r\nhelp teams and entire companies systematically shape their behavior on\r\nIM platforms in the desired direction. At Time Is Ltd., we currently measure\r\nthe following seven best practices:\r\nThread use: It helps create organized discussions\r\naround specific messages, and they let users discuss a topic in more\r\ndetail without adding clutter to a channel or direct message\r\nconversation.\r\nMention use: Mentioning specific people in messages\r\nin both public and private channels is one effective way to avoid\r\noverwhelming users with a large number of messages that are not relevant\r\nto them.\r\nShort messages use: Shorter messages often mean\r\nmore messages, more messages mean more notifications, and more\r\nnotifications mean more distractions, more frequent context switching,\r\nand decreased productivity.\r\nEmoji use: People should use emojis instead of\r\nshort messages as they are less distracting and more friendly to other\r\npeople’s attention.\r\nBatching: Responding to chat messages round the\r\nclock can be detrimental to employees’ productivity as it can distract\r\nthem from focused work. Better strategy is checking messages every one\r\nor two hours instead of continuous handling of all incoming\r\nmessages.\r\nInactive channels: Non-archived channels that show\r\nno activity are just clutter that makes it difficult to navigate and\r\ncollaborate on the chat platform.\r\nTransparency: Direct and group messages have their\r\nplace in chat, especially when discussing sensitive issues or when\r\ntrying to avoid spamming other employees. However, when majority of chat\r\ncommunication occurs in direct and group messages, there is a higher\r\nrisk that information important for task alignment, problem-solving, or\r\ndecision will be hidden in them and out of view from relevant\r\npeople.\r\n\r\nWould you add some other best practices for IM that have worked well\r\nfor you and that would make sense to measure?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-09-slack-best-practices/./im.jpg",
    "last_modified": "2023-02-09T10:41:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-slack-batching/",
    "title": "Always messaging",
    "description": "Let's take a look at two concepts from computer science that can be used in the workplace to improve people's focus and productivity, and expose two methods for measuring their related behaviors when collaborating on instant messaging platforms.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-17",
    "categories": [
      "instant messaging",
      "collaboration",
      "focus time",
      "distractions",
      "operationalization",
      "measurement"
    ],
    "contents": "\r\nWhen dealing with a large number of tasks and frequent task\r\nswitching, two related concepts originating from computer science can be\r\nused: batch\r\nprocessing and interrupt\r\ncoalescing.\r\nIn computing, these terms refer to a situation where computers wait\r\nuntil a fixed interval and check everything, rather than contextually\r\nswitching and processing separate, uncoordinated interrupts from their\r\nvarious sub-components.\r\nWhen transposed into the world of human workers, this design\r\nprinciple can manifest in checking emails or instant messages every one\r\nor two hours instead of continuously handling all incoming emails and\r\nmessages. Such an arrangement prevents fragmentation of people’s time\r\nand provides them with more focus time they need for deep work and\r\nexperiencing flow.\r\nBut how to measure this behavior so that a number can be put on it to\r\nenable people to better shape their behavior in this regard? At Time is\r\nLtd. we are currently experimenting with two different approaches:\r\nClustering of sent emails/messages using K-Means or PAM and calculation\r\nof time gaps between start/end points of identified clusters, including\r\nthe start and end of the working day. The larger the gaps, the stronger\r\nthe signal of batch behavior.\r\nPercentage of emails/messages sent during the 3 busiest working\r\nhours (defined by the number of emails/messages sent) during a given\r\nday. The higher the proportion, the stronger the signal of batch\r\nbehavior. This approach is inspired by the 2016\r\nstudy by Mark et al. “Email Duration, Batching and\r\nSelf-interruption: Patterns of Email Use on Productivity and Stress”,\r\nwhere the authors used similar approach in the domain of email\r\ncommunication.\r\n\r\nThere are advantages and disadvantages to both methods (face\r\nvalidity, accuracy, sensitivity to edge cases, computational complexity,\r\netc.), but irrespective of these, which one would you prefer to see in\r\nyour collaboration report? To get a better idea of what outputs both of\r\nthe above approaches generate, you can take a look at the attached\r\ngraphs showing the prevalence of batch behavior during one of my work\r\nweeks on Slack according to these two approaches.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-slack-batching/./slack.jpg",
    "last_modified": "2023-02-08T19:02:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-overloaded-employees/",
    "title": "Warning system for overloaded employees",
    "description": "What tools and/or signals can we use to identify employees at increased risk of overload? Let's take a look at some of the options we have in this regard.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-12",
    "categories": [
      "overload",
      "layoffs",
      "retention",
      "engagement"
    ],
    "contents": "\r\nOne of the negative side-effects of layoffs and efforts to achieve\r\nthe same (or ideally more) with fewer employees can be an increased\r\nworkload for those who stay because they have to do the work of those\r\nwho have left, which may lead to an increased risk of overload,\r\ndisengagement, and voluntary quits.\r\nTo prevent this from happening, it is useful to combine active\r\nlistening (through engagement surveys, pulse surveys, stay interviews,\r\nsimple chat, etc. ) with signals that can be obtained from the traces\r\nleft by people in various digital workplace tools, such as project\r\nmanagement systems (ClickUp, Jira, Asana, etc.), version control systems\r\n(GitLab, GitHub, etc.), calendars, instant messaging, or emails.\r\nAt Time is Ltd., we are\r\ncurrently focusing on the following metrics that could be useful in this\r\nrespect:\r\nNumber of assigned tasks\r\nTask close rate\r\nNumber of assigned tasks that other people’s tasks depend on\r\nNumber of commits\r\nNumber of code reviews\r\nResponse time to received messages and e-mails\r\nAmount of focus time available\r\nAmount of distracted time\r\nAmount of time spent working after hours or on weekends\r\nBy checking the distribution of these metrics across individual team\r\nmembers and their changes over time, it is possible to identify\r\nemployees at higher risk of overload, as well as opportunities for a\r\nmore even distribution of the workload.\r\nWhat tools and/or signals do you use in your company to identify\r\nemployees at increased risk of overload?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-overloaded-employees/./overload.jpg",
    "last_modified": "2023-02-08T22:31:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/",
    "title": "Evaluation of the results of the evidence-based HRM knowledge test",
    "description": "Have we made any progress in knowledge of evidence-based HRM practices in the last 20 years? Apparently not. But let's look at the details.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-08",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I shared an app\r\nthat tests knowledge of evidence-based HRM practices using items from Rynes, Colbert, and\r\nBrown’s 2002 study on HR practitioners’ beliefs about effective HR\r\npractices. The fact that more than 140 people completed the test allowed\r\nme to compare our current results with those of the participants in the\r\noriginal study (959 HR practitioners, mostly HR managers, with an\r\naverage of 13.8 years of HR experience).\r\nSo how did we do overall?\r\nOn average, we had 19.4 items out of 35 correct, i.e., we had a 55%\r\nsuccess rate, which is very close to the results of the original study\r\nwhere respondents had an average 57% success rate (and also pretty close\r\nto the 50% success rate corresponding to random choice). So these\r\nresults suggest that we have not progressed much as a group over the\r\nlast 20 years, however, see the disclaimer at the very end of the\r\npost.\r\n\r\nIn which HRM area did we have the largest & smallest\r\nknowledge gaps?\r\nThe biggest gap was in the staffing area (44% success rate) and the\r\nsmallest was in the training & employee development area (67%\r\nsuccess rate).\r\nIn which items did we do best?\r\nLeadership training is effective because good leaders are made, not\r\nborn (95% success rate).\r\nLecture-based training is not generally superior to other forms of\r\ntraining delivery (92% success rate).\r\nWhen pay must be reduced or frozen, a company can do something to\r\nreduce employee dissatisfaction and dysfunctional behaviors (90% success\r\nrate).\r\nIn which items did we do the worst?\r\nScoring positive on drug tests doesn’t mean one will be any less\r\nreliable or productive employee (11.6% success rate).\r\nSetting performance goals is, on average, more effective for\r\nimproving organizational performance than encouraging employees to\r\nparticipate in decision-making (12.3% success rate).\r\nMost errors in performance appraisals cannot be eliminated by\r\nproviding training that describes the kinds of errors managers tend to\r\nmake and suggesting ways to avoid them (15% success rate).\r\nIn which items were we most unsure?\r\nOlder adults don’t learn more from training than younger adults (38%\r\nuncertain).\r\nIntegrity tests that try to predict whether someone will steal, be\r\nabsent, or otherwise take advantage of an employer work well in practice\r\n(34% uncertain).\r\nCompanies with merit pay systems tend to have higher performance\r\nthan companies without them (17% uncertain).\r\nPlease keep in mind that the comparison presented here is not\r\nentirely an apples-to-apples comparison due to several limitations of\r\nthe data collection method (e.g., we don’t know the sociodemographics of\r\nthe new participants, new evidence may have emerged that does not match\r\nthe correct answers in the original study, some people may have taken\r\nthe test multiple times, etc.).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/./testEval.jpg",
    "last_modified": "2023-01-08T21:07:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-meetings-improvement/",
    "title": "How to improve effectiveness of meetings?",
    "description": "What suggestions do people have for improving the effectiveness of meetings? Let's check it out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-05",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nIn my last\r\npost I outlined the reasons why people think the meetings they\r\nattend are in/effective. Let’s now look at what they suggest can be done\r\nto improve the effectiveness of meetings. Who knows, maybe in these\r\nrecommendations you’ll find an alternative to canceling most internal\r\nmeetings like they did at Shopify.\r\nThe list below is again based on Geimer\r\net al.’s 2015 paper “Meetings at work: Perceived effectiveness and\r\nrecommended improvements”.\r\nPEOPLE SIDE: 👉 Come prepared 👉 Arrive on time 👉\r\nOpen to change 👉 Actively listen to what others are saying 👉 Display\r\nprofessionalism during the meeting\r\nMEETING STRUCTURE & ORGANIZATION SIDE: 👉\r\nDistribute appropriate information via e-mail instead of in meeting 👉\r\nAllow time to prepare for meetings 👉 Provide meaningful agenda 👉\r\nClarify plan of action 👉 Use or rotate a facilitator/chair 👉 Invite\r\nappropriate attendees 👉 Pay attention to timing limit, start/end on\r\ntime 👉 Shorten meetings 👉 Hold meetings at appropriate intervals &\r\nmeet only when necessary 👉 Make the meeting environment more\r\ncomfortable\r\nMEETING ACTIVITIES SIDE: 👉 Make meetings more\r\ninteractive & seek input from all attendees 👉 Stay focused on the\r\ntopic 👉 Prioritize items 👉 Break into smaller groups (brainstorming,\r\netc.) 👉 Delegate responsibilities and set deadlines for assigned\r\ntasks\r\nMEETING OUTCOMES SIDE: 👉 Record and distribute\r\nmeeting minutes 👉 Follow up with proposed solutions\r\nIs there anything that you think is missing in the list, especially\r\nin the context of the fact that since 2015 the workplaces have been\r\noperating more in remote/hybrid mode?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-meetings-improvement/./meetingsDalle.png",
    "last_modified": "2023-02-08T19:29:27+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-02-08-ineffective-meetings/",
    "title": "Why are meetings in/effective?",
    "description": "If you are a regular organiser or attendee of meetings, you may be interested in what people think about the reasons why the meetings they attend are in/effective, as this can give you a better chance of contributing to making your meetings more effective and meaningful for you and others.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-03",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nThe list below is based on Geimer\r\net al.’s 2015 paper “Meetings at work: Perceived effectiveness and\r\nrecommended improvements”.\r\nPEOPLE SIDE: 🚫 Late arrive 🚫 Unprepared attendees\r\n🚫 One-way (top-down) communication 🚫 Lack of open-mindedness &\r\nempathy 🚫 Self-promotion, people talk just to appear to add value, and\r\nhidden agenda 🚫 People interrupt/talk during meeting 🚫 Interpersonal\r\nconflicts, incivility, and disrespect\r\nMEETING ORGANIZATION SIDE: ✅ Agenda ✅ Distribution\r\nof agenda in advance 🚫 Lack of direction/goals ✅ Chaired effectively\r\n🚫 Meetings held just to have them, just a routine with no real purpose\r\n🚫 Appropriate parties are not invited and inappropriate parties are\r\ninvited 🚫 Too many attendees 🚫 Time conflicts 🚫 Meet at inappropriate\r\nintervals 🚫 Meetings take too long 🚫 Takes time to travel to\r\nmeeting\r\nMEETING ACTIVITIES SIDE: 🚫 Insufficient\r\ninteraction, meeting activities are monotonous and boring 🚫 No new\r\ninformation 🚫 Discussion gets off target 🚫 Core issues not discussed\r\n🚫 Lack of clarity about what the attendee is supposed to do\r\nMEETING OUTCOMES SIDE: 🚫 Inaction post-meeting 🚫\r\nDecisions have already been made, just a rubber stamp\r\nDo you identify with the above reasons? Is there anything that you\r\nthink is missing from the list, particularly as the workplaces have been\r\noperating more remotely/hybrid way since 2015?\r\nP.S. In the next post let’s check what suggestions people have for\r\nimproving the effectiveness of meetings.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-ineffective-meetings/./meetingsDalle.png",
    "last_modified": "2023-02-08T19:29:53+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2022-12-17-good-manager/",
    "title": "Signals of a good manager",
    "description": "Do you think it's possible to find signals in collaboration (meta)data that someone is a good manager? Let's give it some thought.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-17",
    "categories": [
      "google",
      "oxygen project",
      "collaboration data",
      "performance management"
    ],
    "contents": "\r\nMany of you have probably already heard about Google’s\r\nOxygen project and its findings on the factors differentiating the\r\nhighest and lowest-rated managers based on performance reviews, employee\r\nengagement surveys, interviews, and other sources of employee feedback.\r\nThe final list included the following eight characteristics:\r\nIs a good coach.\r\nEmpowers the team and does not micromanage.\r\nExpresses interest in and concern for team members’ success and\r\npersonal well-being.\r\nIs productive and results-oriented.\r\nIs a good communicator - listens and shares information.\r\nHelps with career development.\r\nHas a clear vision and strategy for the team.\r\nHas key technical skills that help him or her advise the team.\r\nDo you think it would be possible to find any signals, however weak,\r\nof the presence of some of these managers’ characteristics in the\r\ncollaboration (meta)data?\r\nOf the collaboration metrics we currently work with at Time is Ltd., I would bet on the\r\nfollowing:\r\nNumber of 1-on-1 meetings managers have with their direct reports\r\nand new hires [concern for employees’ success and personal\r\nwell-being]\r\nNumber of team meetings managers have with their teams [information\r\nsharing, alignment on vision, strategy, and tactics)\r\nAfter-hours or weekend work and workday length of managers’ direct\r\nreports [well-being & work-life balance]\r\nManagers’ presence at meetings of their direct reports, excluding\r\n1-on-1s and team meetings [micromanagement]\r\nManagers being in CC/BCC of emails sent by their direct reports\r\n[micromanagement]\r\nTime it takes managers to respond to emails from their direct\r\nreports [interest in and concern for team members]\r\nDirect reports having skip-level meetings [career development &\r\nnew career opportunities]\r\nWould you agree? Or what other signals of a good manager would you\r\nlook for in collaboration (meta)data?\r\nP.S. In a later\r\nupdate, the list of top manager characteristics at Google also\r\nincludes the characteristic “Collaborates across Google”, which is\r\nrelatively straightforward to measure using collaboration data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-good-manager/./worlds-best-boss-funny-the-office-micromanagement.gif",
    "last_modified": "2022-12-17T12:36:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-evidence-based-hrm-knowledge-test/",
    "title": "Evidence-based HRM knowledge test",
    "description": "Interested in testing your knowledge of evidence-based HRM practices? If so, click and get started.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-15",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management",
      "shiny app"
    ],
    "contents": "\r\n\r\n\r\n\r\nIf you are interested in testing your knowledge of evidence-based HRM practices in the following five areas…\r\nManagement practices\r\nGeneral employment practices\r\nTraining & Employee development\r\nStaffing\r\nCompensation & Benefits\r\n… then give a try on the test, which is based on the items used in Rynes, Colbert, and Brown’s 2002 study “HR practitioners’ beliefs about effective HR practices: a comparison of research and practice”.\r\nI built a simple shiny app that administers you the test, scores your answers, and compares your results to the results of the participants in the original study (959 HR professionals, mostly HR managers, with an average of 13.8 years of experience in HR). You can also use it to check the accuracy of your answers at the item level to fill in specific gaps in your knowledge.\r\nHere is the link to the app.\r\nFeel free to share your results in the comments. Think of it as a form of public commitment to making some progress in evidence-based HRM in the coming year 😉 To walk the talk, I attached my own results. As you can see, it’s not bad, but there is still room for improvement, especially in the general employment practices area 😃\r\n\r\nP.S. Keep in mind that the test is based on evidence available up to 2002, so it is possible that some correct answers or their contingencies may have changed in that time. For all of them, consider, for example, the adjustment of the estimate of the magnitude of the predictive validity of personnel selection procedures in their most recent meta-analysis by Sackett et al. (2022). If you come across any such discrepancy, it would be great if you share it with others in the comments.\r\nUpdate: With more than 140 people completing the test, I was able to compare our current results with those of the participants in the original study. You can see the results of the comparison in the post Evaluation of the results of the evidence-based HRM knowledge test. Spoiler: It’s not very good 😯\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-evidence-based-hrm-knowledge-test/./manOnTheRoad.jpg",
    "last_modified": "2023-04-11T20:07:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-collaboration-during-vacations/",
    "title": "Can you really unplug?",
    "description": "With the Christmas holidays approaching, the following question is more relevant than ever, with the exception of the summer vacations: Can we really disconnect from work during the vacations? And what can collaboration data tell us about this?",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-10",
    "categories": [
      "vacation",
      "well-being",
      "work-life balance",
      "collaboration data"
    ],
    "contents": "\r\nAlthough most people know they need time off to stay mentally sharp, productive, and resilient over the long term, many confess they work when they’re on vacation.\r\nThe data we collect and analyze at Time is Ltd. confirms this sad truth. As the first chart illustrates, on more than 75% of vacation days people show some collaborative activity, here measured by attending non-recurring meetings, sending emails, and editing shared files.\r\n\r\nHowever, the same data also suggests what could be part of the remedy for this unsatisfactory state of affairs. The second chart shows that there is a small but not insignificant positive relationship between the time that managers and their direct reports spend collaborating during vacation. The chart also shows that managers tend to collaborate more intensely during vacation than their direct reports, which is probably not too much of a surprise.\r\n\r\nThe data is thus in line with the quite often mentioned suggestion that managers should be better role models for their direct reports on how to behave during vacation. As in other areas of people management, it is not enough to lay down the rules - the “playing captain” must play by those rules as well.\r\nHow are you doing in this respect? And do you have any tricks that help you unplug from work during vacations?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-collaboration-during-vacations/./outOfOffice.jpg",
    "last_modified": "2023-11-18T16:54:07+01:00",
    "input_file": "collaboration-during-vacations.knit.md"
  },
  {
    "path": "posts/2022-12-07-mission-scale-from-tas/",
    "title": "One does not simply do a business without getting lost",
    "description": "Breaking down one weekend association.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-07",
    "categories": [
      "team assessment survey",
      "mission",
      "purpose",
      "goals",
      "metrics",
      "review"
    ],
    "contents": "\r\n\r\n\r\n\r\nAs I was scrolling through one of my feeds over the weekend, I came across a funny meme that resonated with my recent experience on a family trip that expressed the deep truth that “One does not simply do a road trip without getting lost.” 😄\r\n\r\nBesides that, and that’s why I’m writing about it here on my blog, it also reminded me of the results of a study I did together with Rastislav Duris and Slavka Silberg, on the characteristics of more than 80 teams from different industries and composed of more than 800 people using the Team Assessment Survey, Dr. Gordon Curphy’s survey that measures some of the basic factors that determine team performance.\r\nSpecifically, I was reminded of the results on the Mission scale, which consists of the following four items:\r\nPurpose: Team members are clear about the team’s purpose.\r\nGoals: The team has a set of overall goals.\r\nMetrics: Metrics and benchmarks have been identified for each team goal.\r\nReviews: The team regularly reviews progress on team goals.\r\nAs you can see in the attached chart, the results of the “Heartbeat analysis” can be briefly summarized as “We know where we’re going, at least some of us know the points to get there, but we’re not sure if we’re on the right track and if we should change our original plans.”\r\n\r\nThat sounds a lot like the description of our last family trip. 😅 Do you think you’re better off in this regard in your team or company? And if so, what tips would you give others on how to improve in this respect?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-07-mission-scale-from-tas/./lost_in_desert.jpg",
    "last_modified": "2023-04-11T20:17:08+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-02-change-detection/",
    "title": "How to quickly navigate dashboard users to what they need to know?",
    "description": "Let's take a look at some tips and tricks to make dasboards more useful for their users.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-01",
    "categories": [
      "dashboard",
      "contextualization",
      "time series",
      "change detection",
      "python"
    ],
    "contents": "\r\nSince the beginning of our efforts at Time is Ltd. to develop a\r\ncomprehensive collaboration analytics platform, we have created several\r\nhundred collaboration metrics. As you can probably imagine, with such a\r\nhuge amount of metrics, it was quite difficult for users to get\r\nsomething useful out of the platform.\r\nTo make the metrics provided more digestible, we’ve enhanced the\r\nplatform using several approaches, from simply selecting metrics with\r\nthe most straightforward call to action and creating apps for very\r\nspecific use cases to providing users with comparative, historical,\r\nintuitively scaled, and equivalent information. Brent Dykes described\r\nmany of these sense-making methods very neatly in his article Contextualized\r\nInsights: Six Ways To Put Your Numbers In Context.\r\nHowever, even after several rounds of these improvements, there was\r\nstill a need to help users quickly find areas that might be worth their\r\nattention and deeper exploration. One approach we took was based on the\r\n(validated) assumption that collaboration metrics are relatively stable\r\nover time and that the intentional and unintentional behavioral changes\r\nbehind these metrics evolve rather slowly. We therefore decided to\r\ndetect the most significant changes over the last 3 months.\r\nFor this purpose, we chose a method that is related to the Bollinger Bands\r\nmethod used in time series analysis or “Heartbeat Analysis” used in\r\nsurvey response analysis. Specifically, we look at the standard\r\ndeviation and average of the month-to-month changes for each metric,\r\nscale the changes to z-scores, and then identify the metrics with the\r\nhighest absolute value of average change over the last 3 months. To\r\nillustrate, see the attached chart where the metrics are sorted by\r\nmagnitude of their change in descending order from left to right and top\r\nto bottom.\r\n\r\nIf you want to try this method on your own data, you can use its\r\nPython implementation, which is on my\r\nGitHub page.\r\nWhat other methods do you find useful in identifying values or\r\nchanges that might be worth users’ attention? Feel free to share them in\r\nthe comments for inspiration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-02-change-detection/./dashboard.jpg",
    "last_modified": "2022-12-02T19:10:27+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-26-meeting-matrix/",
    "title": "Eisenhower matrix for meetings",
    "description": "Meet the Eisenhower matrix for meetings ;)",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "meeting culture",
      "meeting overload",
      "focus time",
      "collaboration culture"
    ],
    "contents": "\r\nI assume many, if not all, of you are familiar with the\r\nEisenhower matrix, a simple tool for prioritizing tasks based on a\r\ncombination of their importance and urgency. It assumes 4 types of tasks\r\n- important & urgent, important & not urgent, not important\r\n& urgent, not important & not urgent - and for each of them\r\noffers a simple recommendation on what to do with them - do it, schedule\r\nit, delegate it, and delete it. By following these rules, people should\r\nbe able to more successfully combat the “mere-urgency” effect, eliminate\r\ntime-wasters in their lives, and create more space to make progress\r\ntoward their goals.\r\n\r\nSomething similar can be created also for meetings, which are big\r\ntime and money guzzlers and deserve to be treated accordingly. Only\r\ninstead of the dimensions of importance and urgency, we will use the\r\nsize and length of meetings. The resulting matrix assumes 4 types of\r\nmeetings with corresponding recommendations on what to do with them:\r\nSmall & Short: These meetings are a bit tricky\r\nbecause they often involve useful meetings, e.g., short syncs of teams\r\nworking on some specific task or project or one-on-one meetings between\r\nmanagers and their direct reports, however, when there is a lot of them,\r\nthey may cause calendar fragmentation and drop in available focus time;\r\nto avoid this, one can think of batching such meetings into larger\r\nblocks of two or three meetings with appropriate small breaks in between\r\nto avoid meeting fatigue and late arrivals.\r\nSmall & Long: These meetings are great for\r\nbrainstorming, problem-solving, or decision-making, so make sure you use\r\nthem primarily for that purpose and don’t waste them on low-value-added\r\nactivities.\r\nLarge & Short: These meetings often serve only\r\nto disseminate information and can therefore be safely replaced by less\r\nintrusive asynchronous collaboration tools such as email, instant\r\nmessaging or some kind of knowledge management tool.\r\nLarge & Long: These meetings are quite often a\r\nwaste of people’s time by not allowing everyone to meaningfully\r\ncontribute and by making them both physically and mentally exhausted, so\r\ntry to move these meetings into the third quadrant if your goal is\r\nsimple information dissemination or into the second quadrant if the goal\r\nis to solve some problem or to make some important decision.\r\nThe meeting matrix is by no means a panacea for meeting overload, but\r\nby following the rules above, people should be able to better protect\r\ntheir time for focused work and make meetings more efficient,\r\nmeaningful, and valuable for themselves and the company. Just be aware\r\nthat unlike the Eisenhower matrix, in the case of the meeting matrix,\r\npeople need to rely more on others to follow these rules in order for\r\nits positive effect to materialize. So get ready to update your\r\n(hopefully already existing) team agreement on your collaboration\r\nculture.\r\nBtw, what is your guess as to how much time you spend in each cell of\r\nthe meeting matrix? For comparison I attach my monthly numbers from\r\nOctober.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-meeting-matrix/./meetingMatrix.png",
    "last_modified": "2022-11-26T13:48:43+01:00",
    "input_file": {},
    "preview_width": 850,
    "preview_height": 565
  },
  {
    "path": "posts/2022-11-26-resources-on-retention-and-downsizing/",
    "title": "Some resources on staff retention and downsizing",
    "description": "Probably due to the current situation in the talent market, where many companies are laying people off and at the same time are worried about losing their key employees, a few people have contacted me in recent weeks asking for some tips on evidence-based approaches to dealing with retention and downsizing.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "employee retention",
      "employee turnover",
      "layoffs",
      "downsizing",
      "people analytics",
      "evidence-based management"
    ],
    "contents": "\r\nI referred them to the following resources - maybe some of you will\r\nfind them useful as well:\r\nEmployment\r\nDownsizing and Its Alternatives guideline from the SHRM’s Science-to-Practice\r\nSeries.\r\nRetaining\r\nTalent guideline from the SHRM’s Science-to-Practice\r\nSeries.\r\nRubenstein et al.’s meta-analysis\r\nof voluntary turnover predictors; you can also use this\r\napp visually summarizing its results.\r\nSpeer et al.’s article Here\r\nto stay or go? Connecting turnover research to applied attrition\r\nmodeling.\r\nDemo\r\ndashboard for analysis of employee turnover.\r\nFeel free to share any other resources on this topic you think might\r\nbe useful to others.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-resources-on-retention-and-downsizing/./retentionDownsizing.png",
    "last_modified": "2022-11-26T14:17:06+01:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 720
  },
  {
    "path": "posts/2022-11-18-interventions-reducing-gender-pay-gap/",
    "title": "Evidence-based interventions that help reduce the gender pay gap",
    "description": "Pay inequality between men and women is not only an ethical and legal issue for companies, but also a marketing issue - it can have a negative impact on their \"employer brand\" and attractiveness as an employer. This means that if companies want to attract and retain talented employees, they must be able to ensure that they treat men and women equally in this respect. Let's look at what the existing evidence tells us about what might help us with this.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-18",
    "categories": [
      "gender pay gap",
      "evidence-based management"
    ],
    "contents": "\r\nIf you are responsible for DEI initiatives in your company, you might\r\nbe interested in a paper by The\r\nBehavioural Insights Team that lists several possible interventions\r\nto close the gender pay gap, divided into three categories based on the\r\nextent to which their effectiveness is supported by empirical\r\nevidence.\r\n🥇 Actions with well-documented effectiveness:\r\nIncluding more women on shortlists in recruitment and promotion. *️\r\nUse of tasks assessing job skill levels in the selection of new\r\nemployees.\r\nUse of structured interviews in recruitment and promotion.\r\nEncouraging salary negotiation through disclosure of existing salary\r\nranges.\r\nIntroducing transparent promotion and reward processes.\r\nAppointing a manager or establishing a corporate diversity task\r\nforce.\r\n🥈 Potentially promising actions, but requiring further\r\nevidence of their effectiveness:\r\nIncreasing work flexibility for men and women.\r\nSupporting shared parental leave.\r\nRecruiting former employees who have had to interrupt their careers\r\nfor a prolonged period for various personal reasons.\r\nOffering mentoring and sponsorship.\r\nOffering networking programs.\r\nSetting internal targets.\r\n🥉 Actions with mixed evidence of their\r\neffectiveness:\r\nTraining on the topic of unconscious bias.\r\nDiversity training.\r\nLeadership development training.\r\nDemographically diverse selection panels in external and internal\r\nrecruitment.\r\nFor those interested, here is the original document for a closer\r\nlook.\r\n\r\n\r\nThis browser does not support PDF files. Please download the PDF file to\r\nview it: Download PDF.\r\n\r\n\r\n\r\nA final note. As useful as it is to know which interventions have a\r\ndecent chance of reducing gender pay inequality, an integral part of\r\nthis fight is to regularly check the existence of this inequality across\r\nthe organisation and within different types of organisational processes.\r\nAs Alessandro\r\nLinari aptly noted in a discussion on Linkedin, “The best way to\r\nkeep the gender pay gap under control is still to do a periodic pay\r\nequity analysis across the organisation and address any identified gap.\r\nI say unfortunately because there are a thousand different ways where\r\npay differences still manage to sneak into the workforce, through hiring\r\nand promotion practices, but also retention policies, bonus allocation,\r\nperformance management, and others.” If you want to learn more\r\nabout some of the technical details of such an analysis, check out one\r\nof my previous posts on this topic (unfortunately, it is now only\r\navailable in Czech, but you can use one of the online translators to get\r\naround this limitation 😉).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-18-interventions-reducing-gender-pay-gap/./genderPayGap.jpg",
    "last_modified": "2022-11-18T12:26:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-15-people-related-metrics-distribution/",
    "title": "It's perfectly normal not to be normal",
    "description": "And it definitely applies to the shape of the distribution of many HR metrics. Let's look at this in a little more detail.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-15",
    "categories": [
      "hr metrics",
      "normal distribution",
      "collaboration",
      "performance",
      "r"
    ],
    "contents": "\r\nThe fact is that many HR practitioners overestimate the frequency\r\nwith which the phenomena they commonly encounter in their practice have\r\na normal, bell-shaped, symmetrical distribution.\r\nThis is very much the case, for example, in the area of communication\r\nand collaboration that we deal with at Time is Ltd., as illustrated in\r\nthe attached chart with some of our collaboration metrics, which show a\r\nwide range of distributions from log-normal and power law to\r\nexponential, gamma, Weibull and beta.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./collaborationMetrics.rds\")\r\n\r\n# External network size metric\r\nexternalNetworkSizeG <- mydata %>%\r\n    dplyr::filter(metric == \"externalNetworkSize\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(breaks = seq(0,120,20)) +\r\n    ggplot2::labs(\r\n      x = \"EXTERNAL NETWORK SIZE / PERSON / MONTH\",\r\n      y = \"FREQUENCY\",\r\n      title = \"External network size\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n  )\r\n  \r\n\r\n# Focus rate metric\r\nfocusRatePrctG <- mydata %>%\r\n    dplyr::filter(metric == \"focusRatePrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF FOCUS TIME / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Available focus time\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n\r\n# Multitasking in meetings metric\r\nmultitaskingInMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"multitaskingInMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"MULTITASKING RATE / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Meeting participations with multitasking\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Breaks between meetings metric\r\nbreaksBetweenMeetingsMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"breaksBetweenMeetingsMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,600,120)) +\r\n    ggplot2::labs(\r\n      x = \"LENGTH OF BREAK / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Lenght of breaks between meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Collaboration time metric\r\ncollaborationTimePersonHrsDayG <- mydata %>%\r\n    dplyr::filter(metric == \"collaborationTimePersonHrsDay\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" hrs\"), breaks = seq(0,16,2)) +\r\n    ggplot2::labs(\r\n      x = \"TIME SPENT COLLABORATING / PERSON / DAY\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Time spent collaborating\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Supervised meetings metric\r\nmicromngMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"micromngMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF SUPERVISED MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of supervised meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Recurring meetings metric\r\nrecurringMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"recurringMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"% OF RECURRING MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of recurring meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call ending metric\r\ncallEndingMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callEndingMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(-60,60,20)) +\r\n    ggplot2::labs(\r\n      x = \"PLANNED VS. ACTUAL END OF MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Planned vs. actual end of meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call delay metric\r\ncallDelayMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callDelayMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,40,5)) +\r\n    ggplot2::labs(\r\n      x = \"DELAY OF ONLINE MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Late arrivals to online meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\ng <- (externalNetworkSizeG + focusRatePrctG + multitaskingInMeetingsPrctG) / \r\n  (breaksBetweenMeetingsMinutesG + collaborationTimePersonHrsDayG + micromngMeetingsPrctG) / \r\n  (recurringMeetingsPrctG + callEndingMinutesG + callDelayMinutesG) +\r\n  patchwork::plot_annotation(\r\n    title = 'Distribution of selected collaboration metrics',\r\n    theme = theme(\r\n      plot.title = element_text(size = 26, margin=margin(20,0,12,0))\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBut it also applies to more traditional areas of HR concern such as\r\nindividual or team performance. For many job roles, especially\r\nknowledge-based ones, job performance has a power-law distribution,\r\ni.e., only a few percent of individuals or teams have disproportionately\r\nhigh performance and most have performance below the statistical average\r\n(many of you have probably heard of Pareto’s law, and the 80/20 rule in\r\nthis context). More detailed information on this particular topic can be\r\nfound, for example, in the following two articles - The\r\nBest and the Rest: Revisiting the Norm of Normality of Individual\r\nPerformance by O’Boyle Jr. & Aguinis (2012) and Team\r\nPerformance: Nature and Antecedents of Nonnormal Distributions by\r\nBradley & Aguinis (2022).\r\nWhy bother with that? Well, because, based on incorrect assumptions\r\nabout the distribution of specific phenomena, companies may make\r\ndecisions that ultimately harm them. For example, suppose a company\r\napplies the assumption of normal distribution in evaluating employees’\r\nperformance that actually has power law distribution. In that case, it\r\nwill result in underestimating the contribution of the best performers\r\nand overestimating the contribution of the worst performers, which may\r\nbe negatively reflected in various decisions regarding reward &\r\ncompensation, learning & development, promotions, succession\r\nplanning, etc.\r\nLessons learned? For frequent decisions or decisions with a\r\nsignificant expected impact, it is worth checking that our underlying\r\nassumptions match reality.\r\nBtw, this also applies to personal life. Here’s an example from mine:\r\nI thought I was in control of watching movies and TV shows on streaming\r\nplatforms, but I started getting signals from those around me that I was\r\nspending too much time there. So as a proper ‘Quantified Selfer’, I decided to\r\ntrack my daily screen time for a month. To my surprise, I found that I\r\nwas indeed spending a lot more time there than I thought and wished. I\r\nthen put in place simple solutions to prevent me from watching more than\r\nI should - I have started to pay extra fees into the family budget for\r\nwatching movies (loss aversion), I\r\nhave pre-selected time slots for watching movies (implementation\r\nintention), I’ve removed streaming apps from my phone (lowering salience),\r\nI’ve stopped watching movies alone (social control), and I’m also\r\nplaying with the idea of asking my wife to change the PINs (preventing\r\nimpulsive\r\nwatching).\r\nP.S. If you ever need to check the shape distribution of any of your\r\nmetrics, you should definitely try the amazing fitdistrplus\r\nR package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-15-people-related-metrics-distribution/./distributions.png",
    "last_modified": "2022-11-17T14:57:23+01:00",
    "input_file": {},
    "preview_width": 962,
    "preview_height": 670
  },
  {
    "path": "posts/2022-10-27-bayesian-belief-updating/",
    "title": "A visual introduction to Bayesian belief updating",
    "description": "Teacher: \"Bayesian belief updating involves combining existing or prior beliefs with an assessment of the strength of new evidence.\" Student: \"And could I please see this in action?\"",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-27",
    "categories": [
      "bayesian belief updating",
      "bayesian inference",
      "judgment",
      "forecasting",
      "critical thinking",
      "r"
    ],
    "contents": "\r\nWhen trying to reduce uncertainty, even very small pieces of\r\ninformation count if you are patient and have some tool to combine them\r\neffectively. I recently re-read Philip\r\nTetlock’s book Superforecasting\r\nand came across an excellent illustration of such a tool: Bayesian belief\r\nupdating.\r\n“Imagine you are sitting with your back to a billiards table. A\r\nfriend rolls a ball onto the table and it stops at a random spot. You\r\nwant to locate the ball without looking. How? Your friend rolls a second\r\nball, which stops at another random spot. You ask, “Is the second ball\r\nto the left or the right of the first?” Your friend says, “To the left.”\r\nThat’s an almost trivial scrap of information. But it’s not nothing. It\r\ntells you that the first ball is not on the extreme left edge of the\r\ntable. And it makes it just a tad more likely that the first ball is on\r\nthe right side of the table. If your friend rolls another ball on the\r\ntable and the procedure is repeated, you get another scrap of\r\ninformation. If he says, “It’s to the left,” the likelihood of the first\r\nball being on the right side of the table increases a little more. Keep\r\nrepeating the process and you slowly narrow the range of the possible\r\nlocations, zeroing in on the truth—although you will never eliminate\r\nuncertainty entirely.”\r\nAfter reading this paragraph, I thought it would be much more\r\npedagogically compelling (and fun 😉) to see this update process live\r\nand in action. What a great opportunity to learn how to work with the gganimate\r\nR package.\r\nHere is the code I put together.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # data manipulation and visualization\r\nlibrary(gganimate) # animation of the charts\r\nlibrary(bayestestR) # Highest Density Interval computation\r\n\r\n# specifying the width of the billiards table  \r\nfield <- seq(0,178,1)\r\n\r\n# likelihoood function for the situation when the second ball stops to the right of the first one \r\nrightLikelihood <- (178-field)/178\r\n# likelihoood function for the situation when the second ball stops to the left of the first one \r\nleftLikelihood <- 1-rightLikelihood\r\n# flat prior for the very beginning of the updating process  \r\nfirstPrior <- rep(1/179, 179)\r\n\r\n# position where the first ball stopped\r\npoint <- 88\r\n\r\n# Bayesian belief updating\r\n# creating shell dataframe for final results\r\nposteriors <- data.frame()\r\n\r\n# setting random seed for ensuring reproducibility\r\nset.seed(1234)\r\n\r\n# specifying the number of trials\r\nfor(i in 1:500){\r\n  \r\n  # throwing the second ball\r\n  pointTrial <- runif(n = 1, min = 0, max = 178)\r\n  \r\n  # determining whether the second ball stopped to the left or to the right of the first one \r\n  side <- ifelse(pointTrial > point, \"right\", \"left\")\r\n  \r\n  # selecting appropriate prior\r\n  if(i==1){\r\n    \r\n    prior <- firstPrior\r\n    \r\n  } else{\r\n    \r\n    prior <- posteriors %>% \r\n      dplyr::filter(trial == i-1) %>% \r\n      dplyr::pull(posterior)\r\n    \r\n  }\r\n  \r\n  # combining prior and likelihood (evidence) and transforming the result into probabilities\r\n  if(side == \"right\"){\r\n    \r\n    likelihood <- rightLikelihood * prior \r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  } else{\r\n    \r\n    likelihood <- leftLikelihood * prior\r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  }\r\n  \r\n  # putting results into the dataframe\r\n  suppDf <- data.frame(\r\n    posterior = probability,\r\n    trial = i,\r\n    side = side,\r\n    pointTrial = pointTrial\r\n  )\r\n  \r\n  # computing Highest Density Interval\r\n  sampling <- sample(x = field, size = 10000, replace = TRUE, prob = probability)\r\n  \r\n  hdi <- bayestestR::hdi(sampling, ci = 0.95)\r\n  \r\n  suppDf <- suppDf %>%\r\n    dplyr::mutate(\r\n      lhdi = hdi$CI_low,\r\n      hhdi = hdi$CI_high\r\n    )\r\n  \r\n  # putting results into the shell dataframe \r\n  posteriors <- dplyr::bind_rows(posteriors, suppDf)\r\n\r\n}\r\n\r\n# adjusting data for visualization purposes\r\nposteriorsDf <- posteriors %>%\r\n  dplyr::group_by(trial) %>%\r\n  dplyr::mutate(place = dplyr::row_number()-1) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    hdi = ifelse(place >= lhdi & place <= hhdi, \"yes\", \"no\")\r\n  )\r\n\r\n# creating charts\r\nmyAnimation <- ggplot2::ggplot(data = posteriorsDf, aes(x = place, y = posterior)) +\r\n  ggplot2::geom_area(data = posteriorsDf %>% dplyr::filter(hdi == 'yes'), fill = 'light blue') +\r\n  ggplot2::geom_line(size = 1) +\r\n  ggplot2::geom_point(aes(x = pointTrial, y = 0), size = 4) +\r\n  ggplot2::geom_point(aes(x = point, y = 0), size = 4, color = \"red\") +\r\n  ggplot2::geom_text(aes(x = 8, y = 0.1, label = stringr::str_glue(\"Trial: {trial}\\n95% HDI: [{lhdi}, {hhdi}]\")), color = \"grey\") +\r\n  ggplot2::scale_y_continuous(limits = c(0,0.105)) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" cm\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Bayesian belief updating in the billiards table thought experiment\",\r\n    subtitle = \"Target place: 88 cm\",\r\n    x = 'PLACE ON THE BILLIARDS TABLE', \r\n    y = 'POSTERIOR PROBABILITY',\r\n    caption = \"\\n\\nThe static red point corresponds to a random and unknown location of the first ball. The moving black point then corresponds to the location where the second,\\nrepeatedly rolled ball randomly ended up. The area in blue corresponds to the 95% Highest Density Interval of the posterior distribution. All points inside this\\ninterval have a higher probability density than points outside this interval.\"\r\n    ) +\r\n  gganimate::transition_time(trial) +\r\n  gganimate::ease_aes('linear')\r\n\r\n# animating chart\r\ngganimate::animate(myAnimation, nframes = 125, fps = 5, height = 6, width = 11, units = \"in\", res = 125)\r\n\r\n# saving animated chart as a .gif file\r\ngganimate::anim_save(filename = \"./bayesianBelifUpdating.gif\", animation = last_animation())\r\n\r\n\r\n\r\nAnd here is the resulting animation of the Bayesian belief updating\r\nprocess across 500 trials.\r\n\r\nFor those who would like to incorporate Bayesian reasoning into their\r\nmanagerial decision-making under uncertainty, I can recommend this\r\narticle by Brian T.\r\nMcCann.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-27-bayesian-belief-updating/./bayesian-belief-updating.jpg",
    "last_modified": "2022-11-17T14:59:19+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-24-police-cadet-evaluation-dataset/",
    "title": "Police cadet evaluation dataset",
    "description": "A \"new\" real-world dataset useful for training in people analytics.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-24",
    "categories": [
      "people analytics",
      "data",
      "recruitment",
      "hiring",
      "training"
    ],
    "contents": "\r\nWhile cleaning out my (very) old computer, I came across a hidden\r\ngem: a dataset with real-world data about police cadet evaluation. It\r\nwas part of a tutorial from Peltarion, an AI\r\nsoftware company providing specialized software (Synapse) for\r\ncreating, training, evaluating, and deploying artificial neural networks\r\nand other adaptive systems (recently acquired by King). AFAIK,\r\nthis dataset is not part of any publicly available database with\r\ntraining datasets, so it may add a bit to the portfolio of possibilities\r\nfor those involved and interested in people analytics.\r\nThe data were collected as part of an effort by the National\r\nPolice Services Agency and the Dutch\r\nMinistry of Justice and Security to objectively examine whether the\r\ndata collected at the time of graduation of police cadets can be used to\r\npredict the requirements for passing the standard five-year evaluation.\r\nThe main reason for the study was to find the key indicators for the\r\nthen 20% failure rate, which was considered unacceptable (data were\r\ncollected in the late 1990s), and to study the effects of lowering\r\nadmission standards (accepting cadets with past criminal records and\r\nlowering the minimum grade from 5.5 to 4.0).\r\nThe dataset has the following characteristics:\r\n2000 observations\r\n9 attributes:\r\nAge: the age at which the cadet started studying to\r\nbecome a police officer.\r\nAvG: average grade at the time of graduation (scale\r\n1-10).\r\nChdn: number of children at the time of\r\ngraduation.\r\nExEd: extra university-level or equivalent\r\neducation (years).\r\nCR: criminal record (0=No, 1=Yes).\r\nSex: sex of the cadet (0 = Male, 1 = Female).\r\nSecE: other experience in the security sector (0 =\r\nNo, 1 = Yes).\r\nAvgE: average yearly evaluation score (The average\r\nof five years. The evaluation is performed by a committee of 10 senior\r\npolice officers. Scale 1-5). This is a help attribute and not for use as\r\ninput.\r\nFinalE: final evaluation. Fail if average yearly\r\nevaluation score (Avg) < 2.0 otherwise pass. (1610 Pass / 390 Fail).\r\nThis is the target attribute.\r\n\r\nHere is a table you can use to check and download the data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and making user-friendly data table\r\nlibrary(tidyverse)\r\nlibrary(DT)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./policeCadetEvaluation.csv\")\r\n\r\n# adjusting the data type for some variables for tabulation and visualization purposes\r\ndata <- data %>%\r\n  mutate(\r\n    CR = as.factor(CR),\r\n    Sex = as.factor(Sex),\r\n    SecE = as.factor(SecE),\r\n    FinalE = as.factor(FinalE)\r\n  )\r\n\r\n# defining the table\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy', 'csv', 'excel'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nAnd here is a pairplot showing the distribution and relationships\r\nbetween variables in the dataset.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for the pairplot data visualization\r\nlibrary(GGally)\r\n\r\n# defining custom function for diagonal continuous variable chart  \r\nmy_dens <- function(data, mapping) {\r\n  ggplot(data = data, mapping = mapping) +\r\n    geom_density(alpha = 0.6, color = NA) \r\n}\r\n\r\n# pairplot\r\nGGally::ggpairs(\r\n  data = data,\r\n  title = \"Police cadet evaluation dataset\",\r\n  mapping=ggplot2::aes(fill = FinalE),\r\n  lower=list(\r\n    combo = wrap(\"facethist\", binwidth=1, alpha = 0.6),\r\n    continuous = wrap(\"points\", alpha = 0.3, size = 0.7),\r\n    discrete = wrap(\"facetbar\", alpha = 0.6)\r\n    ),\r\n  upper=list(\r\n    discrete = wrap(\"box\", alpha = 0.6),\r\n    combo = wrap(\"box\", alpha = 0.6)\r\n  ),\r\n  diag = list(\r\n    continuous = my_dens,\r\n    discrete = wrap(\"barDiag\", alpha = 0.6)\r\n    )\r\n  ) +\r\n  ggplot2::scale_fill_manual(values=c(\"Fail\" = \"#e53935\", \"Pass\" = \"#00897b\")) +\r\n  labs(caption = \"\\nThe color indicates the pass/fail result of the final evaluation, the target attribute.\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 12, hjust = 0),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you want to download the dataset, you can do so here via the table\r\nabove or via my\r\nGitHub page where you can also find more information about the\r\ndataset. Happy analysis 😉\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-24-police-cadet-evaluation-dataset/./Politie_Nederland_nieuw_uniform.jpg",
    "last_modified": "2022-11-17T15:01:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-conditional-inference-tree/",
    "title": "Divide and... understand",
    "description": "Finding the breakpoint when people start to score significantly higher/lower on a given criterion - the use case for the Conditional Inference Tree algorithm.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-18",
    "categories": [
      "conditional inference tree",
      "decision tree",
      "machine learning",
      "statistics",
      "interpretability",
      "prediction",
      "r"
    ],
    "contents": "\r\nWhen correlating collaboration metrics with business criteria that\r\nour clients are interested in, such as the size of the internal network\r\nof salespeople vs. their sales performance, we often encounter the\r\nquestion of where the breakpoint is when people start to score\r\nsignificantly higher/lower on a given criterion.\r\nTo answer this question, I find very handy the Conditional\r\nInference Tree algorithm - a non-parametric class of decision trees\r\nthat, unlike traditional decision trees, use a significance/permutation\r\ntest (corrected for multiple testing) to select covariates to split and\r\nrecurse the variable.\r\nWhen applied to just one numerical predictor, it will provide a set\r\nof partitions that allow you to split that predictor into bins in such a\r\nway that you end up with statistically significant differences between\r\nsome of the identified bins. With this information in hand, it is much\r\neasier for you to find the “sweet spots” (there may be more than one)\r\nwhere the criterion starts to behave differently in relation to the\r\npredictor values. See charts below for illustration.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and visualuzation \r\nlibrary(tidyverse)\r\n\r\n# defining normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\n# creating artificial dataset with internal network size and sales performance variables\r\ninternalNetworkSize = seq(-6, 6, 0.1)\r\nsalesPerf = 1*(internalNetworkSize**3) + 2*(internalNetworkSize**2) + 1*internalNetworkSize + 3\r\nsalesPerf_noise = 70 * rnorm(mean = 0, sd = 1, n=length(salesPerf))\r\nsalesPerformance = salesPerf + salesPerf_noise\r\n\r\n# putting data into dataframe and making some transformations of the variables\r\ndata <- data.frame(\r\n  internalNetworkSize = internalNetworkSize,\r\n  salesPerformance = salesPerformance\r\n) %>%\r\n  dplyr::mutate(\r\n    internalNetworkSize = normalize(internalNetworkSize),\r\n    salesPerformance = normalize(salesPerformance),\r\n    internalNetworkSize = internalNetworkSize*189,\r\n    salesPerformance = salesPerformance*100\r\n  )\r\n\r\n# visualizing relationship between internal network size and sales performance\r\nggplot2::ggplot(data = data, aes(x = internalNetworkSize, y = salesPerformance)) +\r\n  ggplot2::geom_point(color = \"#4d009d\", size = 3, alpha = 0.8) +\r\n  ggplot2::labs(\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE\",\r\n    y = \"SALES PERFORMANCE\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,200, 20), limits = c(0,200)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for ctree algorithm and visualization of the results of statistical tests\r\nlibrary(partykit)\r\nlibrary(ggstatsplot)\r\n\r\n# defining formula\r\nfmla <- as.formula(\"salesPerformance ~ internalNetworkSize\")\r\n\r\n# binning internal network size variiablle using ctree algorithm\r\nctree <- partykit::ctree(\r\n  fmla,\r\n  data = data,\r\n  na.action = na.exclude,\r\n  control = partykit::ctree_control(minbucket = ceiling(round(0.05*nrow(data))))\r\n)\r\n\r\n# plotting resulting tree\r\n#plot(ctree)\r\n\r\n# number of identified bins\r\n#bins = partykit::width(ctree)\r\n\r\n# extracting bin borders\r\ncutvct = data.frame(matrix(ncol=0,nrow=0)) # Shell\r\nn = length(ctree) # Number of nodes\r\nfor (i in 1:n) {\r\n  cutvct = rbind(cutvct, ctree[i]$node$split$breaks)\r\n}\r\ncutvct = cutvct[order(cutvct[,1]),] # sorting / converting to an ordered vector (asc)\r\ncutvct = ifelse(cutvct<0,trunc(10000*cutvct)/10000,ceiling(10000*cutvct)/10000) # rounding to 4th decimal place to avoid borderline cases\r\n\r\n# adding minimum and maximum values\r\ncutvct <- append(cutvct, min(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct <- append(cutvct, max(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct = cutvct[order(cutvct)]\r\n\r\n# creating bin categories\r\nvalueCat <- cut(x = data %>% dplyr::pull(\"internalNetworkSize\"), breaks = cutvct, include.lowest = TRUE)\r\n\r\n# creating supplementary dataframe for visualization purposes \r\nsuppDf <- data %>%\r\n  dplyr::select(internalNetworkSize, salesPerformance) %>%\r\n  dplyr::mutate(category = valueCat) %>%\r\n  dplyr::filter(category != \"NA\")\r\n\r\n# visualizing relationship between internal network size and sales performance using ggbetweenstats from ggstatsplot package\r\nggstatsplot::ggbetweenstats(\r\n  data = suppDf %>% as.data.frame(),\r\n  x = category,\r\n  y = salesPerformance,\r\n  type = \"robust\"\r\n) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\"), breaks = seq(0,100,20)) +\r\n  ggplot2::labs(\r\n    y = \"SALES PERFORMANCE\",\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE (BINNED)\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n    ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you are dealing with similar use cases, give it a try. And if you\r\nuse any other tools/approaches for this, feel free to share them in\r\nreturn.\r\nP.S. Thanks to Filip\r\nTrojan, my former boss and colleague from the Deloitte Advanced\r\nAnalytics team, who introduced me to this tool. I still benefit from it\r\nto this day 🙏💪\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-18-conditional-inference-tree/./decision-tree-analysis.jpg",
    "last_modified": "2022-10-25T10:37:02+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-11-timeboxing/",
    "title": "Timeboxing. Does it really work?",
    "description": "Checking with real-world collaboration data whether timeboxing has a protective function in terms of time available for focused work.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-11",
    "categories": [
      "timeboxing",
      "timeblocking",
      "regression analysis",
      "control variables"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I addressed the question of why people don’t make more use of\r\ntimeboxing, a time\r\nmanagement tool relying on a well-researched self-regulatory technique\r\ncalled implementation\r\nintention - planning what you will do, when, and how.\r\nQuite surprisingly, I found in our collaborative data that there is\r\nnot a positive but a slightly negative relationship between the amount\r\nof time for focused work and the amount of blocked working time in the\r\ncalendar, which I interpreted to mean that people who have plenty of\r\ntime for focused work usually do not have a strong need to block time\r\nfor focused work in their calendars.\r\nHowever, based on these results, one colleague wondered whether this\r\nresult actually speaks against the usefulness of this tool. To answer\r\nher question properly, we should avoid comparing apples with pears and\r\ncontrol for the effect of the number of collaborative activities people\r\nparticipate in, as it can be assumed that those who spend more time\r\ncollaborating with others have less time for focused work and also use\r\nthe timeboxing technique more.\r\nUsing this approach and our clients’ collaborative data, I looked at\r\nthe relationship between the proportion of work time blocked on the\r\ncalendar and the time available for focused work (i.e. no meetings, no\r\nad-hoc calls, no email or instant messaging), and found that the\r\nmarginal effect of timeboxing is in line with the positive effect of the\r\ntimeboxing technique on the time available for focused work. The effect\r\nis not huge (each percentage point of working time blocked in the\r\ncalendar yields on average .19% of focus rate), however, timeboxing\r\nseems to be saved, phew 😉\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-11-timeboxing/./time-blocking.png",
    "last_modified": "2022-10-25T06:50:42+02:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-09-17-multilevel-modeling/",
    "title": "Multilevel modeling in people analytics",
    "description": "Don't chase (statistical) ghosts and use multilevel models instead!",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-17",
    "categories": [
      "multilevel modeling",
      "hierarchical modeling",
      "mixed models",
      "nested data",
      "bayesian inference",
      "collaboration",
      "employee engagement",
      "r"
    ],
    "contents": "\r\nIn one of our projects,\r\nwhere we were trying to find out how collaborative behavior relates to\r\nemployee engagement, we repeatedly came across patterns that reminded\r\nthe client of internally well-known differences between the behavior of\r\nteams from different parts of the company. For example, we found that\r\nmore frequent participation in short and small meetings was related to\r\nlower employee engagement. This pattern matched well the client’s\r\nobservation that one particular part of the company had regular daily\r\nstand-up meetings and also lower engagement scores compared to the rest\r\nof the company due to some other aspects of their work. As further\r\nanalysis confirmed, this pattern was really just a statistical artifact\r\ncaused by the coincidence of these two facts.\r\nOne way analysts can protect themselves from this type of misleading\r\nconclusions is by using multilevel or hierarchical\r\nmodels that take into account the fact that the data\r\nhave a nested structure, i.e. that some observations are not\r\nindependent of each other because they belong to the same higher-order\r\ngroup, e.g. to an organizational unit (one of the basic assumptions of\r\nmost statistical models in use).\r\nThis is well illustrated in the graphs below. They show that the\r\nrelationship between the number of monthly 1:1s that employees have with\r\ntheir line manager and their subjectively perceived support from their\r\nline manager is slightly positive across most groups of teams (shown by\r\ncolored dots and lines), but when all teams are analyzed together, the\r\nrelationship is rather negative (shown by black dots and lines).\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(scales)\r\nlibrary(patchwork)\r\nlibrary(ggtext)\r\n\r\ndata <- readr::read_csv(\"./data.csv\")\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support while taking into account differences between organizational units \r\ng1 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(aes(col = Unit), size = 2.5, alpha = 0.5) + \r\n  ggplot2::geom_smooth(aes(col = Unit), method = 'lm', alpha=0.2, se = F) +\r\n  labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Unit A\" = \"#20066b\", \"Unit B\" = \"#e56b61\", \"Unit C\" = \"#b4ba0d\", \"Unit D\" = \"#32b2c7\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support without taking into account differences between organizational units \r\ng2 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(size = 2.5, alpha = 0.5) +\r\n  ggplot2::geom_smooth(method = 'lm', alpha=0.2, linetype = \"solid\", color = \"black\", se = F) +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- g2 + g1\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Relationship between the number of 1:1s and perceived managerial support across**\r\n  <br>\r\n  **and within organizational units**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,12,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# fitting Bayesian hierarchical linear regression model\r\nmodel <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones + (1 + oneonones | Unit)),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(model)\r\n# plot(model)\r\n# brms::pp_check(model, ndraws = 100)\r\n\r\n\r\n# fitting Bayesian non-hierarchical linear regression model\r\nmodelNonHierarchical <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(modelNonHierarchical)\r\n# plot(modelNonHierarchical)\r\n# brms::pp_check(modelNonHierarchical, ndraws = 100)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(emmeans)\r\nlibrary(tidybayes)\r\n\r\n# marginal effect of 1:1s in the Bayesian hierarchical linear regression model\r\navg_marginal_effect <- model %>% \r\n  emmeans::emmeans(~ oneonones,\r\n                   at = list(oneonones = seq(0, 6, by = 0.1)),\r\n                   epred = TRUE,\r\n                   re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf1 <- ggplot2::ggplot(avg_marginal_effect, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Hierarchical linear regression model\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n# marginal effect of 1:1s in the Bayesian non-hierarchical linear regression model\r\navg_marginal_effect_nonHierarchical <- modelNonHierarchical %>% \r\n  emmeans::emmeans(\r\n    ~ oneonones,\r\n    at = list(oneonones = seq(0, 6, by = 0.1)),\r\n    epred = TRUE,\r\n    re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf2 <- ggplot2::ggplot(avg_marginal_effect_nonHierarchical, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Non-hierarchical linear regression model\"\r\n  ) +\r\n    ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ngf <- gf2 + gf1\r\ngf <- gf + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Posterior average marginal effect of 1:1 meetings on perceived managerial support**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,0,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(gf)\r\n\r\n\r\n\r\n\r\nWithout the use of the hierarchical model (and/or a careful post-hoc\r\nvisual check of alternative explanations), we would reach a completely\r\nopposite (and incorrect) conclusion about the relationship between the\r\nnumber of 1:1s and perceived support from the line manager (a phenomenon\r\nknown as Simpson’s\r\nparadox). In this particular case, it is relatively easy to\r\nrecognize that something may be wrong, but the situation is not always\r\nso obvious and intuitive. In these other cases, it is advantageous to\r\nhave some tool at hand to compensate for our imperfect intuition and\r\nlimited knowledge and imagination. Hierarchical models are one such\r\ntool.\r\nIf you don’t already have it in your analytics toolbox, be sure to\r\ngive it a try. If you work with R, you can use the lme4\r\nor brms\r\npackages to implement it. In a Python\r\nenvironment, you can use the statsmodels or\r\nPyMC3 libraries to\r\ndo this. And if you’re more used to drag-and-drop tools, then JASP or jamovi (both open-source alternatives\r\nto SPSS)\r\nwill give you access to various mixed models through an easy-to-use\r\ngraphical interface.\r\nFor an accessible discussion of this topic in the context of people\r\nanalytics, including other useful tools for working with hierarchical\r\ndata, see also the excellent articles by Paul van der\r\nLaken, John\r\nLipinski, and Max\r\nBlumberg:\r\nSimpson’s\r\nParadox: Two HR examples with R code.\r\nHow\r\nto Avoid Aggregation Errors and Simpson’s Paradox in HR Analytics: Part\r\n1\r\nHow\r\nto Avoid Aggregation Errors and Simpson’s Paradox in HR Analytics: Part\r\n2\r\nWhy\r\nPeople Analytics should NOT be using regression to predict team\r\noutcomes\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-multilevel-modeling/./groupsofpeople.jpg",
    "last_modified": "2022-09-17T20:03:50+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-and-personality/",
    "title": "Collaboration and personality",
    "description": "Personality is not fate, at least when it comes to the level of engagement in corporate communication and collaboration.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-15",
    "categories": [
      "collaboration",
      "communication",
      "networking behavior",
      "personality",
      "big five"
    ],
    "contents": "\r\nOne of our clients once\r\nasked us to what extent employees’ level of engagement in corporate\r\ncommunication and collaboration is driven by their personality and to\r\nwhat extent by their job role, the conditions in which they work, and\r\nother factors outside their personality.\r\nOur first answer was that the latter probably plays a more\r\nsignificant role than the former, but it was difficult to answer more\r\nspecifically because we did not yet have all the data we needed to\r\nquantify the tightness of this relationship. This motivated me to look\r\nat existing research on this topic to help us better set our apriori\r\nexpectations on this issue.\r\nWith the help of metaBus, an\r\namazing platform for curating, searching, and summarizing research\r\nfindings from the social and organizational sciences, I was able to get\r\nthe results of over 100 studies on the relationship between employees’\r\nBig\r\n5 personality traits and the amount of interaction and networking\r\nbehavior they engage in. Among the criteria for the amount of\r\ninteraction were variables like contact frequency, frequency of\r\nparticipation, communication frequency, meeting frequency, hours of\r\ninteraction, interaction duration, etc. For networking behavior there\r\nwere criteria as liaison, building networks, relationship building,\r\nnetwork activity, maintaining contacts, increasing internal visibility,\r\nnetwork ability, and informal network.\r\n\r\n\r\nShow code\r\n\r\n# The following concepts were used to search for relevant studies on the metaBus platform (their respective codes are given in brackets):\r\n# Big 5 (20443)  \r\n# Amount of interaction (20287) \r\n# Networking behavior (80017)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # for data manipulation and visualization \r\nlibrary(ggridges) # for data visualization\r\nlibrary(ggtext) # for enabling markdown in ggplots\r\nlibrary(patchwork) #  for combining ggplots\r\n\r\n# data preparation\r\n# uploading data\r\ninteraction <- readr::read_csv(\"./interactionAmount.csv\")\r\nnetworking <- readr::read_csv(\"./networkingBehavior.csv\")\r\n\r\n# preparing dataset for amount of interaction concept\r\ninteractionPrep <- interaction %>%\r\n  dplyr::filter(\r\n    # removing non-relevant personality characteristics \r\n    !Var1 %in% c(\"Empathic concern\"),\r\n    # limiting to studies conducted at the individual level\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Extraversion\") | stringr::str_detect(Var1, \"Extroversion\") | stringr::str_detect(Var1, \"extraversion\") ~ \"Extraversion\",\r\n      stringr::str_detect(Var1, \"Openness to experience\") | stringr::str_detect(Var1, \"openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"agreeableness\") ~ \"Agreeableness\",\r\n      stringr::str_detect(Var1, \"emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# preparing dataset for networking behavior concept\r\nnetworkingPrep <- networking %>%\r\n  dplyr::filter(\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"Emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# data visualization\r\n# creating chart for the amount of interaction concept\r\ninteractionChart <- interactionPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#e56b61\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.text.x = element_text(),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n)\r\n\r\n# creating chart for the networking behavior concept\r\nnetworkingChart <- networkingPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#32b2c7\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text.y = element_blank(),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts together\r\ng <- interactionChart + networkingChart \r\n\r\n# adding title and caption\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:20pt;font-weight:bold;'>**Do Big 5 traits predict** \r\n    <span style='color:#e56b61;'>**the amount of interaction**<\/span> **&**\r\n    <span style='color:#32b2c7;'>**networking behavior**<\/span> **of employees?**\r\n    <\/span>\",\r\n  \r\n  caption = \"The solid vertical lines represent quartile values.\\nBased on studies found on the metaBus platform using the concepts 'Big 5' (code: 20443), 'Amount of interaction' (code: 20287), and 'Networking behavior' (code: 80017).\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(10,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 9, hjust = 0)\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nAs can be seen from the graphs above, the relationship between\r\npersonality and the amount of interaction and networking behavior goes\r\nin the expected direction. Agreeable, open, and extraverted employees\r\nand to some extent also conscientious and emotionally stable employees\r\ntend to engage more in interactions with others and in networking.\r\nHowever, the relationships found are relatively weak. The middle 80% of\r\nobserved effects range from an absolute value of .02 to .24, so across\r\nthe studies shown, small effects prevail. And even in the case of the\r\nstrongest effect (r = .38), personality “explains” only 14% of\r\nthe variability in the networking behavior. There is therefore ample\r\nscope for the influence of a range of other factors.\r\nHow about you? Are you able to engage in interactions and networking\r\nin a way that supports your career, work performance, or other positive\r\noutcomes, perhaps despite your natural tendencies due to your\r\npersonality setup? Feel free to share your experience and thoughts in\r\nthe comments. Btw, you can find interesting information on this topic in\r\nthe excellent book 8\r\nSteps to High Performance by Marc Effron, specifically\r\nin chapters 4 and 6.\r\nCaveat: The graphs represent only a simple summary\r\nof the effects observed in the selected studies, not a proper\r\nmeta-analysis. If you’re interested in the specific analysis steps and\r\nchoices behind the graphs shown, you can check the code above or go to\r\nmy GitHub\r\npage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-and-personality/./personalityandcollaboration.jpeg",
    "last_modified": "2022-09-17T14:19:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-overload-and-bottlenecks/",
    "title": "Hot spots of collaboration overload and collaboration bottlenecks and how to find them",
    "description": "One of the most useful insights that can be gleaned from collaboration data is where hot spots of potential collaboration overload and/or collaboration bottlenecks may exist in a company. Such insight can be especially valuable these days, when many companies are trying to fight the upcoming economic downturn by achieving more with less.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-08",
    "categories": [
      "collaboration overload",
      "collaboration bottlenecks",
      "organizational network analysis",
      "r"
    ],
    "contents": "\r\nOne simple way to identify such hot spots is to compare the\r\noutbound and inbound collaborative activities in which\r\nteams or individuals participate. The greater the difference between the\r\ntwo in favor of inbound collaboration activities, the stronger the\r\nsignal that collaboration overload and/or bottlenecks may be a problem\r\nfor that team or individual.\r\nTo illustrate, take a look at the attached charts that show the\r\npatterns of collaboration between several teams via Slack. The distances\r\nbetween teams, the thickness, and the direction of the arrows between\r\nthem tell us who is collaborating with whom and how much. The size of\r\nthe nodes then represents the amount of inbound and outbound\r\ncollaborative activities that the teams participate in, respectively.\r\nBased on the differences between them, the bar chart below shows us\r\nwhere the inbound collaboration activities outweigh the outbound ones\r\nthe most, and therefore where the risk of collaboration overload and/or\r\nbottlenecks is greatest.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nlibrary(patchwork)\r\n\r\n# uploading data with nodes/ties based on the current frequency of communication via Slack\r\nnodesR <- readr::read_csv(\"./nodes.csv\") \r\ntiesR <- readr::read_csv(\"./ties.csv\")\r\n\r\n# specifying cut-off value for showing only the x% of strongest edges\r\nprob = 1\r\n\r\n# changing coding of individual nodes in the network\r\nties <- tiesR %>%\r\n  dplyr::left_join(nodesR, by = c(\"from\" = \"id\")) %>%\r\n  dplyr::select(-from) %>% \r\n  dplyr::rename(from = name) %>%\r\n  dplyr::left_join(nodesR, by = c(\"to\" = \"id\")) %>%\r\n  dplyr::select(-to) %>%\r\n  dplyr::rename(to = name) %>%\r\n  dplyr::select(from, to, weight) %>%\r\n  dplyr::mutate(\r\n    from = stringr::str_to_title(from),\r\n    to = stringr::str_to_title(to),\r\n    from = stringr::str_replace(from, \"Development\", \"Dev\"),\r\n    to = stringr::str_replace(to, \"Development\", \"Dev\"),\r\n    from = stringr::str_replace(from, \"Dev Management\", \"Dev - Management\"),\r\n    to = stringr::str_replace(to, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n  \r\nnodes <- nodesR %>%\r\n  dplyr::select(-id) %>%\r\n  dplyr::mutate(\r\n    name = stringr::str_to_title(name),\r\n    name = stringr::str_replace(name, \"Development\", \"Dev\"),\r\n    name = stringr::str_replace(name, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n\r\n\r\n# making the network from the data frame \r\ng <- igraph::graph_from_data_frame(d = ties, vertices = nodes, directed = TRUE)\r\n\r\n# setting name of the network\r\ng$name <- \"Collaboration via Slack\"\r\n\r\n# assigning ids to nodes\r\nV(g)$id <- seq_len(vcount(g))\r\n\r\n# cutoff value for showing only the x% of strongest edges\r\ncutoff <- quantile(ties$weight, probs = prob)[[1]]\r\n\r\n# visualizing the inbound network\r\nset.seed(1234)\r\ninG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = inbound), alpha = 0.5, fill = \"#32b2c7\", color = \"#32b2c7\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n    ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"INBOUND COLLABORATION ACTIVITIES\")\r\n    )\r\n\r\n\r\n\r\n# visualizing the outbound network\r\nset.seed(1234)\r\noutG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = outbound), alpha = 0.5, fill = \"#46c8ae\", color = \"#46c8ae\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"OUTBOUND COLLABORATION ACTIVITIES\")\r\n  )\r\n\r\n\r\n# bar chart with info about difference between inbound and outbound collaboration activities\r\ninoutDiffG <- nodes %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(name, inoutDiff), y = inoutDiff)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = ifelse(nodes$inoutDiff > 0, \"#e56b61\", \"#20066b\"), alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(breaks = seq(-200,200,50)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"DIFFERENCE BETWEEN IN/OUTBOUND COLLABORATION ACTIVITIES\",\r\n    y = \"DIFFERENCE BETWEEN THE NUMBER OF IN/OUTBOUND INSTANT MESSAGES\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 30, margin=margin(0,0,12,0), hjust = 0.5),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 24, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 22, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- (inG + outG) / inoutDiffG\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nWe see that the “hottest” hot spots are in teams Dev-Management and\r\nDev-Frontend. While this is not definitive proof that we have real\r\nproblems in these two specific teams, it should be a strong enough\r\nsignal to take notice and try to verify our suspicion with additional\r\ninformation, such as checking some relevant business metrics or simply\r\nasking a few people we know should be affected, if there is a problem.\r\nIf the initial suspicion is confirmed, appropriate action should be\r\ntaken, e.g. consider the relevance of some requests, possibly redirect\r\nthem to other teams, automate some tasks, expand the team and recruit\r\nnew people, etc.\r\nFor more tips on how to leverage collaboration data in the current\r\nuncertain economic times, I recommend reading the articles Top\r\n7 Collaboration Metrics to Utilize in an Economic Crisis by Jan Rezab and Top\r\n6 Metrics to measure during an economic downturn by Shwetha Pai.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-overload-and-bottlenecks/./Organizational-network.jpg",
    "last_modified": "2022-09-17T18:33:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-back-to-back-meetings/",
    "title": "Are back-to-back meetings for good or bad?",
    "description": "A short post about the practice of back-to-back meetings and how to determine when it's for bad and when it's rather for good.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-01",
    "categories": [
      "meeting habits",
      "back-to-back meetings"
    ],
    "contents": "\r\n“Context, Context, Context” could be the headline of this post.\r\nWhen we address the issue of good meeting habits with our clients, the length of breaks\r\nbetween successive meetings is one of the first metrics we focus on.\r\nAs many of you probably know from your firsthand experience,\r\nconsecutive meetings with no breaks, a.k.a. back-to-back\r\nmeetings, have many detrimental effects, from overload\r\nand exhaustion to not being adequately prepared for subsequent\r\nmeetings and arriving\r\nlate to them.\r\nAs useful as the above metric is, it does not tell the whole story\r\nand can lead to invalid conclusions and see a problem where there is\r\nnone. Data from one of our teams illustrates this well.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading datasets from the platform\r\ndata1 <- readr::read_delim(\"./timeisltd-chart1.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata2 <- readr::read_delim(\"./timeisltd-chart2.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata3 <- readr::read_delim(\"./timeisltd-chart3.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\n\r\n# Time between successive meetings\r\ng1 <- data1 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#20066b\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time between successive meetings\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Number of back-to-back meetings in a row\r\ng2 <- data2 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = as.numeric(cat)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#e56b61\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" mtgs\", accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Number of back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Time spent in back-to-back meetings in a row\r\ng3 <- data3 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = factor(cat, levels = c(\"31-60 Min\", \"61-90 Min\", \"91-120 Min\", \"121-150 Min\", \"151-180 Min\", \"181+ Min\"), ordered = TRUE)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#46c8ae\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time spent in back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# putting graphs together\r\ng <- g1/g2/g3\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBased purely on the time between successive meetings, we could\r\nconclude that a given team suffers from an unhealthy frequency of\r\nback-to-back meetings, as in more than a third of cases, there is no\r\nbreak between meetings. However, if we look at how long the series of\r\nback-to-back meetings tend to be (in 75% of cases it’s only 2 meetings\r\nin a row) and how much time people spend in them (in 42% of cases it’s\r\nbetween 31-60 minutes and in 30% of cases it’s between 61-90 minutes),\r\nthen the resulting picture is less pessimistic and more indicative of a\r\nrather healthy level of effort to protect time for focused\r\nwork by batching meetings into short blocks\r\nthat do not come at the cost of exhausting people and making meetings\r\nless effective.\r\nWhat is your approach to back-to-back meetings? Do you try to always\r\nhave at least a 5-minute buffer between two consecutive meetings? And\r\nhow successful are you at this? Are you aware of situations where it is\r\nappropriate to batch meetings into tight blocks without breaks? And do\r\nyou have a limit on how many meetings to put in a row? Feel free to\r\nshare your thoughts and experiences in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-back-to-back-meetings/./backtobackmeetings.jpg",
    "last_modified": "2022-09-17T17:37:26+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-20-people-analytics-popularity-after-covid/",
    "title": "The impact of the COVID pandemic on the popularity of people analytics",
    "description": "Many people analytics professionals think that after the COVID pandemic, organizations are more willing to listen to their insights and recommendations. Can we find any empirical support for their hunch? Let's check it out with data provided by Google Trends and segmented regression analysis of interrupted time series.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-29",
    "categories": [
      "people analytics",
      "hr analytics",
      "covid pandemic",
      "segmented regression analysis",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nPeople analytics popularity after pandemic\r\nData preparation\r\nModeling\r\nWhat do the model and data tell us?\r\n\r\nPeople analytics popularity after pandemic\r\nThere is a fairly common perception among the people analytics professionals with whom I am in contact that after the COVID pandemic, companies are much more willing to use the insights provided by people analytics teams and incorporate them into their business-related decision-making processes.\r\nI wondered if I could find any empirical support for this feeling in the surge of global web search interest in “people analytics” and “HR analytics” terms on Google after the pandemic outbreak, assuming the pandemic broke out in March 2020.\r\nLet’s start our quest with a simple visual inspection of a line chart showing the trend of worldwide web search interest in “people analytics” and “HR analytics” terms on Google from January 2007 to July 2022. (You can replicate this chart using the Google Trends website and the search terms, time range, location, and source for searches described above. If you are familiar with R, you can use the code below.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for getting data from Google Trends\r\nlibrary(gtrendsR)\r\n# uploading libraries for data manipulation\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\n# setting parameters for Google Trends\r\nsearchTerms   <- c(\"people analytics\", \"hr analytics\")\r\nlocation      <- \"\" # global\r\ntime          <- \"2007-01-01 2022-07-31\"\r\nsource         <- \"web\"\r\n\r\n# getting Google Trends data\r\ngtrendsResult <- gtrendsR::gtrends(\r\n    keyword = searchTerms, \r\n    geo     = location, \r\n    time    = time,\r\n    gprop   = source\r\n    )\r\n\r\n# cleaning data\r\ngtrendsResultDf <- gtrendsResult %>%\r\n    purrr::pluck(\"interest_over_time\") %>%\r\n    dplyr::select(date, hits, keyword) %>%\r\n    dplyr::mutate(date = lubridate::ymd(date))\r\n\r\n\r\n\r\nTowards the end of the time series, somewhere between September 2019 and March 2020, it seems that the trend stops increasing and starts to stagnate, except for the very last part of the graph, which shows a sharp increase in searches for both terms, but this may just be the result of the improved data collection system from January 2022 onwards (as indicated by the vertical line in the graph with a note). Thus, the data seems to suggest the opposite of what we would expect if a pandemic were to have a positive effect on interest in people analytics.\r\nHowever, after combining the results for the two search terms and plotting them on a graph together with the unadjusted regression trend lines, the resulting picture gives a slightly different impression. There appears to be little to no decline in interest in people analytics immediately after the pandemic outbreak, but a steeper slope of change after the pandemic.\r\n\r\n\r\nShow code\r\n\r\n# normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\ngtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100\r\n  ) %>%\r\n    # creating new pandemic variable \r\n  dplyr::mutate(\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ \"After the pandemic outbreak\",\r\n      TRUE ~ \"Before the pandemic outbreak\"\r\n    )\r\n  )%>%\r\n  ggplot2::ggplot(aes(x = date, y = interestInPeopleAnalytics, color = pandemic)) +\r\n  ggplot2::geom_point() +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggplot2::scale_x_date(breaks = \"1 year\", date_labels = \"%Y\") +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"Interest in people analytics\",\r\n    title = \"Interest in people analytics before and after the pandemic outbreak\",\r\n    caption = \"The solid lines represent unadjusted regression model trend lines before and after the pandemic outbreak.\"\r\n  ) +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nTo make the picture a little bit clearer, let us take the help of inferential statistics to answer the question we are interested in. The ideal analytical tool for our use case is a segmented regression analysis of interrupted time series that enables estimation of the changes in levels and trends of search interest before and after after a known ‘intervention’ or ‘interruption’ (i.e., a change that could potentially affect the outcome variable). It does so by segmenting the time series data into different periods based on the known interruption points and modeling these segments separately. The model used has the following general structure:\r\n\\[Y_{t} = β_{0} + β_{1}*time_{t} + β_{2}*intervention_{t} + β_{3}*time after intervention_{t} + e_{t}\\]\r\nThe β0 coefficient estimates the baseline level of the outcome variable at time zero; β1 coefficient estimates the change in the mean of the outcome variable that occurs with each unit of time before the intervention (i.e. the baseline trend); β2 coefficient estimates the level change in the mean of the outcome variable immediately after the intervention (i.e. from the end of the preceding segment); and β3 estimates the change in the trend in the mean of the outcome variable per unit of time after the intervention, compared with the trend before the intervention (thus, the sum of β1 and β3 equals to the post-intervention slope). For a better understanding of the model, take a look at the illustrative chart taken from Turner et al. (2021) below.\r\n\r\nSince we are dealing with correlated and truncated data, we should also include two additional terms in our model, an autocorrelation term and a truncation term, to handle these specific properties of our data.\r\nData preparation\r\nBut first, let’s prepare the data we will need for this type of analysis. Specifically, we will need the following five variables:\r\nsearch interest in people analytics – numerical variable representing search interest in people analytics relative to the highest point on the chart for the given region and time; this variable is truncated within the interval between values of 0 and 100; a value of 100 is the peak popularity for the term; a value of 50 means that the term is half as popular; a score of 0 means that there was not enough data for this term; search interest for two monitored terms (“people analytics” and “HR analytics”) was combined by simple summation and then normalized to a range of 0 to 100; this variable serves as a dependent (criterion) variable;\r\nelapsed time – numerical variable representing the number of months that elapsed from the beginning of the time series; this variable enables estimation of the size and direction of the overall trend in the data before the intervention;\r\npandemic – dichotomic variable indicating the presence/absence of pandemic; as already mentioned above, for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020; this variable enables estimation of the level change in the interest in people analytics immediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak – numerical variable representing the number of months that elapsed from the beginning of pandemic; this variable enables estimation of the change in the trend in the interest in people analytics after the outbreak of pandemic;\r\nmonth – categorical variable representing specific month within a year; this variable enables controlling for the effect of seasonality.\r\n\r\n\r\nShow code\r\n\r\n# munging data\r\nmydata <- gtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100,\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), ordered = FALSE)\r\n  ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(date) %>%\r\n    # creating new variables elapsed time, pandemic, and time elapsed after pandemic outbreak\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ 1,\r\n      TRUE ~ 0\r\n    ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic),\r\n    pandemic = as.factor(ifelse(pandemic == 1, \"After the pandemic outbreak\", \"Before the pandemic outbreak\"))\r\n  ) %>%\r\n  # final selection of variables\r\n  dplyr::select(\r\n    date, interestInPeopleAnalytics, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n  )\r\n\r\n\r\nHere is a table with the resulting data we will use for our analysis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making more user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nModeling\r\nWe can now fit the model to the data and test what it tells us about the impact of the pandemic on people’s search interest in people analytics. We will use the brms r package for this, which allows us to make inferences about the model parameters within a Bayesian inferential framework. For this reason, we must also specify some additional parameters (e.g. chains, iter or warmup) of the Markov Chain Monte Carlo (MCMC) algorithm that generates posterior samples of our model’s parameters.\r\nThe Bayesian framework also allows us to specify priors for the estimated parameters and use them to incorporate our domain knowledge into the analysis. The specified priors are important for both parameter estimation and hypothesis testing because they define our initial information state before we consider our data. Here, we will use relatively broad, uninformative, and only slightly regularizing priors (that is, the inference results will be very close to the results of standard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors \r\npriors <- c(brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model\r\nmodel <- brms::brm(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  seed = 1234,\r\n  sample_prior = TRUE, \r\n  control = list(adapt_delta = 0.9),\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n\r\nBefore making any inferences, we should perform several validation checks to ensure that the mechanics of the MCMC algorithm worked well and that we can use the generated posterior samples to make inferences about the parameters of our model. There are many ways to do this, but here we will only use a visual check of the MCMC chains. We want the plots of these chains to look like a hairy caterpillar, indicating the convergence of the underlying Markov chain to stationarity and the convergence of the Monte Carlo estimates to population quantities, respectively. As can be seen in the graph below, we can observe the characteristics we are looking for in the MCMC chains described above. (For additional MCMC diagnostics procedures, see for example Bayesian Notes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains\r\nbayesplot::mcmc_trace(\r\n  model,\r\n  facet_args = list(nrow = 6)\r\n) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the model parameters\"\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nIt is also important to check how well the model fits the data. To do this, we can use the posterior predictive check, which uses a specified number of selected posterior values of the model parameters to show how well the fitted model predicts the observed data. In the graph below we see that the model fits the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model fit\r\n# specifying the number of samples\r\nndraws = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  ndraws = ndraws\r\n) +\r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive check (using {ndraws} samples)\")\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nWhat do the model and data tell us?\r\nWe can now use the parameters of our model to obtain information about our main question. Specifically, we are interested in the value of the coefficient of the pandemic and the time after pandemic terms in our model. They represent how much and in what direction the search interest in people analytics changed after the pandemic outbreak.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the fitted model\r\nsummary(model)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: mydata (Number of observations: 187) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.67      0.07     0.53     0.82 1.00    36988    34009\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                           -19.75      7.34   -35.53\r\nelapsedTime                           0.47      0.04     0.42\r\npandemicBeforethepandemicoutbreak    10.96      4.99     1.62\r\nelapsedTimeAfterPandemic              0.65      0.29     0.10\r\nmonthFeb                              1.74      1.43    -1.08\r\nmonthMar                              2.77      1.87    -0.93\r\nmonthApr                              4.42      2.09     0.30\r\nmonthMay                              2.61      2.19    -1.69\r\nmonthJun                              0.60      2.25    -3.80\r\nmonthJul                              0.35      2.28    -4.16\r\nmonthAug                             -2.33      2.28    -6.83\r\nmonthSep                              1.67      2.22    -2.69\r\nmonthOct                             -0.56      2.10    -4.67\r\nmonthNov                             -1.01      1.88    -4.71\r\nmonthDec                             -6.14      1.44    -8.96\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            -6.98 1.00    30645    29199\r\nelapsedTime                           0.56 1.00    44619    28942\r\npandemicBeforethepandemicoutbreak    21.28 1.00    44289    44331\r\nelapsedTimeAfterPandemic              1.26 1.00    54010    46306\r\nmonthFeb                              4.55 1.00    37661    48486\r\nmonthMar                              6.43 1.00    26135    39337\r\nmonthApr                              8.51 1.00    22840    37029\r\nmonthMay                              6.91 1.00    21100    35358\r\nmonthJun                              5.03 1.00    20495    36016\r\nmonthJul                              4.81 1.00    20712    37600\r\nmonthAug                              2.16 1.00    21240    37079\r\nmonthSep                              6.07 1.00    22312    36687\r\nmonthOct                              3.56 1.00    24241    39183\r\nmonthNov                              2.68 1.00    27803    43248\r\nmonthDec                             -3.31 1.00    40223    50422\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     5.02      0.29     4.49     5.63 1.00    66571    52402\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nThe following graph shows the posterior distribution of the pandemic parameter. We can see that it is on the right-hand side of the zero value, which supports the claim that there is an effect of the pandemic on people’s interest in people analytics immediately after the pandemic outbreak; however, it is on the opposite side of the zero value than we would expect if the pandemic were to have a positive effect on people’s interest in people analytics. Thus, this suggests that immediately after the pandemic outbreak, interest in people analysis declined slightly (somewhere between ~1 and ~20 points, as indicated by the 95% credible interval).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(tidybayes)\r\n\r\nparamVizBeforethepandemicoutbreak <- model %>%\r\n  tidybayes::gather_draws(b_pandemicBeforethepandemicoutbreak) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizBeforethepandemicoutbreak$value)\r\n\r\nparamVizBeforethepandemicoutbreak <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_pandemicBeforethepandemicoutbreak parameter \r\nggplot2::ggplot(paramVizBeforethepandemicoutbreak, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\",\r\n    y = \"Density\",\r\n    x = \"pandemicBeforethepandemicoutbreak\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(posterior)\r\n\r\n# extracting posterior samples\r\nsamplesBeforethepandemicoutbreak <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being higher than 0\r\nsum(samplesBeforethepandemicoutbreak$b_pandemicBeforethepandemicoutbreak > 0) / nrow(samplesBeforethepandemicoutbreak)\r\n\r\n[1] 0.9902778\r\n\r\nNow let’s check the second key parameter of our model, the time after the pandemic term. In this case, the posterior distribution is again on the right-hand side of zero value, but now this result is consistent with the claim that there is a positive effect of the pandemic on people’s interest in people analytics, specifically in terms of the change in slope after the pandemic. Compared to the pre-pandemic trend, the post-pandemic trend is steeper by ~0 to ~1 point per month (as indicated by the 95% confidence interval). We should bear in mind, however, that this effect may in fact only be an artifact caused by the improved data collection system from January 2022, as mentioned at the very beginning of this post.\r\n\r\n\r\nShow code\r\n\r\nparamVizElapsedTimeAfterPandemic <- model %>%\r\n  tidybayes::gather_draws(b_elapsedTimeAfterPandemic) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizElapsedTimeAfterPandemic$value)\r\n\r\nparamVizElapsedTimeAfterPandemic <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_elapsedTimeAfterPandemic parameter \r\nggplot2::ggplot(paramVizElapsedTimeAfterPandemic, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\",\r\n    y = \"Density\",\r\n    x = \"elapsedTimeAfterPandemic\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamplesElapsedTimeAfterPandemic <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_elapsedTimeAfterPandemic coefficient being higher than 0\r\nsum(samplesElapsedTimeAfterPandemic$b_elapsedTimeAfterPandemic > 0) / nrow(samplesElapsedTimeAfterPandemic)\r\n\r\n[1] 0.9897361\r\n\r\nThe overall resulting picture thus partially supports the impression of many of my people analytics fellows about the growing importance of people analytics in HR and business leaders’ decision making. However, given that the Google search interest in people analytics is a fairly distant proxy for its actual use in HR and business practice, we should take these results with a grain of salt and try to find other data sources that would support our results. For example, Frank Corrigan, head of analytics at Ponder, came up with the idea of analyzing changes in postings for people analytics job positions over time. A good inspiration for anyone willing to spend some time getting at and analyzing such data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-popularity-after-covid/./people-analytics.jpg",
    "last_modified": "2023-07-03T21:41:53+02:00",
    "input_file": "people-analytics-popularity-after-covid.knit.md"
  },
  {
    "path": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/",
    "title": "People Analytics Challenge from Orgnostic: Plan for high growth",
    "description": "A brief summary of my participation in Orgnostic's People Analytics Challenge.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-19",
    "categories": [
      "recruitment channels",
      "workforce planning",
      "descriptive statistics",
      "r"
    ],
    "contents": "\r\nDuring the last few nights, I had the opportunity to participate in\r\nan interesting People Analytics Challenge from Orgnostic, a company providing people\r\nanalytics platform that links scattered HR and finance data, run surveys\r\non top, analyses the data, and get answers to the critical questions\r\nabout organizations and their employees.\r\nOut of three possible challenges, I chose one that was quite far from\r\nwhat I’m currently usually working on or what I’ve worked on in the\r\npast. The chosen task was to analyze the effectiveness of\r\nrecruiting sources for a company that plans to double in size\r\nfrom its current 750+ employees.\r\nAlthough the dummy data provided was quite limited for obvious\r\nreasons and did not allow to answer all relevant questions (but you\r\ncould also use your own data which would not suffer from this\r\nshortcoming), after combining them and enriching them slightly based on\r\nrealistic assumptions, it was possible to arrive at quite interesting\r\ninsights and recommendations. See for yourself - the resulting\r\npresentation is attached to this post below.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it:\r\nDownload\r\nPDF.\r\n\r\n\r\n\r\nIf you’d like to look more into the guts of the analyses conducted,\r\nyou can find both the data provided by Orgnostic and the R script used\r\nto analyze it on my\r\nGitHub page.\r\nP.S. Oh, I almost forgot - there are exciting prizes in the form of\r\ntickets to HRtechX Copenhagen or HR Technology Conference in Las Vegas.\r\nSo please keep your fingers crossed for me 🤞😉\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/./recruitmentChannels.png",
    "last_modified": "2022-08-29T18:41:35+02:00",
    "input_file": {},
    "preview_width": 864,
    "preview_height": 515
  },
  {
    "path": "posts/2022-06-11-visual-inference-statistics/",
    "title": "Visual statistical inference",
    "description": "Visual statistical inference represents a valid alternative to standard statistical inference, and as a by-product it also helps with building intuition about the difference between signal and noise. Give it a try.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-06-13",
    "categories": [
      "statistical inference",
      "visual statistical inference",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nDo you have any experience with visual\r\nstatistical inference? Not only is it a valid alternative\r\nto standard statistical inference, but as a by-product it helps with\r\nbuilding intuition about the\r\ndifference between signal and noise.\r\nThe basis of the method is a so-called lineup\r\nprotocol that places a graph of the actual data between arrays\r\nof graphs of null data that are generated by a method consistent with\r\nthe null hypothesis. The lineup is shown to one or more observers who\r\nare asked to identify the graph that differs. If an observer can pick\r\nout the actual data as different from the others, this gives weight to\r\nthe statistical significance of the pattern in the graph.\r\nBecause people usually have a hard time recognizing randomness and\r\ntend to see patterns even where there are none, there is also a\r\nso-called Rorschach protocol that only displays graphs\r\ncreated with null datasets and which is used by observers to calibrate\r\ntheir eyes for variation due to sampling.\r\nYou can try it for yourself in the graphs below, which show the\r\nrelationship between two variables from my current area of work\r\n(collaboration\r\nanalytics), namely between time\r\navailable for focused work and the use of timeboxing\r\n(productivity enhancing technique of assigning a fixed unit of time to\r\nan activity within which a planned activity takes place).\r\nUse the first array of graphs (the Rorschach protocol) to calibrate\r\nyour eye for randomness and then try to identify the actual data in the\r\nsecond array of graphs (the lineup protocol). What’s your guess? Which\r\ngraph matches the actual data (1-20) and what relationship do you see in\r\nthe data? You can give your guess in the comments.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(nullabor)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./visualInferenceData.RDS\")\r\n\r\n# lineup protocol\r\nset.seed(1234)\r\nd <- lineup(null_permute(\"propBlockedTime\"), mydata)\r\n\r\nlplot <- ggplot(data=d, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"LINEUP PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n\r\n# Rorschach protocol\r\ndr <- rorschach(null_permute(\"propBlockedTime\"), mydata, n = 20, p = 0)\r\n\r\nrplot <- ggplot(data=dr, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"RORSCHACH PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n# combining plots\r\nfplot <- rplot / lplot\r\n\r\nprint(fplot)\r\n\r\n\r\n\r\n\r\nHere’s a check on your guess. The actual data are shown in chart #\r\n12. As you can see in the chart below, the relationship between time for\r\nfocused work and the use of timeboxing is slightly negative, which makes\r\npretty good sense, because people who have enough time for focused work\r\nusually don’t have such a strong need to block out time for focused work\r\nin their calendar.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot(aes(x = focusRate, y = propBlockedTime)) +\r\n  geom_point(size = 2, alpha = 0.5) +\r\n  geom_smooth(method = \"lm\", se = F) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format()) +\r\n  labs(\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\",\r\n    title = \"RELATIONSHIP BETWEEN FOCUSED TIME AND TIMEBOXING USAGE\",\r\n    caption = \"\\nThe blue line in the graph represents the linear regression line.\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can also test the relationship between time for focused work and\r\nthe use of timeboxing more formally by fitting a Bayesian\r\nbeta regression model to the data. As you can see in the summary\r\ntables and charts below, the null value is safely outside the 95%\r\ncredible interval of the mean of the focus rate parameter, and the\r\nmarginal effect of focus rate clearly shows its negative relationship\r\nwith the predicted proportion of working time blocked in the calendar.\r\nNote also that the relationship is non-linear, i.e. the marginal effect\r\nof the focus rate is different depending on its level.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(bayesplot)\r\nlibrary(tidybayes) \r\nlibrary(ggdist)       \r\n\r\n# fitting Bayesian beta regression model\r\nmodel <- brms::brm(\r\n  bf(\r\n    propBlockedTime ~ focusRate,\r\n    phi ~ focusRate\r\n    ),\r\n  data=mydata,\r\n  family= Beta(),\r\n  seed = 1234,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  chains = 4,\r\n  cores = 6,\r\n  control = list(\r\n    adapt_delta = 0.9,\r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model\r\nsummary(model)\r\n\r\n\r\n Family: beta \r\n  Links: mu = logit; phi = log \r\nFormula: propBlockedTime ~ focusRate \r\n         phi ~ focusRate\r\n   Data: mydata (Number of observations: 306) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nPopulation-Level Effects: \r\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept        -0.85      0.25    -1.33    -0.35 1.00    61653\r\nphi_Intercept     2.09      0.39     1.30     2.84 1.00    59323\r\nfocusRate        -0.02      0.00    -0.03    -0.02 1.00    57209\r\nphi_focusRate     0.01      0.01    -0.01     0.02 1.00    56423\r\n              Tail_ESS\r\nIntercept        54084\r\nphi_Intercept    51459\r\nfocusRate        55167\r\nphi_focusRate    51573\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the estimated parameters\r\nposterior_beta <- model %>% \r\n  gather_draws(`b_.*`, regex = TRUE) %>% \r\n  mutate(component = ifelse(str_detect(.variable, \"phi_\"), \"Precision\", \"Mean\"),\r\n         intercept = str_detect(.variable, \"Intercept\")) %>%\r\n  filter(intercept == FALSE)\r\n\r\nggplot(posterior_beta, aes(x = .value, y = fct_rev(.variable), fill = component)) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  stat_halfeye(aes(slab_alpha = intercept), \r\n               .width = c(0.8, 0.95), point_interval = \"median_hdi\") +\r\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\r\n  scale_slab_alpha_discrete(range = c(1, 0.4)) +\r\n  guides(fill = \"none\", slab_alpha = \"none\") +\r\n  labs(\r\n    x = \"COEFFICIENT\", \r\n    y = \"\",\r\n    title = \"POSTERIOR DISTRIBUTION OF THE ESTIMATED PARAMETERS\",\r\n    caption = \"\\n80% and 95% credible intervals shown in black\") +\r\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing marginal effect of the focus rate\r\nmodel_pred <- model %>% \r\n  epred_draws(newdata = expand_grid(focusRate = seq(30, 100, by = 1)))\r\n\r\nggplot(model_pred , aes(x = focusRate, y = .epred)) +\r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Purples\") +\r\n  labs(x = \"FOCUS RATE\", \r\n       y = \"PREDICTED PROPORTION OF BLOCKED WORKING TIME\",\r\n       fill = \"Credible interval\",\r\n       title = \"MARGINAL EFFECT OF THE FOCUS RATE\"\r\n       ) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format(accuracy = 1)) +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nIf you would like to apply the visual statistical inference approach\r\nto your own data, you can easily do so using the nullabor R\r\npackage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-11-visual-inference-statistics/./lineup.jpg",
    "last_modified": "2022-06-19T21:18:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-06-standard-and-trend-predictors/",
    "title": "Standard vs. trend predictors",
    "description": "When modeling a phenomenon, one usually can't get by with just raw data but must use one's domain knowledge to select and transform the most relevant variables from raw data to be able to successfully grasp regularities in the domain of one's interest. Let's look at one simple example of such feature engineering from the domain of collaboration analytics.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-02-06",
    "categories": [
      "predictive analytics",
      "feature engineering",
      "employee experience",
      "employee engagement",
      "employee satisfaction",
      "employee attrition",
      "collaboration data"
    ],
    "contents": "\r\nAs predictive analytics practitioners know, trend variables can be more useful in many situations for predicting certain phenomena than standard variables that simply refer to the state of the world at a particular time point or period.\r\nFor example, when trying to predict employee attrition, a downward trend in the use of a piece of company equipment, such as a printer/copier, over the 6 months prior to the resignation may be more predictive than the absolute number of pages printed/copied over the same period.\r\nThis is also true for our domain we focus on at Time is Ltd. where, among other things, we try to use collaboration data to infer some aspects of employee experience.\r\nTo illustrate, the attached chart shows the distribution of the typical daily amount of time people spend by collaboration for two groups of employees - one with above-average scores and the other with below-average scores on the employee satisfaction survey. As you can see, there is little difference between the two groups in terms of the average daily amount of time people spend by collaboration over the last six months (see the density plots), but there is a fairly clear difference in the trend of this metric over the same period, suggesting that less satisfied employees may be suffering from increasing collaboration overload (see the line charts with trend lines for individual employees and the estimated overall linear trend).\r\n\r\n\r\n\r\nDo you have a similar experience with or just a strong hunch about other metrics in your area of expertise? Let me know in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-06-standard-and-trend-predictors/./trendGraph.jpg",
    "last_modified": "2022-02-09T09:23:54+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-30-meeting-planning/",
    "title": "Fighting meeting overload",
    "description": "One of the most effective ways to fight meeting overload is to better plan meetings in terms of the time we spend in them. Let's look at how data can tell us how much room for improvement we have in this area.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-30",
    "categories": [
      "meeting planning",
      "meeting effectiveness"
    ],
    "contents": "\r\nOne of the recommended ways to save time in meetings is to plan them better in terms of the time we allocate for them. As in other activities, even here the well-known Parkinson’s rule applies that “work expands so as to fill the time available for its completion.” When this is combined with the automatic use of default meeting lengths, it leads to spending more time in meetings than is necessary.\r\nFor this reason, Steven Rogelberg suggests in his book The Surprising Science of Meetings that all meeting times should be reduced by 5-10 percent by default.\r\nTo assess whether you have room for improvement in this regard, it is useful to compare actual and planned meeting lengths. For illustration, the attached chart shows the distribution of the typical differences between actual and planned meeting lengths for each of our teams organizing online meetings over the course of a year. It clearly shows that a large proportion of teams are organizing meetings longer than necessary, by an average of 4 minutes. So in the case of our company Time is Ltd., there definitely seems to be room for implementing Steven Rogelberg’s suggestion.\r\n\r\n\r\nShow code\r\n\r\n# uploading package\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata <- readRDS(\"./tardiness.rds\")\r\n\r\n# preparing data for density plot\r\nmydata <- with(density(data %>% pull(tardiness)), data.frame(x, y)) %>%\r\n  mutate(col = ifelse(x >= 0, \"A\", \"B\"))\r\n\r\n# visualizing data\r\nmydata %>%\r\n  ggplot() +\r\n  geom_rug(data = filter(data, tardiness >=0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 1, position = \"identity\") +\r\n  geom_rug(data = filter(data, tardiness <0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 0.5, position = \"identity\") +\r\n  \r\n  geom_area(data = filter(mydata, col == 'A'), aes(x = x, y = y), fill = '#4d009d', alpha = 1) + \r\n  geom_area(data = filter(mydata, col == 'B'), aes(x = x, y = y),  fill = '#4d009d', alpha = 0.5) +\r\n  \r\n  geom_label(aes( x=-15.25, y=0.06, label=\" Shorter than planned \"), fill = \"#a67fce\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n  geom_label(aes( x=10.5, y=0.04, label=\" Longer than planned \"), fill = \"#4d009d\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n\r\n  labs(\r\n    x = \"TYPICAL DIFFERENCE BETWEEN ACTUAL AND PLANNED LENGTHS OF MEETINGS\",\r\n    y = \"DENSITY\",\r\n    title = \"Do our online meetings end on time?\",\r\n    subtitle = str_glue(\"On average, our teams organize online meetings {round(abs(mean(data$tardiness)),1)} minutes longer than necessary.\"),\r\n    caption = \"\\nPositive values indicate that online meetings tend to overrun; negative values indicate that online meetings are planned longer than they need to be.\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::label_number(suffix = \" min\"), breaks =  seq(-40, 20, 10)) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.2,.98),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.size = unit(0, \"cm\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt is also worth noting the reverse situation where meetings take longer than planned, as a late end to one meeting becomes a late start to the next meeting.\r\nHow do you feel about finishing meetings too early or too late? Are both similarly unpleasant for you? And isn’t actually having a shorter meeting than planned something positive?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-30-meeting-planning/./meetingPlanning.jpg",
    "last_modified": "2022-01-30T18:27:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-18-probability-words/",
    "title": "How do we perceive probability words?",
    "description": "Have you ever wondered exactly how much chance of success people give a project when they say they believe in it? If so, then you may find this post useful, as it attempts to answer that question at least in part with data.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-18",
    "categories": [
      "probability",
      "perception"
    ],
    "contents": "\r\nNowadays - probably also due to the Covid pandemic and the associated predictions - we are more and more frequently encountering various probabilistic statements, but these are often expressed not in terms of precise numerical probabilities, but in terms of relatively vague probability words such as “probably”, “maybe”, “unlikely”, etc.\r\nSince people may imagine different probabilities under these words, it would be useful to have something like a glossary to help us decipher these words and indicate what people usually mean when they use them.\r\nFortunately, there are some studies that examine what numerical probabilities people typically associate with probability words.\r\nFor this purpose, I used a collection of 123 responses to the Wade Fagen-Ulmschneider’s internet survey and created two similar graphs based on them. The first shows the distribution of the numerical probabilities that people associate with each word, and these are sorted in the graph by the median value of the corresponding probability in descending order. The second graph then differs only in that the words are sorted by the size of the interquartile range in descending order.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggridges)\r\nlibrary(ggpubr)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./survey-results.csv\")\r\n\r\n# getting ordered list of words based on the median value of corresponding probabilities\r\nwordsMedian <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(median = median(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, median)\r\n      )\r\n  \r\nlevelsMedian <- levels(wordsMedian$word)\r\n\r\n# getting ordered list of words based on the IQR of corresponding probabilities\r\nwordsVariability <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(sd = IQR(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, sd)\r\n  )\r\n\r\nlevelsVariability <- levels(wordsVariability$word)\r\n\r\n# graph 1\r\ng1 <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  mutate(word = factor(word, levels = levelsMedian, ordered = TRUE)) %>%\r\n  ggplot(aes(x = probability, y = word)) + \r\n  geom_density_ridges(\r\n    fill = \"#4d009d\",\r\n    alpha = 0.85,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 1, height = 0),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n    quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n  labs(\r\n    fill = \"Trend size\",\r\n    x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n    y = \"\",\r\n    title = \"How do people perceive probability words?\",\r\n    caption = \"\\nThe words are sorted by the median value of the corresponding probability in descending order.\\nThe white dashed lines represent the median values.\\nSource: A collection of 123 responses to an internet survey by Wade Fagen-Ulmschneider.\"\r\n  ) +\r\n  scale_fill_gradient2(\r\n    low = \"red\",\r\n    mid = \"white\",\r\n    high = \"blue\",\r\n    midpoint = 0,\r\n    space = \"Lab\"\r\n  ) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.text.x = element_text(),\r\n        legend.position = \"right\",\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  ) +\r\n  guides(\r\n    fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n  )\r\n\r\n\r\n# graph 2\r\ng2 <- data %>%\r\n    select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n    pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n    mutate(word = factor(word, levels = levelsVariability, ordered = TRUE)) %>%\r\n    ggplot(aes(x = probability, y = word)) + \r\n    geom_density_ridges(\r\n      fill = \"#4d009d\",\r\n      alpha = 0.85,\r\n      scale = 1,\r\n      jittered_points = TRUE,\r\n      position = position_points_jitter(width = 1, height = 0),\r\n      point_shape = '|', point_size = 1, point_alpha = 1, \r\n      quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n      quantile_fun=function(x,...)median(x)\r\n    ) +\r\n    scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n    labs(\r\n      fill = \"Trend size\",\r\n      x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n      y = \"\",\r\n      title = \"What probability words are the most noisy?\",\r\n      caption = \"\\nThe words are sorted by the size of the interquartile range in descending order.\\n\\n\"\r\n    ) +\r\n    scale_fill_gradient2(\r\n      low = \"red\",\r\n      mid = \"white\",\r\n      high = \"blue\",\r\n      midpoint = 0,\r\n      space = \"Lab\"\r\n    ) +\r\n    theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n          plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n          plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n          axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n          axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n          legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n          legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n          axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n          axis.text.x = element_text(),\r\n          legend.position = \"right\",\r\n          axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n          axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n          panel.background = element_blank(),\r\n          panel.grid.major.y = element_blank(),\r\n          panel.grid.major.x = element_blank(),\r\n          panel.grid.minor = element_blank(),\r\n          axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n          axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n          plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n          plot.title.position = \"plot\",\r\n          plot.caption.position =  \"plot\"\r\n    ) +\r\n    guides(\r\n      fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n    )\r\n\r\n# combining graphs\r\nggarrange(g1, g2, ncol = 2, nrow = 1)\r\n\r\n\r\n\r\n\r\nThe first graph can thus help us to use the right word, which in the mind of the other person is most likely to evoke the same probability we want to express. The second graph can then help us to identify the most noisy probability words, for which we will know to ask for a more precise definition because we will be aware that people may imagine very different probabilities under these words.\r\nHow about your perception of probability words? Is there anything in the graphs that surprised you? Would you expect differences between cultures? And what about other demographics? Btw, the original dataset also includes some demographic variables such as age, gender, and education level, so I’ll probably come back to this question in a future post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-18-probability-words/./Probability-Word-Cards.jpg",
    "last_modified": "2022-01-30T17:55:43+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/",
    "title": "Hofstede's theory of cultural dimensions",
    "description": "Cultural diversity brings both positive effects and some challenges. To deal with the latter, it is useful to have some kind of map to help people better navigate the cultural specificities of people from different societies. Hofstede's theory of cultural dimensions is useful for such a purpose. Let's check how dis/similar countries are on these cultural dimensions with a simple app that could help us better understand, manage and appreciate cultural differences a little better.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-03",
    "categories": [
      "cultural diversity",
      "cultural awareness",
      "international management",
      "crosscultural communication",
      "shiny app"
    ],
    "contents": "\r\nOne of the advantages of switch to remote working is that companies can expand the pool of talent from which they choose their employees, without being too constrained by country or even continental boundaries. However, the resulting cultural diversity can bring not only positive effects (e.g. a broader set of perspectives, a more diverse skill base, local market knowledge and insight, better creativity and innovation, etc.) but also some challenges (e.g. risk of prejudice or negative cultural stereotypes, misinterpretation of communication, conflicting work styles, different understanding of professional etiquette, etc.).\r\nBetter knowledge and awareness of the cultural specificities of the societies from which people come is one way of dealing with these challenges. In this respect, Hofstede’s theory of cultural dimensions may be useful to us. Just as Big-5 theory facilitates our understanding of other people’s personalities, Hofstede’s theory facilitates our understanding of their cultural background by describing their social values and releated behaviors through the following six cultural dimensions:\r\n\r\nImage source: https://corporatefinanceinstitute.com/resources/knowledge/other/hofstedes-cultural-dimensions-theory/\r\nYou can easily check how countries are doing on these six dimensions on the Hofstede Insights website. To make it easier to compare cultural differences/similarities between countries, I built a simple app that projects the cultural profiles of countries into 2D space using dimensionality reduction technique called UMAP (Uniform manifold approximation and projection). By selecting a specific cultural dimension, you can see how it is distributed across countries and continents. In addition, you can select some specific countries in the comparator and compare them across all six cultural dimensions.\r\nCheck it out here ➡️ https://peopleanalyticsblog.shinyapps.io/Hofstede_Cultural_Dimensions/\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/./six-dimensions-hofstedes-cultural-dimensions-theory.jpg",
    "last_modified": "2023-04-11T20:08:25+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-19-makers-and-managers-schedule/",
    "title": "Makers' schedule and managers' schedule in collaboration data",
    "description": "Many of us have probably already heard of Paul Graham's two types of schedules - one that meets the needs of makers and one that meets the needs of managers. But can these two types of schedules be found in any real collaborative data? Let's find out.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-19",
    "categories": [
      "schedule types",
      "makers and managers",
      "collaboration data"
    ],
    "contents": "\r\nI am sure that many of you have heard of the two types of schedules\r\nas described by Paul Graham in his famous article Maker’s Schedule,\r\nManager’s Schedule:\r\nThe manager’s schedule [is] embodied in the traditional\r\nappointment book, with each day cut into one hour intervals. [By]\r\ndefault you change what you’re doing every hour. But [makers] generally\r\nprefer to use time in units of half a day at least. You can’t write or\r\nprogram well in units of an hour. That’s barely enough time to get\r\nstarted. When you’re operating on the maker’s schedule, meetings are a\r\ndisaster. A single meeting can blow a whole afternoon, by breaking it\r\ninto two pieces each too small to do anything hard in. Plus you have to\r\nremember to go to the meeting.\r\nI recently realized that I have only seen illustrative pictures on\r\nthis topic so far, but not any real data. This inspired me to look at\r\nour own collaboration data at Time\r\nIs Ltd. and see if these two schedule categories can be found\r\nthere.\r\nWhen I contrasted the data on the average number of meetings per day\r\nand the average time between meetings, there were indeed categories of\r\npeople who either have relatively more meetings with relatively shorter\r\nbreaks (managers), or have relatively fewer meetings with\r\nrelatively longer breaks (makers).\r\n\r\nBut beyond that, there was a third type, which I called\r\nbatchers - they have relatively fewer meetings with relatively\r\nshorter breaks, which is a good strategy when you have to be both\r\nmanager and creator, which may be the case for more and more people as\r\nwe move to remote working.\r\nIn the charts below you can see how typical monthly calendars of\r\nthese three types of schedulers look like.\r\n\r\n\r\nWhat we cannot see in our own data, but could theoretically be there,\r\nis a fourth category I call overtimers, who have relatively\r\nmore meetings but manage to keep relatively longer breaks in between.\r\nHowever, this can only be achieved by making the meetings more spread\r\nout over time, i.e. at the cost of working after hours.\r\nHow about you? Where would you fit in? And is there anyone among you\r\nwho would fit into the fourth, missing category?\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-19-makers-and-managers-schedule/./maker-schedule-vs-manager-schedule.jpg",
    "last_modified": "2022-06-19T22:21:49+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-16-linkedin-connections-analysis/",
    "title": "R Shiny app for LinkedIn connections analysis",
    "description": "An introduction of a simple R Shiny application for analysing LinkedIn connections.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-16",
    "categories": [
      "linkedin",
      "external networks",
      "social network analysis",
      "shiny app"
    ],
    "contents": "\r\nIf you like to use the end of the year as an opportunity for deeper self-reflection, you might enjoy the following simple app I have put together over the past weekend.\r\n➡️ https://peopleanalyticsblog.shinyapps.io/linkedIn_connections_analysis/\r\nOnce you upload your LinkedIn connections data to the app (you can easily download the data by following the instructions in the app or in this video), it automatically generates basic descriptive statistics about your LinkedIn connections:\r\nCumulative number of connections over time\r\nNumber of established connections by years, months, and days of the week\r\nTop N companies by the number of established connections\r\nTop N positions by their frequency among your connections (based on whole position titles, bigrams and single words)\r\nProportion of connections by their gender (based on your connections’ first name)\r\n\r\nUnfortunately, since there is no information about your connections’ connections in the data, the app cannot perform more advanced SNA-type of analyses on it. Still, I think you may find some of the statistics useful, or at least interesting and entertaining.\r\nYou can take it as a kind of Christmas gift for my fellow LinkedIn users. Enjoy exploring your connections! And if you’d like to explore and better manage also your company’s internal collaboration networks, then check out what we do at Time is Ltd.\r\nP.S. The data you upload is not permanently stored anywhere. The app runs on the shinyapps.io server. If you don’t want to upload your own data, but would still like to see what the analysis output looks like, you can download and then upload ready-made sample data from the app.\r\nP.P.S. Big thanks to Sebastian Vorac for bringing me to this idea and for UX review. Any remaining errors are, of course, mine alone.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-16-linkedin-connections-analysis/./linkedinLogo.png",
    "last_modified": "2023-04-11T20:08:53+02:00",
    "input_file": {},
    "preview_width": 3753,
    "preview_height": 2352
  },
  {
    "path": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/",
    "title": "Overview of predictors of voluntary employee turnover",
    "description": "An introduction of a simple R Shiny application to facilitate extraction and digestion of information from meta-analysis of predictors of voluntary employee turnover.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-12",
    "categories": [
      "great resignation",
      "employee turnover",
      "turnover predictors",
      "meta-analysis",
      "shiny app"
    ],
    "contents": "\r\nAlthough the ‘Great Resignation’ in some parts of the world may be due in no small part to factors specific to the COVID-19 pandemic, it is still useful in this context to draw on the extensive research on employee turnover carried out in the run-up to the pandemic.\r\nA useful overview of such findings is provided, for example, by a 2017 meta-analysis by Rubenstein et al. that summarizes the significance of 57 predictors of voluntary turnover from 9 different domains based on 316 studies from 1975 to 2016 involving more than 300,000 people.\r\nTo make it easier to assimilate these findings, I extracted them from the original article and visualized them in a simple shiny app that helps one to quickly explore and grasp the estimated magnitude, direction, and reliability of the effect of each factor, along with information on the degree of their actionability. The last feature is based purely on my own judgement, so please take it with a grain of salt, or adjust it in your mind using your own judgement. Try it out and let me know if you find it useful.\r\n➡️ https://peopleanalyticsblog.shinyapps.io/voluntary_turnover_predictors/\r\n\r\nAnd here is the original research paper on which the shiny app is based.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/./great_resignation.png",
    "last_modified": "2023-04-11T20:09:22+02:00",
    "input_file": {},
    "preview_width": 2700,
    "preview_height": 1800
  },
  {
    "path": "posts/2021-01-29-paygap/",
    "title": "Firemní audit rozdílu mezi platy mužů a žen",
    "description": "Platová nerovnost mezi muži a ženami není pro firmy jen záležitostí etickou a právní, ale také marketingovou - může mít totiž negativní dopad na jejich \"employer brand\" a atraktivitu coby zaměstnavatele. To znamená, že pokud firmy chtějí přilákat a také si udržet talentované zaměstnance, musí být schopny zajistit, že se u nich s muži a ženami bude v tomto ohledu zacházet stejně. Prvním krokem k tomu je zjistit, jak velký je rozdíl mezi platy mužů a žen ve firmě a do jaké míry ho lze vysvětlit jinými faktory než je samotné pohlaví zaměstnance. V tomto článku demonstruji, jak takovou analýzu provést s pomocí analytického nástroje R a dat, která má většina firem běžně k dispozici. Stručně se zmiňuji rovněž o tom, jaké mohou být případné další kroky a doporučení vyplývající z výsledků provedné analýzy.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-05-17",
    "categories": [
      "gender pay gap",
      "gender pay audit",
      "regression analysis",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nCo to je gender pay\r\ngap a jak ho měřit?\r\nProč se\r\nzabývat platovou nerovností ve Vaší firmě?\r\nAudit platové\r\nnerovnosti mezi muži a ženami\r\nPlán analýzy\r\nDostupná\r\ndata\r\nPříprava dat k analýze\r\nExplorační\r\nanalýza\r\nStatistický model platové\r\nnerovnosti\r\nVýsledky\r\nanalýzy\r\nMožné další\r\nkroky\r\n\r\nCo to je gender pay\r\ngap a jak ho měřit?\r\nGender pay gap (GPG), v překladu genderová příjmová\r\nnerovnost nebo příjmová propast mezi muži a ženami, označuje\r\ntypický rozdíl mezi platovým ohodnocením pracujících žen a\r\nmužů. Obvykle je GPG vyjadřována procenty, poměrem typické\r\nhrubé hodinové (či roční) mzdy ženy k typické mzdě muže nebo poměrem\r\nrozdílu mezi typickou mzdou mužů a žen vůči typické mzdě mužů.\r\nBez ohledu na způsob měření GPG, je dobře doloženým faktem, že ženy\r\njsou obecně hůře placeny než muži, jakkoli se tento rozdíl\r\npostupem času zmenšuje. Rozdíly v platech se přitom mohou v\r\njednotlivých zemích poměrně dost lišit. Názorně to ilustruje níže\r\nuvedený graf, který ukazuje vývoj (neadjustované) GPG (definované jako\r\npoměr rozdílu mediánové mzdy zaměstnaných mužů a žen a mediánové mzdy\r\nzaměstnaných mužů) v průběhu několika minulých let v zemích OECD.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ngpgoecd <- readr::read_csv(\"./DP_LIVE_29012021212234147.csv\")\r\n\r\n# creating color palette\r\n# list of R color Brewer's palettes: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\r\nnbCols <- length(unique(gpgoecd$LOCATION))\r\nmyColors <- colorRampPalette(brewer.pal(8, \"Set1\"))(nbCols)\r\n\r\n# creating a graph\r\ng <- gpgoecd %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(LOCATION, Value), y = Value, fill = LOCATION,\r\n                      text = paste('Země: ', LOCATION,\r\n                                 '<\/br><\/br>GPG: ', round(Value))))+\r\n  ggplot2::geom_col() +\r\n  ggplot2::facet_wrap(~ TIME, nrow = 4) +\r\n  ggplot2::labs(x = \"\",\r\n                y = \"GPG\",\r\n                title = \"Genderová příjmová nerovnost v zemích OECD v letech 2016-2019\") +\r\n  ggthemes::theme_few() +\r\n  ggplot2::scale_fill_manual(values = myColors) +\r\n  ggplot2::theme(legend.position = \"\",\r\n                 legend.title = element_blank(),\r\n                 axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n\r\n\r\n# making the graph interactive\r\nplotly::ggplotly(\r\n  g, \r\n  width = 800,\r\n  height = 700,\r\n  tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nDůvodů pro nevyváženost příjmů žen a mužů pravděpodobně existuje\r\nvětší množství. Mezi nejčastěji uváděné důvody patří:\r\nDiskriminace na pracovišti. Stejné práce je\r\nodměňována rozdílně čistě na základě pohlaví pracovníka.\r\nGenderové stereotypy. Předsudky ohledně výkonnosti,\r\nschopností a vlastností žen mají za následek oslabení jejich pozic a\r\nvytváření tzv. „skleněného stropu“, tj. neviditelné bariéry, na kterou\r\nženy naráží při snaze o kariérní postup na lépe placené pozice.\r\nSegregace trhu. Odvětví, v nichž je tradičně\r\nzaměstnáváno více žen než mužů jako je zdravotnictví, školství nebo\r\nveřejná správa, jsou společností vnímána jako méně prestižní, a tedy i\r\nhůře odměňována.\r\nRodinný život. Ženy většinou nesou větší část\r\nzátěže spojené s rodinným životem (např. při odchodu na mateřskou\r\ndovolenou, při péči o nemocné děti či jiné členy domácnosti), což jim\r\nvýznamně stěžuje jejich snahu o kariérní růst.\r\nV situaci, kdy při reportování GPG nerozlišujeme mezi různými důvody\r\npro platovou nerovnost, hovoříme o tzv. neadjustované\r\nGPG. Pro potřeby firemního auditu platové nerovnosti je však\r\ndůležité zjistit rovněž tzv. adjustovanou GPG, která se\r\nsnaží vyjádřit míru platové nerovnosti, která je způsobena čistě\r\npohlavím zaměstnance. Zatímco adjustovaná GPG umožňuje firmě\r\nidentifikovat možnou diskriminaci na pracovišti, neadjustovaná GPG (při\r\nneprokázané adjustované GPG) může poukazovat na existenci problémů jako\r\njsou genderové stereotypy či nedostatečná podpora žen při snaze skloubit\r\nsvůj osobní a profesní život. Pro firmy je tak užitečné sledovat oba\r\nukazatele.\r\nProč se\r\nzabývat platovou nerovností ve Vaší firmě?\r\nI kdybychom odhlédli od etických či právních aspektů platové\r\nnerovnosti mezi muži a ženami, je ve velice pragmatickém zájmu každé\r\nfirmy, aby se tento druh nespravedlnosti v jejím systému odměňování\r\nnevyskytoval. V době sociálních sítí a platforem na hodnocení firem\r\njejich současnými i bývalými zaměstnanci (za všechny zmiňme např. Glassdoor nebo\r\nčeský Atmoskop) se totiž\r\ninformace o nerovném přístupu může velice snadno rozšířit mezi\r\npotenciální i stávající zaměstnance, kteří ji mohou zohlednit při svém\r\nrozhodování, zda se v dané firmě ucházet o práci, resp. zda v ní i\r\nnadále zůstat.\r\nTuto skutečnost dokládají např. výsledky průzkumu\r\nprovedeného společností Glassdoor, podle kterého cca 67 % (U.S.)\r\nzaměstnanců by se neucházelo o práci tam, kde by si myslelo, že muži a\r\nženy mají nerovné platové podmínky.\r\nAudit platové\r\nnerovnosti mezi muži a ženami\r\nStejně jako při řešení jakéhokoli jiného problému, i v tomto případě\r\nplatí, že v první řadě je především potřeba ověřit, že nějaký\r\nproblém k řešení vůbec existuje. K tomu poslouží\r\nfiremní audit platové nerovnosti mezi muži a ženami.\r\nTen prostřednictvím analýzy platových, demografických a organizačních\r\ndat ověří, zda máme nějaké doklady pro to, že v dané společnosti\r\nexistují platové rozdíly mezi zaměstnanci spojené s jejich pohlavím.\r\nTeprve na základě výsledků takové analýzy je možné se začít poohlížet po\r\nmožných opatřeních v oblastech náboru, odměňování a/nebo povyšování,\r\nkterá by mohla pomoct nespravedlivé platové nerovnosti odstranit nebo\r\nalespoň zmírnit.\r\nNíže uvedený příklad takového auditu vychází z článku How\r\nto Analyze Your Gender Pay Gap: An Employer’s Guide od Andrew\r\nChamberlaina, Ph.D., hlavního ekonoma a vedoucího výzkumu ve\r\nspolečnosti Glassdoor.\r\nPlán analýzy\r\nAnalýzu platové nerovnosti mezi muži a ženami provedeme v\r\nnásledujících několika krocích:\r\nNačteme si data, která obsahují informace o platech vzorku\r\nzaměstnanců, jejich pohlaví, demografických a organizačních\r\ncharakteristikách, na kterých budeme testovat naše hypotézy. Za tímto\r\núčelem použijeme ilustrační data\r\nposkytnutá společností Glassdoor.\r\nV případě potřeby si upravíme data tak, aby lépe vyhovovala potřebám\r\nnaší analýzy.\r\nProvedeme explorační analýzu, která nám poskytne základní představu\r\no našich datech.\r\nSpočítáme si neadjustovanou GPG.\r\nS pomocí hierarchické regresní analýzy vytvoříme statistický model\r\nGPG, který nám umožní lépe rozlišit “vliv” různých faktorů, včetně\r\njejich interakcí, na pozorované rozdíly v platech mužů a žen.\r\nOvěříme, zda samotné pohlaví zaměstance - při zohlednění “vlivu”\r\nostatních faktorů, ke kterým máme k dispozici nějaká data - hraje\r\nnějakou významnější roli ve výši platu, který zaměstnanec dostává.\r\nOvěříme, zda pohlaví zaměstnance neinteraguje s některými dalšími\r\nfaktory při predikci výše jejich mzdy.\r\nDostupná data\r\n\r\n\r\nShow code\r\n\r\ndata <- readr::read_csv(\"./GenderPay_Data.csv\")\r\n\r\n\r\n\r\nK dispozici máme následující data ke vzorku 1000 zaměstnanců:\r\nTyp pozice, na které zaměstnanec pracuje (jobTitle)\r\nPohlaví zaměstnance (gender)\r\nVěk zaměstnance (age)\r\nHodnocení pracovního výkonu zaměstnance (perfEval)\r\nÚroveň vzdělání zaměstnance (edu)\r\nOddělení, ve kterém zaměstnanec pracuje (dpt)\r\nMíra seniority zaměstnance (seniority)\r\nZákladní mzda zaměstnance (basePay)\r\nBonusová složka platu zaměstnance (bonus)\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames = FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nPříprava dat k analýze\r\nZe zběžné kontroly povahy našich dat je patrné, že ne každá z\r\nproměnných je v našem datasetu reprezentována pomocí adekvátního\r\ndatového typu. Před samotnou analýzou si tedy budeme muset naše data\r\nještě trochu upravit.\r\n\r\n\r\nShow code\r\n\r\ndplyr::glimpse(data)\r\n\r\n\r\nRows: 1,000\r\nColumns: 9\r\n$ jobTitle  <chr> \"Graphic Designer\", \"Software Engineer\", \"Warehous~\r\n$ gender    <chr> \"Female\", \"Male\", \"Female\", \"Male\", \"Male\", \"Femal~\r\n$ age       <dbl> 18, 21, 19, 20, 26, 20, 20, 18, 33, 35, 24, 18, 19~\r\n$ perfEval  <dbl> 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,~\r\n$ edu       <chr> \"College\", \"College\", \"PhD\", \"Masters\", \"Masters\",~\r\n$ dept      <chr> \"Operations\", \"Management\", \"Administration\", \"Sal~\r\n$ seniority <dbl> 2, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 3, 3, 5, 4, 3, 5,~\r\n$ basePay   <dbl> 42363, 108476, 90208, 108080, 99464, 70890, 67585,~\r\n$ bonus     <dbl> 9938, 11128, 9268, 10154, 9319, 10126, 10541, 1024~\r\n\r\nKonkrétně budeme chtít upravit všechny textové proměnné (pracovní\r\npozice, pohlaví, úroveň vzdělání a pracovní oddělení) a dvě numerické\r\nproměnné (hodnocení pracovního výkonu a míru seniority) na faktorové\r\nproměnné. Ke třem z těchto nově vytvořených faktorových proměnných\r\n(úroveň vzdělání, hodnocení pracovního výkonu a míra senirotity) je\r\npotom potřeba přidat informaci o správném pořadí jejich jednotlivých\r\nkategorií, protože reprezentují ordinální proměnné, u kterých lze\r\nsmysluplně hovořit o relativním pořadí kategorií ve smyslu vyšší/nižší,\r\nresp. větší/menší. Takto upravená data již odpovídají typu informací,\r\nkteré reprezentují, a můžeme je tedy začít používat pro analýzu našeho\r\nproblému.\r\n\r\n\r\nShow code\r\n\r\nmydata <- data %>%\r\n  dplyr::mutate_if(is.character, as.factor) %>%\r\n  dplyr::mutate(edu = factor(edu, ordered = TRUE, levels = c(\"High School\", \"College\", \"Masters\", \"PhD\")),\r\n                perfEval = factor(as.character(perfEval), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")),\r\n                seniority = factor(as.character(seniority), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")))\r\n\r\n\r\n\r\nExplorační analýza\r\nV níže uvedených tabulkách jsou uvedeny základní popisné statistiky k\r\njednotlivým proměnným. Můžeme z nich vyčíst např. to, že našich 1000\r\nzaměstnanců je relativně rovnoměně rozdělených do jednotlivých kategorií\r\nz hlediska pracovní pozice, pohlaví, hodnocení pracovního výkonu, úrovně\r\nvzdělání, oddělení, ve kterém pracují, i míry jejich seniority. Dále se\r\nz nich můžeme dozvědět, že prostředních 50 % zaměstnanců je ve věku mezi\r\n29 a 54 lety, jejich roční základní mzda se pohybuje od 76 850 do 111\r\n558 USD a jejich bonusy za rok činí 4 849 až 8 026 USD.\r\n\r\n\r\nShow code\r\n\r\nskimr::skim(mydata)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nmydata\r\nNumber of rows\r\n1000\r\nNumber of columns\r\n9\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n6\r\nnumeric\r\n3\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\njobTitle\r\n0\r\n1\r\nFALSE\r\n10\r\nMar: 118, Sof: 109, Dat: 107, Fin:\r\n107\r\ngender\r\n0\r\n1\r\nFALSE\r\n2\r\nMal: 532, Fem: 468\r\nperfEval\r\n0\r\n1\r\nTRUE\r\n5\r\n5: 209, 4: 207, 1: 198, 3: 194\r\nedu\r\n0\r\n1\r\nTRUE\r\n4\r\nHig: 265, Mas: 256, Col: 241, PhD:\r\n238\r\ndept\r\n0\r\n1\r\nFALSE\r\n5\r\nOpe: 210, Sal: 207, Man: 198, Adm:\r\n193\r\nseniority\r\n0\r\n1\r\nTRUE\r\n5\r\n3: 219, 2: 209, 1: 195, 5: 193\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n41.39\r\n14.29\r\n18\r\n29.00\r\n41.0\r\n54.25\r\n65\r\n▇▇▆▆▇\r\nbasePay\r\n0\r\n1\r\n94472.65\r\n25337.49\r\n34208\r\n76850.25\r\n93327.5\r\n111558.00\r\n179726\r\n▂▇▇▃▁\r\nbonus\r\n0\r\n1\r\n6467.16\r\n2004.38\r\n1703\r\n4849.50\r\n6507.0\r\n8026.00\r\n11293\r\n▂▇▇▆▂\r\n\r\n Z hlediska námi analyzovaného problému jsou pro nás ale\r\ndůležitější vztahy mezi jednotlivými proměnnými, zejména mezi pohlavím a\r\nostatními proměnnými a jejich různými kombinacemi. Rychlý přehled o\r\nněkterých těchto vztazích nám může poskytnout níže uvedený graf, který\r\nzobrazuje souvislosti mezi jednotlivými dvojicemi proměnných a s pomocí\r\nbarevného kódování navíc nese informaci o tom, jak se tyto souvislosti\r\nliší mezi pohlavími. V grafu můžeme např. vidět, že se v případě\r\nněkterých pracovních pozic významně liší relativní zastoupení mužů a\r\nžen. V menší míře se zdá tento rozdíl platit i v případě úrovně\r\nvzdělání. Určitý rozdíl mezi muži a ženami se zdá existovat rovněž ve\r\nvýši jejich základní mzdy (narozdíl od bonusové složky, která se zdá být\r\nu mužů a žen obdobně vysoká).\r\n\r\n\r\nShow code\r\n\r\nGGally::ggpairs(mydata, aes(color = gender, alpha = 0.4)) +\r\n  ggplot2::theme(\r\n      strip.text.x = element_text(\r\n        size = 22),\r\n      strip.text.y = element_text(\r\n        size = 22)\r\n      ) +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\")\r\n\r\n\r\n\r\n\r\nVizuální dojem o rozdílné výši základní mzdy u mužů a žen potvrzuje i\r\ndetailnější analýza tohoto rozdílu. Ta ukazuje, že v našem vzorku\r\nmediánová mzda žen činí 89913.5 USD a mediánová mzda mužů 98223 USD. To\r\nodpovídá rozdílu 8309.5 USD, resp. neadjustované GPG (definované jako\r\npoměr rozdílu mediánové mzdy mužů a žen a mediánové mzdy mužů) 8.5 %.\r\nMíra platové nerovnosti se tak v námi sledované firmě zdá být spíše\r\nnižší, srovnatelná s celkovou hodnotou tohoto ukazatele v zemích jako je\r\nnapř. Švédsko nebo Nový Zéland (viz graf z úvodu tohoto článku).\r\nPokud bychom chtěli zohlednit míru naší nejistoty při odhadu\r\nvelikosti rozdílu mezi typickým platem mužů a žen, která je daná tím, že\r\npracujeme pouze se vzorkem zaměstnanců a nikoli s celou firmou, měli\r\nbychom sáhnout po inferenční statistice. Při použití bayesovského\r\nekvivalentu t-testu pro dva nezávislé výběry získáme takto informaci o\r\nposteriorní distribuci velikosti tohoto rozdílu. Na grafu níže můžeme\r\nvidět, že 95% interval kredibility se nachází v rozmezí od 5511 do 11615\r\nUSD, s mediánovou hodnotou 8392 USD. Z grafu také můžeme vyčíst, že\r\ndostupná data mluví silně v neprospěch nulové hypotézy o neexistenci\r\nrozdílu mezi průměrným platem mužů a žen - viz velmi nízká hodnota\r\nlogaritmu Bayesova\r\nfaktoru ve prospěch nulové hypotézu BF01.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nggstatsplot::ggbetweenstats(\r\n  data = mydata,\r\n  x = gender,\r\n  y = basePay,\r\n  type = \"bayes\",\r\n  title = \"Rozdíl v základní mzdě mezi muži a ženami\",\r\n  palette = \"Dark2\"\r\n) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::labs(x = \"\")\r\n\r\n\r\n\r\n\r\nSamotný fakt rozdílné výše základní mzdy u mužů a žen ale\r\nještě nemusí automaticky znamenat, že by se za ním skrývala diskriminace\r\nžen. Pozorovaný rozdíl může být totiž např. způsobený tím, že\r\nženy zaměstnané v námi sledované firmě mají typicky nižší vzdělání než\r\nve stejné firmě zaměstnaní muži. A vzhledem k tomu, že výše vzdělání (z\r\nhlediska “meritokratické spravedlnosti” zcela neproblematicky) pozitivně\r\nkoreluje s výší platu, projeví se tato souvislost v nižší typické mzdě\r\nžen (ponechme nyní stranou otázku, v jaké míře mají ženy obecně přístup\r\nk vyššímu vzdělání ve společnosti, kde daná firma působí). Tuto hypotézu\r\nse zdají podporovat i dva níže uvedené grafy, které vizualizují vztah\r\nmezi úrovní vzdělání zaměstnance a výší jeho základní mzdy, resp.\r\nsouvislost mezi pohlavím zaměstnance a úrovní jeho vzdělání.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, y = basePay)) +\r\n  PupillometryR::geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8, fill = \"#a9b2d1\") +\r\n  ggplot2::geom_point(aes(y = basePay), position = position_jitter(width = .15), size = .5, alpha = 0.8, color = \"#a9b2d1\") +\r\n  ggplot2::geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5, fill = \"#a9b2d1\") +\r\n  ggplot2::expand_limits(x = 5.25) +\r\n  ggplot2::guides(fill = FALSE) +\r\n  ggplot2::guides(color = FALSE) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      ),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::theme(panel.border = element_blank()) +\r\n  ggplot2::labs(title = \"Vztah mezi úrovní vzdělání a výší základní mzdy\",\r\n       x = \"\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, fill = gender)) +\r\n  ggplot2::geom_bar(position = \"fill\") +\r\n  ggplot2::scale_fill_hue() +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"Míra zastoupení můžů a žen v jednotlivých kategoriích úrovně vzdělání\",\r\n                x = \"\",\r\n                y = \"\",\r\n                fill = \"\") +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\") +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nPodobných kombinovaných souvislostí může v našich datech (a v\r\nrealitě, kterou reprezentují) existovat větší množství. Pokud by čtenář\r\nchtěl vztahy mezi různými kombinacemi proměnných prozkoumat sám a\r\ndetailněji, může za tímto účelem využít tuto\r\ninteraktivní aplikaci, kde jsou nahraná naše data a kde lze snadno\r\nrůzným způsobem vizualizovat zadané kombinace proměnných. Viz níže\r\nuvedená ukázka využití této aplikace při vizualizaci vztahu mezi výší\r\nplatu, pohlavím a pracovní pozicí, včetně počtu zaměstnanců v\r\njednotlivých kombinovaných kategoriích. Z tohoto konkrétního grafu je\r\ndobře patrné, že ženy jsou ve srovnání s muži disproporčně méně\r\nzastoupeny na dvou nadprůměrně odměňovaných pozicích Manager a\r\nSoftware Engineer a naopak disproporčně více jsou zastoupeny na\r\npodprůměrně platově ohodnocené pozici Marketing Associate.\r\n\r\n Důležitou kategorií vztahů mezi proměnnými, kterou bychom měli\r\nprozkoumat, pokud se chceme co nejblíže dostat k příčinám pozorovaných\r\nnerovností v platech mužů a žen a dobře zacílit případné intervence,\r\njsou tzv. interakce. Ty popisují situace, kdy vztah\r\nmezi dvěma proměnnými závisí na hodnotě nějaké třetí proměnné. Nás zde\r\nbude konkrétně zajímat interakce mezi naší hlavní nezávislou proměnnou\r\n(prediktorem), tj. pohlavím zaměstnance, a dalšími nezávislými\r\nproměnnými (např. věkem, úrovní vzdělání, hodnocením pracovního výkonu,\r\npracovní pozicí nebo oddělením) ve vztahu k naší závislé proměnné\r\n(kritériu), tedy základní mzdě.\r\nPříkladem vizualizace tohoto druhu vztahu mezi proměnnými je níže\r\nuvedený graf, ze kterého můžeme vyčíst, že ženy mají sice v průměru\r\nnižší základní mzdu než muži napříč celým věkovým spektrem (viz níže\r\npoložená regresní přímka pro skupinu žen), ale fakt, že zobrazené\r\nregresní přímky jsou rovnoběžné, svědčí pro to, že v rámci obou skupin\r\nplatí stejný typ vztahu mezi věkem a výší platu, a tedy že mezi pohlavím\r\na věkem ve vztahu k výši mzdy nedochází k žádné interakci. Pokud by se\r\nexistence takové interakce potvrdila i při zohlednění dalších\r\nrelevantních faktorů, mělo by to pro nás být podnětem k další exploraci\r\ntoho, co se pozorovaným rozdílem skrývá.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = age, y = basePay, fill = gender, colour = gender, group = gender)) +\r\n  ggplot2::geom_point(size = 1L, position = \"jitter\", alpha = 0.5) +\r\n  ggplot2::geom_smooth(span = 1L, method = \"lm\") +\r\n  ggplot2::scale_fill_brewer(palette = \"Dark2\") +\r\n  ggplot2::scale_color_brewer(palette = \"Dark2\") +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"Vztah mezi věkem zaměstnanců a výší jejich základní mzdy\",\r\n                fill = \"\",\r\n                color = \"\") +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nStatistický model platové\r\nnerovnosti\r\nAbychom dokázali izolovat vliv samotného pohlaví zaměstnanců na výši\r\nplatu a zohlednit přitom zároveň vliv všech ostatních relevantních\r\nfaktorů, včetně některých jejich interakcí, musíme sáhnout po\r\nkomplexnějším nástroji než je popisná statistika. A tímto nástrojem je\r\nstatistické modelování.\r\nStatistické modelování, podobně jako jakékoli jiné modelování ve\r\nvědě, ale i v běžném životě, není ničím jiným než snahou\r\nvytvořit menší a zjednodušený model našeho světa, který však\r\njeho chování odráží dostatečně věrně na to, abychom s jeho pomocí mohli\r\nčinit úsudky a předpovědi o skutečném světě a zakládat na něm svá\r\nrozhodnutí (k tomuto tématu viz srozumitelně napsaný\r\npopularizující článek Modeluji,\r\ntedy jsem od Josefa Šlerky).\r\nStatistické modelování se potom od jiných druhů modelování liší v tom,\r\nže se ve větší míře opírá o nástroje matematické statistiky a teorie\r\npravděpodobnosti.\r\nPřekvapivě mnoho jevů našeho světa se dá úspěšně modelovat a\r\npředpovídat pomocí relativně jednoduchých statistických modelů\r\nzobecněné lineární regrese (Generalized Linear\r\nModels, GLM). Ty předpokládají, že závislá proměnná, transformovaná\r\nprostřednictvím tzv. linkovací funkce (link\r\nfunction), je funkcí lineární kombinace nezávislých proměnných.\r\nNejznámější z této rodiny statistických modelů je klasický\r\nlineární model, který předpokládá normální rozdělení závislé\r\nproměnné, resp. reziduí (chyb) okolo predikované/ očekávané střední\r\nhodnoty závislé proměnné (viz ilustrativní obrázek níže).\r\n\r\nVzhledem k tomu, že námi modelovaná proměnná základní mzdy se zdá mít\r\nnormální, nebo téměř normální rozdělení (viz některé grafy v části\r\nvěnované explorační analýze), můžeme i my sáhnout po tomto statistickém\r\nmodelu. Jako nezávislé proměnné v našem modelu použijeme všechny nám\r\ndostupné prediktory, spolu s interakcemi mezi proměnnou pohlaví na\r\nstraně jedné a proměnnými úrovně vzdělání, seniority, věku a hodnocení\r\npracovního výkonu na straně druhé. Protože zaměstnanci tvoří přirozené\r\nshluky v rámci oddělení, napříč kterými se liší výše mzdy a také by se\r\nmohla lišit povaha vztahu mezi pohlavím zaměstnance a výší jeho mzdy,\r\npoužijeme hierarchickou/víceúrovňovou variantu modelu lineární\r\nregrese, která umožňuje, aby hodnoty vybraných parametrů modelu\r\nvariovaly v závilosti na příslušnosti zaměstnanců do konkrétního\r\noddělení.\r\nK odhadu hodnot parametrů našeho modelu použijeme inferenční\r\nrámec bayesovské statistiky, která ve srovnání s\r\nfrekventistickou statistikou nabízí bohatší a intuitivně snáze\r\nuchopitelné výstupy. Pro apriorní distribuci parametrů modelu použijeme\r\ndefaultní, široké a neinformativní hodnoty, takže výsledky analýzy budou\r\nnominálně podobné těm, které bychom získali při použití tradičnější\r\nfrekventistické inferenční statistiky.\r\n\r\n\r\nShow code\r\n\r\n# defining and running the model\r\n\r\nmodel <- brms::brm(\r\n  basePay | trunc(lb = 0) \r\n  ~ 1 \r\n  + jobTitle \r\n  + gender \r\n  + age \r\n  + perfEval \r\n  + edu \r\n  + seniority \r\n  + gender:edu \r\n  + gender:seniority \r\n  + gender:age \r\n  + gender:perfEval \r\n  + (1 + gender | dept),  \r\n  data = mydata %>% dplyr::mutate_if(is.factor, as.character),\r\n  family = gaussian(link = \"identity\"),\r\n  iter = 3000,\r\n  chains = 3,\r\n  cores = 6,\r\n  warmup = 1000,\r\n  seed = 2809,\r\n  control = list(\r\n    adapt_delta = 0.99, \r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\nVýsledky analýzy\r\nDříve než přistoupíme k interpretaci výsledků analýzy je dobré si\r\nověřit, že náš statistický model dokáže dostatečně věrně napodobit či\r\nsimulovat data reprezentující firemní realitu, na jejíž vlastnosti\r\nchceme s pomocí tohoto modelu usuzovat. Za tímto účelem můžeme použít\r\nnástroj posteriorní prediktivní kontroly (posterior predictive\r\ncheck), který ověřuje, jak moc dobře námi zvolený a odhadnutý model\r\npredikuje pozorovaná data na základě vzorku posteriorních hodnot jeho\r\nparametrů. Z níže uvedeného grafu je dobře patrné, že náš model si z\r\ntohoto hlediska nevede vůbec špatně.\r\nPo této kontrole (a také po ověření dalších technických\r\nnáležitostí, jako je např. konvergence MCMC\r\nřetězců, které umožňují odhadnout posteriorneí distribuci parametrů i\r\nkomplexnějších statistických modelů jako je ten náš) můžeme začít\r\nvyužívat parametry našeho modelu k usuzování na pravděpodobné vlastnosti\r\nnámi studované firemní reality.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model's fit\r\n\r\n# specifying the number of samples\r\nnsamples = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posteriorní prediktivní kontrola modelu za použití vzorku o velikoti n = {nsamples}\")\r\n    )\r\n\r\n\r\n\r\n\r\nNíže je uveden souhrn informací o našem odhadnutém modelu. Primárně\r\nnás zajímá hodnota parametru pohlaví (genderMale) v sekci\r\nvěnované efektům na úrovni celé populace (Population-Level\r\nEffects). 95% interval kredibility (Credible Interval),\r\nkterý udává kam v posteriorním rozdělení spadá hodnota nepozorovaného\r\nparametru s 95% pravděpodobností, se nachází v rozmezí od -3750.04 USD\r\ndo 9081.92 USD, se střední hodnotou 2717.57. Tzn., že podle našeho\r\nmodelu má muž - při zohlednění ostatních faktorů a jejich vybraných\r\ninterakcí - typicky o cca 2700 USD vyšší základní mzdu než její ženský\r\nprotějšek. Analýza našich dat tak do určité míry podporuje hypotézu o\r\nexistenci platové diskriminace na základě pohlaví zaměstnance v námi\r\nstudované firmě. Síla důkazu ve prospěch této hypotézy však není nijak\r\nvýrazná, což vyplývá z toho, že 95% interval kredibility zahrnuje vedle\r\nkladných hodnot i nulovou hodnotu a záporné hodnoty parametru pohlaví\r\njako jeho plauzibilní hodnoty.\r\n\r\n\r\nShow code\r\n\r\nsummary(model)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: basePay | trunc(lb = 0) ~ 1 + jobTitle + gender + age + perfEval + edu + seniority + gender:edu + gender:seniority + gender:age + gender:perfEval + (1 + gender | dept) \r\n   Data: mydata %>% dplyr::mutate_if(is.factor, as.characte (Number of observations: 1000) \r\n  Draws: 3 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 6000\r\n\r\nGroup-Level Effects: \r\n~dept (Number of levels: 5) \r\n                          Estimate Est.Error l-95% CI u-95% CI Rhat\r\nsd(Intercept)              3659.02   2392.77  1181.83  9771.07 1.00\r\nsd(genderMale)             1808.55   1724.07    78.57  6119.45 1.00\r\ncor(Intercept,genderMale)     0.30      0.53    -0.83     0.98 1.00\r\n                          Bulk_ESS Tail_ESS\r\nsd(Intercept)                 2653     2634\r\nsd(genderMale)                3466     3552\r\ncor(Intercept,genderMale)     6333     4308\r\n\r\nPopulation-Level Effects: \r\n                            Estimate Est.Error  l-95% CI  u-95% CI\r\nIntercept                   29216.96   3175.03  23007.14  35319.81\r\njobTitleDriver              -3633.29   1488.22  -6452.10   -760.74\r\njobTitleFinancialAnalyst     3749.02   1419.10   1008.10   6517.61\r\njobTitleGraphicDesigner     -2832.43   1454.75  -5687.49     37.72\r\njobTitleIT                  -1869.36   1438.48  -4666.71    969.94\r\njobTitleManager             31411.39   1495.39  28444.26  34352.28\r\njobTitleMarketingAssociate -16475.88   1390.53 -19181.98 -13758.33\r\njobTitleSalesAssociate        316.91   1428.70  -2488.94   3110.02\r\njobTitleSoftwareEngineer    13286.51   1416.67  10487.13  16055.90\r\njobTitleWarehouseAssociate  -1040.96   1491.96  -3955.62   1848.02\r\ngenderMale                   2717.57   3238.37  -3750.04   9081.92\r\nage                           995.31     33.84    927.62   1061.68\r\nperfEval2                     246.70   1444.36  -2609.56   3094.73\r\nperfEval3                   -1515.48   1463.94  -4327.32   1361.02\r\nperfEval4                     183.45   1438.20  -2567.99   3065.99\r\nperfEval5                    1433.01   1470.70  -1405.12   4405.58\r\neduHighSchool                -417.81   1302.59  -2969.86   2140.64\r\neduMasters                   4149.49   1321.31   1548.67   6728.00\r\neduPhD                       7627.28   1342.16   5025.37  10270.97\r\nseniority2                   8000.31   1527.46   4989.21  10992.21\r\nseniority3                  17954.08   1486.71  15132.57  20868.40\r\nseniority4                  30596.32   1613.05  27406.56  33680.11\r\nseniority5                  39640.70   1530.88  36626.89  42651.21\r\ngenderMale:eduHighSchool    -1926.51   1868.39  -5590.15   1674.20\r\ngenderMale:eduMasters         780.52   1829.37  -2755.04   4346.52\r\ngenderMale:eduPhD           -3154.41   1842.55  -6757.49    436.31\r\ngenderMale:seniority2         898.79   2074.27  -3179.87   4874.13\r\ngenderMale:seniority3        -354.32   2018.99  -4315.71   3647.04\r\ngenderMale:seniority4       -2847.26   2095.05  -7044.18   1157.86\r\ngenderMale:seniority5       -3538.92   2098.61  -7564.69    565.50\r\ngenderMale:age                 16.69     44.85    -71.88    104.73\r\ngenderMale:perfEval2         -603.35   2084.98  -4768.78   3466.38\r\ngenderMale:perfEval3         1601.12   2062.41  -2375.63   5640.84\r\ngenderMale:perfEval4         -471.22   2020.88  -4447.73   3466.87\r\ngenderMale:perfEval5        -2795.97   2013.44  -6768.31   1080.21\r\n                           Rhat Bulk_ESS Tail_ESS\r\nIntercept                  1.00     2311     3592\r\njobTitleDriver             1.00     3819     4183\r\njobTitleFinancialAnalyst   1.00     3294     4284\r\njobTitleGraphicDesigner    1.00     3372     3844\r\njobTitleIT                 1.00     4040     4704\r\njobTitleManager            1.00     3617     4213\r\njobTitleMarketingAssociate 1.00     3495     4556\r\njobTitleSalesAssociate     1.00     3885     4347\r\njobTitleSoftwareEngineer   1.00     3327     4760\r\njobTitleWarehouseAssociate 1.00     3601     4249\r\ngenderMale                 1.00     3031     4168\r\nage                        1.00     5997     4475\r\nperfEval2                  1.00     4400     3950\r\nperfEval3                  1.00     4457     4393\r\nperfEval4                  1.00     4276     4384\r\nperfEval5                  1.00     4239     4453\r\neduHighSchool              1.00     4654     4274\r\neduMasters                 1.00     4692     4883\r\neduPhD                     1.00     4686     4832\r\nseniority2                 1.00     3890     4236\r\nseniority3                 1.00     3995     4891\r\nseniority4                 1.00     4234     4872\r\nseniority5                 1.00     4092     4550\r\ngenderMale:eduHighSchool   1.00     4494     4650\r\ngenderMale:eduMasters      1.00     4323     4943\r\ngenderMale:eduPhD          1.00     4369     4810\r\ngenderMale:seniority2      1.00     3919     4684\r\ngenderMale:seniority3      1.00     4535     5116\r\ngenderMale:seniority4      1.00     4379     4976\r\ngenderMale:seniority5      1.00     4594     4807\r\ngenderMale:age             1.00     5968     4280\r\ngenderMale:perfEval2       1.00     4211     4414\r\ngenderMale:perfEval3       1.00     4106     4449\r\ngenderMale:perfEval4       1.00     3960     4438\r\ngenderMale:perfEval5       1.00     3865     4221\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma 10095.58    236.20  9649.25 10569.40 1.00    10053     4303\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nPokud bychom chtěli přesněji vyjadřit míru, s níž naše data v rámci\r\nnašeho modelu favorizují hodnoty parametru pohlaví větší než nula (tj.\r\nhodnoty, které jsou v souladu s hypotézou o existenci platové\r\ndiskriminace na základě pohlaví v neprospěch žen), můžeme se podívat na\r\nposteriorní distribuci tohoto parametru a jednoduše na něm spočítat, s\r\njakou pravděpodobností nabývá kladných hodnot.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the model's b_genderMale parameter \r\n\r\nparamViz <- model %>%\r\n  tidybayes::gather_draws(\r\n    b_genderMale\r\n    ) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramViz$value)\r\n\r\nparamViz <- tibble(x = dens$x, y = dens$y)\r\n\r\n\r\nggplot2::ggplot(\r\n  paramViz,\r\n  aes(x,y)\r\n    ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x > 0),\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x <= 0),\r\n    fill = \"grey\"\r\n  ) +\r\n  ggplot2::geom_line(\r\n  ) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-15000, 15000, 5000)) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(\r\n    title = \"Posteriorní distribuce parametru pohlaví zaměstnance\",\r\n    y = \"Density\",\r\n    x = \"genderMale\"\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(model)\r\n\r\n# probability of b_genderMale coefficient being higher\r\nprop <- sum(samples$b_genderMale > 0) / nrow(samples)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Bayesian hypothesis test\r\nthe_test <- brms::hypothesis(model, \"genderMale > 0\")\r\n\r\n\r\n\r\nPo provedení tohoto výpočtu nám vychází hodnota 80 %. To je v souladu\r\ns předchozím tvrzením, že důkaz ve prospěch testované hypotézy není\r\npříliš silný. Další možností by bylo použití tzv. Bayesova faktoru,\r\nkterý vyjadřuje míru s níž dostupná data favorizují testovanou hypotézu\r\nve srovnání s modelem odpovídajícím nulové hypotéze. Ten má pro naši\r\nhypotézu hodnotu 4, což odpovídá významnému, ale zdaleka nikoli silnému\r\nči rozhodnému důkazu ve prospěch naší hypotézy.\r\nVedle parametru pohlaví může být pro nás potenciálně užitečné podívat\r\nse také na vztah základní mzdy a ostatních prediktorů použitých v našem\r\nmodelu. Za tímto účelem můžeme použít vizualizaci marginálních efektů\r\njednotlivých prediktorů, které vyjadřují vztah mezi prediktorem a\r\nkritériem při zohlednění vlivu ostatních prediktorů. Takto např. můžeme\r\nna jednom z grafů vidět, že vztah mezi úrovní vzdělání a výší základního\r\nplatu se má tendenci u mužů a žen lišit. Na jiném grafu si můžeme zase\r\nvšimnout toho, že rozdíl mezi základní mzdou mužů a žen má tendenci\r\nnarůstat s tím, jak klesá seniorita zaměstnanců. Tyto a další podobné\r\nvhledy nám mohou pomoct přiblížit se k důvodům za pozorovanými\r\nnerovnostmi v platech mužů a žen.\r\n\r\n\r\nShow code\r\n\r\n# plotting marginal effects of predictors used \r\n# Note: Conditional vs. Marginal Relationships: The regression coefficients in generalized linear mixed models represent conditional effects in the sense that they express comparisons holding the cluster-specific random effects (and covariates) constant. For this reason, conditional effects are sometimes referred to as cluster-specific effects. In contrast, marginal effects can be obtained by averaging the conditional expectation μij over the random effects distribution. Marginal effects express comparisons of entire sub-population strata defined by covariate values and are sometimes referred to as population-averaged effects.In linear mixed models (identity link), the regression coefficents can be interpreted as either conditional or marginal effects. However, conditional and marginal effects differ for most other link functions.\r\n\r\nmarginalEffplots <- plot(\r\n  brms::marginal_effects(\r\n    model, \r\n    effects = c(\"jobTitle\", \"age\", \"perfEval\", \"edu\", \"seniority\", \"gender:edu\", \"gender:seniority\", \"gender:age\", \"gender:perfEval\"),\r\n    probs = c(0.025, 0.975)),\r\n  ask = FALSE\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# putting all graphs with marginal effects together  \r\nggpubr::ggarrange(\r\n  plotlist = marginalEffplots, \r\n  nrow = 9,\r\n  ncol = 1\r\n)\r\n\r\n\r\n\r\n\r\nMožné další kroky\r\nI v situaci, kdy analýza dat nepodpoří naše podezření na existenci\r\nplatové diskriminace na základě pohlaví zaměstnance, je stále možné, že\r\nza pozorovaným rozdílem v platech mužů a žen jsou jiné faktory, které s\r\npohlavím zaměstance nějak souvisí. Např. skutečnost, že jsou ženy méně\r\nreprezentované na lépe placených seniornějších pozicích, by mohla\r\nsvědčit o tom, že se ženy na pracovišti mohou potýkat s genderovými\r\nstereotypy a že při snaze o kariérní postup na lépe placené pozice\r\nnarážejí na tzv. “skleněný strop“. Pro učinění takového závěru je však\r\nzapotřebí získat další data, a to spíše kvalitativní povahy, taková,\r\nkterá sbírá a analyzuje např. organizační\r\nči firemní\r\nantropologie.\r\nV situaci, kdy máme dostatečně silné důkazy pro to, že se za\r\npozorovanou platovou nerovností mezi muži a ženami skrývají faktory\r\nsouvisející s pohlavím zaměstnance, je možné začít se poohlížet po\r\nmožných řešeních. Stejně jako při identifikaci problému, i při hledání\r\nzpůsobu jeho řešení je dobré držet se zásad na\r\ndůkazech založeného managementu a volit pouze řešení s dostatečně\r\nempiricky doloženou účinností, která zároveň dávají smysl ve specifickém\r\nkontextu dané firmy.\r\nUžitečný přehled možných akcí, které zaměstnavatelé mohou podniknout\r\ns cílem snížit GPG ve své organizaci, vytvořila známá skupina odborníků\r\nna behaviorální vědy v rámci tzv. The\r\nBehavioral Insights Team, která svého času vznikla pro to, aby\r\nbritské vládě pomáhala realizovat účinnou politiku založenou na\r\ndůkazech. V dokumentu s názvem Reducing the gender pay gap and\r\nimproving gender equality in organisations: Evidence-based actions for\r\nemployers tato skupina odborníků uvádí několik možných intervencí,\r\nkteré řadí do tří kategorií podle toho, jak dobře je jejich účinnost\r\npodložená empirickými důkazy.\r\nMezi akce s dobře doloženou účinností řadí\r\nnásledující intervence:\r\nZahrnutí většího počtu žen do užších seznamů v rámci výběru nových\r\nzaměstnanců a povyšování.\r\nPoužívání úloh posuzujících úroveň pracovních dovedností v rámci\r\nvýběru nových zaměstnanců.\r\nPoužívání strukturovaného interview v rámci výběru nových\r\nzaměstnanců a povyšování.\r\nPodpora vyjednávání o výši platu pomocí zvěřejnění existujícího\r\nplatového rozmezí.\r\nZavedení transparentních procesů povyšování a odměňování.\r\nJmenování manažera či zřízení pracovní skupiny pro firemní\r\ndiverzitu.\r\nMezi potenciálně slibné akce, které ale vyžadují další důkazy\r\no své účinnosti, řadí následující postupy:\r\nZvýšení pracovní flexibility pro muže a pro ženy.\r\nPodporu sdílené rodičovské dovolené.\r\nNábor bývalých zaměstnanců, kteří museli z různých osobních důvodů\r\nna delší dobu přerušit svou kariéru.\r\nNabídku mentoringu and sponsorshipu.\r\nNabídku networkingových programů.\r\nNastavení interních cílů.\r\nA mezi akce se smíšenými doklady o jejich účinnosti\r\npotom řadí následující opatření:\r\nŠkolení věnované tématu nevědomých předsudků.\r\nŠkolení v oblasti diverzity.\r\nŠkolení věnované rozvoji leadershipu.\r\nDemograficky různorodé výběrové panely v rámci externího i interního\r\nnáboru.\r\nZde je pro zájemce originální dokument k bližšímu prostudování.\r\n\r\n\r\nTento prohlížeč nepodporuje soubory PDF. Pro zobrazení si, prosím, PDF\r\nsoubor stáhněte: Stáhnout PDF.\r\n\r\n\r\n\r\nSkript k analýze je k dispozici ke stažení v podobě Jupyter Notebooku\r\nna mých GitHub\r\nstránkách.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-29-paygap/./PayGap.png",
    "last_modified": "2022-08-30T17:05:43+02:00",
    "input_file": {},
    "preview_width": 1438,
    "preview_height": 897
  },
  {
    "path": "posts/2020-12-31-segmentedregression/",
    "title": "Modeling impact of the COVID-19 pandemic on people’s interest in work-life balance and well-being",
    "description": "Illustration of Bayesian segmented regression analysis of interrupted time series data with a testing hypothesis about the impact of the COVID-19 pandemic on increase in people's search interest in work-life balance and well-being.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "well-being",
      "work-life balance",
      "covid pandemic",
      "segmented regression",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nSearch interest in work-life balance and well-being\r\nBayesian segmented regression\r\nSome necessary sanity checks\r\nResults of the analysis\r\n\r\nThe turn of the year, which is full of all sorts of resolutions to change for the better in our private lives and in our organizations, is a good time to remind ourselves that analytic tools can be very helpful in our efforts to make these resolutions come true. One way they can help us is by verifying that we have really achieved our stated goals and that we are not just fooling ourselves into believing so. We need to keep in mind Richard Feynman’s famous principle of critical thinking…\r\n\r\n\r\nOne of the tools that can help us with that is segmented regression analysis of interrupted time series data (thanks to Masatake Hirono for pointing me to its existence). It allows us to model changes in various processes and outcomes that follow interventions, while controlling for other types of changes (e.g. trends and seasonality) that may have occurred regardless of the interventions. It is thus very useful for data analysis conducted within studies with a quasi experimental study design that are often in the organizational context the best alternative to the “gold standard” of randomized controlled trials (RCTs) that are not always realizable or politically acceptable.\r\nSearch interest in work-life balance and well-being\r\nFor illustration, let’s use this tool for testing hypothesis about people’s increased interest in topics related to work-life balance and well-being due to the COVID-19 pandemic and subsequent changes in the way people work. As a proxy measure of this interest we will use worldwide search interest data over the last 10 years from Google Trends using search terms work-life balance and well-being (see Fig. 1 and 2 below).\r\nFig. 1: Interest in “work-life balance” topic over the last 10 years measured as a search interest by Google Trends. The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\r\n\r\nFig. 2: Interest in “well-being” topic over the last 10 years measured as a search interest by Google Trends. The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\r\n\r\nBased solely on the visual inspection of the graphs, it is pretty difficult to tell whether there was some effect of the COVID-19 pandemic or not, especially in the case of work-life balance (for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020). For sure it’s not a job for “inter-ocular trauma test” when the existence of the effect hits you directly between the eyes. We need to rely here on inferential statistics and its ability to help us with distinguishing signal from noise.\r\nBefore conducting the analysis itself, we need to wrangle the data from Google Trends a little bit using the recipe presented in the Wagner, Zhang, and Ross-Degnan’s paper. Specifically, we need the following five variables (or six, given that we have two dependent variables):\r\nsearch interest – numerical variable representing search interest relative to the highest point on the chart for the given region and time; this variable is truncated within the interval between values of 0 and 100; a value of 100 is the peak popularity for the term; a value of 50 means that the term is half as popular; a score of 0 means that there was not enough data for this term; this variable serves as a dependent (criterion) variable;\r\nelapsed time – numerical variable representing the number of months that elapsed from the beginning of the time series; this variable enables estimation of the size and direction of the overall trend in the data;\r\npandemic – dichotomic variable indicating the presence/absence of pandemic; as already mentioned above, for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020; this variable enables estimation of the level change in the interest in work-life balance and well-being immediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak – numerical variable representing the number of months that elapsed from the beginning of pandemic; this variable enables estimation of the change in the trend in the interest in work-life balance and well-being after the outbreak of pandemic;\r\nmonth – categorical variable representing specific month within a year; this variable enables controlling for the effect of seasonality.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for data manipulation\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndfWorkLifeBalance <- readr::read_csv(\"./workLifeBalanceGoogleTrendData.csv\")\r\ndfWellBeing <- readr::read_csv(\"./wellBeingGoogleTrendData.csv\")\r\n\r\ndfAll <- dfWorkLifeBalance %>%\r\n  # joining both datasets\r\n  dplyr::left_join(\r\n    dfWellBeing, by = \"Month\"\r\n    ) %>%\r\n  # changing the format and name of Month variable\r\n  dplyr::mutate(\r\n    Month = stringr::str_glue(\"{Month}-01\"),\r\n    Month = lubridate::ymd(Month)\r\n    ) %>%\r\n  dplyr::rename(\r\n    date = Month\r\n    ) %>%\r\n  # creating new variable month\r\n  dplyr::mutate(\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, \r\n                   levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   ordered = FALSE)\r\n    ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(\r\n    date\r\n    ) %>%\r\n  # creating new variables\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= \"2020-03-01\" ~ 1,\r\n      TRUE ~ 0\r\n      ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic)\r\n  ) %>%\r\n  dplyr::mutate(\r\n    pandemic = as.factor(case_when(\r\n        pandemic == 1 ~ \"After the pandemic outbreak\",\r\n        TRUE ~ \"Before the pandemic outbreak\"\r\n        ))\r\n  ) %>%\r\n  # changing order of variables in df\r\n  dplyr::select(\r\n    date, workLifeBalance, wellBeing, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n    )\r\n\r\n\r\n\r\nHere is a table with the resulting data we will use for testing our hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  dfAll,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nTable 1: Final dataset used for testing hypothesis about impact of the COVID-19 pandemic on people’s interest in work-life balance and well-being.\r\n\r\nBayesian segmented regression\r\nWe will model our data using common segmented regression models that have following general structure:\r\n\\[Y_{t} = β_{0} + β_{1}*time_{t} + β_{2}*intervention_{t} + β_{3}*time after intervention_{t} + e_{t}\\]\r\nThe β0 coefficient estimates the baseline level of the outcome variable at time zero; β1 coefficient estimates the change in the mean of the outcome variable that occurs with each unit of time before the intervention (i.e. the baseline trend); β2 coefficient estimates the level change in the mean of the outcome variable immediately after the intervention (i.e. from the end of the preceding segment); and β3 estimates the change in the trend in the mean of the outcome variable per unit of time after the intervention, compared with the trend before the intervention (thus, the sum of β1 and β3 equals to the post-intervention slope). For a better understanding of the model, take a look at the illustrative chart below.\r\n\r\nSince we are dealing with correlated and truncated data, we should also include two additional terms in our model, an autocorrelation term and a truncation term, to handle these specific properties of our data.\r\nNow let’s fit the models to the data and check what they tell us about the effect of pandemic on people’s search interest in work-life balance and well-being. We will use brms r package that enables making inferences about statistical models’ parameters within Bayesian inferential framework. Because of that, we also need to specify some additional parameters (e.g. chains, iter or warmup) of the Markov Chain Monte Carlo (MCMC) algorithm that will generate posterior samples of our models’ parameters.\r\nBayesian framework also enables us to specify priors for estimated parameter and through them include our domain knowledge in the analysis. The specified priors are important for both parameter estimation and hypothesis testing as they define our starting information state before we take into account our data. Here we will use rather wide, uninformative, and only mildly regularizing priors (it means that the results of the inference will be very close to the results of standard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\nbrms::get_prior(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors for predictors in both models \r\npriors <- c(set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model for work-life balance\r\nmodelWorkLifeBalance <- brms::brm(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 12345,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n# defining the statistical model for well-being\r\nmodelWellBeing <- brms::brm(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 678910,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n\r\n\r\nSome necessary sanity checks\r\nBefore making any inferences, we should make some sanity checks to be sure that the mechanics of the MCMC algorithm worked well and that we can use generated posterior samples for making inferences about our models’ parameters. There are many ways for doing that, but here we will use only visual check of the MCMC chains. We want plots of these chains look like hairy caterpillar which would indicate convergence of the underlying Markov chain to stationarity and convergence of Monte Carlo estimators to population quantities, respectively. As can be seen in Graph 1 and 2 below, in case of both models we can observe wanted characteristics of the MCMC chains described above. (For additional MCMC diagnostics procedures, see for example Bayesian Notes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains for the modelWorkLifeBalance \r\nbayesplot::mcmc_trace(\r\n  modelWorkLifeBalance,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWorkLifeBalance's parameters\"\r\n    )\r\n\r\n\r\n\r\nGraph 1: Trace plots of Markov chains for individual parameters of the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting the MCMC chains for the modelWellBeing \r\nbayesplot::mcmc_trace(\r\n  modelWellBeing,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWellBeing's parameters\"\r\n    )\r\n\r\n\r\n\r\nGraph 2: Trace plots of Markov chains for individual parameters of the modelWellBeing.\r\n\r\nIt is also important to check how well the models fit the data. We can use for this purpose posterior predictive checks that use specified number of sampled posterior values of models’ parameters and show how well the fitted models predict observed data. We can see in Graphs 3 and 4 that both models fit the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWorkLifeBalance fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWorkLifeBalance, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWorkLifeBalance (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\nGraph 3: Posterior predictive checks comparing simulated/replicated data under the fitted modelWorkLifeBalance with the observed data.\r\n\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWellBeing fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWellBeing, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWellBeing (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\nGraph 4: Posterior predictive checks comparing simulated/replicated data under the fitted modelWellBeing with the observed data.\r\n\r\nResults of the analysis\r\nNow, after having sufficient confidence that - using terminology from the Richard McElreath’s book Statistical Rethinking - our “small worlds” can pretty accurately mimic the data coming from our real,“big world”, we can use our models’ parameters to learn something about our research questions. Our primary interest is in the coefficient value of the pandemicBeforethepandemicoutbreak and elapsedTimeAfterPandemic terms in our models. It expresses how much and in what direction people’s search interest in work-life balance and well-being changed immediately after the outbreak of pandemic, and how slope of the trend changed after the pandemic, respectively.\r\nIn Graph 5 and 6 we can see posterior distribution of the pandemicBeforethepandemicoutbreak parameter in our two models. In both cases the posterior distribution of the pandemic term is (predominantly or completely) on the left side of the zero value, which supports the claim about existence of the effect of pandemic on people’s increased search interest in work-life balance and well-being immediately after the outbreak of pandemic. As is apparent from the graphs, for well-being (Graph 6) this evidence is much stronger than for work-life balance (Graph 5), which corresponds to impression we might have when looking at the original Google Trends charts shown in Fig. 1 and 2.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for \r\nlibrary(tidybayes)\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\nGraph 5: Visualization of the posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\nGraph 6: Visualization of the posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing.\r\n\r\nTo generate more summary statistics about posterior distributions (and also some diagnostic information like Rhat or ESS), we can use summary() function.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWorkLifeBalance \r\nsummary(modelWorkLifeBalance)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.21      0.09     0.03     0.40 1.00     7232     5932\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            69.59     11.51    47.51\r\nelapsedTime                          -0.05      0.04    -0.13\r\npandemicBeforethepandemicoutbreak   -13.90     10.15   -34.52\r\nelapsedTimeAfterPandemic              0.40      1.59    -2.77\r\nmonthFeb                              3.50      4.47    -5.29\r\nmonthMar                              4.49      4.99    -5.30\r\nmonthApr                              6.45      5.10    -3.44\r\nmonthMay                              8.81      5.10    -1.18\r\nmonthJun                             -2.91      5.05   -12.82\r\nmonthJul                             -7.40      5.02   -17.18\r\nmonthAug                             -2.51      5.01   -12.52\r\nmonthSep                             -2.20      5.01   -11.87\r\nmonthOct                              5.35      5.01    -4.68\r\nmonthNov                             12.31      4.91     2.56\r\nmonthDec                             -0.64      4.51    -9.37\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            92.59 1.00     4930     4892\r\nelapsedTime                           0.03 1.00     8111     5233\r\npandemicBeforethepandemicoutbreak     5.76 1.00     5569     5338\r\nelapsedTimeAfterPandemic              3.63 1.00     5567     5543\r\nmonthFeb                             12.29 1.00     3805     4101\r\nmonthMar                             14.36 1.00     3316     4648\r\nmonthApr                             16.44 1.00     3225     4565\r\nmonthMay                             18.82 1.00     3148     4520\r\nmonthJun                              7.14 1.00     3008     4287\r\nmonthJul                              2.42 1.00     3148     4652\r\nmonthAug                              7.42 1.00     3206     4379\r\nmonthSep                              7.58 1.00     3234     4922\r\nmonthOct                             14.99 1.00     3173     4504\r\nmonthNov                             22.21 1.00     2945     4709\r\nmonthDec                              8.24 1.00     3662     5175\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma    11.54      0.77    10.17    13.16 1.00     6603     5274\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWellBeing \r\nsummary(modelWellBeing)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.24      0.10     0.05     0.44 1.00     6461     5967\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            56.14      4.63    46.81\r\nelapsedTime                           0.11      0.02     0.08\r\npandemicBeforethepandemicoutbreak   -21.29      4.11   -29.26\r\nelapsedTimeAfterPandemic              0.50      0.64    -0.78\r\nmonthFeb                              8.61      1.80     5.09\r\nmonthMar                             10.63      2.02     6.70\r\nmonthApr                              9.57      2.02     5.66\r\nmonthMay                              3.52      2.07    -0.52\r\nmonthJun                             -4.35      2.05    -8.43\r\nmonthJul                             -8.05      2.06   -12.10\r\nmonthAug                             -5.76      2.05    -9.68\r\nmonthSep                              4.56      2.05     0.57\r\nmonthOct                              8.13      2.01     4.24\r\nmonthNov                              6.74      2.01     2.83\r\nmonthDec                             -4.95      1.80    -8.39\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            65.13 1.00     4316     5151\r\nelapsedTime                           0.14 1.00     8987     5597\r\npandemicBeforethepandemicoutbreak   -13.07 1.00     5825     5557\r\nelapsedTimeAfterPandemic              1.78 1.00     6106     5784\r\nmonthFeb                             12.19 1.00     3387     4626\r\nmonthMar                             14.59 1.00     2868     4601\r\nmonthApr                             13.59 1.00     2724     4233\r\nmonthMay                              7.60 1.00     2938     4027\r\nmonthJun                             -0.34 1.00     2815     4633\r\nmonthJul                             -4.01 1.00     2749     4036\r\nmonthAug                             -1.77 1.00     2863     4364\r\nmonthSep                              8.55 1.00     2969     3748\r\nmonthOct                             12.22 1.00     2907     3914\r\nmonthNov                             10.73 1.00     2962     4142\r\nmonthDec                             -1.35 1.00     3525     4630\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     4.63      0.31     4.07     5.28 1.00     7355     6173\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\nGiven that for work-life balance model the posterior distribution of pandemic term crosses the zero value, it would be useful to know how strong is the evidence in the favor of hypothesis that pandemic term is lower than zero. For that purpose we can extract posterior samples and use them for calculation of the proportion of values that are larger/smaller than zero. The resulting proportions show that the vast majority (around 92%) of posterior distribution lies below zero.\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(modelWorkLifeBalance, seed = 12345)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being lower than 0\r\nsum(samples$b_pandemicBeforethepandemicoutbreak < 0) / nrow(samples)\r\n\r\n[1] 0.9145\r\n\r\n\r\nNow let’s check the parameter elapsedTimeAfterPandemic. Its posterior distribution in both models “safely” includes zero value, which indicates that there is not huge support for positive change in trend after the outbreak of pandemic.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_elapsedTimeAfterPandemic\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_elapsedTimeAfterPandemic\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\nGraph 7: Visualization of the posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_elapsedTimeAfterPandemic\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_elapsedTimeAfterPandemic\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\nGraph 8: Visualization of the posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWellBeing.\r\n\r\nIn conclusion, we can say that there is some evidence that the COVID-19 pandemic has prompted people to be more interested in topics related to work-life balance and well-being. I wish us all to be able to transform our increased interest in these topics into truly increased quality of our personal and professional lives. It would be a shame not to use that extra incentive many of us have now for making significant change in our lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-31-segmentedregression/./wellBeingData.jpeg",
    "last_modified": "2023-07-03T22:33:50+02:00",
    "input_file": "segmentedregression.knit.md"
  },
  {
    "path": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/",
    "title": "HR analytika a odchodovost zaměstnanců",
    "description": "Které faktory přispívají k odchodovosti zaměstnanců a u kterých konkrétních zaměstnanců je zvýšené riziko, že firmu během několika příštích měsíců opustí? Na tyto otázky se čím dál tím více firem snaží odpovědět pomocí analýzy dat o svých vlastních zaměstnancích. V tomto článku se prostřednictvím analytického nástroje R a vizualizačního nástroje Shiny podíváme, jak může být tento druh HR analytického projektu pro firmy užitečný.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-14",
    "categories": [
      "employee turnover",
      "evidence-based hr",
      "shiny app"
    ],
    "contents": "\r\n\r\nContents\r\nCo je to HR analytika?\r\nOdchodovost zaměstnanců a její prediktory\r\nNa důkazech založená pro-retenční opatření\r\n\r\nCo je to HR analytika?\r\nHR analytika ve své podstatě vychází ze známé zásady managementu, že co nelze měřit, nelze ani (efektivně) řídit a zlepšovat, a aplikuje tuto zásadu na lidské zdroje. V několika posledních letech potom k tomu navíc přidává nadstavbu v podobě pokročilejších analytických postupů, které mají větší potenciál přijít s hlubšími vhledy a s doporučeními s větším efektem. Ale ať už využíváte pouze základní reporting nebo nějakou pokročilejší analytiku, cíl je vždy stejný – snažit se s pomocí dat a jejich analýzy žádoucím způsobem ovlivnit jednotlivé HR procesy, které organizacím pomáhají dosahovat jejich strategických cílů. Názorně to ilustruje níže uvedené schéma zachycující mechanismus propojující HR procesy s (nejen) finančními výsledky organizace (Paauwe & Richardson, 1997).\r\n\r\nHR analytika pomáhá optimalizovat nastavení tohoto mechanismu tím, že umožňuje nalézat odpovědi na některé klíčové otázky, jako např.:\r\nKterými kanály se k nám dostávají ti nejlepší kandidáti?\r\nJaké charakteristiky od sebe odlišují úspěšné a neúspěšné kandidáty?\r\nKteré faktory přispívají k úspěšnému onboardingu?\r\nKterá „kápéíčka“ mají nejsilnější vazbu na finanční výsledky firmy?\r\nJaké tréninky vedou s nejvyšší pravděpodobností ke zlepšení pracovního výkonu?\r\nKteré intervence mají největší dopad na zaměstnanci pociťovaný well-being nebo work-life balance?\r\nCo u zaměstnanců zvyšuje, nebo naopak snižuje míru jejich angažovanosti?\r\nKde se v organizaci nachází izolovaná sila a úzká hrdla znemožňující efektivní komunikaci a spolupráci mezi jednotlivými zaměstnanci, týmy nebo i celými odděleními?\r\nKdo představuje skrytý talent, který je potřeba podchytit a dále rozvíjet?\r\nKde lze očekávat odpor v souvislosti s plánovanými změnami ve firmě a kdo naopak může být jejich ambasadorem a katalyzátorem?\r\nKteré faktory přispívají k odchodovosti zaměstnanců a u kterých konkrétních zaměstnanců je zvýšené riziko, že firmu během několika příštích měsíců opustí?\r\nOdchodovost zaměstnanců a její prediktory\r\nPrávě posledně jmenovaný způsob využití HR analytiky často představuje jeden z prvních druhů HR analytických projektů, kterými se ve firmách s HR analytikou začíná, a to z dobře pochopitelného důvodu. S nežádoucími odchody zaměstnanců jsou totiž spojené vysoké přímé i nepřímé náklady, takže i poměrně mírné snížení odchodovosti zaměstnanců může představovat značnou úsporu, kterou ocení management každé firmy. Naléhavost tohoto problému navíc ještě zvyšuje současná fáze ekonomického cyklu s rekordně nízkou mírou nezaměstnanosti, která v kombinaci s různými on-line platformami na zprostředkování práce motivuje mnoho lidí k hledání nového místa, kde, jak doufají, bude práce zajímavější, smysluplnější a lépe placená a kde kolegové budou sympatičtější a šéfové inspirativnější. Viz také graf níže, který na datech z USA názorně dokládá těsnost vztahu mezi mírou nezaměstnanosti a mírou dobrovolné odchodovosti zaměstnanců (r = -0,95, p < 0,001 ).\r\n\r\n\r\n\r\nVzhledem k palčivosti tohoto problému, který trápí nejednu firmu, není žádným velkým překvapením, že se tématu odchodovosti zaměstnanců věnovalo a stále věnuje velké množství různých studií. Takto např. na konci roku 2017 vyšla rozsáhlá meta-analýza od autorů Rubensteina, Eberlyové a Leeho, kteří syntetizovali výsledky více než 300 dílčích výzkumů týkajících se prediktorů odchodovosti. Můžeme se tak oprávněně ptát, co nového nám může přinést HR analytika zaměřená na odchodovost zaměstnanců realizovaná pouze v jediné organizaci. Nebylo vše podstatné k tomuto tématu již objeveno? (K této otázce viz např. tento inspirativní a trochu provokativní článek od Thomase Rasmussena.) Je pravda, že není příliš pravděpodobné, že při analýze vašich vlastních dat narazíte na nějaký naprosto nový faktor související s odchodovostí. Na druhou stranu je rovněž pravda, že každá organizace je v něčem jedinečná, takže některé z retenčních faktorů pro danou organizaci budou pravděpodobně více a jiné méně důležité. Tato informace o relativní důležitosti jednotlivých retenčních faktorů je potom klíčová při nastavování retenčního plánu a HR analytika může být při tomto velice nápomocná.\r\nNa důkazech založená pro-retenční opatření\r\nS pomocí tohoto dashboardu - vytvořeného prostřednictvím analytického nástroje R a vizualizačního nástroje Shiny a za využití ukázkových dat od společnosti IBM - si můžete sami vyzkoušet, jak užitečné by pro Vás mohly být výstupy z takového HR analytického projektu zaměřeného na odchodovost zaměstnanců. Dashboard obsahuje informace, které pomáhají (nejen) managementu zodpovědět řadu klíčových otázek, které stojí na počátku každého účinného plánu na retenci zaměstnanců, jako např.:\r\nKolik zaměstnanců nás ročně opouští?\r\nKteré skupiny zaměstnanců odcházejí nejčastěji?\r\nJaký je externí benchmark? Jsme na tom podobě jako konkurence v oboru?\r\nPředstavuje pro nás stávající úroveň odchodovosti závažný problém, a vyplatí se nám ho tedy řešit?\r\nZ jakých důvodů lidé obecně nejčastěji odcházejí ze zaměstnání?\r\nJaké faktory přispívají k odchodu specificky našich zaměstnanců?\r\nJaká pro-retenční opatření jsou obecně k dispozici?\r\nJaká pro-retenční opatření bychom měli zvolit vzhledem k pravděpodobným důvodům odchodů našich zaměstnanců?\r\nNa jaké skupiny zaměstnanců se především zaměřit z hlediska prevence jejich odchodovosti?\r\nU kterých konkrétních zaměstnanců existuje zvýšené riziko, že odejdou, a na jaké konkrétní retenční faktory se u nich zaměřit v rámci pravidelného stay interview?\r\nJak je z výše uvedeného výčtu otázek patrné, dashboard obsahuje informace, které při svém rozhodování mohou využít nejen HR manažeři, ale také HR business partneři nebo přímo team-leadeři a linioví manažeři jednotlivých týmů či oddělení. Kromě toho dashboard obsahuje také řadu technických detailů o použitém predikčním modelu a samotná data, které stojí v pozadí všech prezentovaných vizualizací a analýz. S jejich pomocí tak HR/Business analytik může např. hledat optimální způsob, jak nastavit skórovací algoritmus, aby se maximalizoval pozitivní efekt pro-retenčních opatření, nebo může v dostupných datech sám hledat nějaké další užitečné informace. Více viz již samotný dashboard, z něhož můžete níže vidět několik screenshotů.\r\nScreenshot části dashboardu, která obsahuje různé řezy odchodovostí zaměstnanců, a dává tak dobrý přehled o tom, které skupiny zaměstnanců jsou odchodovostí nejvíce ohrožené.\r\nScreenshot části dashboardu, která obsahuje informace o pravděpodobnosti odchodu jednotlivých zaměstnanců společně s dalšími informacemi, které mohou posloužit jako podklad pro individuální intervence s cílem předejít nežádoucím odchodům zaměstnanců.\r\n\r\nScreenshot části dashboardu, která obsahuje informace o výkonu/kvalitě statistického modelu použitého k identifikaci významných prediktorů odchodovosti zaměstnanců a k odhadu pravděpodobnosti odchodu jednotlivých zaměstnanců.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/./Types-of-Employee-Attrition.png",
    "last_modified": "2023-04-11T20:14:01+02:00",
    "input_file": {},
    "preview_width": 860,
    "preview_height": 396
  },
  {
    "path": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/",
    "title": "Moneyball v HR",
    "description": "Přes popularitu tématu HR analytiky mezi HR profesionály je stále relativně málo společností, které HR analytiku reálně a systematicky využívají. Jednou z možných příčin je to, že tradiční HR mnohdy postrádá analytický mindset a některé z kompetencí, které jsou klíčové pro úspěšnou realizaci HR analytických projektů. V takové situaci může být užitečné podívat se ve větším detailu na celkovou logiku i na konkrétní analytické kroky nějakého úspěšného příkladu využití HR analytiky k optimalizaci některého z HR procesů s pozitivním dopadem na obchodní výsledky společnosti. V tomto článku se tímto způsobem podíváme na známý příběh oaklandského baseballového týmu \"Áček\", jehož management poměrně radikálně - a podle všeho i úspěšně - přehodnotil svůj dosavadní přístup k výběru nových hráčů na základě výstupů statistické analýzy sabermetrických dat o herním chování hráčů. Využijeme při tom volně dostupný statistický software R a veřejně dostupnou databázi historických údajů o výsledcích v americké baseballové lize.",
    "author": [
      {
        "name": "Luděk Stehlík",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "employee selection",
      "correlation analysis",
      "multivariate regression analysis",
      "structural equation modeling",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\n1.\r\nkrok: Začít od konce aneb strategický rámec HR analytiky\r\n2. krok: Definice\r\nproblému a kvantifikace cíle\r\n3. krok: Kladení otázek a\r\nměření\r\n4. krok: Kladení\r\ndalších otázek a další měření\r\n5.\r\nkrok: Propojení dílčích vhledů aneb organizace jako stroj\r\n6. krok:\r\nIntervence\r\nOmezení HR analytiky\r\nZávěr\r\n\r\nHR analytika už dnes není ve světě HR žádnou horkou novinkou. Téměř\r\nvšichni z oboru už o HR analytice něco slyšeli, něco o ní vědí a\r\npřípadně se jí už také pokouší ve svých organizacích v nějaké podobě\r\nzavádět. Zároveň většinou uznávají její důležitost při transformaci HR z\r\npodpůrné a administrativní funkce na funkci, která dokáže organizacím\r\nbezprostředně pomáhat dosahovat jejich strategických cílů. Navzdory\r\ntomuto všeobecnému povědomí o HR analytice a navzdory řadě úspěšně\r\nrealizovaných HR analytických projektů (viz např. série článků od Davida Greena - článek\r\n1, článek\r\n2, článek\r\n3, článek\r\n4) překvapivě málo organizací HR analytiku reálně a systematicky\r\nvyužívá. Tento stav reflektují i výsledky výzkumu 2018\r\nHuman Capital Trends od společnosti Deloitte, ze kterých vyplývá, že\r\norganizace si většinou uvědomují strategickou důležitost výzvy, kterou\r\npředstavuje datifikace HR, zároveň se ale necítí být na čelení této\r\nvýzvě příliš dobře připraveny. Už nějakou dobu platí, že když už se v\r\norganizaci s HR daty nějak pracuje, tak je to většinou pouze na úrovni\r\nnějakého základního reportingu vybraných HR metrik a KPIs typu náklady\r\nna nábor, délka období neobsazenosti volné pracovní pozice, míra\r\nne/dobrovolné odchodovosti zaměstnanců, počet zaměstnanců na jednoho HR\r\nbusiness partnera apod. Slabinou tohoto přístupu je, že takto sledované\r\nmetriky jsou často relevantní pouze pro monitorování a řízení\r\nefektivnosti HR coby nákladového střediska, ale již méně pro dosahování\r\nstrategických cílů organizace. Spíše výjimečně se potom v tomto kontextu\r\nvyužívají nějaké pokročilejší analytiky, které obecně mají větší\r\npotenciál přicházet s doporučeními s přímým dopadem na schopnost\r\norganizací dosahovat svých strategických cílů.\r\nVýsledky výzkumu\r\nprovedeného společnostmi MIT Sloan Management Review a SAS\r\nnaznačují, že tento nevyužitý potenciál HR analytiky má dvě hlavní\r\npříčiny. První z nich je to, že tradiční HR mnohdy postrádá analytický\r\nmindset a některé z kompetencí, které jsou klíčové pro úspěšnou\r\nrealizaci HR analytických projektů (přehled těchto kompetencí a důsledků\r\njejich absence či nedostatečné úrovně viz např. tento\r\nčlánek od Mortena Kamp\r\nAndersena). Ve stejném duchu Josh Bersin ve své zprávě\r\nHR\r\nTechnology Disruptions for 2018 konstatuje, že zvládnutí základních\r\nanalytických dovedností patří mezi nejdůležitější prediktory efektivní\r\nimplementace HR analytiky v organizacích: “Equip all HR staff with\r\nbasic data literacy skills. All HR practitioners should know basic\r\nstatistical concepts, where to find data, how to slice and dice it, how\r\nto read a dashboard, and how to bring data and analytics to bear on\r\nbusiness issues. Our research reveals that such basic skills are among\r\nthe most important predictors of high-performing people\r\nanalytics.”\r\nDruhou hlavní příčinou je potom to, že HR analytické projekty\r\nnebývají ukotveny v rámci nějaké širší strategie, jak data systematicky\r\nvyužívat při řízení lidských zdrojů, navíc způsobem, který by byl\r\nsladěný se strategickými cíli společnosti. Zde platí praxí osvědčená\r\npravda projektového managementu, že při implementaci projektů je potřeba\r\nvždy začínat od konce. V kontextu HR analytických projektů to tedy\r\nznamená začínat nikoli od dat, ale od toho, k čemu mají být HR\r\nanalytické výstupy použity. A očekávání managementu je, že HR analytika\r\nbude v posledku hlavně pomáhat zlepšovat obchodní výsledky společnosti.\r\nNázorně to ilustruje níže uvedené schéma (převzaté z článku\r\nMaxe Blumberga),\r\nkteré zachycuje předpokládaný kauzální řetězec spojující HR procesy s\r\nobchodními výsledky. Úkolem HR analytiky je potom s pomocí dat a\r\nanalytických nástrojů tyto dvě oblasti propojit a zjistit, jak\r\noptimalizací prvního zajistit zlepšení toho druhého.\r\n\r\nŘadě organizací by v tomto ohledu mohl být inspirací známý příběh oaklandského\r\nbaseballového týmu „Áček“, který se stal předlohou pro knihu Moneyball a z\r\nní vycházející stejnojmenný film.\r\nPrávě tento příběh jako jeden z prvních ukázal a mezi širokou veřejností\r\nzpopularizoval možnosti využití statistické analýzy ve světě sportu a\r\npotažmo také v rámci řízení lidských zdrojů. Díky radikální změně\r\ndosavadního přístupu k výběru nových hráčů, který se začal více opírat o\r\nvýstupy statistické analýzy sabermetrických\r\ndat o herním chování hráčů, dokázal management oaklandského\r\nbaseballového týmu „Áček“ přijímat rozhodnutí, která z jednoho z\r\nnejchudších týmů americké baseballové ligy učinila jeden z\r\nnejúspěšnějších týmů soutěže (měřeno počtem vítězství v základní části\r\nsoutěže a počtem postupů do play-off). Abychom mohli tento příběh plně\r\nvytěžit coby inspiraci, jak analyzovat svá vlastní zaměstnanecká data,\r\nbude užitečné, když se na jednotlivé analytické kroky, které stály v\r\npozadí úspěchu oklandských “Áček”, podíváme trochu podrobněji. A učiníme\r\ntak za využití volně dostupného statistického softwaru R a veřejně\r\ndostupné databáze\r\nhistorických údajů o výsledcích v americké baseballové lize.\r\n1.\r\nkrok: Začít od konce aneb strategický rámec HR analytiky\r\nJak bylo uvedeno výše, často podceňovaným krokem při zavádění HR\r\nanalytiky do firem a organizací je zasazení HR analytiky do nějakého\r\nširšího strategického rámce, ze kterého by jasně vyplývalo, čemu má\r\nvlastně HR analytika sloužit. HR analytika je pouze nástroj, konkrétně\r\nnástroj na zodpovídání otázek, resp. na testování různých hypotéz. To,\r\nzda bude tento nástroj užitečný, závisí na tom, zda si dokážeme klást ty\r\nsprávné otázky. To je přitom z velké části dáno tím, zda si jsme vědomi,\r\njaké jsou strategické cíle naší organizace. Jen ve světle těchto cílů\r\ndává smysl klást si nějaké otázky, sbírat a analyzovat nějaká data za\r\núčelem nalezení odpovědí na položené otázky a posléze činit nějaká\r\nkonkrétní rozhodnutí na základě nalezených odpovědí. V případě\r\noaklandských „Áček“ byl cíl jasný – kvalifikovat se do play-off.\r\n2. krok: Definice\r\nproblému a kvantifikace cíle\r\nPaul\r\nDePodesta, kterého generální manažer oaklandských „Áček“ Billy Beane přijal\r\ndo týmu jako statistického analytika, redukoval tento cíl na celkem\r\njednoduchý matematický problém: Kolik zápasů musí tým vyhrát, aby se\r\nkvalifikoval do play-off? K zodpovězení této otázky DePodesta potřeboval\r\nhistorická data o počtu vítězství jednotlivých týmů v minulých sezónách\r\na o tom, zda se jim podařilo postoupit do play-off, či nikoli.\r\n\r\n\r\nShow code\r\n\r\n# Načtěme si knihovnu, která nám umožní si načíst a předpřipravit data k analýze a také je i vizualizovat. \r\nlibrary(tidyverse)\r\n\r\n# Načteme si naše data.\r\nbaseball <- read_csv(\"baseball.csv\")\r\n\r\n# Pro získání lepší představy o nich se podívejme na jejich prvních deset řádků.\r\nhead(baseball, 10)\r\n\r\n\r\n# A tibble: 10 x 15\r\n   Team  League  Year    RS    RA     W   OBP   SLG    BA Playoffs\r\n   <chr> <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>\r\n 1 ARI   NL      2012   734   688    81 0.328 0.418 0.259        0\r\n 2 ATL   NL      2012   700   600    94 0.32  0.389 0.247        1\r\n 3 BAL   AL      2012   712   705    93 0.311 0.417 0.247        1\r\n 4 BOS   AL      2012   734   806    69 0.315 0.415 0.26         0\r\n 5 CHC   NL      2012   613   759    61 0.302 0.378 0.24         0\r\n 6 CHW   AL      2012   748   676    85 0.318 0.422 0.255        0\r\n 7 CIN   NL      2012   669   588    97 0.315 0.411 0.251        1\r\n 8 CLE   AL      2012   667   845    68 0.324 0.381 0.251        0\r\n 9 COL   NL      2012   758   890    64 0.33  0.436 0.274        0\r\n10 DET   AL      2012   726   670    88 0.335 0.422 0.268        1\r\n# ... with 5 more variables: RankSeason <dbl>, RankPlayoffs <dbl>,\r\n#   G <dbl>, OOBP <dbl>, OSLG <dbl>\r\n\r\nShow code\r\n\r\n# Pro následující analýzy si potom vytvořme podmnožinu dat, která měli k dispozici v Oaklandu v roce 2002, kdy se děj Moneyballu převážně odehrává.\r\nmoneyball <- baseball %>%\r\n  filter(Year < 2002)\r\n\r\n\r\n\r\nPodíváme-li se na data mezi lety 1996–2001, tj. na data z relativně\r\nnedávné minulosti (vztaženo k roku 2002, kdy se děj Moneyballu převážně\r\nodehrává), z grafického vyjádření vztahu mezi počtem vítězství v\r\nzákladní částí soutěže a postupem do play-off je dobře patrné, že čím\r\nvíce zápasů tým vyhraje v základní soutěži, tím větší je šance, že se\r\ntaké dostane do play-off.\r\n\r\n\r\nShow code\r\n\r\n# Vytvořme si graf zachycující vztah mezi počtem vítězství v základní části soutěže a postupem do play-off\r\nmoneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select (W, Playoffs) %>%\r\n  mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs)))+\r\n  geom_point(size = 2)+\r\n  scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5))+\r\n  scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"Tým nepostoupil do play-off\",\"Tým postoupil do play-off\"))+\r\n  ggtitle(\"Postupy týmů do play-off mezi lety 1996-2001\")+\r\n  ylab(\"\")+\r\n  xlab(\"Počet vítězství v základní části soutěže\")+\r\n  theme(legend.position = \"bottom\",\r\n        axis.ticks.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.text.x = element_text(size=11),\r\n        axis.title.x = element_text(size=11),\r\n        legend.text = element_text(size=11),\r\n        legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nS daty, která máme k dispozici, máme tu výhodu, že můžeme vztah mezi\r\npočtem vítězství v základní části soutěže a šancí na postup do play-off\r\npřesně kvantifikovat. Provedeme-li podrobnější analýzu našich dat, ukáže\r\nse, že velkou (přibližně 95%) šanci na postup do play-off má tým tehdy,\r\nkdyž v základní části vyhraje minimálně 95 zápasů. Těchto 95 vítězství\r\npředstavuje dobře definovaný a kvantifikovaný cíl, kterého by se\r\noaklandská „Áčka“ měla snažit dosáhnout.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si opět data mezi lety 1996-2001. \r\nmoneyball2 <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995)\r\n\r\n# Vytvořme si seznam několika různých hodnot počtu vítězství v základní části soutěže.\r\npocet_vitezstvi <- seq(60,115,5)\r\nucast_v_playoff <- vector(mode=\"numeric\", length=length(pocet_vitezstvi))\r\nplayoff_data <- data.frame(pocet_vitezstvi = pocet_vitezstvi, ucast_v_playoff = ucast_v_playoff)\r\n\r\n# Vypočtěme si, jaká je pravděpodobnost postupu do play-off při různém počtu vítězství v základní části soutěže.\r\nfor(i in 1:nrow(playoff_data)){\r\n playoff_data$ucast_v_playoff[i] <- length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i] & moneyball2$Playoffs == 1])/length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i]]) \r\n}\r\n\r\n# A nyní si vztah mezi počtem vyhraných zápasů v základní části soutěže a pravděpodobností účasti v play-off vizualizujme.\r\nggplot(playoff_data, aes(x = pocet_vitezstvi, y = ucast_v_playoff))+\r\n  geom_point(size = 2)+\r\n  geom_line()+\r\n  ggtitle(\"Souvislost mezi počtem výher v základní části soutěže a\\npravděpodobností postupu týmu do play-off (1996-2001)\")+\r\n  ylab(\"Pravděpodobnost postupu týmu do play-off\")+\r\n  xlab(\"Počet vítězství v základní části soutěže\")+\r\n  scale_x_continuous(limits=c(60,115), breaks = seq(60,115,5))+\r\n  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1))+\r\n  theme(axis.text = element_text(size=11),\r\n        axis.title = element_text(size=11))\r\n\r\n\r\n\r\n\r\n3. krok: Kladení otázek a\r\nměření\r\nS takto definovaným a kvantifikovaným cílem si potom můžeme klást\r\ndalších otázky, na které když si dokážeme odpovědět, zvýšíme tím naše\r\nšance na to, že tohoto cíle dosáhneme. V případě oaklandských „Áček“ se\r\nmůžeme ptát, díky čemu tým dosahuje v zápasech vítězství? Celkem zjevná\r\nodpověď zní, že díky tomu, že dokáže získat více bodů než jeho soupeři.\r\nOtázkou ale je, přesně o kolik bodů navíc musí tým získat, aby v\r\nzákladní části soutěže dosáhl na minimálně 95 vítězství. K zodpovězení\r\ntéto otázky opět potřebujeme historická data (údaje o vyhraných a\r\nprohraných bodech) a relativně jednoduchý statistický model zvaný lineární\r\nregrese, pomocí kterého můžeme popsat vztah mezi počtem vyhraných\r\nzápasů v základní části soutěže a rozdílem mezi vyhranými a prohranými\r\nbody. Z níže uvedeného grafu je zřejmé, že mezi těmito dvěma proměnnými\r\nje velice těsný vztah a že spolu velice silně korelují.\r\n\r\n\r\nShow code\r\n\r\n# Vypočtěme si rozdíl mezi vyhranými a prohranými body\r\nmoneyball <- moneyball %>%\r\n  mutate(RD = RS - RA)\r\n\r\n# Graficky si znázorněme vztah mezi počtem vyhraných zápasů v základní části soutěže a rozdílem mezi vyhranými a prohranými body\r\nlibrary(ggpubr)\r\nggplot(moneyball, aes(x = RD , y = W))+\r\n  geom_point(alpha = 0.5, size = 2)+\r\n  geom_smooth(method = \"lm\", se = FALSE)+\r\n  ggtitle(\"Vztah mezi počtem vítězství v základní části soutěže a\\nrozdílem mezi vyhranými a prohranými body\")+\r\n  xlab(\"Rozdíl mezi počtem vyhraných a prohraných bodů\")+\r\n  ylab(\"Počet vítězství\")+\r\n  theme(axis.title = element_text(size = 11),\r\n        axis.text = element_text(size = 11))+\r\n  scale_x_continuous(limits = c(-350,350), breaks = seq(-350,350,50))+\r\n  scale_y_continuous(limits = c(40, 120), breaks = seq(40,120,10))+\r\n  stat_cor(method = \"pearson\", label.x = 175, label.y = 45)\r\n\r\n\r\n\r\n\r\nPři použití modelu lineární regrese můžeme vztah mezi těmito dvěma\r\nproměnnými popsat trochu podrobněji.\r\n\r\n\r\nShow code\r\n\r\n# Regresní analýza vztahu mezi mezi počtem vyhraných zápasů v základní části soutěže a rozdílem mezi vyhranými a prohranými body \r\nreg_model1 <- glm(W ~ RD, data = moneyball, family = \"gaussian\")\r\nsummary(reg_model1)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = W ~ RD, family = \"gaussian\", data = moneyball)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-14.2662   -2.6509    0.1234    2.9364   11.6570  \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 80.881375   0.131157  616.67   <2e-16 ***\r\nRD           0.105766   0.001297   81.55   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 15.51641)\r\n\r\n    Null deviance: 117164  on 901  degrees of freedom\r\nResidual deviance:  13965  on 900  degrees of freedom\r\nAIC: 5037\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nVýsledná regresní rovnice nám říká, že očekávaný počet vítězství\r\n= 80.88 + 0.106 x Rozdílový skór. Tzn., že při vyrovnaném poměru\r\nvyhraných a prohraných bodů můžeme očekávat, že tým vyhraje přibližně 80\r\nzápasů za sezónu, a že když se rozdílové skóre navýší o deset bodů,\r\nmůžeme očekávat, že tým vyhraje v průměru o jeden zápas za sezónu navíc.\r\nKlíčové je ale pro nás to, že s pomocí této rovnice a s trochou algebry\r\nsi můžeme jednoduše vypočítat, že k dosažení minimálně 95 vítězství za\r\nsezónu potřebuje tým vyhrát přibližně o 133 bodů více, než kolik jich se\r\nsoupeři prohraje ((95 - 80.88) / 0.106).\r\n4. krok: Kladení\r\ndalších otázek a další měření\r\nTímto zjištěním se náš cíl opět trochu více specifikuje a vyvolává\r\ndalší otázky. Otázka, která se téměř sama nabízí, se týká charakteristik\r\nhráčů, které nejlépe předpovídají počet vyhraných a prohraných bodů, a\r\ntím tedy také pravděpodobnost postupu týmu do play-off. DePodesta na\r\nzákladě svých analýz zjistil, že počet vyhraných bodů nejtěsněji souvisí\r\ns procentem případů, kdy se hráč dostane na metu (tzv. On-Base\r\nPercentage - OBP), a to, jak daleko se hráč dostane při svém odpalu\r\n(tzv. Slugging Percentage - SLG). Analogické statistiky pro\r\ntýmy soupeřů (OOBP a OSLG) potom stejně dobře předpovídají počet\r\nprohraných bodů. Když vztah mezi těmito proměnnými popíšeme opět pomocí\r\nmodelu lineární regrese, můžeme se s jeho pomocí pokusit předpovědět,\r\njak si tým povede příští sezónu. Taková předpověď by přitom mohla být\r\npotenciálně velice užitečná, protože na jejím základě bychom případně\r\nmohli upravit některá svá rozhodnutí o koupi nebo prodeji vybraných\r\nhráčů. Pojďme tuto předpověď vytvořit pro tým oaklandských „Áček“ pro\r\nsezónu 2002 na základě dat z let 1962-2001. Z předchozí analýzy již\r\nvíme, že…\r\nPočet vítězství = 80.88 + 0.106 x (Počet vyhraných bodů - Počet\r\nprohraných bodů).\r\nNyní potřebujeme určit, jaký bude pravděpodobný počet vyhraných a\r\nprohraných bodů. Pomůžeme si opět regresní analýzou.\r\n\r\n\r\nShow code\r\n\r\n# Regresní analýza vztahu mezi mezi počtem vyhraných bodů v základní části soutěže a dvěma vybranými hráčskými/týmovými statistikami \r\nregModel2 = lm(RS ~ OBP + SLG, data=moneyball)\r\nsummary(regModel2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RS ~ OBP + SLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-70.838 -17.174  -1.108  16.770  90.036 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -804.63      18.92  -42.53   <2e-16 ***\r\nOBP          2737.77      90.68   30.19   <2e-16 ***\r\nSLG          1584.91      42.16   37.60   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 24.79 on 899 degrees of freedom\r\nMultiple R-squared:  0.9296,    Adjusted R-squared:  0.9294 \r\nF-statistic:  5934 on 2 and 899 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\n# Regresní analýza vztahu mezi mezi počtem prohraných bodů v základní části soutěže a dvěma vybranými hráčskými/týmovými statistikami \r\nregModel3 = lm(RA ~ OOBP + OSLG, data=moneyball)\r\nsummary(regModel3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RA ~ OOBP + OSLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-82.397 -15.178  -0.129  17.679  60.955 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -837.38      60.26 -13.897  < 2e-16 ***\r\nOOBP         2913.60     291.97   9.979 4.46e-16 ***\r\nOSLG         1514.29     175.43   8.632 2.55e-13 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.67 on 87 degrees of freedom\r\n  (812 observations deleted due to missingness)\r\nMultiple R-squared:  0.9073,    Adjusted R-squared:  0.9052 \r\nF-statistic: 425.8 on 2 and 87 DF,  p-value: < 2.2e-16\r\n\r\nS pomocí regresní analýzy jsme zjistili, že…\r\nPočet vyhraných bodů = -804.63 + 2737.77 x OBP + 1584.91 x\r\nSLGPočet prohraných bodů = -837.38 + 2913.60 x OOBP + 1514.29 x\r\nOSLG.\r\nSe znalostí hráčských/týmových statistik oaklandských „Áček“ za rok\r\n2001 se nyní můžeme pokusit předpovědět nejdříve počet vyhraných a\r\nprohraných bodů a potom také předpokládaný počet vítězství v základní\r\nčásti soutěže. Při formulování této předpovědi vycházíme z předpokladu,\r\nže se složení týmu v průběhu sezóny 2002 nebude (např. z důvodu zranění\r\nhráčů) příliš lišit od jeho složení v roce 2001.\r\n\r\n\r\nShow code\r\n\r\n# Hráčské/týmové statistiky oaklandských „Áček“ za rok 2001\r\nOBP_OAK <- moneyball$OBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nSLG_OAK <- moneyball$SLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOOBP_OAK <- moneyball$OOBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOSLG_OAK <- moneyball$OSLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\n\r\n# Pravděpodobné hodnoty vybraných statistik oaklandských \"Áček\" pro rok 2002 vypočítané s pomocí odhadnutých regresních modelů\r\npocet_vyhranych_bodu_pred <- round(-804.63 + 2737.77*OBP_OAK + 1584.91*SLG_OAK)\r\npocet_prohranych_bodu_pred <- round(-837.38 + 2913.60*OOBP_OAK + 1514.29*OSLG_OAK)\r\npocet_vitezstvi_pred <- round(80.88 + 0.106 * (pocet_vyhranych_bodu_pred - pocet_prohranych_bodu_pred), 0)\r\n\r\n# Skutečné hodnoty vybraných statistik oaklandských \"Áček\" pro rok 2002\r\npocet_vyhranych_bodu_real <- baseball$RS[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_prohranych_bodu_real <- baseball$RA[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_vitezstvi_real <- baseball$W[which(baseball$Team==\"OAK\" & baseball$Year == 2002)]\r\n\r\n# Tabulka porovnávající statistické předpovědi se skutečností \r\npred <- c(pocet_vyhranych_bodu_pred, pocet_prohranych_bodu_pred, pocet_vitezstvi_pred)\r\nreal <- c(pocet_vyhranych_bodu_real, pocet_prohranych_bodu_real, pocet_vitezstvi_real)\r\ntable <- data.frame(\"Předpověd\" = pred, \"Skutečnost\" = real)\r\nrow.names(table) <- c(\"Vyhrané body\", \"Prohrané body\", \"Počet vítězství\")\r\ntable\r\n\r\n\r\n                Předpověd Skutečnost\r\nVyhrané body          836        800\r\nProhrané body         635        654\r\nPočet vítězství       102        103\r\n\r\nPorovnání našich předpovědí s reálnými výsledky za sezónu 2002\r\nukazuje, že se nám podařilo velice přesně předpovědět výsledky v\r\nnadcházející ligové sezóně, a významně tak snížit míru naší nejistoty\r\npři jejím plánování.\r\n5.\r\nkrok: Propojení dílčích vhledů aneb organizace jako stroj\r\nMatt Dancho ve\r\nsvé metodice k datově-analytickým projektům doporučuje, abychom se při\r\nsnaze o pochopení obchodního problému organizace na danou organizaci\r\ndívali jako na druh stroje, který má určité vstupy, procesy a výstupy.\r\nTuto metaforu stroje můžeme nyní využít k tomu, abychom všechny výše\r\nuvedené dílčí vhledy spojili do jednotného rámce. V něm budou mít\r\noaklandská “Áčka” podobu jednoduchého stroje na výrobu postupů do\r\nplay-off - viz obrázek níže.\r\n\r\nZe schématu je dobře patrné, jak tento stroj funguje: Jeho výstupy\r\njsou postupy do play-off, kterých dosahuje tak, že se snaží vyhrát více\r\nzápasů, resp. získat více bodů než soupeřící týmy; k tomu využívá vstupy\r\nv podobě schopnosti hráčů hrát dobře na pálce a v poli; vstupem\r\novlivňujícím chod stroje jsou rovněž obdobné schopnosti hráčů\r\nsoupeřících týmů. Jedná se samozřejmě o velmi zjednodušený kauzální\r\nmodel fungování týmu oakladnských “Áček”, ale jak konstatuje slavný\r\nstatistický aforismus, modely jsou\r\nvždy nepřesné, ale některé z nich jsou užitečné.\r\nJakkoli naše modely fungování organizace budou vždy neúplné, je\r\ndůležité ověřit, zda tyto modely i přes svou omezenost v dostatečné míře\r\nodrážejí realitu tak, jak nám ji zprostředkovávají dostupná data. Za\r\ntímto účelem můžeme použít statistickou metodu strukturálního\r\nmodelování, která umožňuje formalizovat naše představy o vzájemných\r\nvztazích mezi několika různými proměnnými a zhodnotit míru souladu\r\ntěchto našich představ s dostupnými daty. Teprve po takovém zhodnocení\r\nvěrohodnosti modelu je rozumné na něm zakládat svá další rozhodnutí.\r\nPojďme tedy tuto metodu použít rovněž na náš nově vytvořený model\r\nfungování týmu oaklandských “Áček” a ověřit míru jeho věrohodnosti.\r\n\r\n\r\nShow code\r\n\r\n# Data, která budeme potřebovat pro ověření věrohodnosti našeho modelu fungování oaklandských \"Áček\" \r\nsem_data <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select(RS, RA, RD, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n\r\n# Definice modelu, která je v souladu s výše uvedeným schématem\r\nlibrary(lavaan)\r\noak_model <- '\r\n     Playoffs ~ W\r\n     W ~ RS + RA\r\n     RA ~ OOBP + OSLG\r\n     RS ~ OBP + SLG \r\n'\r\n# Odhad parametrů modelu\r\nfit_oak_model <- sem(oak_model, data = sem_data, missing = \"pairwise\", estimator = \"WLSMV\", ordered = \"Playoffs\")\r\nsummary(fit_oak_model, standardized = T, fit.measures = T, rsq = T)\r\n\r\n\r\nlavaan 0.6-9 ended normally after 148 iterations\r\n\r\n  Estimator                                       DWLS\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        14\r\n                                                      \r\n                                                  Used       Total\r\n  Number of observations                            90         176\r\n  Number of missing patterns                         1            \r\n                                                                  \r\nModel Test User Model:\r\n                                              Standard      Robust\r\n  Test Statistic                                 8.354      10.470\r\n  Degrees of freedom                                15          15\r\n  P-value (Chi-square)                           0.909       0.789\r\n  Scaling correction factor                                  1.159\r\n  Shift parameter                                            3.260\r\n       simple second-order correction                             \r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                               167.943     150.692\r\n  Degrees of freedom                                 6           6\r\n  P-value                                        0.000       0.000\r\n  Scaling correction factor                                  1.119\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    1.000       1.000\r\n  Tucker-Lewis Index (TLI)                       1.016       1.013\r\n                                                                  \r\n  Robust Comparative Fit Index (CFI)                            NA\r\n  Robust Tucker-Lewis Index (TLI)                               NA\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.000       0.000\r\n  90 Percent confidence interval - lower         0.000       0.000\r\n  90 Percent confidence interval - upper         0.040       0.067\r\n  P-value RMSEA <= 0.05                          0.964       0.903\r\n                                                                  \r\n  Robust RMSEA                                                  NA\r\n  90 Percent confidence interval - lower                     0.000\r\n  90 Percent confidence interval - upper                        NA\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  SRMR                                           0.108       0.108\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                           Robust.sem\r\n  Information                                 Expected\r\n  Information saturated (h1) model        Unstructured\r\n\r\nRegressions:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n  Playoffs ~                                                     \r\n    W                  0.234    0.025    9.181    0.000     0.234\r\n  W ~                                                            \r\n    RS                 0.093    0.006   15.189    0.000     0.093\r\n    RA                -0.094    0.006  -16.031    0.000    -0.094\r\n  RA ~                                                           \r\n    OOBP            3158.695  360.178    8.770    0.000  3158.695\r\n    OSLG            1520.258  213.163    7.132    0.000  1520.258\r\n  RS ~                                                           \r\n    OBP             3621.290  258.284   14.021    0.000  3621.290\r\n    SLG             1418.260  144.885    9.789    0.000  1418.260\r\n  Std.all\r\n         \r\n    0.993\r\n         \r\n    0.682\r\n   -0.726\r\n         \r\n    0.564\r\n    0.452\r\n         \r\n    0.606\r\n    0.425\r\n\r\nIntercepts:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.000                                0.000\r\n   .W                 96.691   11.568    8.358    0.000    96.691\r\n   .RA              -808.808  116.566   -6.939    0.000  -808.808\r\n   .RS             -1041.496   73.943  -14.085    0.000 -1041.496\r\n  Std.all\r\n    0.000\r\n    8.629\r\n   -9.367\r\n  -12.661\r\n\r\nThresholds:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs|t1       22.653    6.998    3.237    0.001    22.653\r\n  Std.all\r\n    8.578\r\n\r\nVariances:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.101                                0.101\r\n   .W                  7.779    2.306    3.374    0.001     7.779\r\n   .RA               536.111   97.833    5.480    0.000   536.111\r\n   .RS               449.043   80.457    5.581    0.000   449.043\r\n  Std.all\r\n    0.014\r\n    0.062\r\n    0.072\r\n    0.066\r\n\r\nScales y*:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs           1.000                                1.000\r\n  Std.all\r\n    1.000\r\n\r\nR-Square:\r\n                   Estimate \r\n    Playoffs           0.986\r\n    W                  0.938\r\n    RA                 0.928\r\n    RS                 0.934\r\n\r\n\r\n\r\nShow code\r\n\r\n# Grafické znázornění modelu fungování oaklandských \"Áček\" \r\nlibrary(semPlot)\r\nsemPaths(fit_oak_model, \r\n         whatLabels=\"std\", \r\n         intercepts=FALSE, \r\n         style=\"lisrel\",\r\n         nCharNodes=0,\r\n         nCharEdges=0,\r\n         curveAdjacent = TRUE,\r\n         title=TRUE,\r\n         layout=\"tree2\",\r\n         curvePivot=TRUE,\r\n         rotation =3)\r\n\r\n\r\n\r\n\r\nVýstupy provedené tzv. pěšinkové\r\nanalýzy, která je speciálním typem strukturálního modelování,\r\nnaznačují, že námi navržený model je v souladu s daty, která máme k\r\ndispozici (viz “příznivé” hodnoty indexů shody, resp. neshody jako je\r\nTLI a CFI, resp. RMSEA, a také vysoké hodnoty standardizovaných\r\nregresních koeficientů). Dávají nám tak dobrý důvod věřit, že naše další\r\nkroky a rozhodnutí, která založíme na tomto modelu, budou mít žádoucí\r\nefekt na požadované výstupy, tj. na postup oaklandských “Áček” do\r\nplay-off.\r\n6. krok: Intervence\r\nNa základě výše uvedených zjištění začal management oaklandských\r\n„Áček“ do svého týmu vybírat hráče, kteří sice nevyhovovali tradičním\r\nkritériím, podle kterých hráčští skauti posuzovali kvalitu baseballových\r\nhráčů, ale za to vykazovali přesně ty charakteristiky, které podle\r\nDePodestových analýz předpovídaly počet vyhraných a prohraných bodů, a\r\npotažmo tedy také pravděpodobnost účasti v play-off, která byla hlavním\r\ncílem managementu. Díky tomu, že konkurenční týmy důležitost těchto\r\nhráčských statistik podceňovaly a naopak přeceňovaly jiné, méně důležité\r\nproměnné (např. míru úspěšnosti odpalů, tzv. Batting Average),\r\nmohl management oaklandských „Áček“ relativně levně skupovat hráče,\r\nkteří jim umožňovali dosahovat stanoveného cíle. Výsledkem bylo to, že\r\noaklandská „Áčka“ vyhrávala zhruba o 20 zápasů za sezónu více než stejně\r\n„chudé“ týmy a přibližně stejně tolik zápasů jako 2krát až 3krát bohatší\r\nkonkurence - viz graf níže.\r\n\r\n\r\nShow code\r\n\r\n# Načtěme si potřebná data Lahmanovy baseballové databáze, která je veřejně přístupná na adrese http://seanlahman.com/baseball-archive/statistics/\r\nmzdyHracu <- read_csv(\"salaries.csv\")\r\nvyhryTymu <- read_csv(\"teams.csv\")\r\n\r\n# Vypočtěme si průměrnou sumu mezd vyplácených jednotlivými týmy svým hráčům v letech 1998-2001 \r\nprumerna_suma_MezdHracu <- mzdyHracu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypočtěme si pro jednotlivé týmy průměrný počet výher za sezónu v letech 1998-2001\r\nprumerny_pocet_vyher <- vyhryTymu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyjádřeme si graficky vztah mezi počtem výher a množstvím peněz, které týmy vynakládají na mzdy svých hráčů \r\nlibrary(ggrepel)\r\nprumerna_suma_MezdHracu %>%\r\n  left_join(prumerny_pocet_vyher, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hráčů a počet vítězství v letech 1998-2001\")+\r\n  xlab(\"Průměrná suma mezd hráčů (USD)\")+\r\n  ylab(\"Průměrný počet výher za sezónu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\"))+\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07))\r\n\r\n\r\n\r\n\r\nOmezení HR analytiky\r\nPřes veškerou přidanou hodnotu, kterou HR analytika pro organizaci\r\nmůže mít, je vhodné si vůči ní zachovat zdravou míru skepse a být si\r\nvědom jejích omezení. Níže uvádím přehled několika z nich.\r\nKvalita a užitečnost výstupů HR analytiky je závislá na kvalitě\r\ndat, která do ní vstupují. Jako kdekoli jinde i zde platí okřídlené\r\nrčení „rubbish in, rubbish out“. Schopnost získat potřebná data\r\nvčas, v dostatečné kvalitě a v dostatečném množství přitom představuje\r\njedno z nejužších hrdel celého procesu zavádění HR analytiky v\r\norganizacích.\r\nHR analytika pracuje s historickými daty a vychází z předpokladu,\r\nže minulost je dobrým prediktorem budoucnosti. Ale jak nás na to\r\nopakovaně upozorňují odborníci jako Nassim\r\nTaleb nebo Philip\r\nTetlock, tento vztah mezi minulostí a budoucností platí pouze do\r\nurčité míry a pouze v relativně krátkém časovém horizontu. Na každém\r\nrohu na nás číhá nějaká potenciální černá\r\nlabuť, která může postavit na hlavu všechno, co jsme se na základě\r\nnašich minulých zkušeností naučili brát jako samozřejmou\r\njistotu.\r\nNe každé prostředí je stejně předvídatelné jako svět sportu.\r\nPoměr signálu\r\na šumu se může napříč různými oblastmi významně lišit a čím více\r\npřevládá náhodný šum nad signálem, tím méně jsou výstupy z HR analytiky\r\nužitečné. Příkladem zde může být relativně neúspěšná snaha předpovídat\r\nto, jak si baseballové týmu povedou v play-off. Na rozdíl od základní\r\nčásti soutěže, kde se hraje dostatek zápasů na to, aby se vyrušil vliv\r\nnáhodného štěstí a smůly, v pětizápasových kolech play-off hraje náhoda\r\ntak významnou roli, že souvislost mezi celkovým počtem vítězství v\r\nzákladní části a pořadím týmu v play-off je téměř nulová.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si data mezi lety 1994-2011, kdy v play-off hraje 8 týmů.\r\nmoneyball3 <- moneyball %>%\r\n  filter(Year < 2012 & Year > 1993)\r\n  \r\n# Výpočtěme si Kendallovu pořadovou korelaci mezi mezi celkovýmm počtem vítězství v základní části soutěže a pořadím týmu v play-off mezi lety 1994-2011. \r\nsuppressWarnings(cor.test(~ W + RankPlayoffs, data = moneyball3, method = \"kendall\"))\r\n\r\n\r\n\r\n    Kendall's rank correlation tau\r\n\r\ndata:  W and RankPlayoffs\r\nz = -0.48318, p-value = 0.629\r\nalternative hypothesis: true tau is not equal to 0\r\nsample estimates:\r\n        tau \r\n-0.05541167 \r\n\r\nČísla mají tu zvláštní moc, že dokážou v člověku velice snadno\r\nvzbudit dojem, že toho víme mnohem více než je tomu ve skutečnosti. Je\r\nvšak dobré si být vědom toho, že každá statistická předpověď je vždy\r\nzatížena nějakou mírou chyby, tu větší, tu menší. Velkou výhodou\r\nstatistických modelů je to, že tato chyba je u nich explicitně\r\nvyčíslena, takže s ní lze dopředu počítat a zohlednit ji při následném\r\nrozhodování. Tato „upřímnost“ ohledně své vlastní omylnosti paradoxně\r\nmnohdy staví statistické modely do horšího světla než jinak méně přesné\r\nintuitivní úsudky expertů, pro které podobné údaje o míře jejich\r\nomylnosti většinou nejsou vůbec k dispozici.\r\nVelikost výhody, kterou nám zavedení HR analytiky dává, může být\r\nzávislá na tom, zda podobné postupy využívá také naše konkurence. Opět\r\nto lze celkem dobře doložit na oaklandských „Áčkách“. Jejich výsledky se\r\nmezi lety 2002 až 2012, tj. v době po zveřejnění Moneyballu, kdy již\r\nvšechny týmy měly příležitost seznámit se s principy prediktivní\r\nanalytiky a zavést ji do své praxe, začaly více přibližovat výsledkům\r\npodobně „chudých“ soupeřů a naopak jejich bohatší soupeři jim svým\r\nvýkonem zase trochu odskočili - viz graf níže. Z toho mimo jiné vyplývá,\r\nže s tím, jak se stále více společností bude při řízení lidských zdrojů\r\nspoléhat na výstupy z HR analytiky, přestane být HR analytika nějakou\r\nzásadní konkurenční výhodou a stane se z ní něco, co organizaci “pouze”\r\numožní držet krok s konkurencí.\r\n\r\n\r\nShow code\r\n\r\n# Vypočtěme si průměrnou sumu mezd vyplácených jednotlivými týmy svým hráčům v letech 2002-2012 \r\nprumerna_suma_MezdHracu2 <- mzdyHracu %>%\r\n  filter(yearID > 2001 & yearID <= 2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypočtěme si pro jednotlivé týmy průměrný počet výher za sezónu v letech 2002-2012\r\nprumerny_pocet_vyher2 <- vyhryTymu %>%\r\n  filter(yearID > 2001 & yearID <=2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyjádřeme si graficky vztah mezi počtem výher a množstvím peněz, které týmy vynakládají na mzdy svých hráčů \r\nprumerna_suma_MezdHracu2 %>%\r\n  left_join(prumerny_pocet_vyher2, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hráčů a počet vítězství v letech 2002-2012\")+\r\n  xlab(\"Průměrná suma mezd hráčů (USD)\")+\r\n  ylab(\"Průměrný počet výher za sezónu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\")) +\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(3e+07,2e+08), breaks = seq(3e+07,2e+08,2e+07))\r\n\r\n\r\n\r\n\r\nZávěr\r\nNa příkladu oaklandského baseballového mužstva jsme takto mohli\r\nsledovat obvyklý postup aplikace HR analytiky na určitý druh problému,\r\nkterý se snaží v dané organizaci vyřešit. Vzhledem ke specifickému\r\npředmětu podnikání oaklandských „Áček“ bylo tímto cílem dosáhnout\r\npostupu do play-off a to v situaci, kdy management neměl dostatek\r\nfinančních prostředků na zaplacení hráčů považovaných dle tradičních\r\nměřítek za kvalitní a perspektivní. Od tohoto cíle se potom odvíjela\r\nřada kroků, které blíže specifikovaly jeho povahu a identifikovaly\r\nfaktory (mimo jiné i ty personální), které s jeho dosažením souvisí. Na\r\nzákladě této znalosti potom bylo možné formulovat určité předpovědi a\r\nučinit jistá rozhodnutí, která zvýšila pravděpodobnost toho, že se\r\npodaří vytčeného cíle dosáhnout. Přestože tento příběh o využití HR\r\nanalytiky se odehrál ve světě sportu, jeho logika je platná i v kontextu\r\ntradičnějšího typu organizací. Ostatně ve všech typech\r\norganizací jde nakonec především o to mít na správném místě a ve správný\r\nčas ty správné lidi - jedině tak tyto organizace mohou\r\nsystematicky dosahovat svých strategických cílů.\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/./Pitcher-at-the-mound.jpg",
    "last_modified": "2022-09-17T18:41:49+02:00",
    "input_file": {}
  }
]
