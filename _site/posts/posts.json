[
  {
    "path": "posts/2023-11-22-gender-gap-in-hiring-decisions/",
    "title": "Evidence on the presence of gender bias in selection settings",
    "description": "Interesting results from a pre-registered meta-analysis of 44 years of field experiments on gender gaps in hiring decisions.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-22",
    "categories": [
      "gender discrimination",
      "field experiments",
      "meta-analysis",
      "open science",
      "forecasting"
    ],
    "contents": "\r\nSchaerer et al.¬†(2023) conducted a pre-registered meta-analysis of 44 years of field experiments on gender gaps in hiring decisions with quite interesting results:\r\nDiscrimination against women for male-typed and balanced jobs decreased across time.\r\nDiscrimination against men for female-typed jobs remained stable across time.\r\nAverage effect is small - the average odds of male applicants to receive a callback is 0.91 times the odds of equally qualified female applicants (with 95% CI from 0.86 to 0.97).\r\nHeterogeneity of true effects is very high, specifically 83% of total variance across studies can be attributed to heterogeneity rather than random chance, so the average effect is not so telling.\r\nIn addition to the meta-analysis, the study also included a forecasting challenge in which researchers and laypeople attempted to accurately estimate both time-trends and the current pervasiveness of gender biases in selection settings. Forecasters expected observed decline, but overestimated the degree of remaining bias.\r\n\r\n\r\n\r\nNote on the attached chart: In all figures, odds ratios above 1 indicate a greater preference for male applicants and odds ratios below 1 indicate greater preference for female applicants.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-22-gender-gap-in-hiring-decisions/./gendergap.jpg",
    "last_modified": "2023-11-26T11:42:26+01:00",
    "input_file": "gender-gap-in-hiring-decisions.knit.md"
  },
  {
    "path": "posts/2023-11-18-job-demands-job-control-wellbeing/",
    "title": "Surprising finding on the impact of job demands and control on workers‚Äô well-being",
    "description": "I just came across an interesting and surprising result from a Bayesian meta-analysis on the effect of the interaction between job demands and job control on worker well-being.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-18",
    "categories": [
      "well-being",
      "job demands",
      "job control",
      "meta-analysis"
    ],
    "contents": "\r\nAccording to Huth & Chung-Yan (2023), contrary to the position of many theories in the field of occupational health and stress, job control does not reduce the negative impact of job demands on workers‚Äô well-being.\r\nAs you can see from the table below, the data provided strong evidence for the absence of the interaction between job demands and control.\r\n\r\n\r\n\r\nAt the same time, however, the authors themselves emphasize that ‚Äú[these] findings do not suggest that job demands and job control are not important work design features when considering the well-being of workers. Their direct effects on worker well-being are well-established in past research.‚Äù\r\n‚ÄúThe important conclusion of [the] study is that increased job control cannot offset the deleterious impact that high workloads have on workers. [This means that assuming] that employee well-being is a priority, workload should be restricted irrespective of the positive benefits of increasing employee control.‚Äù\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-18-job-demands-job-control-wellbeing/./pic.png",
    "last_modified": "2023-11-19T12:35:40+01:00",
    "input_file": "job-demands-job-control-wellbeing.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-11-09-personas-based-on-ml-local-interpretation-algos/",
    "title": "Personas based on ML local interpretation algorithms",
    "description": "A demonstration of one method useful for sharing insights from fitted ML models.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-10",
    "categories": [
      "machine learning",
      "interpretability",
      "personas",
      "storytelling",
      "r",
      "python"
    ],
    "contents": "\r\nThere is one very useful albeit relatively underused method of sharing insights from fitted ML models, at least in my professional bubble.\r\nIt‚Äôs a method of identifying personas based on outputs from ML local interpretation algorithms, which provide information about the specific drivers of predictions for individual observations.\r\nIts implementation is pretty straightforward:\r\nFit a ML model.\r\nGenerate predictions for each observation using the fitted model.\r\nIdentify drivers of predictions for individual observation units using a ML local interpretation algorithm, e.g., LIME or SHAP.\r\nUse the data points from steps 2 and 3 to identify clusters with similar characteristics.\r\nName and describe personas corresponding to the identified clusters.\r\nLet‚Äôs see this in action using the Python code below. First, we need to create an artificial dataset on which we will demonstrate the method described above. We‚Äôll create a classification dataset - imagine, for example, that we‚Äôre trying to predict sales performance based on some collaboration metrics, but feel free to imagine any scenario you like - and prepare a training and testing set to train our ML.\r\n\r\n\r\nShow code\r\n\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\n\r\n\r\n# defining the number of samples and features for the dataset\r\nn_samples = 10000\r\nn_features = 10\r\n\r\n# creating the dataset\r\nX, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=6, n_redundant=4, n_clusters_per_class=3, flip_y=0.27, class_sep=1, random_state=1979)\r\n\r\n# creating a df from X and y\r\ndf = pd.DataFrame(X, columns=['feature_{}'.format(i) for i in range(n_features)])\r\ndf['criterion'] = y\r\n\r\n# splitting the dataset into training and test sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1979)\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\nlibrary(reticulate)\r\n\r\n# table dataviz\r\nDT::datatable(\r\n  py$df,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nNow we can fit and fine-tune our ML (XGBoost) model.\r\n\r\n\r\nShow code\r\nfrom xgboost import XGBClassifier\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\n# fitting a XGBoost model with hyperparameter tuning and 10-fold cross-validation\r\n# initializing the XGBoost classifier\r\nxgb_model = XGBClassifier(random_state=1979)\r\n\r\n# defining the parameter grid for hyperparameter tuning\r\nparam_grid = {\r\n  'n_estimators': [100, 200],\r\n  'max_depth': [3, 5, 7],\r\n  'learning_rate': [0.01, 0.1, 0.2]\r\n}\r\n\r\n# setting up the grid search with 10-fold cross-validation\r\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=10, scoring='f1')\r\n\r\n# fitting the model with hyperparameter tuning\r\ngrid_search.fit(X_train, y_train)\r\nGridSearchCV(cv=10,\r\n             estimator=XGBClassifier(base_score=None, booster=None,\r\n                                     callbacks=None, colsample_bylevel=None,\r\n                                     colsample_bynode=None,\r\n                                     colsample_bytree=None,\r\n                                     early_stopping_rounds=None,\r\n                                     enable_categorical=False, eval_metric=None,\r\n                                     gamma=None, gpu_id=None, grow_policy=None,\r\n                                     importance_type=None,\r\n                                     interaction_constraints=None,\r\n                                     learning_rate=None, max_bin=None,\r\n                                     max_cat_to_onehot=None,\r\n                                     max_delta_step=None, max_depth=None,\r\n                                     max_leaves=None, min_child_weight=None,\r\n                                     missing=nan, monotone_constraints=None,\r\n                                     n_estimators=100, n_jobs=None,\r\n                                     num_parallel_tree=None, predictor=None,\r\n                                     random_state=1979, reg_alpha=None,\r\n                                     reg_lambda=None, ...),\r\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\r\n                         'max_depth': [3, 5, 7], 'n_estimators': [100, 200]},\r\n             scoring='f1')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10,\r\n             estimator=XGBClassifier(base_score=None, booster=None,\r\n                                     callbacks=None, colsample_bylevel=None,\r\n                                     colsample_bynode=None,\r\n                                     colsample_bytree=None,\r\n                                     early_stopping_rounds=None,\r\n                                     enable_categorical=False, eval_metric=None,\r\n                                     gamma=None, gpu_id=None, grow_policy=None,\r\n                                     importance_type=None,\r\n                                     interaction_constraints=None,\r\n                                     learning_rate=None, max_bin=None,\r\n                                     max_cat_to_onehot=None,\r\n                                     max_delta_step=None, max_depth=None,\r\n                                     max_leaves=None, min_child_weight=None,\r\n                                     missing=nan, monotone_constraints=None,\r\n                                     n_estimators=100, n_jobs=None,\r\n                                     num_parallel_tree=None, predictor=None,\r\n                                     random_state=1979, reg_alpha=None,\r\n                                     reg_lambda=None, ...),\r\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\r\n                         'max_depth': [3, 5, 7], 'n_estimators': [100, 200]},\r\n             scoring='f1')estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\r\n              colsample_bylevel=None, colsample_bynode=None,\r\n              colsample_bytree=None, early_stopping_rounds=None,\r\n              enable_categorical=False, eval_metric=None, gamma=None,\r\n              gpu_id=None, grow_policy=None, importance_type=None,\r\n              interaction_constraints=None, learning_rate=None, max_bin=None,\r\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\r\n              max_leaves=None, min_child_weight=None, missing=nan,\r\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\r\n              num_parallel_tree=None, predictor=None, random_state=1979,\r\n              reg_alpha=None, reg_lambda=None, ...)XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\r\n              colsample_bylevel=None, colsample_bynode=None,\r\n              colsample_bytree=None, early_stopping_rounds=None,\r\n              enable_categorical=False, eval_metric=None, gamma=None,\r\n              gpu_id=None, grow_policy=None, importance_type=None,\r\n              interaction_constraints=None, learning_rate=None, max_bin=None,\r\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\r\n              max_leaves=None, min_child_weight=None, missing=nan,\r\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\r\n              num_parallel_tree=None, predictor=None, random_state=1979,\r\n              reg_alpha=None, reg_lambda=None, ...)\r\n\r\nShow code\r\n\r\n# getting the best estimator\r\nbest_xgb_model = grid_search.best_estimator_\r\n\r\nThe classification performance metrics below show that the fitted model performs well on the test data, so we can safely proceed further.\r\n\r\n\r\nShow code\r\nfrom sklearn.metrics import classification_report, roc_auc_score\r\nimport numpy as np\r\n\r\n# predictions on the test set\r\ny_pred = best_xgb_model.predict(X_test)\r\ny_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1] \r\n\r\n# classification report\r\nreport = classification_report(y_test, y_pred)\r\nprint(report)\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.81      0.78      0.80       977\r\n           1       0.80      0.83      0.81      1023\r\n\r\n    accuracy                           0.81      2000\r\n   macro avg       0.81      0.80      0.80      2000\r\nweighted avg       0.81      0.81      0.80      2000\r\n\r\nShow code\r\n# ROC AUC score\r\nroc_auc = roc_auc_score(y_test, y_pred_proba)\r\nprint('ROC AUC score: ', np.round(roc_auc, 2))\r\nROC AUC score:  0.85\r\n\r\nNow we will generate LIME explanations for each observation in the testing dataset and standardize them for later analysis and visualization (including the predicted probabilities of the positive class).\r\n\r\n\r\nShow code\r\n\r\nimport lime.lime_tabular\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\n# using LIME for local interpretation\r\n# initializing the LIME explainer\r\nexplainer = lime.lime_tabular.LimeTabularExplainer(\r\n    training_data=X_train,\r\n    feature_names=['Feature_{}'.format(i) for i in range(X_train.shape[1])],\r\n    class_names=['Low Performance', 'High Performance'],\r\n    mode='classification',\r\n    random_state=1234\r\n)\r\n\r\n\r\n# df for storing the LIME explanations for all observation\r\nexplanations_df = pd.DataFrame()\r\nfeature_names = df.columns[:-1].tolist()\r\n\r\n# generating LIME explanations for each observation in the test set\r\nfor i in range(X_test.shape[0]):\r\n    \r\n    # predicted probability for the positive class\r\n    predicted_class_proba = y_pred_proba[i]\r\n    \r\n    # generating the LIME explanation\r\n    exp = explainer.explain_instance(X_test[i], best_xgb_model.predict_proba, num_features=X_train.shape[1])\r\n    exp_list = exp.as_list()\r\n    \r\n    feature_values = {name: 0 for name in feature_names}\r\n    # looping through the employee's conditions and updating feature_values accordingly\r\n    for condition, value in exp_list:\r\n        for feature_name in feature_names:\r\n            if feature_name in condition.lower():\r\n                feature_values[feature_name] = value\r\n                break\r\n\r\n    # adding the predicted probability for the positive class\r\n    feature_values['predicted_class_proba'] = predicted_class_proba\r\n\r\n    supp_df = pd.DataFrame(feature_values, index=[0])  \r\n    explanations_df = pd.concat([explanations_df, supp_df], ignore_index=True) \r\n\r\n  \r\n# standardizing all features (including the probability for the positive class)\r\nscaler = StandardScaler()\r\nexplanations_scaled = scaler.fit_transform(explanations_df)\r\nexplanations_scaled_df = pd.DataFrame(explanations_scaled)\r\nexplanations_scaled_df.columns = explanations_df.columns\r\n\r\nUsing the UMAP 2D projection of the prediction explanations and predicted probabilities, we can see that that are several clusters of observations with similar predicted probabilities and their drivers.\r\n\r\n\r\nShow code\r\nimport umap\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"white\")\r\n\r\n# visualizing the personas using UMAP\r\n# initializing and fitting UMAP\r\nreducer = umap.UMAP(n_components=2, n_neighbors=50, min_dist=0.01, metric='euclidean', random_state=1979, n_jobs=1)\r\nembedding = reducer.fit_transform(explanations_scaled)  \r\n\r\n# plotting the explanations and predicted probability in 2D scatterplot \r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c='lightblue', s=50, alpha=0.5)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\nplt.show()\r\n\r\n\r\nA clustering algorithm, such as HDBSCAN, can help us identify the clusters.\r\n\r\n\r\nShow code\r\nimport hdbscan\r\nimport matplotlib.patches as mpatches\r\n\r\n# HDBSCAN clustering\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=10, cluster_selection_epsilon=0.3, prediction_data=True)\r\nclusterer.fit(embedding)\r\nHDBSCAN(cluster_selection_epsilon=0.3, min_cluster_size=25, min_samples=10,\r\n        prediction_data=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HDBSCANHDBSCAN(cluster_selection_epsilon=0.3, min_cluster_size=25, min_samples=10,\r\n        prediction_data=True)\r\n\r\nShow code\r\nclusters = clusterer.labels_\r\n\r\n# adding the cluster labels to the dataframe\r\nexplanations_df['cluster'] = clusters\r\n\r\n# plotting the clusters\r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\ncmap = plt.cm.get_cmap('tab20')\r\nnorm = plt.Normalize(clusters.min(), clusters.max())\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=50, cmap=cmap, norm=norm, alpha=0.5)\r\npatches = [mpatches.Patch(color=cmap(norm(i)), label=f'Cluster {i}') for i in np.unique(clusters)]\r\nplt.legend(handles=patches, fontsize=12)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities. The clusters were identified using HDBSCAN.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\nplt.show()\r\n\r\n\r\nThere seem to be about eight clusters and some outliers (cluster -1). Let‚Äôs look at how they differ in terms of predicted probabilities. According to the chart below, there appear to be four clusters with increased predicted probabilities (clusters 3, 5, 6, and 7), three with decreased predicted probabilities (clusters 0, 2, and 4), and one with more mixed predictions (cluster 1).\r\n\r\n\r\nShow code\r\nimport matplotlib.colorbar as colorbar\r\n\r\n# plotting the distribution of probability of positive classes\r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=y_pred_proba, cmap='viridis', s=50, alpha = 0.5)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\ncbar = plt.colorbar(scatter)\r\ncbar.set_label('Predicted class probability', rotation=270, labelpad=15)\r\nplt.show()\r\n\r\n\r\nNow we can check for the selected clusters which features and in which direction most affect their respective predicted probabilities. For example, we can see from the table below that the clusters with lower predicted probabilities (clusters 0, 2 and 4) are driven either by the respective values in features 6 and 9 (cluster 0), or by the respective values in features 0, 3, 7 and 9 (cluster 2), or by the respective values in feature 0 (cluster 4).\r\n\r\n\r\nShow code\r\n\r\n# creating a summary for each cluster across all feature drivers of predicted probabilities of the positive class\r\ntab1 <- py$explanations_df %>% \r\n  dplyr::group_by(cluster) %>% \r\n  dplyr::summarise_all(~median(., na.rm = TRUE))\r\n\r\n# tab dataviz\r\nDT::datatable(\r\n  round(tab1,2),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 10, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  formatStyle(\r\n    names(tab1 %>% dplyr::select(-cluster, -predicted_class_proba)),\r\n    background = styleColorBar(range(tab1 %>% dplyr::select(-cluster, -predicted_class_proba)), 'lightblue'),\r\n    backgroundSize = '98% 90%',\r\n    backgroundRepeat = 'no-repeat',\r\n    backgroundPosition = 'center'\r\n  ) \r\n\r\n\r\n\r\nCombined with information on the median values of these specific features, we can get a good idea of the people who tend to under-perform and ‚Äúwhy‚Äù. For example, people in cluster 0 score too high in feature 6 and too low in feature 9; people in cluster 2 score too low in features 0, 3, 7 and 9; and people in cluster 4 score too low in features 0. Given these differences, it would be useful to consider different approaches to try to improve the sales performance of people based on information about which persona they belong to. We could also repeat a similar analysis for clusters with higher predicted probabilities to see which combination of features tends to be associated with higher performance.\r\n\r\n\r\nShow code\r\n\r\n# creating a df with X and y from testing part of the dataset\r\ntest_df = pd.DataFrame(X_test, columns=['feature_{}'.format(i) for i in range(n_features)])\r\ntest_df['predicted_class_proba'] = y_pred_proba\r\ntest_df['cluster'] = clusters\r\n\r\n\r\n\r\nShow code\r\n\r\n# creating a summary for each cluster across all raw data and predicted probability of the positive class\r\ntab2 <- py$test_df %>% \r\n  dplyr::group_by(cluster) %>% \r\n  dplyr::summarise_all(~median(., na.rm = TRUE))\r\n\r\n# tab dataviz\r\nDT::datatable(\r\n  round(tab2,2),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 10, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  formatStyle(\r\n    names(tab2 %>% dplyr::select(-cluster, -predicted_class_proba)),\r\n    background = styleColorBar(range(tab2 %>% dplyr::select(-cluster, -predicted_class_proba)), 'lightblue'),\r\n    backgroundSize = '98% 90%',\r\n    backgroundRepeat = 'no-repeat',\r\n    backgroundPosition = 'center'\r\n  ) \r\n\r\n\r\n\r\nMaybe you‚Äôll find the method described here useful in one of your ML projects. Happy data sleuthing üôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-09-personas-based-on-ml-local-interpretation-algos/./persona_lm_illustration.png",
    "last_modified": "2023-11-13T16:54:11+01:00",
    "input_file": "personas-based-on-ml-local-interpretation-algos.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-10-24-sentiment-analysis-validation/",
    "title": "Sentiment analysis of employee survey comments using zero-shot classification",
    "description": "An attempt to validate a zero-shot sentiment classification.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-10-24",
    "categories": [
      "zero-shot learning",
      "ai",
      "machine learning",
      "sentiment analysis",
      "employee survey",
      "python"
    ],
    "contents": "\r\nRecently, I experimented with zero-shot classification - a machine learning task where the model classifies data into categories it hasn‚Äôt encountered during training - to determine the sentiment of comments from an employee survey (positive, negative, mixed, neutral).\r\nI chose this approach as an alternative to the traditional lexicon and rule-based NLTK sentiment analyzer. Using the transformer architecture of the model (bart-large-mnli from Facebook), I aimed to capture context from entire comments when gauging their sentiment. I admit that I had some doubts because the model wasn‚Äôt specifically trained for sentiment analysis. Instead, it generalizes its understanding from the MNLI tasks to deduce sentiments.\r\nFortunately, each comment was tied to specific statements that also received ratings on a standard 0-10 scale. This allowed me to cross-reference the sentiment classification with the ratings people assigned to these statements.\r\nSo, how did the model do? As the attached chart shows, the average sentiment classification probability is pretty compellingly consistent with the ratings on the standard scale.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(readr)\r\n\r\nmydata <- readr::read_csv(\"./mydata.csv\")\r\n\r\nlabel_data <- mydata %>% \r\n  dplyr::group_by(sentiment_category) %>% \r\n  dplyr::slice(1)\r\n\r\nmydata %>% \r\n  ggplot2::ggplot(aes(x = Score, y = avg_sentiment_score, color = sentiment_category)) +\r\n  ggplot2::geom_line(linewidth = 1.5) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::scale_y_continuous(limits = c(0,NA), breaks = seq(0,1,0.1)) +\r\n  ggplot2::scale_x_continuous(limits = c(-1,10), breaks = seq(0,10,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Mixed\"=\"#7b00e7\", \"Negative\"=\"#db370e\", \"Neutral\"=\"grey\", \"Positive\"=\"#208600\")) +\r\n  ggplot2::geom_text(data=label_data, aes(label=sentiment_category), nudge_x=-0.5, hjust=0.75, vjust = 0, size = 5, fontface = \"bold\") +\r\n  ggplot2::labs(\r\n    x = \"SURVEY ITEM RATING (0-10)\",\r\n    y = \"AVERAGE PROBABILITY OF SENTIMENT CATEGORY\",\r\n    title = \"Validation of zero-shot sentiment classification\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"none\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nSeems like I can add another useful tool to my toolbox. If you are interested in trying it out, you can use a short code snippet below for it.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nfrom transformers import pipeline\r\n\r\n# sentiment analysis with zero-shot classification using the facebook/bart-large-mnli model\r\nsentiment_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\r\ncomments = df['comments'].to_list()\r\n# defining the candidate labels \r\ncandidate_labels = [\"positive\", \"negative\", \"neutral\", \"mixed\"]\r\n# setting the hypothesis template\r\nhypothesis_template = \"The sentiment of this employee feedback is {}.\"\r\n# estimating the sentiment labels\r\nprediction = sentiment_classifier(comments, candidate_labels, hypothesis_template=hypothesis_template)\r\nprediction = pd.DataFrame(prediction)\r\n# creating columns with predicted sentiment (label with the highest probability) and sentiment scores for individual labels  \r\ndf['sentiment_label'] = prediction['labels'].apply(lambda x: x[0])\r\ndf['sentiment_score_positive'] = prediction.apply(lambda x: x['scores'][x['labels'].index('positive')], axis=1)\r\ndf['sentiment_score_negative'] = prediction.apply(lambda x: x['scores'][x['labels'].index('negative')], axis=1)\r\ndf['sentiment_score_neutral'] = prediction.apply(lambda x: x['scores'][x['labels'].index('neutral')], axis=1)\r\ndf['sentiment_score_mixed'] = prediction.apply(lambda x: x['scores'][x['labels'].index('mixed')], axis=1)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-24-sentiment-analysis-validation/./cover_pic.png",
    "last_modified": "2023-10-24T12:56:56+02:00",
    "input_file": "sentiment-analysis-validation.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-10-23-nlp-llm-and-onboarding/",
    "title": "Using NLP & LLM to combat 'tip-of-the-tongue' moments during onboarding",
    "description": "How to make onboarding experience a little bit smoother with the help of NLP and LLM.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-10-10",
    "categories": [
      "onboarding",
      "nlp",
      "llm",
      "embeddings",
      "ai"
    ],
    "contents": "\r\nI believe that many of you have had a similar experience when you joined a new company: you have taken a ton of notes from various meetings and trainings, you know that the answer to your immediate question is in there somewhere, but it is too hard to find the right notes, so you capitulate and decide to try to find the answer in a different way, e.g.¬†by asking a more tenured colleague.\r\nTo make better use of my notes from my current onboarding, I created a quick and dirty app that uses embeddings to find the relevant files with my notes and LLM to summarise answers to my questions.\r\nSo, for example, if I need to find out who owns a certain business process, I can just type my question into the app and quickly get an answer, including tips on what files to look at to verify the answer or find other related information.\r\n\r\nIf you want to check out the code behind the app, you can find it in this GitHub repo.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-23-nlp-llm-and-onboarding/./lost_info.jpg",
    "last_modified": "2023-10-23T19:47:23+02:00",
    "input_file": "nlp-llm-and-onboarding.knit.md"
  },
  {
    "path": "posts/2023-09-24-exploration-vs-exploitation-tradeoff/",
    "title": "Exploration vs. Exploitation trade-off in our calendars",
    "description": "As I was going through my calendar recently to check who I had already met during my onboarding at Sanofi, I realized that one way to look at the calendar is through the lens of the Exploration vs. Exploitation trade-off. What lessons can we take from this?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-25",
    "categories": [
      "calendar",
      "time management",
      "decision making",
      "exploration vs exploration trade-off"
    ],
    "contents": "\r\nAs a ‚Äúmaker‚Äù, I‚Äôm used to using my time in larger blocks of time to do focused work on various data products. Now, however, my time tends to be spread out between a series of 30-45 minute long meetings that help me navigate a complex organization like Sanofi, explore opportunities for collaboration, and make valuable contacts that can help me deliver on tasks in the future.\r\n\r\nIllustration of the Maker‚Äôs vs.¬†Manager‚Äôs schedule.\r\nI wondered if I could gain any potentially useful insights from this particular ‚Äúintuition pump‚Äù. I took the help of the book ‚ÄúAlgorithms to Live By‚Äù by Brian Christian and Tom Griffiths and found at least five such insights:\r\nAlthough one has a primary mode of operation (e.g.¬†exploitation for makers, exploration for managers), one should not completely ignore the other mode and should allocate a small, consistent portion of one‚Äôs time to ensure that one does not miss valuable insights or opportunities that lie outside one‚Äôs primary mode of operation (based on the Epsilon-Greedy strategy to solve the Multi-Armed Bandit problem).\r\nOver time, as one becomes more aware of the options available, the need for exploration may decrease, allowing for more targeted exploitation of known best opportunities (based on the Decaying Epsilon strategy to solve the Multi-Armed Bandit problem).\r\nAlthough our primary focus is on delivery, we should remain open and try new options that have the highest potential upside, despite the associated high uncertainty (based on the Upper Confidence Bound strategy to solution of the Multi-Armed Bandit problem).\r\nIf you plan to stay with the company for a while, you should be open to further exploration, as newly found valuable opportunities can be used later in the future, and vice versa (btw, this could be a useful signal of intentions to leave, as some research suggests that people actually tend to resolve this type of trade-offs this way).\r\nBe Bayesian, i.e., choose your paths based on your current beliefs, regularly check the balance between exploration and exploitation on your calendar, and update your beliefs and make new, more informed decisions as evidence of outcomes accumulates (based on the Thompson Sampling strategy to solution of the Multi-Armed Bandit problem).\r\nMaybe you‚Äôll find some of these insights helpful in finding a better balance between exploring new paths and following familiar ones, leading to more effective use of your time. If nothing else, take this as a recommendation to read the book mentioned above - there‚Äôs plenty in there for inspiration.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-24-exploration-vs-exploitation-tradeoff/./multiArmedBandit.png",
    "last_modified": "2023-09-24T20:26:53+02:00",
    "input_file": "exploration-vs-exploitation-tradeoff.knit.md",
    "preview_width": 1400,
    "preview_height": 544
  },
  {
    "path": "posts/2023-09-17-bayesian-simulation/",
    "title": "Harnessing Bayesian analysis for business process simulation",
    "description": "A demonstration of how the outputs of Bayesian analysis can be used to simulate business processes while preserving inherent uncertainties.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-17",
    "categories": [
      "bayesian statistics",
      "business process simulation",
      "python",
      "pymc"
    ],
    "contents": "\r\nOne of the advantages of doing statistical analysis in a Bayesian framework is that its generative part makes it a natural fit for business process simulation, which incorporates all the uncertainties inherent in the statistical models we use to capture the patterns of interest.\r\nOnce the parameters of the models have been estimated, their corresponding posterior distributions can be easily sampled and used in combination with a range of input values to simulate the expected outcomes, including the associated uncertainty that needs to be taken into account when making decisions.\r\nTo illustrate, imagine, for example, that as a CHRO you want to estimate the number of potential new employees brought in by employees who choose to participate in a new referral program. Fortunately, you have data from a small, three-month pilot of this program in which you offered participation to a small random sample of employees. Using a combination of simple Binomial and Poisson models, you can easily arrive at a reasonable estimate of the outcome of interest when you introduce the program to a larger portion of the company.\r\nLets‚Äô implement this simple illustrative example with PyMC, a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods.\r\nFirst, let‚Äôs upload the data from the pilot program. The first table includes 150 employees who were randomly selected and offered participation in the pilot program. The second table then shows 42 employees who chose to participate in the program and the number of potential new employees they brought in.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\n\r\n# table with all pilot nominees\r\nnominees=pd.read_excel(\"./dataBayesSim.xlsx\", sheet_name=\"nominees\")\r\n\r\n# table with all pilot participants\r\nparticipants=pd.read_excel(\"./dataBayesSim.xlsx\", sheet_name=\"participants\")\r\n\r\n# showing first few rows of the tables\r\nnominees.head(5)\r\n  employeeID  participation\r\n0         e1              0\r\n1         e2              0\r\n2         e3              0\r\n3         e4              0\r\n4         e5              0\r\n\r\nShow code\r\nparticipants.head(5)\r\n  employeeID  referrals\r\n0         e7          2\r\n1         e9          4\r\n2        e10          2\r\n3        e11          2\r\n4        e25          1\r\n\r\nIn order to estimate the expected number of new potential employees after the introduction of a new referral program to a larger part of the company, we want to model the probability that nominees actually participate in the program and the expected number of potential candidates that a participant brings in. To do this, we can fit Binomial and Poisson models to the data, respectively. As mentioned above, we will do this in a Bayesian framework that will make it easier to deal with uncertainty later in our simulation. A side note: for the sake of brevity, I omit the usual sanity checks that should be performed before drawing any conclusions from fitted models - e.g., checking for convergence of Markov chains or posterior predictive checks for how well the fitted models predict observed data.\r\nLet‚Äôs start with the first model. From the summary below, we see that the estimated probability of participating in the programme is between 0.21 and 0.35.\r\n\r\n\r\nShow code\r\nimport pymc as pm\r\n\r\nShow code\r\n# estimating the participation rate\r\nnominated = nominees.shape[0]\r\nparticipated = nominees['participation'].sum()\r\n\r\n# setting up the model\r\nwith pm.Model() as participationModel:\r\n  # assigning a flat Beta prior for p\r\n  p = pm.Beta(\"p\", alpha=1, beta=1)\r\n  \r\n  # defining likelihood\r\n  obs = pm.Binomial(\"obs\", p=p, n=nominated, observed=participated)\r\n  \r\n  # running mcmc\r\n  idata = pm.sample(3000, tune=500, chains=3, cores=1)\r\n  \r\n  # generating posterior predictive sample\r\n  participationModelPosterior = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\r\n\r\n\r\n\r\nShow code\r\nimport arviz as az\r\n\r\n# trace plot showing the evolution of parameter vector over the iterations of Markov chain(s)\r\n#az.plot_trace(idata, kind=\"trace\", divergences=\"bottom\", show=True)\r\n\r\n# posterior predictive check\r\n#az.plot_ppc(participationModelPosterior, num_pp_samples=500, random_seed=7, alpha=0.3, textsize=14, kind='kde', show=True)\r\n\r\n# tabular and visual summary of the posterior probability distribution of the p parameter value\r\naz.summary(idata).round(2)\r\n   mean    sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat\r\np  0.28  0.04    0.22     0.35        0.0      0.0    4338.0    6229.0    1.0\r\n\r\nShow code\r\naz.plot_posterior(idata, hdi_prob=.95, show=True)\r\n\r\n\r\nThe second model then suggests that program participants brought in an average of 1.7 to 2.5 referrals.\r\n\r\n\r\nShow code\r\n# setting up the Poisson model\r\nwith pm.Model() as referralModel:\r\n  # weakly informative exponential prior for lambda parameter with mean 3\r\n  lambda_ = pm.Exponential('lambda', 1/3)\r\n  # alternative flat prior for lambda parameter\r\n  #lambda_ = pm.Uniform('lambda', lower=0, upper=25)\r\n  \r\n  # Poisson likelihood\r\n  y_obs = pm.Poisson('y_obs', mu=lambda_, observed=participants['referrals'])\r\n  \r\n  # running mcmc\r\n  trace = pm.sample(3000, tune=500, chains=3, cores=1)\r\n  \r\n  # generating posterior predictive sample\r\n  referralModelPosterior = pm.sample_posterior_predictive(trace, extend_inferencedata=True)\r\n\r\n\r\n\r\nShow code\r\n# trace plot showing the evolution of parameter vector over the iterations of Markov chain(s)\r\n#az.plot_trace(trace, kind=\"trace\", divergences=\"bottom\", show=True)\r\n\r\n# posterior predictive check\r\n#az.plot_ppc(referralModelPosterior, num_pp_samples=500, random_seed=7, alpha=0.3, textsize=14, kind='kde', show=True)\r\n\r\n# tabular and visual summary of the posterior probability distribution of the p parameter value\r\naz.summary(trace).round(2)\r\n        mean    sd  hdi_3%  hdi_97%  ...  mcse_sd  ess_bulk  ess_tail  r_hat\r\nlambda   2.1  0.22    1.69     2.51  ...      0.0    3676.0    6056.0    1.0\r\n\r\n[1 rows x 9 columns]\r\n\r\nShow code\r\naz.plot_posterior(trace, hdi_prob=.95, show=True)\r\n\r\n\r\nWe can now sample the posterior distributions of the parameters p and lambda, insert them into the dataframe, and for each row/combination calculate the expected number of referrals brought in by participating employees when the program is rolled out to the entire population of 1500 employees. As you can see below, using the IQR, our CHRO can expect recruiters to reach 790 to 990 potential candidates once the new company-wide referral program is in place.\r\n\r\n\r\nShow code\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"white\")\r\n\r\n# sampling from the posterior distribution the parameters p and lambda\r\nposterior = pd.DataFrame({\r\n    'p': idata['posterior']['p'].values.flatten(),\r\n    'lambda': trace['posterior']['lambda'].values.flatten()\r\n})\r\n\r\n# computing expected number of referrals with 1500 nominees\r\nposterior['expectedReferrals'] = 1500*posterior['p']*posterior['lambda']\r\n\r\n# computing summary statistics\r\nm = posterior['expectedReferrals'].mean().round(1)\r\nQ1 = np.percentile(posterior['expectedReferrals'], 25).round(1)\r\nQ2 = np.percentile(posterior['expectedReferrals'], 50).round(1)\r\nQ3 = np.percentile(posterior['expectedReferrals'], 75).round(1)\r\n\r\n# visualizing results\r\nsns.histplot(posterior['expectedReferrals'], bins=30, kde=True, color='#5b7db6').set(xlabel =\"Number of new referrals\", ylabel = \"Count\")\r\nplt.gcf().suptitle('Expected referrals for program rollout to all 1500 employees', fontsize=13)\r\nplt.gca().set_title(f'Mean={m}, Q1={Q1}, Median={Q2}, Q3={Q3}', fontsize=10)\r\nplt.show()\r\n\r\n\r\nAnd it doesn‚Äôt have to end there. For example, this estimate can be combined with other inputs, e.g.¬†the cost of a new referral program, the cost of an alternative solution, etc., to make a better informed decision, taking into account the existing uncertainty.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-17-bayesian-simulation/./plot.png",
    "last_modified": "2023-09-17T19:26:16+02:00",
    "input_file": "bayesian-simulation.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-09-03-dag-and-double-ml/",
    "title": "A plausible model of data-generating process eats ML algorithms for breakfast",
    "description": "An illustration of one of the lessons I took away from studying the use of meta-learners for causal inference.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-03",
    "categories": [
      "causal inference",
      "double machine learning",
      "dag",
      "python"
    ],
    "contents": "\r\nI‚Äôm just in the middle of studying the use of meta-learners for causal inference, i.e.¬†how to repurpose conventional ML models to estimate treatment effect. One of the lessons I took away from this study so far is that no matter how fancy the ML algorithm we use, without a plausible model of the data-generating process, we cannot hope for an unbiased estimate of the treatment effect as without it, it is difficult to select relevant covariates and avoid those that introduce bias into our estimation (e.g.¬†colliders, mediators and their descendants).\r\nTo illustrate and reinforce this lesson for myself, I coded a small example of estimating the average treatment effect of training performance on productivity using synthetic data with a known data-generating process and the Double ML method.\r\nFirst, we create a DAG of the causal relationships behind our data. We see that, according to this DAG, employee productivity is affected by years of experience, job fit, available resources, and training performance, which is our variable of interest. Employee satisfaction is also part of the DAG, which is affected by available resources, employee productivity, and training performance.\r\n\r\n\r\nShow code\r\n\r\nimport networkx as nx\r\nimport matplotlib.pyplot as plt\r\n\r\n# initializing DAG\r\nG = nx.DiGraph()\r\n\r\n# adding nodes\r\nnodes = ['Years of experience', 'Resources', 'Job_fit', 'Training', 'Productivity', 'Satisfaction', 'Cognitive ability']\r\nG.add_nodes_from(nodes)\r\n\r\n# adding edges\r\nedges = [('Years of experience', 'Productivity'),\r\n         ('Cognitive ability', 'Productivity'),\r\n         ('Cognitive ability', 'Training'),\r\n         ('Resources', 'Productivity'),\r\n         ('Resources', 'Satisfaction'),\r\n         ('Job_fit', 'Productivity'),\r\n         ('Training', 'Productivity'),\r\n         ('Training', 'Satisfaction'),\r\n         ('Productivity', 'Satisfaction')]\r\n\r\nG.add_edges_from(edges)\r\n\r\n# drawing DAG\r\npos = nx.fruchterman_reingold_layout(G, seed=5, iterations = 500, k = 2)\r\nlabels = {node: node for node in G.nodes()}\r\nnx.draw(G, pos, with_labels=True, labels=labels, node_color='lightblue', font_weight='bold', node_size=1500, font_size=10)\r\nplt.title(\"DAG\")\r\nplt.show()\r\n\r\n\r\nNow let‚Äôs generate synthetic data corresponding to this DAG.\r\n\r\n\r\nShow code\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# generating synthetic data\r\nnp.random.seed(42)\r\nn = 1000\r\n\r\n# generating features years of experience, resources, job fit, and cognitive ability\r\nX = np.random.normal(0, 1, (n, 4))\r\n\r\n# setting the true causal effect of training performance \r\ntrue_causal_effect = 3.5\r\n\r\n# training is influence by cognitive ability\r\ntraining = 1.8 * X[:, 3] + np.random.normal(0, 1, n)\r\n\r\n# productivity is influenced by all features \r\nproductivity = 2 * X[:, 0] + 1 * X[:, 1] + 0.5 * X[:, 2] + 2.2 * X[:, 3] + true_causal_effect * training + np.random.normal(0, 1, n)\r\n\r\n# defining an employee satisfaction variable that is influenced by resources, productivity, and training\r\nsatisfaction = 0.3 * X[:, 1] +  0.2 * productivity + 1.3 * training + np.random.normal(0, 1, n)\r\n\r\n# creating a final dataFrame\r\ndf = pd.DataFrame(X, columns=['years_of_experience', 'resources', 'job_fit', 'cognitive_ability'])\r\ndf['training'] = training\r\ndf['productivity'] = productivity\r\ndf['satisfaction'] = satisfaction \r\n\r\nWithout DAG, i.e., without understanding the data-generating process behind our data, we might be tempted to ‚Äúthrow‚Äù all available variables into our estimator. We can give it a try and use Double ML for that - a popular framework designed to provide unbiased and consistent estimates of treatment effects in the presence of high-dimensional controls while reducing the risk of overfitting. The common Double ML procedure looks as follows:\r\nRandomly splitting the data into two parts: one for estimating the control function and the other for estimating the treatment effect.\r\nUsing the first part of the data to train a machine learning model to predict the outcome variable based on covariates (without the treatment variable). Similarly, training another model to predict the treatment variable based on covariates.\r\nUsing the second part of the data to form the residuals for outcome and treatment variable and running a simple linear regression of outcome residuals on treatment residuals.\r\nRepeating steps 1-3 but switching the roles of the two data splits and averaging the estimates to get the final average treatment estimate.\r\nLet‚Äôs apply this approach to our data and use XGBoost models to estimate the control function and see how successful we will be in our efforts to estimate the known causal effect of training on productivity.\r\n\r\n\r\nShow code\r\n\r\nimport numpy as np\r\nfrom xgboost import XGBRegressor\r\nfrom sklearn.linear_model import LinearRegression\r\n\r\n# creating a variable for data splits\r\nnp.random.seed(42)\r\ndf['part'] = np.random.choice([0, 1], size=len(df))\r\n\r\nestimates = []\r\n\r\nfor p in [0,1]:\r\n    \r\n    # auxiliary variables for switching the roles of the two data splits\r\n    firstPart = 1-p\r\n    secondPart = p-0\r\n    \r\n    # used covariates\r\n    covariates = ['years_of_experience', 'resources', 'job_fit', 'cognitive_ability', 'satisfaction']\r\n  \r\n    # preparing datasets\r\n    X_First = df.loc[df['part'] == firstPart, covariates].values\r\n    y_Outcome_First = df.loc[df['part'] == firstPart,'productivity'].values\r\n    y_Treatment_First = df.loc[df['part'] == firstPart,'training'].values\r\n    \r\n    X_Second = df.loc[df['part'] == secondPart, covariates].values\r\n    y_Outcome_Second = df.loc[df['part'] == secondPart,'productivity'].values\r\n    y_Treatment_Second = df.loc[df['part'] == secondPart,'training'].values\r\n    \r\n    # controlling for covariates for productivity using XGBoost\r\n    model_Outcome = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Outcome.fit(X_First, y_Outcome_First)\r\n    residual_Outcome = y_Outcome_Second - model_Outcome.predict(X_Second)\r\n    \r\n    # controlling for covariates for training using XGBoost\r\n    model_Treatment = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Treatment.fit(X_First, y_Treatment_First)\r\n    residual_Treatment = y_Treatment_Second - model_Treatment.predict(X_Second)\r\n    \r\n    # Part 3: Estimate the causal effect using the residuals with Linear Regression\r\n    model_causal = LinearRegression()\r\n    model_causal.fit(residual_Treatment.reshape(-1, 1), residual_Outcome)\r\n    double_ml_effect = model_causal.coef_[0]\r\n    \r\n    estimates.append(double_ml_effect)\r\n  \r\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\r\n\r\nShow code\r\nprint(\"The estimated Average Treatment Effect is:\", sum(estimates)/len(estimates))\r\nThe estimated Average Treatment Effect is: 2.8913607224732614\r\n\r\nWe see that the estimated causal effect of training is approximately 2.9, quite far from the known value of 3.5. How so? Well, if you look at the DAG above, you can see that by using all the variables as covariates, not only have we correctly blocked the backdoor by controlling for employee cognitive ability, but we have also introduced bias into our estimation by controlling for satisfaction, which is a collider - a variable that is affected by both the outcome (productivity) and the treatment (training). And controlling for colliders leads to spurious correlations between variables or, as in our case, deflates the size of the estimated effect. We can easily check this using DAGitty, which is a wonderful browser-based environment for creating and analysing causal diagrams.\r\n\r\nSo let‚Äôs repeat the estimation, but now without satisfaction variable as covariate. As you can see below, we are now much closer to the actual causal effect.\r\n\r\n\r\nShow code\r\n\r\n# creating a variable for data splits\r\nnp.random.seed(42)\r\ndf['part'] = np.random.choice([0, 1], size=len(df))\r\n\r\nestimates = []\r\n\r\nfor p in [0,1]:\r\n    \r\n    # auxiliary variables for switching the roles of the two data splits\r\n    firstPart = 1-p\r\n    secondPart = p-0\r\n    \r\n    # used covariates\r\n    covariates = ['years_of_experience', 'resources', 'job_fit', 'cognitive_ability']\r\n  \r\n    # preparing datasets\r\n    X_First = df.loc[df['part'] == firstPart, covariates].values\r\n    y_Outcome_First = df.loc[df['part'] == firstPart,'productivity'].values\r\n    y_Treatment_First = df.loc[df['part'] == firstPart,'training'].values\r\n    \r\n    X_Second = df.loc[df['part'] == secondPart, covariates].values\r\n    y_Outcome_Second = df.loc[df['part'] == secondPart,'productivity'].values\r\n    y_Treatment_Second = df.loc[df['part'] == secondPart,'training'].values\r\n    \r\n    # controlling for covariates for productivity using XGBoost\r\n    model_Outcome = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Outcome.fit(X_First, y_Outcome_First)\r\n    residual_Outcome = y_Outcome_Second - model_Outcome.predict(X_Second)\r\n    \r\n    # controlling for covariates for training using XGBoost\r\n    model_Treatment = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Treatment.fit(X_First, y_Treatment_First)\r\n    residual_Treatment = y_Treatment_Second - model_Treatment.predict(X_Second)\r\n    \r\n    # Part 3: Estimate the causal effect using the residuals with Linear Regression\r\n    model_causal = LinearRegression()\r\n    model_causal.fit(residual_Treatment.reshape(-1, 1), residual_Outcome)\r\n    double_ml_effect = model_causal.coef_[0]\r\n    \r\n    estimates.append(double_ml_effect)\r\n  \r\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\r\n\r\nShow code\r\nprint(\"The estimated Average Treatment Effect is:\", sum(estimates)/len(estimates))\r\nThe estimated Average Treatment Effect is: 3.477088075986001\r\n\r\nDespite the simplicity of the example presented, I think it nicely demonstrates that by using fancy ML algorithms like XGBoost, we are not relieved of the need to have a plausible model of the data-generating process when estimating causal effects from observational data, and we can‚Äôt just blindly rely on some magical powers of ML to squeeze what we need out of whatever input we give it.\r\nJust a side note: If you want to make your life a little bit easier when using Double ML, you can use the DoubleML library for Python and R. The code below illustrates this library in action on our synthetic data.\r\n\r\n\r\nShow code\r\n\r\nfrom doubleml import DoubleMLData, DoubleMLPLR\r\n\r\nnp.random.seed(42)\r\n\r\n# specifying data and roles of individual variables\r\ndml_data = DoubleMLData(df, y_col='productivity', d_cols='training', x_cols=['years_of_experience', 'resources', 'job_fit', 'cognitive_ability'])\r\n\r\n# specifying ML model(s) used for estimation of the nuisance parts\r\nml_xgb = XGBRegressor(eta = 0.1, n_estimators =25)\r\n\r\n# initializing and parametrizing the model object which will be used to perform the estimation\r\ndml_plr_xgb = DoubleMLPLR(\r\n  dml_data,\r\n  ml_l = ml_xgb,\r\n  ml_m = ml_xgb,\r\n  n_folds = 5,\r\n  n_rep = 10,\r\n  score = 'partialling out',\r\n  dml_procedure = 'dml2')\r\n\r\n# estimation and inference\r\ndml_plr_xgb.fit()\r\n<doubleml.double_ml_plr.DoubleMLPLR object at 0x000001A9267BACE0>\r\n\r\nShow code\r\ndml_plr_xgb.summary\r\n              coef   std err          t  P>|t|     2.5 %    97.5 %\r\ntraining  3.426416  0.049597  69.085291    0.0  3.329208  3.523624\r\n\r\nShow code\r\ndml_plr_xgb.confint()\r\n             2.5 %    97.5 %\r\ntraining  3.329208  3.523624\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-03-dag-and-double-ml/dag-and-double-ml_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-09-03T22:52:43+02:00",
    "input_file": "dag-and-double-ml.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-08-22-r-and-power-bi/",
    "title": "Embedding R (or Python) ML models in Power BI dashboards",
    "description": "In my new job, we currently rely a lot on Power BI when presenting people-related insights to our stakeholders. Since I know Power BI quite superficially and we also want to share insights from more complex analyses with our stakeholders, I spent part of the weekend studying how to incorporate ML models created in R or Python into Power BI dashboards. I put my learnings in this blog post. It's definitely not rocket science, but it may still shorten the learning path for some of you who are in a similar situation.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-08-22",
    "categories": [
      "power bi",
      "r",
      "python",
      "machine learning"
    ],
    "contents": "\r\nNote to start: Although the example presented in this post is demonstrated in R, it could be analogously implemented in Python as well.\r\nFor demonstration, I will use the well-known IBM artificial attrition dataset. First, we need to train the model. I suppose you know the drill and know all the steps to go through to get a useful and reliable model. The following R script implements the whole process of data preparation, (XGBoost) model tuning, training, and validation. After we have the prediction model ready, we have to save it, as we will use it later in the Power BI (PBI) dashboard.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\nlibrary(recipes)\r\nlibrary(themis)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors()) %>%\r\n  themis::step_smote(Attrition, over_ratio = 1)\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n  )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n    tune::collect_predictions(parameters = xgb_best) %>% \r\n    yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  parsnip::fit(data_train)\r\n\r\n\r\n# variable importance\r\nxgb_fit %>%\r\n  tune::extract_fit_parsnip() %>%\r\n  vip::vip(num_features = 10, geom = \"col\")\r\n\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n    yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# saving the final model\r\nsaveRDS(xgb_fit, \"./final_xgb_model.RDS\")\r\n\r\n# saving testing data as a new dataset that will be used in Power BI dashboard\r\ndata_test %>%\r\n  dplyr::select(-Attrition) %>%\r\n  writexl::write_xlsx(\"./newData.xlsx\")\r\n\r\n\r\nNow we can move on to PBI. We use the testing data as a new dataset that we will score by the trained model and show stakeholders how the predicted flight risk varies by department, job role, and gender.\r\nFirst, we should check in the PBI settings that PBI has access to our R (or Python) instance (File -> Options and setting -> GLOBAL/R scripting/Python scripting). If so, we can upload new data using the Get data dialog and go to the Transform tab in the Query editor. Here, we should first check that the data types match those in R when preparing the model, and then we can load and run our scoring algorithm using the Run R script dialog. The following script will do the job.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(workflows)\r\n\r\n# uploading trained model\r\nmodel <- readRDS(\"PathTo/YourModel.RDS\")\r\n\r\n# generate predictions\r\nprediction <- predict(model, dataset) %>% \r\n  dplyr::bind_cols(predict(model, dataset, type = \"prob\")) %>%\r\n  dplyr::rename(\r\n    predictedCat = .pred_class,\r\n    probLeave = .pred_1,\r\n    probStay = .pred_0\r\n  )\r\n\r\n# enriching original dataset\r\nprediction <- cbind(dataset, prediction)\r\n\r\n\r\nAfter running the script, our original data will be enriched with the generated predictions. We can apply this transformation, save it and exit the Query editor using the Close & Apply button. We will then have flight risk predictions that we can visualize along with the original data, allowing us to see how predicted flight risk varies by department, job role, and gender, among other things. Again, we could use R or Python for this purpose, as PBI does not offer some types of data visualization by default, especially those related to visualizing data variability. For example, to create the raincloud plots below, I used the ggplot2 and ggdist packages in R. Below is a script used within the R visual that implements one of the plots. To fine-tune them, I recommend using RStudio (if you have it on your computer), which is accessible directly from the R visual editor in PBI.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n# dataset <- data.frame(probLeave, Department)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggdist)\r\n\r\ndataset %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(Department, probLeave, mean, .na_rm = TRUE), y = probLeave)) + \r\n  ggdist::stat_halfeye(\r\n    adjust = .5, \r\n    width = .6, \r\n    .width = 0, \r\n    justification = -.3, \r\n    point_colour = NA,\r\n    fill = \"#bca36b\",\r\n    color = \"#bca36b\"\r\n  ) + \r\n  ggplot2::geom_boxplot(\r\n    width = .25, \r\n    outlier.shape = NA\r\n  ) +\r\n  ggplot2::stat_summary(fun.y=mean, geom=\"point\", shape=20, size=5, color=\"#bca36b\", fill=\"#bca36b\") +\r\n  geom_point(\r\n    size = 1.3,\r\n    alpha = .3,\r\n    position = position_jitter(seed = 1, width = .09, height = .008\r\n    )\r\n  ) + \r\n  ggplot2::coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"PROBABILITY OF LEAVING\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 strip.text.x = element_text(size = 11, face = \"plain\"),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=c(.95,.88),\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(0,0,0,0),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nWe may also want to allow dashboard users to enter custom values for specific personas of interest and let the scoring algorithm predict the corresponding risk of leaving the company. To make the demonstration of this feature somewhat easier, I will use a different prediction model that uses only some of the strongest predictors.\r\nTo enable this feature, we need to create several parameters that the user can set. For numeric predictors we can use the New parameter dialog box on the Modeling tab - we set the name of the parameters, their data type, their minimum, maximum, and default values, and confirm we want to show corresponding sliders in the dashboard. For categorical parameters, we need to create and then upload a table with all possible combinations of values of the categorical parameters used. When using R, we can use for example the expand.grid function to do this. The fields from this table are then used and displayed in the dashboard as single-selection filters. And all these parameters then serve as input to the R visual, where we load the model, change the names of some parameters to match those expected by the model, generate a prediction, and create the resulting visualization. All these steps are implemented by the following R script.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n# dataset <- data.frame(JobLevelParameter Value, YearsWithCurrManagerParameter Value, OverTime, Department, JobSatisfaction)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(workflows)\r\n\r\ndata <- dataset %>%\r\n    dplyr::rename(\r\n        JobLevel = `JobLevelParameter Value`, \r\n        YearsWithCurrManager = `YearsWithCurrManagerParameter Value`\r\n    )\r\n\r\n# uploading trained model\r\nmodel <- readRDS(\"PathTo/YourModel2.RDS\")\r\n\r\n# generate predictions\r\nprediction <- predict(model, data) %>% \r\n  dplyr::bind_cols(predict(model, data, type = \"prob\")) %>%\r\n  dplyr::rename(\r\n    predictedCat = .pred_class,\r\n    probLeave = .pred_1,\r\n    probStay = .pred_0\r\n  )\r\n\r\n# enriching original dataset\r\nprediction <- cbind(data, prediction)\r\n\r\nprediction %>%\r\n  ggplot2::ggplot(aes(x = 1, y = probLeave)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#444492\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::geom_text(aes(label = round(probLeave, 2)), nudge_y = 0.04, color = \"black\", size = 7, fontface = \"bold\") +\r\n  scale_y_continuous(limits = c(0,1.08), breaks = seq(0,1,0.1)) +\r\n  labs(\r\n    x = \"\",\r\n    y = \"PROBABILITY OF LEAVING\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 strip.text.x = element_text(size = 11, face = \"plain\"),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=c(.95,.88),\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(0,0,0,0),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nThis completed the work on our local computer. You can download the final dashboard here. If you use it, be sure to update the paths to the trained model in both the Query Editor and the R visuals. The next step is to set up the Power BI Service and install and configure the on-premises data gateway so that the R scripts will work in dashboards shared with others. More on this in a future blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-08-22-r-and-power-bi/./r_with_powerbi.png",
    "last_modified": "2023-08-24T21:28:55+02:00",
    "input_file": "r-and-power-bi.knit.md",
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2023-07-20-vocational-interests/",
    "title": "Vocational interests don't seem so uninteresting after all",
    "description": "Quite surprising (at least to me) findings on the validity of vocational interests for predicting a range of important work outcomes.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-20",
    "categories": [
      "vocational interests",
      "meta-analysis",
      "predictive validity",
      "job performance"
    ],
    "contents": "\r\nTo be honest, until recently, I tended to underestimate the importance of vocational interests in job performance prediction and considered questions about them during a job interview as a formality. In my defense, this view has also been supported by the low estimates of their predictive validity reported by classics such as Schmidt & Hunter (1998).\r\nHowever, I adjusted my view after coming across the updated validity estimate in the Sackett et al.¬†meta-analysis (2022) and the results of the Nye et al.¬†meta-analysis (2017) on the validity of interests for predicting job performance.\r\nThe latter study reported the following interesting findings:\r\nCorrelation between interest scores and job performance (corrected for both indirect range restriction and unreliability in the criterion) is 0.16 (SE=0.03).\r\n\r\nInterest congruence/match between an individual‚Äôs interests and his or her work is a much stronger predictor of performance outcomes than interest scores alone, with baseline correlations of 0.32 and 0.16, respectively.\r\n\r\nInterests are significantly better predictors of organizational citizenship behavior than other criteria (job performance, task performance, OCB, persistence, CWB, and training performance) but are less valid for predicting CWB and task performance.\r\nIf you tend to think about vocational interests as I have until recently, perhaps these two studies will help you update your priors a little bit üòâ\r\nNote: The attached schemes are taken from another excellent resource on this topic by Nye et al.¬†(2012).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-20-vocational-interests/./scheme2.png",
    "last_modified": "2023-07-20T19:09:20+02:00",
    "input_file": "vocational-interests.knit.md",
    "preview_width": 491,
    "preview_height": 283
  },
  {
    "path": "posts/2023-07-14-induced-centrality/",
    "title": "Induced centralities",
    "description": "A post about useful complement to common ONA centrality measures.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-14",
    "categories": [
      "organizational network analysis",
      "centrality measures",
      "r"
    ],
    "contents": "\r\nWhile reading the excellent publication Social Networks at Work from SIOP‚Äôs Organizational Frontiers series (btw, highly recommended to all PA professionals), I came across the interesting and useful concept of induced centrality.\r\nIt sort of reverses the logic of the common ONA centrality measures, which focus primarily on what one gets from the surrounding network of connections, and instead shows how individual nodes contribute to some global network characteristic of interest, i.e.¬†what one does for the network as a whole.\r\nIts calculation is quite simple and straightforward - you just need to first calculate the global characteristic of the network that you are interested in as a reference point, e.g.¬†its coherence, and then calculate how this measure changes when you remove individual nodes from the network. From this, you can deduce that the nodes that cause the most change in a specific direction contribute the most to a given measure.\r\nIn addition to its versatility and the interesting angle it offers, it can also be very useful in making visible otherwise hidden and invisible ‚Äúheroes‚Äù who contribute to the greater good under the radar of public recognition.\r\nWhat follows is a small demonstration of using induced centrality to estimate which people play the role of expressive leaders who shorten the lengths of paths in the network. It‚Äôs an implementation of the idea briefly described in the aforementioned publication Social Networks at Work:\r\n‚ÄúFor example, suppose one theorizes that there are certain individuals in groups (perhaps called expressive leaders) who provide a certain social glue such that they tend to shorten the lengths of paths in the network (see, for example, the Heidi Roizen case by McGinn and Tempest, 2010). This sounds like we should use closeness centrality, since it is concerned with path lengths. But there are two problems with this. First of all, closeness centrality only counts the shortest paths, and not the circuitous paths that things such as gossip often take. Second, closeness gets at how long it takes for information to reach a given node, who is then presumed to benefit from this information. But the concept we‚Äôve just outlined is about individuals who enable others to have short paths so that the whole group benefits. Closeness was not designed to measure this, and doesn‚Äôt. However an induced centrality measure can be created to measure exactly this: to what degree paths lengthen when you remove each node from the network.‚Äù\r\nFirst, let‚Äôs upload the data used for the demonstration and create the network object. I will use a dataset that captures information-sharing links between 15 members of my friendship network.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# uploading data\r\ndf <- readxl::read_excel(\"./friendshipNetwork.xlsx\")\r\n\r\n# creating network object\r\ng <- igraph::graph_from_data_frame(df, directed=TRUE) \r\n\r\n\r\nWe can now iterate over all directed pairs of nodes and compute the average length of paths between nodes we will use as a reference point. We won‚Äôt use all the paths but only the three shortest paths between each pair of nodes that would enable us to capture some of the circuitous paths mentioned in the problem description above.\r\n\r\n\r\nShow code\r\n\r\nall_nodes <- V(g)\r\ntotal_length <- 0\r\ntotal_paths <- 0\r\n# setting the number of 3 shortest paths between pair of nodes to capture also some of the circuitous paths \r\ntop_shortest_paths <- 3\r\n\r\n# loop for directed network\r\nfor (i in 1:length(all_nodes)) {\r\n  for (j in 1:length(all_nodes)) {\r\n    if (i != j) {\r\n      lengths <- unlist(igraph::all_simple_paths(g, all_nodes[i], all_nodes[j], mode = \"out\"))\r\n      lengths <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n      total_length <- total_length + sum(lengths, na.rm = TRUE)\r\n      total_paths <- total_paths + length(lengths)\r\n    }\r\n  }\r\n}\r\n\r\n# loop for undirected network \r\n# for (i in 1:(length(all_nodes) - 1)) {\r\n#   for (j in (i + 1):length(all_nodes)) {\r\n#     lengths <- all_simple_paths(g, all_nodes[i], all_nodes[j])\r\n#     lengths <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n#     total_length <- total_length + sum(lengths, na.rm = TRUE)\r\n#     total_paths <- total_paths + length(lengths)\r\n#   }\r\n# }\r\n\r\n# computing the average length of paths\r\naverage_length_ref <- total_length / total_paths\r\n\r\n\r\nNow let‚Äôs remove each node one at a time from the network and calculate the average lengths of paths between pairs consisting of the remaining nodes. We also need to deal somehow with situations when node removal leads to the disconnection of previously connected nodes (in such situations, the distance between nodes is by default assumed to be infinite or undefined, which would bias our estimation). I have decided to take the three shortest paths from the complete network and add 1 (this is somewhat equivalent to the additional effort required to find a new bonding connection). After this step, we can subtract the reference point from the obtained values and get the information about the absence of which nodes lengthens the paths between other nodes and thus act as a kind of social glue that facilitates the spread of information between nodes.\r\n\r\n\r\nShow code\r\n\r\n# vector for saving average lengths of paths for individual nodes\r\naverage_lengths <- numeric(length(all_nodes))\r\n\r\nfor (k in 1:length(all_nodes)) {\r\n\r\n  g_new <- g\r\n  g_new <- igraph::delete_vertices(g_new, all_nodes[k])\r\n  all_nodes_new <- V(g_new)\r\n  \r\n  total_length_new <- 0\r\n  total_paths_new <- 0\r\n  \r\n  # for directed network\r\n  for (i in 1:length(all_nodes_new)) {\r\n    for (j in 1:length(all_nodes_new)) {\r\n      if (i != j) {\r\n        # lengths of paths in the network with removed node \r\n        lengths_new <- unlist(igraph::all_simple_paths(g_new, all_nodes_new[i], all_nodes_new[j], mode=\"out\"))\r\n        # lengths of paths in the complete network\r\n        lengths <- unlist(igraph::all_simple_paths(g, all_nodes_new[i]$name, all_nodes_new[j]$name, mode=\"out\"))\r\n        # dealing with situations when node removal leads to disconnection of previously connected nodes by taking 3 shortest paths from the full network and adding 1\r\n        if(is.null(lengths_new) & !is.null(lengths)){\r\n          lengths_new <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n          lengths_new <- lengths_new + 1\r\n        } else{\r\n          lengths_new <- sort(lengths_new, decreasing = FALSE)[1:top_shortest_paths]\r\n        }\r\n        total_length_new <- total_length_new + sum(lengths_new, na.rm = TRUE)\r\n        total_paths_new <- total_paths_new + length(lengths_new)\r\n      }\r\n    }\r\n  }\r\n  \r\n  # for undirected network \r\n  # for (i in 1:(length(all_nodes_new) - 1)) {\r\n  #   for (j in (i + 1):length(all_nodes_new)) {\r\n  #     lengths_new <- length_of_all_paths(g_new, all_nodes_new[i], all_nodes_new[j])\r\n  #     lengths <- length_of_all_paths(g, all_nodes_new[i]$name, all_nodes_new[j]$name)\r\n  #     if(is.null(lengths_new) & !is.null(lengths)){\r\n  #      lengths_new <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n  #      lengths_new <- lengths_new + 1\r\n  #     } else{\r\n  #      lengths_new <- sort(lengths_new, decreasing = FALSE)[1:top_shortest_paths]\r\n  #     }\r\n  #     total_length_new <- total_length_new + sum(lengths_new, na.rm = TRUE)\r\n  #     total_paths_new <- total_paths_new + length(lengths_new)\r\n  #   }\r\n  # }\r\n  \r\n  average_lengths[k] <- total_length_new / total_paths_new\r\n}\r\n\r\n# computing the difference between average and reference point\r\naverage_length_diff <- average_lengths - average_length_ref\r\n\r\n# assigning computed differences to individual nodes\r\nV(g)$avg_length_diff <- average_length_diff\r\n\r\n\r\nThe graph below shows that nodes P2, P8, and P4 are the most critical in this respect.\r\n\r\n\r\nShow code\r\n\r\nggraph::ggraph(g, layout = \"kk\") + # other available layouts: 'star', 'circle', 'gem', 'dh', 'graphopt', 'grid', 'mds', 'randomly', 'fr', 'kk', 'drl', 'lgl'\r\n  ggraph::geom_edge_link(arrow = arrow(length = unit(2.5, 'mm')), end_cap = circle(2, 'mm')) +\r\n  ggraph::geom_node_point(aes(size = avg_length_diff), alpha = 1, color = ifelse(V(g)$avg_length_diff>0, \"#e15759\", \"black\")) +\r\n  ggplot2::scale_size_continuous(range = c(0.1,8)) +\r\n  ggraph::geom_node_label(aes(label = name), repel = TRUE) +\r\n  ggplot2::labs(\r\n    title = \"Expressive leaders who shorten path lengths in the network\",\r\n    subtitle = \"Demonstration of the concept of induced centrality\",\r\n    size = \"Increase in average path length after node removal\",\r\n    caption = \"\\nNodes with an increase greater than 0 are highlighted in red.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,9,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title = element_blank(),\r\n    axis.text = element_blank(),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    axis.line = element_blank(),\r\n    legend.position=\"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2\r\n3.4.0.\r\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere\r\n  instead.\r\nThis warning is displayed once every 8 hours.\r\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning\r\nwas generated.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-14-induced-centrality/./ona.png",
    "last_modified": "2023-07-14T12:09:17+02:00",
    "input_file": "induced-centrality.knit.md",
    "preview_width": 1500,
    "preview_height": 1111
  },
  {
    "path": "posts/2023-07-11-searching-and-querying-aihr-blog-posts/",
    "title": "Searching & querying AIHR blog posts on People Analytics topics",
    "description": "I'm sharing a by-product of my learning about vector database search that may be useful to some of you who want to learn something new about People Analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-11",
    "categories": [
      "aihr",
      "people analytics",
      "vector database",
      "learning by doing"
    ],
    "contents": "\r\nIt‚Äôs an app that allows you to quickly search for People Analytics topics in a database of almost 260 AIHR blog posts and get key ideas and insights from the posts that best match your search topic, with the possibility to go to the original post via a provided link.\r\n\r\nThe cluster analysis revealed that the posts cover more than 30 topics, so there‚Äôs a lot to choose from. Feel free to give it a try and let me know how it works for you. Here‚Äôs a link to the app.\r\nLast but not least, a big ‚Äúthank you‚Äù goes to AIHR and all the contributing authors for putting together such an amazing resource on People Analytics üôè\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-11-searching-and-querying-aihr-blog-posts/./search.jpg",
    "last_modified": "2023-07-11T21:08:22+02:00",
    "input_file": "searching-and-querying-aihr-blog-posts.knit.md"
  },
  {
    "path": "posts/2023-07-03-bayesian-shrinkage/",
    "title": "Using Bayesian shrinkage in reporting employee turnover",
    "description": "When you report turnover rates by team, do you take into account the size of individual teams, or do you take the turnover rate numbers as they are?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-03",
    "categories": [
      "employee turnover",
      "hr reporting",
      "hr metrics",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nIt‚Äôs a no-brainer that when comparing team performance on retention, the departure of one person in small teams with fewer members will have a more significant impact on turnover rates than it does in larger teams. However, this creates room for potential misinterpretation and inadequate actions.\r\nIt‚Äôs related to the well-known phenomenon of variation being more likely in smaller samples. Fans of Daniel Kahneman and Amos Tversky will probably recall the famous cognitive bias of insensitivity to sample size which occurs when people judge the probability of obtaining a statistic regardless of sample size.\r\nOne way to reduce this effect is Bayesian shrinkage. This approach, which serves as a kind of regularization, involves borrowing information from the overall company turnover rate to influence the turnover rate of smaller teams. It works by ‚Äúshrinking‚Äù the turnover rate of smaller teams towards the company average, and thus creating a balance between the observed rate and the company average.\r\nIt‚Äôs not dissimilar to what one intuitively does when deciding what movie to watch or what restaurant to go to, when movies and restaurants vary widely in the number of ratings available.\r\nYou can see this approach in action on the chart below. The smaller the team and the further its turnover rate is from the company-wide turnover rate (the vertical dashed line), the more the turnover rate estimate for that team is shifted towards the company-wide value (the distance between the red cross and the black dot) - see, for example, teams 4 and 6. For comparison, check teams 12 and 9 that don‚Äôt show much of a shrinkage effect due to the big size and small distance from the company-wide turnover rate, respectively.\r\n\r\n\r\nShow code\r\n\r\n# uploading the necessary libraries\r\nlibrary(tidyverse) # data manipulation and dataviz\r\nlibrary(brms) # bayesian stats\r\nlibrary(cmdstanr) # bayesian stats\r\nlibrary(ggdist) # dataviz\r\n\r\n# creating artificial data\r\n# setting a seed for reproducibility\r\nset.seed(123)\r\n# number of teams\r\nnTeams <- 12\r\n# generating team sizes ranging from 10 to 100\r\nteamSizes <- sample(10:100, nTeams, replace = TRUE)\r\n# generating 'true' turnover rates from a beta distribution\r\ntrueRates <- rbeta(nTeams, 2, 10)\r\n# for each team, simulating the number of employees who left\r\nnumberLeft <- rbinom(nTeams, teamSizes, trueRates)\r\n# generating team IDs\r\nteamId <- as.character(1:nTeams)\r\n# creating the data frame\r\nteamsData <- data.frame(teamId, teamSizes, numberLeft)\r\n\r\n# fitting multilevel Bayesian logistic regression with wide, uninformative priors\r\nmodel <- brms::brm(\r\n  numberLeft | trials(teamSizes) ~ 1 + (1 | teamId),\r\n  data = teamsData,\r\n  family = binomial(link = \"logit\"),\r\n  prior = prior(normal(0, 10), class = \"Intercept\"),\r\n  chains = 4,\r\n  iter = 5000,\r\n  control = list(adapt_delta = 0.95),\r\n  seed = 123,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n# model summary\r\n#summary(model)\r\n#fixef(model)\r\n#ranef(model)\r\n\r\n# extracting the posterior samples\r\n#parnames(model)\r\nposteriorDraws <- as_draws_df(model, variable = \"Intercept\", regex = TRUE) %>%\r\n  dplyr::select(-.draw, -.chain, -.iteration, -sd_teamId__Intercept) %>%\r\n  tidyr::pivot_longer(cols = -b_Intercept, names_to = \"teamId\", values_to = \"paramValues\") %>%\r\n  dplyr::mutate(\r\n    teamId = stringr::str_extract(teamId, \"\\\\d+\"),\r\n    team = stringr::str_glue(\"Team {teamId}\"),\r\n    logOdds = paramValues + b_Intercept,\r\n    estimatedTR = exp(logOdds) / (1 + exp(logOdds))\r\n    ) %>%\r\n  dplyr::left_join(teamsData %>% dplyr::select(teamId, teamSizes), by = \"teamId\") %>%\r\n  dplyr::mutate(\r\n    team = stringr::str_glue(\"{team} (n={teamSizes})\"),\r\n    team = forcats::fct_reorder(factor(team), as.numeric(teamId))\r\n    )\r\n\r\n# computing fixed, population-level effect estimate\r\nfixedEffect <- 1 / (1 + exp(-1*fixef(model)[1]))\r\n\r\n# computing observed turnover rate by team\r\nobservedRT <- teamsData %>% \r\n  dplyr::mutate(team = stringr::str_glue(\"Team {teamId}\")) %>%\r\n  dplyr::mutate(\r\n    observedRT = numberLeft/teamSizes,\r\n    team = stringr::str_glue(\"{team} (n={teamSizes})\"),\r\n    team = forcats::fct_reorder(factor(team), as.numeric(teamId))\r\n  )\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# dataviz\r\nggplot2::ggplot(data = posteriorDraws, aes(x=estimatedTR, group = team)) + \r\n  ggdist::stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\", fill = \"skyblue\", normalize = \"groups\") +\r\n  ggplot2::geom_point(data = observedRT, aes(x = observedRT, y = 0, group = team), color = \"red\", inherit.aes = F, size = 2.5, shape=3, stroke = 1.5) +\r\n  ggplot2::geom_vline(xintercept = fixedEffect, linetype = \"dashed\", color = \"#2C2F46\") +\r\n  ggplot2::facet_wrap(~team, ncol = 2) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,1,0.1)) +\r\n  ggplot2::labs(\r\n    y = \"NORMALIZED DENSITY\",\r\n    x = \"ESTIMATED TURNOVER RATE\",\r\n    title = \"Using Bayesian shrinkage in reporting employee turnover\",\r\n    caption = \"\\nThe black solid lines represent the 80% and 95% credibility intervals, respectively. The black dot represents the median of the Highest Confidence Interval.\\nThe vertical dashed line represents the fixed, population-level effect estimate. The red cross represents the observed turnover rate for a given team.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 1),\r\n    axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 9),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nIf you are trying to deal with this effect in your reporting practice, can you share the approach you use and serves you well?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-03-bayesian-shrinkage/./shrinkage.png",
    "last_modified": "2023-07-04T08:15:40+02:00",
    "input_file": "bayesian-shrinkage.knit.md",
    "preview_width": 421,
    "preview_height": 349
  },
  {
    "path": "posts/2023-06-28-job-comparator/",
    "title": "A bet on a new job",
    "description": "Sharing a by-product of my search for a new full-time job.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-29",
    "categories": [
      "job selection",
      "bayesian statistics",
      "r",
      "shiny"
    ],
    "contents": "\r\nBeing in the final phase of my new job search, I wanted to be able to aggregate the information and impressions I gathered during the hiring process, with all the uncertainties, to make the best decision possible.\r\nTo do this, I put together a ‚Äúback-of-the-envelope‚Äù calculation that combines, in a Bayesian way, the impressions one has of various aspects of the jobs one is applying for.\r\n\r\nIt works with several factors that research suggests are related to job satisfaction and that a person has the chance to estimate subjectively to some degree during the hiring process from job ads, interviews, sample tasks, company reviews from current or former employees on Glassdoor, etc. Specifically, it takes into account the following factors:\r\nSalary\r\nJob security\r\nWork-life balance\r\nCareer progression\r\nOrganizational culture\r\nJob content\r\nBenefits\r\nRelationships with supervisors\r\nRelationships with colleagues\r\nOne‚Äôs task is to simply determine, based on the available information, the range of how much he or she can expect to be satisfied with these factors in a given job. The app then aggregates the evidence and estimates the expected overall level of job satisfaction, including a level of uncertainty that can provide a guide as to where the person should try to obtain some additional information to reduce this uncertainty. One can also adjust the weights of each factor based on one‚Äôs personal preferences.\r\nIf you have at least two job offers to choose from, you may find the app as useful as I did. Here‚Äôs a link to the app.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-28-job-comparator/./decisionMaking.jpg",
    "last_modified": "2023-06-30T06:58:24+02:00",
    "input_file": "job-comparator.knit.md"
  },
  {
    "path": "posts/2023-06-27-deloitte-hc-trends-themes/",
    "title": "Themes in Deloitte's Global Human Capital Trends between 2011 and 2023",
    "description": "Reading the latest release of Deloitte Global HC Trends made me wonder what common themes this regular series has been covering throughout its 12 years long history.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-27",
    "categories": [
      "global hc trends",
      "deloitte",
      "openai",
      "r",
      "python"
    ],
    "contents": "\r\nAside from satisfying a simple curiosity, it was also a good opportunity to try out a nerdy combination of various cool DS tools: openAI‚Äôs embeddings for determining trends similarity, UMAP for dimensionality reduction, DBSCAN for cluster analysis, openAI‚Äôs chat completion for cluster summarization and naming, Plotly for interactive dataviz, Shiny for dashboarding, and Python and R for orchestrating it all.\r\nThe result? The analysis revealed 13 distinct themes among the 118 specific trends:\r\nGlobal Talent Management Strategies (23)\r\nLeadership Development and Talent Management (16)\r\nHR Transformation and Innovation (15)\r\nHuman Capital and Workforce Strategies (12)\r\nWorkforce Data and Analytics (10)\r\nCognitive Technologies and Workforce (8)\r\nEmployee-Centric Learning and Development (7)\r\nPerformance Management and Compensation (7)\r\nImproving Employee Experience and Well-being (6)\r\nCloud Computing and HR Transformation (4)\r\nEmployee Engagement and Retention (4)\r\nDiversity in Business Strategy (3)\r\nWorkplace Flexibility Strategies (3)\r\n\r\nIt‚Äôs no wonder I‚Äôve had dejavu feelings about some trends over the years, but that‚Äôs why they are called trends, because they persist over time, right? üòâ\r\nIf you would like to check the analysis output interactively and in greater detail, you can use this simple dashboard.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-27-deloitte-hc-trends-themes/./dttHCTrends.png",
    "last_modified": "2023-06-27T11:10:07+02:00",
    "input_file": "deloitte-hc-trends-themes.knit.md",
    "preview_width": 1000,
    "preview_height": 1000
  },
  {
    "path": "posts/2023-06-22-team-level-predictors-of-innovation/",
    "title": "Team-level predictors of innovation at work",
    "description": "Team processes seem to beat team composition and structure when it comes to innovation at work.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-22",
    "categories": [
      "innovation",
      "team",
      "meta-analysis",
      "people analytics"
    ],
    "contents": "\r\nAt least this is suggested by an interesting meta-analysis of team-level predictors of innovation at work by H√ºlsheger, Anderson, & Salgado (2009).\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data with the results of the meta-analysis \r\ndata <- readxl::read_xlsx(\"./metaAnalysisResults.xlsx\")\r\n#dplyr::glimpse(data)\r\n\r\ndata %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(variable, rho), y = rho, group = area, color = area)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_errorbar(aes(ymin=l95, ymax=h95), width=.2, position=position_dodge(0.05), linewidth = 1) +\r\n  ggplot2::geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_color_manual(values = c(\"Team composition and structure\"=\"#4e79a7\", \"Team process\" = \"#f28e2b\")) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"AVERAGE CORRECTED CORRELATION\",\r\n    title = \"Team-Level Predictors of Innovation at Work\",\r\n    caption = \"\\nThe bars around the point estimates represent the 2.5% lower and 97.5% upper limits of the 95% confidence interval.\\nSource: H√ºlsheger, U. R., Anderson, N., & Salgado, J. F. (2009). Team-level predictors of innovation at work: A comprehensive meta-analysis spanning three decades\\nof research. Journal of Applied Psychology, 94(5), 1128‚Äì1145. https://doi.org/10.1037/a0015978\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,10,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 10, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"top\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.text = element_text(size = 12),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nIf that‚Äôs true, not sure whether it‚Äôs good news or bad news for companies‚Äô innovation initiatives. Is it easier to change processes or team composition? I expect there will be a lot of ‚Äúit depends‚Äù üòâ What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-22-team-level-predictors-of-innovation/./innovation.jpg",
    "last_modified": "2023-06-22T13:09:06+02:00",
    "input_file": "team-level-predictors-of-innovation.knit.md"
  },
  {
    "path": "posts/2023-06-19-latent-class-analysis/",
    "title": "Latent Class Analysis of responses from employee surveys",
    "description": "How listening to a podcast about conspiracies and disinformation inspired me to try out a \"new\" statistical tool popular among sociologists.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-19",
    "categories": [
      "latent class analysis",
      "employee survey",
      "people analytics",
      "r"
    ],
    "contents": "\r\nI recently listened to a podcast about a very interesting study about conspiracies and disinformation in Czech society, and one of the authors of the study, Matous Pilnacek, a sociologist, spoke very enthusiastically and positively during the interview about the analytical possibilities offered by Latent Class Analysis (LCA).\r\nNot being a sociologist, among whom this tool is well-known, I was quite easily impressed and hooked üôÇ LCA allows probabilistic modeling of multivariate categorical data with the assumption that there are latent classes of people who are characterized by a common pattern of probabilities of responses to a set of questions on some categorical, e.g.¬†Likert scale.\r\nLCA is thus a natural fit for identifying subgroups of people with similar work views. Compared to other methods used in this context, such as Factor Analysis, k-means, or hierarchical clustering, it has several advantages:\r\nLCA can handle well categorical observed variables.\r\nLCA estimates probabilities of class membership, so it inherently incorporates uncertainty about which class each individual belongs to.\r\nLCA allows for meaningful analysis of missing answers or non-answers together with proper answers on a Likert scale.\r\nLCA provides a framework (via information criteria like BIC, AIC, or likelihood ratio tests) for comparing models with different numbers of classes.\r\nLCA‚Äôs output is intuitive and easy to interpret.\r\nWhat follows, is a a small demonstration of this tool on artificial employee survey data accompanying the book Predictive HR Analytics: Mastering the HR Metric by Edwards & Edwards (2019). It contains the survey responses of 832 employees on a 1 ‚Äòstrongly disagree‚Äô to 5 ‚Äòstrongly agree‚Äô response scale for a following set of statements.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\n\r\n# uploading legend to the data\r\nlegend <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Legend\") \r\n\r\n# user-friendly table with individual survey items\r\nDT::datatable(\r\n  legend %>% dplyr::mutate(Scale = as.factor(Scale)),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nBefore moving on to modeling, we first need to wrangle the data a bit - select the relevant variables, change their data type, and replace missing values with ‚ÄúPrefer Not to Say‚Äù reply category.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ndata <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Data\")\r\n\r\n# preparing the data for modeling\r\nmydata <- data %>%\r\n  dplyr::select(-sex:-ethnicity) %>%\r\n  dplyr::mutate_all(as.character) %>%\r\n  replace(is.na(.), \"Prefer Not to Say\") %>%\r\n  dplyr::mutate_all(factor, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"Prefer Not to Say\"))\r\n\r\n\r\nNow we can proceed with the modeling. For this we will use poLCA R package. One of the parameters to be set is the expected number of classes. To choose the right number, we need to fit several LCA models with different numbers of classes and, based on information criteria such as BIC or AIC, choose the model with the best balance between model complexity and good fit to the data. Here, I set the parameter to the best value I determined earlier.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(poLCA)\r\n\r\n# specifying and running the model\r\nset.seed(1234)\r\nlca_model <- poLCA::poLCA(\r\n  cbind(ManMot1, ManMot2, ManMot3, ManMot4, ocb1, ocb2, ocb3, ocb4, aut1, aut2, aut3, Justice1, Justice2, Justice3, JobSat1, JobSat2, Quit1, Quit2, Quit3, Man1, Man2, Man3, Eng1, Eng2, Eng3, Eng4, pos1, pos2, pos3)~1, \r\n  data = mydata, \r\n  nclass = 4, \r\n  nrep = 3,\r\n  verbose = FALSE,\r\n  graphs = FALSE\r\n)\r\n\r\n\r\nLet‚Äôs check some of the outputs of the analysis: 1) probability of responses to individual items by people from different classes,\r\n\r\n\r\nShow code\r\n\r\n# looking at the model output\r\nlca_model\r\n\r\nConditional item response (column) probabilities,\r\n by outcome variable, for each class (row) \r\n \r\n$ManMot1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1350 0.4245 0.3092 0.0875 0.0227 0.0211\r\nclass 2:  0.0732 0.1220 0.1463 0.0244 0.0000 0.6341\r\nclass 3:  0.0906 0.1342 0.2384 0.2976 0.2021 0.0371\r\nclass 4:  0.4765 0.3591 0.1274 0.0121 0.0132 0.0118\r\n\r\n$ManMot2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.3345 0.4531 0.1377 0.0534 0.0081 0.0132\r\nclass 2:  0.1220 0.1463 0.0976 0.0244 0.0000 0.6098\r\nclass 3:  0.2274 0.2580 0.2239 0.1508 0.1033 0.0366\r\nclass 4:  0.7507 0.1967 0.0325 0.0080 0.0121 0.0000\r\n\r\n$ManMot3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1180 0.3715 0.3610 0.1077 0.0209 0.0209\r\nclass 2:  0.0732 0.0732 0.2195 0.0000 0.0000 0.6341\r\nclass 3:  0.1005 0.1045 0.2517 0.3043 0.2019 0.0371\r\nclass 4:  0.4354 0.3590 0.1482 0.0252 0.0201 0.0121\r\n\r\n$ManMot4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1965 0.4915 0.1830 0.0853 0.0225 0.0212\r\nclass 2:  0.0732 0.1463 0.0976 0.0488 0.0000 0.6341\r\nclass 3:  0.1437 0.1539 0.2503 0.2418 0.1798 0.0305\r\nclass 4:  0.5329 0.3401 0.1070 0.0000 0.0121 0.0078\r\n\r\n$ocb1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.3092 0.4552 0.1880 0.0344 0.0026 0.0105\r\nclass 2:  0.1463 0.1707 0.0732 0.0000 0.0000 0.6098\r\nclass 3:  0.4200 0.3895 0.1463 0.0135 0.0122 0.0185\r\nclass 4:  0.6336 0.2680 0.0592 0.0272 0.0000 0.0120\r\n\r\n$ocb2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1645 0.4304 0.3103 0.0577 0.0131 0.0240\r\nclass 2:  0.0732 0.1707 0.1463 0.0000 0.0000 0.6098\r\nclass 3:  0.2486 0.4233 0.2233 0.0617 0.0246 0.0185\r\nclass 4:  0.3427 0.3929 0.1963 0.0403 0.0121 0.0156\r\n\r\n$ocb3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1373 0.3711 0.3566 0.0869 0.0293 0.0188\r\nclass 2:  0.0732 0.1707 0.0732 0.0488 0.0244 0.6098\r\nclass 3:  0.2291 0.3319 0.2798 0.0853 0.0493 0.0246\r\nclass 4:  0.3206 0.3222 0.2333 0.0647 0.0396 0.0196\r\n\r\n$ocb4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0971 0.2143 0.2582 0.2185 0.1988 0.0131\r\nclass 2:  0.0732 0.1220 0.0000 0.0976 0.0732 0.6341\r\nclass 3:  0.2431 0.1670 0.1081 0.1557 0.2955 0.0307\r\nclass 4:  0.3205 0.1708 0.1708 0.1759 0.1459 0.0162\r\n\r\n$aut1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5) Pr(6)\r\nclass 1:  0.1914 0.6010 0.1600 0.0476 0.0000     0\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000     1\r\nclass 3:  0.3128 0.3342 0.1095 0.1765 0.0671     0\r\nclass 4:  0.5015 0.4326 0.0659 0.0000 0.0000     0\r\n\r\n$aut2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5) Pr(6)\r\nclass 1:  0.0892 0.5412 0.2574 0.1121 0.0000     0\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000     1\r\nclass 3:  0.1521 0.3402 0.1754 0.2164 0.1159     0\r\nclass 4:  0.4131 0.4234 0.1476 0.0160 0.0000     0\r\n\r\n$aut3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1199 0.5929 0.2172 0.0674 0.0025 0.0000\r\nclass 2:  0.0000 0.0000 0.0244 0.0000 0.0000 0.9756\r\nclass 3:  0.2286 0.3560 0.1749 0.1426 0.0918 0.0061\r\nclass 4:  0.4325 0.4548 0.0964 0.0082 0.0000 0.0081\r\n\r\n$Justice1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0782 0.7351 0.1756 0.0110 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0061 0.0190 0.2754 0.3353 0.3398 0.0245\r\nclass 4:  0.0526 0.3241 0.5002 0.0742 0.0247 0.0242\r\n\r\n$Justice2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0759 0.7355 0.1788 0.0099 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0061 0.0190 0.2458 0.3396 0.3528 0.0367\r\nclass 4:  0.0445 0.2954 0.5556 0.0463 0.0380 0.0202\r\n\r\n$Justice3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0332 0.6473 0.2877 0.0318 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0000 0.0305 0.2225 0.3209 0.3956 0.0305\r\nclass 4:  0.0404 0.2239 0.5407 0.1179 0.0569 0.0202\r\n\r\n$JobSat1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0749 0.7008 0.1795 0.0298 0.0053 0.0097\r\nclass 2:  0.0244 0.0000 0.0000 0.0000 0.0000 0.9756\r\nclass 3:  0.1355 0.3993 0.1967 0.1263 0.1220 0.0202\r\nclass 4:  0.4703 0.4749 0.0267 0.0242 0.0000 0.0041\r\n\r\n$JobSat2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0527 0.6334 0.2540 0.0546 0.0000 0.0053\r\nclass 2:  0.0244 0.0000 0.0000 0.0000 0.0000 0.9756\r\nclass 3:  0.1100 0.2836 0.2498 0.2345 0.1037 0.0183\r\nclass 4:  0.3717 0.5498 0.0388 0.0356 0.0000 0.0040\r\n\r\n$Quit1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0086 0.0503 0.3622 0.5410 0.0193 0.0185\r\nclass 2:  0.0488 0.0000 0.1463 0.1707 0.0244 0.6098\r\nclass 3:  0.1476 0.2228 0.1411 0.2832 0.1870 0.0182\r\nclass 4:  0.0103 0.0176 0.0660 0.3442 0.5619 0.0000\r\n\r\n$Quit2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0076 0.1285 0.2897 0.5154 0.0378 0.0210\r\nclass 2:  0.0732 0.0732 0.0732 0.1463 0.0488 0.5854\r\nclass 3:  0.1804 0.2770 0.1514 0.2373 0.1476 0.0063\r\nclass 4:  0.0103 0.0356 0.0533 0.3251 0.5758 0.0000\r\n\r\n$Quit3\r\n           Pr(1)  Pr(2) Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0065 0.0874 0.365 0.5037 0.0137 0.0237\r\nclass 2:  0.0244 0.0488 0.122 0.1463 0.0488 0.6098\r\nclass 3:  0.1862 0.2099 0.159 0.2557 0.1649 0.0243\r\nclass 4:  0.0081 0.0179 0.058 0.3631 0.5529 0.0000\r\n\r\n$Man1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0161 0.4123 0.4782 0.0434 0.0026 0.0474\r\nclass 2:  0.0244 0.2683 0.0732 0.0244 0.0000 0.6098\r\nclass 3:  0.0183 0.1371 0.4832 0.2147 0.1404 0.0062\r\nclass 4:  0.2745 0.5092 0.1949 0.0094 0.0000 0.0121\r\n\r\n$Man2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0187 0.3700 0.4954 0.0661 0.0051 0.0448\r\nclass 2:  0.0244 0.2683 0.0244 0.0732 0.0000 0.6098\r\nclass 3:  0.0244 0.1232 0.4177 0.2329 0.1956 0.0062\r\nclass 4:  0.2381 0.5147 0.1917 0.0354 0.0081 0.0121\r\n\r\n$Man3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0213 0.3718 0.5068 0.0536 0.0044 0.0422\r\nclass 2:  0.0244 0.2439 0.0732 0.0488 0.0000 0.6098\r\nclass 3:  0.0244 0.0998 0.4262 0.2522 0.1913 0.0062\r\nclass 4:  0.2341 0.5234 0.1969 0.0255 0.0040 0.0160\r\n\r\n$Eng1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0100 0.3362 0.4682 0.1654 0.0158 0.0044\r\nclass 2:  0.0000 0.1463 0.1463 0.0732 0.0488 0.5854\r\nclass 3:  0.1709 0.2392 0.1777 0.2413 0.1526 0.0183\r\nclass 4:  0.2432 0.4776 0.2106 0.0551 0.0040 0.0094\r\n\r\n$Eng2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0406 0.4844 0.4136 0.0581 0.0000 0.0032\r\nclass 2:  0.0732 0.1220 0.1463 0.0244 0.0488 0.5854\r\nclass 3:  0.2601 0.2988 0.1498 0.1815 0.1098 0.0000\r\nclass 4:  0.3312 0.5015 0.1391 0.0088 0.0000 0.0193\r\n\r\n$Eng3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0043 0.4245 0.5074 0.0572 0.0000 0.0065\r\nclass 2:  0.0244 0.0976 0.1463 0.0732 0.0732 0.5854\r\nclass 3:  0.1887 0.2142 0.2704 0.1802 0.1403 0.0061\r\nclass 4:  0.2442 0.5647 0.1576 0.0151 0.0000 0.0183\r\n\r\n$Eng4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0026 0.3724 0.5065 0.1117 0.0026 0.0044\r\nclass 2:  0.0244 0.1463 0.1220 0.0732 0.0488 0.5854\r\nclass 3:  0.2015 0.1987 0.2597 0.2117 0.1283 0.0000\r\nclass 4:  0.2870 0.5095 0.1824 0.0117 0.0000 0.0095\r\n\r\n$pos1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.1046 0.6481 0.2241 0.0073 0.0160\r\nclass 2:  0.0000 0.0488 0.2195 0.0488 0.0732 0.6098\r\nclass 3:  0.0305 0.0479 0.1663 0.4436 0.3117 0.0000\r\nclass 4:  0.0728 0.2523 0.4393 0.1949 0.0127 0.0280\r\n\r\n$pos2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0023 0.1124 0.6717 0.1952 0.0024 0.0159\r\nclass 2:  0.0244 0.0488 0.2439 0.0488 0.0244 0.6098\r\nclass 3:  0.0549 0.0313 0.2471 0.3795 0.2812 0.0061\r\nclass 4:  0.0289 0.2999 0.4990 0.1482 0.0000 0.0240\r\n\r\n$pos3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0608 0.6749 0.2278 0.0205 0.0159\r\nclass 2:  0.0244 0.0000 0.2927 0.0244 0.0488 0.6098\r\nclass 3:  0.0488 0.0180 0.2200 0.3711 0.3421 0.0000\r\nclass 4:  0.0202 0.2505 0.5242 0.1766 0.0086 0.0200\r\n\r\nEstimated class population shares \r\n 0.4564 0.0493 0.197 0.2973 \r\n \r\nPredicted class memberships (by modal posterior prob.) \r\n 0.4639 0.0493 0.1947 0.2921 \r\n \r\n========================================================= \r\nFit for 4 latent classes: \r\n========================================================= \r\nnumber of observations: 832 \r\nnumber of estimated parameters: 583 \r\nresidual degrees of freedom: 249 \r\nmaximum log-likelihood: -29354.91 \r\n \r\nAIC(4): 59875.81\r\nBIC(4): 62629.81\r\nG^2(4): 47676.67 (Likelihood ratio/deviance statistic) \r\nX^2(4): 1.120338e+25 (Chi-square goodness of fit) \r\n \r\n\r\nprobabilities of people belonging to individual classes,\r\n\r\n\r\nShow code\r\n\r\n# probabilities of belonging to individual classes (first 10 rows)\r\nhead(lca_model$posterior, n = 10)\r\n\r\n              [,1] [,2]         [,3]          [,4]\r\n [1,] 1.705863e-01    0 1.088439e-08  8.294137e-01\r\n [2,] 1.208337e-02    0 9.879166e-01  0.000000e+00\r\n [3,] 4.990889e-13    0 1.000000e+00  0.000000e+00\r\n [4,] 9.999903e-01    0 9.649316e-06  6.879833e-09\r\n [5,] 0.000000e+00    1 0.000000e+00  0.000000e+00\r\n [6,] 9.988548e-01    0 3.805196e-06  1.141380e-03\r\n [7,] 1.782575e-13    0 1.000000e+00  0.000000e+00\r\n [8,] 0.000000e+00    0 1.000000e+00  0.000000e+00\r\n [9,] 1.272725e-04    0 1.104259e-11  9.998727e-01\r\n[10,] 1.064019e-09    0 1.000000e+00 1.215934e-244\r\n\r\nand 3) predicted belongings of people to the classes.\r\n\r\n\r\nShow code\r\n\r\n# predicted belongings to the classes\r\nlca_model$predclass\r\n\r\n  [1] 4 3 3 1 2 1 3 3 4 3 1 1 1 4 4 1 1 1 4 1 1 1 3 4 2 3 4 1 4 3 1 4\r\n [33] 3 1 4 3 3 1 3 3 4 4 1 1 3 1 4 1 1 3 1 3 4 1 1 2 1 4 3 1 3 1 4 3\r\n [65] 4 1 4 3 4 1 1 1 4 3 2 4 1 1 4 1 3 3 1 4 1 4 1 3 4 4 4 4 1 1 1 4\r\n [97] 4 1 3 1 3 3 1 4 1 4 3 3 1 2 1 1 4 1 4 1 4 1 4 4 1 4 3 1 4 1 1 3\r\n[129] 1 1 4 4 4 4 4 1 4 3 1 1 3 1 1 1 1 1 4 3 1 3 4 1 4 3 1 1 1 1 3 3\r\n[161] 1 1 1 4 1 1 3 1 1 1 4 3 3 1 1 1 1 1 1 3 1 3 1 1 4 4 3 4 1 4 3 3\r\n[193] 3 3 4 1 4 2 3 4 4 3 1 4 1 4 4 4 4 3 3 1 4 1 4 3 1 3 4 1 1 4 4 1\r\n[225] 4 1 4 4 1 4 4 4 3 4 1 1 4 1 3 1 4 1 4 1 3 4 3 1 1 1 4 1 1 1 4 1\r\n[257] 4 1 1 1 1 3 1 1 4 1 1 1 4 1 1 1 1 1 3 4 4 1 1 4 4 3 4 3 1 3 1 3\r\n[289] 1 2 4 4 3 4 1 4 4 1 1 1 1 1 4 1 4 3 1 1 4 1 1 3 1 4 1 4 1 4 1 4\r\n[321] 4 4 1 4 1 4 3 4 4 1 1 4 4 2 4 1 1 1 1 4 1 4 3 1 4 4 4 1 1 4 3 2\r\n[353] 4 1 4 3 3 1 4 4 3 2 4 1 1 1 4 1 2 1 1 4 1 1 1 1 4 1 1 1 1 2 4 1\r\n[385] 1 1 2 1 1 3 4 1 1 3 1 4 1 1 1 2 1 1 4 1 3 2 1 1 3 1 4 4 1 1 1 1\r\n[417] 1 1 4 4 4 1 1 3 1 1 4 3 1 1 1 1 1 4 1 2 1 4 4 1 1 1 1 1 2 1 4 2\r\n[449] 1 4 1 3 1 1 3 4 3 1 1 3 4 4 4 4 4 3 1 4 1 1 3 3 3 1 1 3 1 4 3 1\r\n[481] 1 1 1 4 4 2 1 3 1 1 1 4 4 1 3 1 1 3 1 1 1 1 3 4 4 4 3 1 1 1 4 3\r\n[513] 4 1 1 4 1 1 1 4 2 1 4 1 4 1 3 1 3 2 4 2 3 4 1 1 4 4 4 1 3 1 4 1\r\n[545] 1 1 4 3 1 4 4 4 1 3 1 2 1 1 3 4 2 3 2 1 1 1 1 4 4 2 1 3 1 4 1 3\r\n[577] 4 3 3 4 1 4 1 2 4 3 1 1 1 1 1 1 3 4 4 1 4 1 1 4 1 1 1 4 1 4 1 1\r\n[609] 3 1 3 1 1 3 4 4 3 1 4 1 3 4 1 1 4 1 4 4 4 1 3 4 2 4 1 1 1 4 1 4\r\n[641] 1 1 1 1 1 3 1 3 1 1 3 1 1 4 1 4 3 1 4 1 2 2 1 3 4 3 1 4 2 3 4 1\r\n[673] 3 3 4 3 1 1 1 1 4 1 1 3 1 3 1 1 3 1 1 1 1 1 1 3 2 1 1 1 4 1 4 4\r\n[705] 4 1 1 1 1 1 1 3 4 3 4 1 3 1 1 1 2 3 3 3 3 3 4 3 1 1 1 3 3 1 3 2\r\n[737] 1 2 3 1 4 1 4 2 3 1 1 4 3 1 3 4 3 4 1 4 1 1 3 4 4 4 3 1 1 2 1 1\r\n[769] 1 1 4 3 3 4 4 1 3 4 2 4 4 3 3 1 3 1 2 4 1 1 3 1 4 3 4 1 1 1 4 1\r\n[801] 2 1 4 1 4 4 4 1 1 4 1 4 4 4 2 4 4 3 4 1 1 3 1 3 3 1 1 4 4 3 3 1\r\n\r\nShow code\r\n\r\n# checking the size of the classes\r\ntable(lca_model$predclass)\r\n\r\n\r\n  1   2   3   4 \r\n386  41 162 243 \r\n\r\nIf we wanted to visualize the results using our own charts, for example, with full-stacked bar charts showing average probability of responses per scale and class, we need to do some data wrangling of information extracted from the fitted model.\r\n\r\n\r\nShow code\r\n\r\n# extracting information from the model for dataviz\r\nscaleNames <- mydata %>% names()\r\nclasses <- 4\r\nresponsesDf <- data.frame()\r\n\r\nfor(s in scaleNames){\r\n  \r\n  df <- data.frame()\r\n  \r\n  counter <- 1\r\n  \r\n  for(v in 1:length(lca_model$probs[[s]])){\r\n    \r\n    p <- lca_model$probs[[s]][v]\r\n    cl <- paste0(\"Class \", counter)\r\n    \r\n    supp <- data.frame(item = s, class = cl, p = p)\r\n    \r\n    df <- rbind(df, supp)\r\n    \r\n    if(counter < classes){\r\n      \r\n      counter <- counter + 1\r\n      \r\n    } else{\r\n      \r\n      counter <- 1\r\n      \r\n    }\r\n    \r\n  }\r\n  \r\n  df <- df %>%\r\n    dplyr::mutate(choice = rep(c(\"Strongly Disagree\",\"Disagree\",\"Neither Agree nor Disagree\",\"Agree\",\"Strongly Agree\",\"Prefer Not to Say\"), each=classes))\r\n  \r\n  responsesDf <- rbind(responsesDf, df)\r\n  \r\n}\r\n\r\n# computing size of individual classes\r\nclassCnts <- table(lca_model$predclass) %>% \r\n  as.data.frame() %>%\r\n  dplyr::mutate(\r\n    Var1 = as.character(Var1),\r\n    Var1 =  paste0(\"Class \", Var1)\r\n  ) %>%\r\n  dplyr::rename(\r\n    class = Var1,\r\n    freq = Freq\r\n    )\r\n\r\n# final df for dataviz\r\ndatavizDf <- responsesDf %>%\r\n  dplyr::mutate(\r\n    scale = stringr::str_remove_all(item, \"\\\\d\"),\r\n    choice = factor(choice, levels = c(\"Strongly Disagree\",\"Disagree\",\"Neither Agree nor Disagree\",\"Agree\",\"Strongly Agree\",\"Prefer Not to Say\"), ordered = TRUE),\r\n    scale = case_when(\r\n      scale == \"aut\" ~ \"Autonomy\",\r\n      scale == \"Eng\" ~ \"Engagement\",\r\n      scale == \"JobSat\" ~ \"Job Satisfaction\",\r\n      scale == \"Justice\" ~ \"Proc.Justice\",\r\n      scale == \"Man\" ~ \"Management\",\r\n      scale == \"ManMot\" ~ \"Mng.Motivation\",\r\n      scale == \"ocb\" ~ \"OCB\",\r\n      scale == \"pos\" ~ \"Org.Support\",\r\n      scale == \"Quit\" ~ \"Quitting Intentions\"\r\n    )\r\n    ) %>%\r\n  dplyr::group_by(scale, class, choice) %>%\r\n  dplyr::summarise(p = mean(p)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::left_join(classCnts, by = \"class\") %>%\r\n  dplyr::mutate(class = stringr::str_glue(\"{class} (n = {freq})\"))\r\n\r\n\r\nIn the resulting graphs, we quickly see that there is one class of people who often prefer not to give their opinion in an employee survey (Class 2), one class of people with a strongly negative view of their employment experience (Class 4), one class of people with a neutral to slightly negative view of work (Class 1), and one class of people with a neutral to positive view of work (Class 3).\r\n\r\n\r\nShow code\r\n\r\n# dataviz\r\ndatavizDf %>%\r\n  ggplot2::ggplot(aes(x = scale, y = p, fill = choice)) +\r\n  ggplot2::scale_x_discrete(limits = rev) +\r\n  ggplot2::geom_bar(stat = \"identity\", position = position_fill(reverse = TRUE)) +\r\n  ggplot2::scale_fill_manual(values = c(\"Strongly Disagree\"=\"#c00000\",\"Disagree\"=\"#ed7d31\",\"Neither Agree nor Disagree\"=\"#ffc000\",\"Agree\"=\"#00b050\",\"Strongly Agree\"=\"#4472c4\",\"Prefer Not to Say\"=\"#d9d4d4\")) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::facet_wrap(~class,nrow = 2) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"AVERAGE PROBABILITY OF RESPONSE\",\r\n    fill = \"\",\r\n    title = \"Latent Class Analysis of responses from the employee survey\"\r\n    ) +\r\n  ggplot2::theme_bw() +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text = element_text(size = 12, face = \"bold\"),\r\n    strip.background = element_rect(fill = \"white\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    legend.box.margin=margin(0,0,0,0),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\",\r\n  ) +\r\n  ggplot2::guides(fill = guide_legend(nrow = 1))\r\n\r\n\r\n\r\nIt would certainly be possible to delve deeper into the results, but for a basic overview of the process of LCA and its outputs, this might be sufficient. If interested, a more detailed introduction to LCA can be found, for example, in this practitioner‚Äôs guide.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-19-latent-class-analysis/./scheme.png",
    "last_modified": "2023-06-20T07:27:31+02:00",
    "input_file": "latent-class-analysis.knit.md",
    "preview_width": 930,
    "preview_height": 525
  },
  {
    "path": "posts/2023-06-05-mindfulness-and-objectivity/",
    "title": "Another positive effect of mindfulness meditation on the horizon?",
    "description": "Is it possibile to improve the objectivity of decision making through mindfulness meditation?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-05",
    "categories": [
      "decision-making",
      "mindfulness",
      "field experiment"
    ],
    "contents": "\r\nMaybe, at least this is suggested by a pre-registered field experiment by Ash et al.¬†(2023), where people practicing mindfulness meditation for 15 minutes a day for 2 weeks compared to an active control (listening to relaxing music) showed a reduced tendency to avoid painful information that may trigger worry or regret, with a likely mechanism for this effect being improved emotion regulation.\r\nIf this were the case, this would be great news for our decision making, as the ability to impartially evaluate all relevant information is one of the keys to good decision making.\r\nIt‚Äôs true that the observed effect was rather small and barely distinguishable from noise and we may be concerned about its reproducibility, but as a Trekkie and a big fan of Mr.¬†Spock, I like the idea that there might be a method that could make us a little more like this cool half-Vulcan and half-human üòâüññ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-05-mindfulness-and-objectivity/./spock.jpg",
    "last_modified": "2023-06-05T10:57:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-05-bayesian-networks-in-people-analytics/",
    "title": "Use of Bayesian networks in people analytics?",
    "description": "Bayesian networks seem to have some interesting properties that could make them useful for various people analytics use cases, but for some reason this is not the case.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-01",
    "categories": [
      "bayesian networks",
      "probabilistic graphical models",
      "people analytics"
    ],
    "contents": "\r\nIn a people analytics project I‚Äôm involved in, we were asked to come up with a prediction model that would perform reasonably well while being very easy to understand and interpret for non-technical users.\r\nIn considering various options, we also came across Bayesian networks (BNs), probabilistic graphical models consisting of nodes and directed edges that represent conditional (and under certain assumptions, causal) relationships between random variables. They seem to have several interesting properties that would suit our needs, namely:\r\nBN models are not ‚Äúblack boxes‚Äù, but all their parameters have a comprehensible interpretation, which may reduce users‚Äô algorithm aversion and increase their willingness to take algorithm outputs into account in their decision-making.\r\nExpert knowledge can easily be used in the development of BN models by building the network structure using expert knowledge of im/plausible (causal) relationships between the variables under study.\r\nThe model lends itself to a user-friendly visualization of its structure, helping to design new models that take into account both data-based constraints and the knowledge of experts in the field.\r\nIt is possible to use the data itself to estimate the possible network topology and thus gain preliminary insight into the structure of the problem domain defined by the available variables.\r\nAfter fitting the model, it can be used for reasoning, i.e., calculating probabilities of interest conditional on the available evidence. This reasoning can be done even with incomplete data, based only on the known values of any combination of the available variables.\r\nThe attached graph is for illustrative purposes only - the output from early experiments with BNs on artificial IBM employee attrition data.\r\nHowever, when searching for information about this method, we found that it is not particularly popular among people analytics and I/O psychology practitioners. Would anyone of the readers happen to have a good or bad experience using this tool and would also be willing to share it? üôè\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-05-bayesian-networks-in-people-analytics/./plot.png",
    "last_modified": "2023-06-05T10:51:22+02:00",
    "input_file": {},
    "preview_width": 1121,
    "preview_height": 746
  },
  {
    "path": "posts/2023-05-26-selection-procedures-validity-update/",
    "title": "Visualizing shifts in validity estimates for selection procedures",
    "description": "Let's take a slopegraph perspective to assess changes in estimates of the validity of selection procedures.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-26",
    "categories": [
      "i/o psychology",
      "validity",
      "employee selection",
      "meta-analysis",
      "data visualization"
    ],
    "contents": "\r\nI suppose that many, if not the majority, of I/O psychology and people analytics folks have already heard about a new meta-analytic estimation of validity for selection procedures that is based on a more realistic range-restriction correction performed by Sackett et al.¬†(2022).\r\nNumerous articles have explored the results of this meta-analysis and its presumed implications for the hiring process. However, what I found lacking was a clear visual representation of the changes in the validity estimates, including the original article.\r\nBecause I needed one for a training I was conducting on people analytics and EB-HRM, I created one. Given the nature of the change to be shown, I chose a slopegraph that nicely and intuitively illustrates a two-point change.\r\n\r\n\r\nShow code\r\n\r\n# uploading the necessary libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(ggrepel)\r\n\r\n# uploading data\r\ndata <- readxl::read_xlsx(\"./data.xlsx\")\r\n#glimpse(data)\r\n\r\n# transforming data\r\ndataLong <- data %>%\r\n  tidyr::drop_na() %>%\r\n  tidyr::pivot_longer(Hunter:Sackett, names_to = \"analysis\", values_to = \"validity\") %>%\r\n  dplyr::mutate(analysis = case_when(\r\n    analysis == \"Hunter\" ~ \"Schmidt & Hunter (1998)\",\r\n    analysis == \"Sackett\" ~ \"Sackett et al. (2022)\",\r\n    TRUE ~ \"unknown\"\r\n    ),\r\n    analysis = factor(analysis, levels = c(\"Schmidt & Hunter (1998)\", \"Sackett et al. (2022)\"))\r\n  )\r\n\r\n# creating custom color palette based on Tableau colors\r\nmy_palette <- c(\r\n  \"#4E79A7\", \"#F28E2C\", \"#E15759\", \"#76B7B2\", \"#59A14F\",\r\n  \"#EDC949\", \"#B07AA2\", \"#FF9DA7\", \"#9C755F\", \"#BAB0AB\",\r\n  \"#2F8AC4\"  # an additional distinct color\r\n)\r\n\r\n# creating the slopegraph\r\ndataLong %>%\r\n  ggplot2::ggplot(aes(x = analysis, y = validity, group = SelectionProcedure)) +\r\n  ggplot2::geom_line(aes(color = SelectionProcedure), linewidth = 1) +\r\n  ggplot2::geom_point(aes(color = SelectionProcedure), size = 3) +\r\n  ggrepel::geom_text_repel(data = dataLong %>% filter(analysis == \"Schmidt & Hunter (1998)\"), aes(label = SelectionProcedure, color = SelectionProcedure), size = 4.5, hjust = 1.2, vjust = 0.5, direction = \"y\", force = 1) +\r\n  ggrepel::geom_text_repel(data = dataLong %>% filter(analysis == \"Sackett et al. (2022)\"), aes(label = SelectionProcedure, color = SelectionProcedure), size = 4.5, hjust = -0.2, vjust = 0.5, direction = \"y\", force = 1) +\r\n  ggplot2::scale_color_manual(values = my_palette) +\r\n  ggplot2::labs(\r\n    title = \"Comparison of employee selection procedures validity estimates\",\r\n    y = \"VALIDITY ESTIMATES\", \r\n    x = \"\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 22, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_blank(),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"none\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nMaybe the visualization will come in handy for you as well when trying to ‚Äúrewire‚Äù your long-held beliefs and assumptions. It should make clearer in what direction and to what extent to do so üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-26-selection-procedures-validity-update/selection-procedures-validity-update_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-05-26T12:52:41+02:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1440
  },
  {
    "path": "posts/2023-05-16-psychometric-network-analysis/",
    "title": "Psychometric network analysis & employee survey data",
    "description": "A demonstration of how psychometric network analysis can be used to gain insights into employee survey data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-16",
    "categories": [
      "network analysis",
      "psychometrics",
      "employee survey",
      "employee engagement",
      "employee satisfaction",
      "r"
    ],
    "contents": "\r\nIf you‚Äôre looking for an alternative to factor analysis for processing employee survey data, consider using psychometric network analysis (hereafter PNA).\r\nIt provides insight into the interdependencies between different topics related to employee experience, and the resulting network diagrams make these insights easily accessible even to non-experts.\r\nIt helps to identify key topics (nodes) that have the most connections or influence within the network, which can be valuable for identifying areas of focus.\r\nAlthough PNA doesn‚Äôt work with latent factors, common community detection algorithms (e.g., Louvain‚Äôs method) can be used to identify clusters of more densely interconnected topics.\r\nIn interpreting the PNA outputs, one can rely on the apparatus of bootstrapping statistics to distinguish signal from noise.\r\nWhat follows is a small demonstration of the use of PNA on employee survey data. For this purpose, I used a sample dataset that accompanies the book Predictive HR Analytics: Mastering the HR Metric by Edwards & Edwards (2019). It contains the survey responses of 832 employees on a 1 ‚Äòstrongly disagree‚Äô to 5 ‚Äòstrongly agree‚Äô response scale for a following set of statements.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\n\r\n# uploading legend to the data\r\nlegend <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Legend\") \r\n\r\n# user-friendly table with individual survey items\r\nDT::datatable(\r\n  legend %>% dplyr::mutate(Scale = as.factor(Scale)),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nAnd here is a table with the survey responses we will analyse.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ndata <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Data\")\r\n\r\n# selecting relevant data\r\nmydata <- data %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\n# user-friendly table with the data used in the analysis\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nWe will use the bootnet R package to estimate the regularized partial correlation network using the Spearman correlation matrix and LASSO regularization, and then select the final network using the Extended Bayesian Information Criterion (with Œ≥ parameter set to 0.5).\r\n\r\n\r\nShow code\r\n\r\n# estimating a regularized partial correlation network\r\nnetwork <- bootnet::estimateNetwork(\r\n  mydata,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE # when TRUE, enforces higher specificity, at the cost of sensitivity\r\n)\r\n\r\nprint(network)\r\n\r\n\r\n=== Estimated network ===\r\nNumber of nodes: 29 \r\nNumber of non-zero edges: 158 / 406 \r\nMean weight: 0.02974358 \r\nNetwork stored in network$graph \r\n \r\nDefault set used: EBICglasso \r\n \r\nUse plot(network) to plot estimated network \r\nUse bootnet(network) to bootstrap edge weights and centrality indices \r\n\r\nRelevant references:\r\n\r\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9 (3), 432-441.\r\n    Foygel, R., & Drton, M. (2010). Extended Bayesian information criteria for Gaussian graphical models. \r\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2014). glasso: Graphical lasso estimation of gaussian graphical models. Retrieved from https://CRAN.R-project.org/package=glasso\r\n    Epskamp, S., Cramer, A., Waldorp, L., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48 (1), 1-18.\r\n    Epskamp, S., Borsboom, D., & Fried, E. I. (2016). Estimating psychological networks and their accuracy: a tutorial paper. arXiv preprint, arXiv:1604.08462.\r\n\r\nThe above brief summary of the estimated network shows that it is a relatively dense network with 158 out of 406 possible connections (39%). Now let‚Äôs plot the network.\r\n\r\n\r\nShow code\r\n\r\n# plotting the estimated network \r\nplot(\r\n  network, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydata),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\"\r\n)\r\n\r\n\r\n\r\nWe can also plot the network in ggplot style, which can be useful when we need more control over the chart, e.g.¬†when we want to plot identified communities/clusters instead of theoretical scales. To do this, we just need to get the necessary information from the network object and do a few data manipulations.\r\n\r\n\r\nShow code\r\n\r\n# uploading \r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# extracting information about the connections form the network object\r\nngMatrix <- network$graph\r\n# inputting to upper part of the matrix zeroes (the edges between items are symmetrical, so we need just a half of the matrix)  \r\nngMatrix[upper.tri(ngMatrix)] <- 0\r\n\r\n# transforming matrix into dataframe\r\nngDf <- ngMatrix %>%\r\n  as.data.frame() %>%\r\n  tibble::rownames_to_column() %>%\r\n  dplyr::rename(itemID = rowname)\r\n\r\n# creating a dataframe with information about connections between items\r\nfromToList <- data.frame()\r\n\r\nfor(i in unique(ngDf$itemID)){\r\n  \r\n  suppDf <- ngDf %>%\r\n    dplyr::filter(itemID == i) %>%\r\n    tidyr::pivot_longer(-itemID, names_to = \"to\", values_to = \"weight\") %>%\r\n    dplyr::filter(weight != 0) %>%\r\n    dplyr::rename(from = itemID) %>%\r\n    dplyr::mutate(\r\n      sign = case_when(\r\n        weight < 0 ~ \"negative\",\r\n        weight > 0 ~ \"positive\",\r\n        TRUE ~ \"zero\"\r\n      ),\r\n      weight = abs(weight)\r\n    )\r\n  \r\n  fromToList <- dplyr::bind_rows(fromToList, suppDf)\r\n  \r\n}\r\n\r\n\r\n# creating a dataframe with information about individual items \r\nitems <- data.frame(\r\n  item = names(mydata),\r\n  scale = legend %>% pull(Scale)\r\n)\r\n\r\n# creating igraph object\r\nigraph_graph <- igraph::graph_from_data_frame(fromToList, directed=FALSE, vertices = items)\r\n\r\n# visualizing the network\r\nset.seed(123)\r\nggraph::ggraph(igraph_graph, layout = \"fr\", maxiter = 500) +  # fr, kk, drl, mds, maxiter = 500 is default\r\n  ggraph::geom_edge_link(aes(edge_width = weight, color = sign), alpha = 0.05) + \r\n  ggraph::geom_node_point(aes(color = scale), size = 5) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  ggraph::theme_graph(background = \"white\") +\r\n  ggraph::scale_edge_color_manual(values = c(\"negative\" = \"red\", \"positive\" = \"blue\")) +\r\n  ggplot2::scale_color_brewer(palette=\"Set1\")\r\n\r\n\r\n\r\nFrom the charts we can quickly gain a basic idea of the internal structure of the data. For example, we can notice that:\r\nitems from the same scales tend to cluster close to each other;\r\nthere are items from different scales that appear to measure motivational aspects of employee attitudes (engagement, organizational citizenship behavior, and quitting intentions), and satisfaction with the ‚Äúexternal forces‚Äù that affect employee experience (management, justice, and perceived organizational support);\r\nthe topic of autonomy is closely related to the topic of managerial motivation;\r\nmost of the negative partial correlations are between items measuring quitting intentions and other survey items.\r\nTo identify most influential topics (nodes) within the network, we can use centrality measures that quantify the relative importance of a node within a network based on different aspects of a node‚Äôs role in the network.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(qgraph)\r\n\r\n# computing centrality measures for individual survey items\r\nqgraph::centralityPlot(network, include = \"all\", orderBy = \"ExpectedInfluence\")\r\n\r\n\r\n\r\nBased on the expected influence centrality measure, which takes into account the presence of both negative and positive edges, the following items appear to be among the most influential ones:\r\nMan3: Management acts decisively.\r\nManMot3: My manager encourages me to do my best.\r\nJustice2: Procedures are applied fairly.\r\naut3: I am given enough leeway to get the job done.\r\nManMot1: My manager motivates me.\r\nBut to know how much we can rely on these measures, we must first test their stability using correlation stability analysis that repeatedly computes centrality indices of subsets of the data and correlates these with the centrality indices of the full data. This process generates a correlation stability coefficient (CS-Coefficient) for each centrality measure. The CS-Coefficient corresponds to the proportion of cases that can be dropped while retaining with 95% certainty a certain level of correlation (e.g., 0.7) between the original centrality measure and the centrality measure of the subset. The higher the CS-Coefficient, the more robust the centrality measure is to the reduction of cases. According to Epskamp & Fried (2018), the CS-coefficient should be above 0.5, and should be at least above 0.25. In our case, we can see the that we can make solid interpretations based on centrality measures of strength (CS(cor = 0.7) = 0.75) and expected influence (CS(cor = 0.7) = 0.75).\r\n\r\n\r\nShow code\r\n\r\n# estimating the stability of centrality measures\r\nbootnet_case_dropping <- bootnet::bootnet(\r\n  network, \r\n  nBoots = 2500,\r\n  type = \"case\",\r\n  nCores = 6,\r\n  statistics = c('strength', 'expectedInfluence', 'betweenness', 'closeness')\r\n)\r\n\r\n# plotting the results\r\nplot(bootnet_case_dropping, 'all')\r\n\r\n\r\nShow code\r\n\r\n# listing the results\r\nbootnet::corStability(bootnet_case_dropping)\r\n\r\n=== Correlation Stability Analysis === \r\n\r\nSampling levels tested:\r\n   nPerson Drop%   n\r\n1      208  75.0 232\r\n2      273  67.2 242\r\n3      337  59.5 254\r\n4      402  51.7 246\r\n5      467  43.9 207\r\n6      532  36.1 259\r\n7      596  28.4 279\r\n8      661  20.6 267\r\n9      726  12.7 251\r\n10     790   5.0 263\r\n\r\nMaximum drop proportions to retain correlation of 0.7 in at least 95% of the samples:\r\n\r\nbetweenness: 0.05 (CS-coefficient is lowest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0, caseMax = 0.127) \r\n\r\ncloseness: 0.05 (CS-coefficient is lowest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0, caseMax = 0.127) \r\n\r\nexpectedInfluence: 0.75 (CS-coefficient is highest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0.672, caseMax = 1) \r\n\r\nstrength: 0.75 (CS-coefficient is highest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0.672, caseMax = 1) \r\n\r\nAccuracy can also be increased by increasing both 'nBoots' and 'caseN'.\r\n\r\nIn a similar way, i.e.¬†using a bootstrapping, we can estimate the stability of the edge weights. This gives us information on how much the edge weights for individual connections vary with 95% confidence intervals. From the chart below, it‚Äôs apparent that some edge weights are more accurate than others, as they show a narrower band. Simultaneously, we observe that the majority of edges closer to zero appear to be non-significant, as they intersect with zero in the bootstrapped samples.\r\n\r\n\r\nShow code\r\n\r\n# estimating the stability of edge weights \r\nbootnet_nonpar <- bootnet::bootnet(\r\n  network, \r\n  nBoots = 1000,\r\n  nCores = 6\r\n  )\r\n\r\n# plotting the results\r\nplot(bootnet_nonpar, labels = FALSE, order = \"sample\")\r\n\r\n\r\n\r\nTo find internal structure in the estimated network, we need not rely solely on the visual inspection of the network diagram, but we can use some of the common community detection algorithms that can be used to identify clusters of more densely connected topics. In the example below, we have merely ‚Äù replicated‚Äù an existing clustering by scale, which is an indication that the authors have managed to construct a survey that measures distinct constructs that they originally intended to measure, but in practice you are likely to observe less distinct patterns more often, which can give you clues about how to improve the construction of the employee survey.\r\n\r\n\r\nShow code\r\n\r\n# identifying communities/clusters\r\nclu <- igraph::cluster_louvain(igraph_graph, weights = fromToList$weight) # cluster_optimal, cluster_louvain, cluster_leading_eigen, cluster_fast_greedy, cluster_walktrap,   cluster_edge_betweenness, cluster_spinglass\r\n\r\n# assigning communities/clusters to nodes\r\nmember <- membership(clu)\r\nV(igraph_graph)$cluster <- as.character(member)\r\n\r\n# visualizing the network with identified communities/clusters\r\nset.seed(123)\r\nggraph::ggraph(igraph_graph, layout = \"fr\", maxiter = 500) +  # fr, kk, drl, mds, maxiter = 500 is default\r\n  ggraph::geom_edge_link(aes(edge_width = weight, color = sign), alpha = 0.05) + \r\n  ggraph::geom_node_point(aes(color = cluster), size = 5) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  ggraph::theme_graph(background = \"white\") +\r\n  ggraph::scale_edge_color_manual(values = c(\"negative\" = \"red\", \"positive\" = \"blue\")) +\r\n  ggplot2::scale_color_brewer(palette=\"Set1\")\r\n\r\n\r\n\r\nAnother use case would be to test for differences in job attitudes‚Äô connectivity and centrality across different groups of employees. To do this, we can use the NetworkComparisonTest R package that implements permutation based hypothesis testing of differences between two networks. Let‚Äôs illustrate it with differences between networks estimated on data coming from females and males. We first filter data for both genders and than estimate and visualize their respective attitudinal networks.\r\n\r\n\r\nShow code\r\n\r\n# filtering data for females and males \r\nmydataFemale <- data %>%\r\n  dplyr::filter(sex == 2) %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\nmydataMale <- data %>%\r\n  dplyr::filter(sex == 1) %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\n# estimating a regularized partial correlation network for females and males\r\nnetworkFemale <- bootnet::estimateNetwork(\r\n  mydataFemale,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE \r\n)\r\n\r\nnetworkMale <- bootnet::estimateNetwork(\r\n  mydataMale,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE \r\n)\r\n\r\n# plotting the estimated networks\r\nplot(\r\n  networkFemale, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydataFemale),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\",\r\n  title = \"Females\"\r\n)\r\n\r\n\r\nShow code\r\n\r\nplot(\r\n  networkMale, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydataMale),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\",\r\n  title = \"Males\"\r\n)\r\n\r\n\r\n\r\nWe can check four basic types of differences between the networks:\r\nThe maximum difference in edge weights (M statistic) that tells us whether the structure of the network is identical across two compared groups, and thus whether the groups differ in the overall ‚Äúshape‚Äù or ‚Äúarchitecture‚Äù of their attitudes.\r\nThe difference in global strength (S statistic) that tells us whether the density of the network is identical across the groups, and thus whether they differ in their openness to change of their attitudes and behaviors (see, for example, Zwicker et al.¬†(2020)).\r\nThe difference in the centrality measures across the groups that tell us whether the groups differ in topics that are most important/influential in their attitudinal network.\r\nThe difference between groups in specific edges. According to van Borkulo et al.¬†(2017), when the M statistic is not statistically significant, it is recommended not to test group-level differences for specific edges as it increases the likelihood of Type 1 error.\r\nAs you can see below, (almost) all test results are statistically non-significant, so we don‚Äôt have sufficiently strong evidence for claiming that there are substantive differences between females and males in their respective attitudinal networks. Only in the case of the Eng1 item (I share the values of this organization.), females show a statistically significantly smaller strength centrality measure compared to males. Given that M statistic is statistically non-significant, we don‚Äôt test statistical significance of differences for specific edges.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(NetworkComparisonTest)\r\n\r\n# testing the differences\r\nset.seed(123)\r\ntestGenderDiff <- NetworkComparisonTest::NCT(\r\n  networkFemale, \r\n  networkMale, \r\n  binary.data=FALSE, \r\n  paired=FALSE,\r\n  weighted=TRUE,\r\n  test.edges=FALSE, \r\n  edges=\"all\",\r\n  p.adjust.methods=\"holm\",\r\n  test.centrality=TRUE, \r\n  centrality=c(\"strength\",\"expectedInfluence\"),\r\n  nodes=\"all\",\r\n  progressbar=FALSE\r\n  )\r\n\r\n# printing test results\r\nprint(testGenderDiff)\r\n\r\n\r\n NETWORK INVARIANCE TEST \r\n Test statistic M:  0.1747811 \r\n p-value 0.63 \r\n\r\n GLOBAL STRENGTH INVARIANCE TEST \r\n Global strength per group:  12.8295 13.39245 \r\n Test statistic S:  0.5629521 \r\n p-value 0.18 \r\n\r\n EDGE INVARIANCE TEST \r\n\r\nNULL\r\n\r\n CENTRALITY INVARIANCE TEST \r\n \r\n         strength expectedInfluence\r\nManMot1         1              1.00\r\nManMot2         1              1.00\r\nManMot3         1              1.00\r\nManMot4         1              1.00\r\nocb1            1              1.00\r\nocb2            1              1.00\r\nocb3            1              1.00\r\nocb4            1              1.00\r\naut1            1              1.00\r\naut2            1              1.00\r\naut3            1              1.00\r\nJustice1        1              1.00\r\nJustice2        1              1.00\r\nJustice3        1              1.00\r\nJobSat1         1              1.00\r\nJobSat2         1              1.00\r\nQuit1           1              1.00\r\nQuit2           1              1.00\r\nQuit3           1              1.00\r\nMan1            1              1.00\r\nMan2            1              1.00\r\nMan3            1              1.00\r\nEng1            0              0.57\r\nEng2            1              1.00\r\nEng3            1              1.00\r\nEng4            1              1.00\r\npos1            1              1.00\r\npos2            1              1.00\r\npos3            1              1.00\r\n\r\nShow code\r\n\r\n# checking observed differences in centrality measures\r\n# testGenderDiff$diffcen.real \r\n\r\n# plotting results of the network structure invariance test\r\n# plot(testGenderDiff,what=\"network\")\r\n# plotting results of global strength invariance test\r\n# plot(testGenderDiff,what=\"strength\")\r\n# plotting results of the edge invariance test\r\n# plot(testGenderDiff,what=\"edge\")\r\n\r\n\r\nI hope you find this post useful and that it inspires you to try PNA on your own data. If you were looking for more authoritative sources on PNA, Sacha Epskamp‚Äôs site is a good place to start. An excellent introduction to the topic can also be found in Letouche & Wille (2022) and Dalege et al.¬†(2017), respectively.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-16-psychometric-network-analysis/psychometric-network-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-05-26T12:58:02+02:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2023-05-10-regression-to-the-mean/",
    "title": "Employee commitment over time & regression to the mean",
    "description": "A nice illustration of the regression to the mean phenomenon in the space of people analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-10",
    "categories": [
      "people analytics",
      "critical thinking",
      "regression to the mean",
      "r"
    ],
    "contents": "\r\nA vendor specializing in employee engagement measurements recently presented their findings that companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced the largest decreases and largest increases in commitment post-layoff, respectively. They post-hoc-hypothesized that highly committed employees feel especially hurt and betrayed when layoffs occur.\r\nHowever, when a similar pattern emerges, a warning light should always flash that regression to the mean (RTM) might actually be behind it. When a variable is imperfectly correlated with another variable, extreme values tend to gravitate towards the mean in subsequent measurements, which can make natural variations in repeated data appear like genuine change.\r\nThis doesn‚Äôt mean that RTM was the sole factor in the aforementioned case. However, to know better, it‚Äôs necessary to control for it.\r\nWhat follows is a simple simulation to illustrate how the reported finding could occur purely or partially due to RTM, and how one might control for it.\r\nFirst, let‚Äôs create correlated employee commitment observations per company from time 1 (T1) and time 2 (T2).\r\n\r\n\r\nShow code\r\n\r\n# uploading necessary libraries\r\nlibrary(tidyverse)\r\n\r\nset.seed(1) # seed for reproducibility\r\nn <- 2000 # number of observations\r\nT1 <- rnorm(n) # generating observations at time 1\r\nT2 <- 0.7*T1 + rnorm(n)*sqrt(1-0.7^2) # generating correlated observations at time 2\r\n\r\n# cor(T1, T2) # checking the correlation\r\n\r\ndf <- data.frame(T1=T1, T2=T2) # putting created variables into dataframe\r\n\r\n\r\nWe also need a benchmark for T1 so that we can calculate the difference between T1 values and T1 benchmark. In addition, we also need to calculate the difference between T2 and T1.\r\n\r\n\r\nShow code\r\n\r\n# computing benchmark for pre-layoff period (T1)\r\nbenchmark <- df %>%\r\n  dplyr::summarise(\r\n    benchmark = mean(T1)\r\n  ) %>%\r\n  dplyr::pull(benchmark)\r\n\r\n# computing differences between T2 and T1 and between T1 and T1 benchmark (average)\r\ndf <- df %>%\r\n  dplyr::mutate(\r\n    timeDiff = T2-T1,\r\n    benchmarkDiff = T1-benchmark\r\n  )\r\n\r\n\r\nThen all we have to do is randomly assign individual observations to the group of companies that made layoffs and those that did not.\r\n\r\n\r\nShow code\r\n\r\n# assigning each observation randomly one of two labels - Layoffs/NoLayoffs \r\ndf1 <- df %>%\r\n  dplyr::mutate(\r\n    Layoffs = sample(c(\"Layoffs\", \"NoLayoffs\"), size = n(), replace = TRUE, prob = c(0.15, 0.85))\r\n  )\r\n\r\n\r\nNow we can contrast the differences between T1 values and T1 benchmark on the one hand and the differences between T2 and T1 on the other. As we can see in the chart below, the pattern matches well with the originally reported finding\r\n- companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced the largest decreases and largest increases in commitment post-layoff, respectively, but now purely as a result of RTM.\r\n\r\n\r\nShow code\r\n\r\n# visualizing relationship between the \r\ndf1 %>%\r\n  dplyr::filter(Layoffs == \"Layoffs\") %>%\r\n  ggplot2::ggplot(aes(x = benchmarkDiff, y = timeDiff)) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = F) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-2,4,1)) +\r\n  labs(\r\n    x = \"DIFFERENCE BETWEEN T1 AND T1 BENCHMARK\",\r\n    y = \"DIFFERENCE BETWEEN T2 AND T1\",\r\n    title = \"Changes in employee commitment purely due to regression to the mean\",\r\n    caption = \"\\nA replication of the original finding that companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced\\nthe largest decreases and largest increases in commitment post-layoff, respectively, relying only on a regression to the mean phenomenon.\\nT1 and T2 refer to commitment measurements at time 1 (prior to a layoff) and time 2 (after a layoff), respectively.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\nIn general, in order to make a valid estimate of the effect of layoffs on employee commitment when RTM is at play, we need to control for its effect. One way to do this is to include the difference between T1 values and T1 benchmark in the linear regression model as illustrated below.\r\n\r\n\r\nShow code\r\n\r\n# modeling T2 while controlling for the effect of regression to the mean\r\nmodel1 <- glm(T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), data = df1)\r\nsummary(model1)\r\n\r\n\r\nCall:\r\nglm(formula = T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), \r\n    data = df1)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-2.54938  -0.47760  -0.01441   0.50541   2.19243  \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       0.01538    0.04210   0.365    0.715    \r\nbenchmarkDiff     0.72305    0.01593  45.382   <2e-16 ***\r\nLayoffsNoLayoffs -0.01620    0.04577  -0.354    0.723    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 0.5458733)\r\n\r\n    Null deviance: 2214.4  on 1999  degrees of freedom\r\nResidual deviance: 1090.1  on 1997  degrees of freedom\r\nAIC: 4470\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nIt is clear from the estimated model that there is not much evidence in favor of the existence of a layoff effect, which should not be surprising since individual observations were purely randomly assigned to groups of companies with and without layoffs. However, when we adjusted the data to better reflect the hypothesized causal mechanism behind the observed pattern, the effect of layoffs was detected as statistically significant.\r\n\r\n\r\nShow code\r\n\r\n# creating a new dataset that better reflects the hypothesized causal mechanism behind the observed pattern\r\ndf2 <- df %>%\r\n  dplyr::rowwise() %>%\r\n  dplyr::mutate(\r\n    Layoffs = sample(c(\"Layoffs\", \"NoLayoffs\"), size = 1, replace = TRUE, prob = c(0.15, 0.85)),\r\n    T2 = ifelse(Layoffs == \"Layoffs\" & T1 >= 0.5, T2 - runif(min = 0.3, max = 1.5, n = 1), T2),\r\n    T2 = ifelse(Layoffs == \"Layoffs\" & T1 <= -0.5 , T2 - runif(min = -0.75, max = 0.2, n = 1), T2)\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(timeDiff = T2-T1) # recomputing timeDiff\r\n\r\n# modeling T2 while controlling for the effect of regression to the mean\r\nmodel2 <- glm(T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), data = df2)\r\nsummary(model2)\r\n\r\n\r\nCall:\r\nglm(formula = T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), \r\n    data = df2)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-2.99200  -0.49173  -0.00417   0.52662   2.52414  \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)      -0.19596    0.04404  -4.449  9.1e-06 ***\r\nbenchmarkDiff     0.66308    0.01662  39.908  < 2e-16 ***\r\nLayoffsNoLayoffs  0.19571    0.04786   4.089  4.5e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 0.5933289)\r\n\r\n    Null deviance: 2135.6  on 1999  degrees of freedom\r\nResidual deviance: 1184.9  on 1997  degrees of freedom\r\nAIC: 4636.7\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nFeel free to share your own experiences and encounters with the phenomena of regression to the mean in your people analytics practice.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-10-regression-to-the-mean/./rtm.gif",
    "last_modified": "2023-05-10T12:50:20+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-04-cv-job-match-career-site/",
    "title": "Improving a company career site with tools from OpenAI",
    "description": "How my own experience of exploring new job opportunities gave me the idea of how the company's career site could be easily improved using OpenAI's tools.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-04",
    "categories": [
      "recruitment",
      "candidate experience",
      "career site",
      "openai",
      "embeddings",
      "gpt",
      "python",
      "shiny"
    ],
    "contents": "\r\nMy recent exploration of new job opportunities has inspired me to consider what I would appreciate as a job candidate when visiting the career site of a company I‚Äôd like to work for. Specifically, I would appreciate the following flow:\r\nUploading my CV on the company‚Äôs career site.\r\nReceiving a list of jobs sorted by the degree of match to my CV.\r\nObtaining an overview of my major mis/matches for the selected job.\r\nHaving the option to ask specific questions about the job (e.g.¬†What could be the biggest challenge for me?)\r\nNot/Applying for the job after considering the provided information.\r\nIt‚Äôs clear that a process like this would make life easier not only for job candidates but also for companies, as more relevant candidates would likely apply for posted vacancies on average (as far as people are looking for similar jobs they have done in the past).\r\nI tested the feasibility of this idea by creating a functional POC career site, with OpenAI tools working behind the scenes, that supports this exact flow for 20 people-analytics-related job ads taken from One Model‚Äôs website with open roles in the people analytics space (by the way, kudos for that, One Model team üëè). You can try it for yourself with your own or sample CV on this webpage.\r\n\r\nLet me know in the comments what you think about the flow and/or how you would improve it to make it even more useful for job candidates and companies.\r\nP.S. It was also a good opportunity to try out Shiny for Python by Posit. Being a regular user of the R version of Shiny, I may be a little bit biased, but IMO it‚Äôs more user-friendly compared to Dash, so if you need to present the results of your Python code in an interactive web application, definitely give it a try.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-04-cv-job-match-career-site/./search.jpg",
    "last_modified": "2023-05-26T12:53:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/",
    "title": "GPT-4's performance in the knowledge test of evidence-based HRM practices",
    "description": "How did GPT-4 perform in the knowledge test of evidence-based HRM practices? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-02",
    "categories": [
      "gpt",
      "ai",
      "evidence-based management",
      "hr management",
      "people management",
      "hr practices"
    ],
    "contents": "\r\nSome time ago, I ‚Äúreplicated‚Äù Rynes, Colbert, and Brown‚Äôs 2002 study on HR practitioners‚Äô beliefs about effective HR practices on a convenience sample of more than 140 LinkedIn users. The results of this ‚Äúreplication‚Äù closely resembled the results of the original study. On average, respondents correctly answered 19.4 out of 35 items, achieving a 55% success rate, which was very close to the 57% average success rate in the original study (and also quite close to the 50% success rate that corresponds to random choice, given the TRUE/FALSE response format).\r\nI was curious to see how GPT-4 would perform in this test, as it had been evaluated on various standardized tests such as the SAT, GRE, Bar Exam, and AP. The prompts used had the following form: Read the following statement and indicate whether it is true or false. Keep in mind that the statement refers to general trends and patterns that apply on average but not necessarily to all cases. When evaluating the statement, ensure that you correctly interpret the words used in the statement and take into account existing scientific evidence. Give me the answer either true or false, without intermediate values, in a boolean way. Finally, briefly explain your reasoning behind your answer. The statement is as follows:‚Ä¶\r\nSo, what were the results? GPT-4 answered 29 out of 35 items right, i.e., it achieved an 83% success rate, which corresponds to the 99th and 97th percentiles in the original and ‚Äúreplicated‚Äù studies, respectively. GPT-4‚Äôs results were thus superior to majority of people who took the test.\r\n\r\nHowever, even when it gave a correct answer, it did not always rely on correct facts and/or valid reasoning, which could be a problem if management decided to act on the answers provided. See the table below to check the details of its responses.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(DT)\r\n\r\n# uploading data\r\nmydata <- readxl::read_xlsx(\"./gpt4Responses.xlsx\")\r\n\r\n# creating user-friendly table\r\nDT::datatable(\r\n  mydata %>% \r\n    dplyr::select(itemId, item, gpt4Response, correctAnswer, gpt4Reasoning, researchEvidence, possibleContingencies) %>%\r\n    dplyr::rename(\"Item ID\"=itemId, Item=item, \"GPT-4 response\"=gpt4Response, \"Correct answer\"=correctAnswer, \"GPT-4 reasoning\"=gpt4Reasoning, \"Research evidence\"=researchEvidence, \"Possible contingencies\"=possibleContingencies),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 3,\r\n    autoWidth = TRUE,\r\n    columnDefs = list(list(width = '500px', targets = c(\"Item\", \"GPT-4 reasoning\", \"Research evidence\", \"Possible contingencies\"))),\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  DT::formatStyle(1:7, 'vertical-align'='top')\r\n\r\n\r\n\r\nThe possible takeaway from this finding? Although GPT-4 can be a handy tool for exploring possible solutions to specific HR-related problems, on its own and in its current form it cannot replace the good old systematic search for and retrieval of evidence, critical evaluation of its reliability and relevance, and its weighing and synthesis as conducted and/or supervised by human experts.\r\nP.S. I didn‚Äôt test the reliability of GPT-4‚Äôs responses, nor did I set its temperature to 0, so it‚Äôs possible that you might obtain somewhat different results if you decide to replicate the test. In addition, please keep in mind that the comparison presented here is not entirely an apples-to-apples comparison, mainly due to the fact that new evidence may have emerged that does not match the correct answers in the original study conducted more than 20 years ago.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/./gpt4.jpg",
    "last_modified": "2023-05-01T22:45:00+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-28-employee-feedback-analysis-using-openai/",
    "title": "Employee feedback analysis using tools from OpenAI",
    "description": "How to use GPT and embeddings from OpenAI for identifying topics and related sentiments in employee feedback.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-28",
    "categories": [
      "openai",
      "gpt",
      "embeddings",
      "employee feedback",
      "employee survey",
      "topic analysis",
      "python",
      "r",
      "shiny app"
    ],
    "contents": "\r\nA while ago, I posted about the potential of using GPT for processing open-ended feedback from employees. I simply inputted a block of text into GPT and asked for a summary of the major topics found in the feedback. Although the output was quite accurate and the information compression achieved was very useful, this approach was somewhat limited in terms of scalability and granularity of the information provided.\r\nTo address these limitations, I experimented with another approach that includes the following steps:\r\nLooping over feedback from individual employees and sending them one by one to GPT.\r\nPrompting GPT to identify all present topics in each feedback, determining their respective sentiments (positive, negative, mixed, or neutral), and extracting the relevant parts of the feedback based on which the topic was identified.\r\nPrompting GPT to categorize identified topics using a provided list of topic categories (e.g., compensation and benefits, work-life balance, collaboration and teamwork, etc.), while taking into account contextual information in the relevant parts of the feedback. Alternatively, categorizing by matching embeddings of identified topics, contextual information, and topic categories.\r\nPlotting the topic categories by the number of their occurrences and type of associated sentiment.\r\nInteractive exploration of specific topics clustered by their semantic similarity based on their respective embeddings and visualized with the help of t-SNE dimensionality reduction technique.\r\nCreating a filterable table with identified topics and all original and extracted information that may be useful for further exploration of the feedback and for checking the precision of the topic identification.\r\nI had to experiment a bit with the prompts and include some data-munging inter-steps to get useful outputs, however, it now works relatively smoothly and provides pretty good results. To test the plausibility of this approach, I tried it on publicly available feedback from more than 300 current and former employees of an unnamed company published on Glassdoor and shared on Kaggle. You can check the results of the analysis yourself in this simple dashboard.\r\n\r\nIn my opinion, it works quite well and could represent a very time- and cost-effective way to gain useful insights from employee open-ended feedback at scale, with the caveat that one has to ensure the security of the processed data, for example, by using a local LLM. Let me know what you think about this approach. And if you are interested in the Python code behind the analysis so you can play with it on your own data, here‚Äôs the link to the GitHub page with the Python code.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-28-employee-feedback-analysis-using-openai/./employeeListening.avif",
    "last_modified": "2023-06-27T17:03:08+02:00",
    "input_file": "employee-feedback-analysis-using-openai.knit.md"
  },
  {
    "path": "posts/2023-04-24-glassdoor/",
    "title": "When flawed statistical & causal reasoning leads to a valid conclusion anyway",
    "description": "Comparison of Glassdoor ratings from current and former employees.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-24",
    "categories": [
      "glassdoor",
      "employee experience",
      "employee satisfaction",
      "employee turnover"
    ],
    "contents": "\r\nOne simple lesson from the observation that former employees tend to rate their employers more harshly on Glassdoor compared to current employees: Strive to retain your employees, and you‚Äôll likely have a more satisfied workforce and better Glassdoor ratings üòÅ\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data (link to the original dataset: https://www.kaggle.com/datasets/davidgauthier/glassdoor-job-reviews/code)\r\n# data <- readr::read_csv(\"./glassdoor_reviews.csv\")\r\n# \r\n# # preparing data\r\n# mydata <- data %>%\r\n#   # selecting relevant vars\r\n#   dplyr::select(firm, current, overall_rating, work_life_balance, culture_values, career_opp, comp_benefits, senior_mgmt) %>%\r\n#   # keeping companies with at least 300 records\r\n#   dplyr::group_by(firm) %>%\r\n#   dplyr::mutate(n = n()) %>%\r\n#   dplyr::ungroup() %>%\r\n#   dplyr::filter(n >= 500) %>%\r\n#   dplyr::select(-n) %>%\r\n#   # renaming employee status and keeping only current and former employees\r\n#   dplyr::mutate(\r\n#     status = tolower(current),\r\n#     status = case_when(\r\n#       stringr::str_detect(status, \"\\\\bcurrent\\\\b\") ~ \"Current employee\",\r\n#       stringr::str_detect(status, \"\\\\bformer\\\\b\") ~ \"Former employee\",\r\n#       TRUE ~ \"Unknown\"\r\n#     )\r\n#     ) %>%\r\n#   dplyr::filter(status != \"Unknown\") %>%\r\n#   dplyr::select(-current) %>%\r\n#   # changing wide format to long one\r\n#   tidyr::pivot_longer(overall_rating:senior_mgmt, names_to = \"rating_dimension\", values_to = \"value\") %>%\r\n#   # removing missing values\r\n#   dplyr::filter(!is.na(value)) %>%\r\n#   # renaming rating dimensions\r\n#   dplyr::mutate(rating_dimension = case_when(\r\n#     rating_dimension == \"overall_rating\" ~ \"Overall rating\",\r\n#     rating_dimension == \"work_life_balance\" ~ \"Work-life balance\",\r\n#     rating_dimension == \"culture_values\" ~ \"Culture values\",\r\n#     rating_dimension == \"career_opp\" ~ \"Career opportunities\",\r\n#     rating_dimension == \"comp_benefits\" ~ \"Compensation & benefits\",\r\n#     rating_dimension == \"senior_mgmt\" ~ \"Senior management\",\r\n#     TRUE ~ \"Unknown\"\r\n#   )\r\n#   ) %>%\r\n#   # removing unknown rating dimensions\r\n#   dplyr::filter(rating_dimension != \"Unknown\")\r\n\r\n# to save space in my GitHub repo, I will upload already filtered dataset saved as .RDS file\r\nmydata <- readRDS(\"./glassdoor_reviews_filtered.RDS\")\r\n\r\n\r\n# dataviz \r\n# computing weighted average probability of a given rating for companies in the sample\r\nvizData <- mydata %>%\r\n  dplyr::group_by(firm, status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    n = n()\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::group_by(firm, status, rating_dimension) %>%\r\n  dplyr::mutate(nAll = sum(n)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    prop = n/nAll,\r\n    wprop = prop*nAll\r\n    ) %>%\r\n  dplyr::group_by(status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    wpropsum = sum(wprop),\r\n    w = sum(nAll)\r\n    ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(fprop = wpropsum/w)\r\n\r\n# chart\r\nvizData %>%\r\n  dplyr::mutate(rating_dimension = factor(rating_dimension, levels = c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\"))) %>%\r\n  ggplot2::ggplot(aes(x = value, y = fprop, fill = forcats::fct_rev(status))) +\r\n  ggplot2::geom_bar(stat = \"identity\", position=\"dodge\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_fill_manual(values = c(\"Current employee\" = \"#4E79A7\", \"Former employee\" = \"gray\")) +\r\n  ggplot2::facet_wrap(~rating_dimension, ncol = 3, scales = \"fixed\") +\r\n  ggplot2::labs(\r\n    title = \"<span style='font-size:22pt;font-weight:bold;'>**Comparison of Glassdoor ratings from** \r\n    <span style='color:#4E79A7;'>**current**<\/span> **and**\r\n    <span style='color:#999696;'>**former employees**<\/span>\r\n    <\/span>\",\r\n    caption = \"\\nBased on a sample of ratings from 792,390 individuals across 165 companies with more than 500 records each.\\nThe values represent the weighted average probability of a given rating for companies in the sample.\",\r\n    x = \"RATING\",\r\n    y = \"PROBABILITY OF RATING\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nOn a more serious note, it may be quite interesting and potentially useful to examine the order of estimated differences in specific areas between current and former employees as it may provide some insights on which areas to focus on when trying to retain employees within the company. We can use a multilevel ordered regression analysis on a random sample of 300 ratings per company for this purpose.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(ordinal)\r\nlibrary(broom.mixed)\r\nlibrary(parameters)\r\n\r\n# modeling responses using multilevel ordered regression analysis\r\nfits <- data.frame()\r\n# looping over individual dimensions\r\nfor(scale in c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\")){\r\n  #print(scale)\r\n  set.seed(1234)\r\n  model <- ordinal::clmm(\"value ~ status + (1 | firm)\", data = mydata %>% dplyr::filter(rating_dimension == scale) %>% dplyr::mutate(value = factor(value,ordered = TRUE)) %>% dplyr::group_by(firm) %>% dplyr::sample_n(300) %>% dplyr::ungroup())\r\n  #summary(model)\r\n  \r\n  # extracting information about fitted models\r\n  supp <- broom.mixed::tidy(model) %>%\r\n    dplyr::filter(term == \"statusFormer employee\") %>%\r\n    dplyr::bind_cols(parameters::ci(model) %>% filter(Parameter == \"statusFormer employee\") %>% select(CI_low, CI_high)) %>%\r\n    dplyr::select(-coef.type) %>%\r\n    dplyr::mutate(scale = scale) %>%\r\n    dplyr::select(scale, everything())\r\n  \r\n  fits <- dplyr::bind_rows(fits, supp)\r\n  \r\n}\r\n\r\nfits %>%\r\n  arrange(estimate)\r\n\r\n                    scale                  term   estimate  std.error\r\n1          Culture values statusFormer employee -0.6750302 0.01712937\r\n2          Overall rating statusFormer employee -0.6373467 0.01707703\r\n3       Senior management statusFormer employee -0.6314480 0.01693096\r\n4    Career opportunities statusFormer employee -0.5943039 0.01693091\r\n5       Work-life balance statusFormer employee -0.5263801 0.01689702\r\n6 Compensation & benefits statusFormer employee -0.3574820 0.01691018\r\n  statistic       p.value     CI_low    CI_high\r\n1 -39.40777  0.000000e+00 -0.7086032 -0.6414573\r\n2 -37.32187 7.251441e-305 -0.6708171 -0.6038763\r\n3 -37.29547 1.943268e-304 -0.6646320 -0.5982639\r\n4 -35.10171 6.347105e-270 -0.6274878 -0.5611199\r\n5 -31.15224 4.729748e-213 -0.5594977 -0.4932626\r\n6 -21.14005  3.407214e-99 -0.3906253 -0.3243386\r\n\r\nCaveat: As the title of this post implies, readers should be aware that numerous biases can distort the portrayal of employee experiences reflected in Glassdoor ratings. Some of the most significant biases include survivorship bias, social desirability, non-response bias, self-selection, and motivated reasoning.\r\nDr.¬†Paul De Young‚Äôs personal experience in this regard is quite telling: ‚ÄúThere is often a high preponderance of phony ratings among so-called current employees on Glassdoor. Beware of bogus ‚Äúpart time‚Äù current employees giving high ratings, especially if the company does not employ a lot of part-time employees.Also, I learned from an HR executive that if you want to get ratings up on Glassdoor, encourage ALL your employees to rate the company. Most often it is the mistreated employees who post because this is an outlet for their misfortune. By getting more employees to rate, chances are your ratings will increase. Watch for actively monitored employers on Glassdoor. You can usually tell a bogus rating because there is a high rating with very few comments in jobs that do not exist. The first thing I do when looking at a company is to filter out the part time employees and look at the impact on the overall scores. If they jump down, you have to wonder about the validity of the ratings. Read the comments, they are more telling. There are all kinds of ways to game the system. Glassdoor is helpful, but doesn‚Äôt always give you a valid picture without looking at the details, which is where the devil lives.‚Äù\r\nHowever, it doesn‚Äôt mean that there is no signal in Glassdoor ratings. For example, behavioral scientists at Culture Amp investigated the relationship between Glassdoor ratings and employee engagement data collected by Culture Amp. The findings suggested a strong correlation between employee engagement and Glassdoor ratings, particularly as the number of reviews increases (r = 0.69 for 100+ reviews). Companies with higher engagement scores tended to have better Glassdoor ratings, including higher CEO approval percentages and a greater likelihood of being recommended as a workplace. The study also identified the five factors with the largest relationship to Glassdoor scores, which included Learning and Development, Service and Quality Focus, Decision-making, Leadership, and Collaboration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-24-glassdoor/./glassdoor.png",
    "last_modified": "2023-04-24T23:58:31+02:00",
    "input_file": {},
    "preview_width": 768,
    "preview_height": 595
  },
  {
    "path": "posts/2023-04-18-multilevel-correlation/",
    "title": "In need of multilevel correlations?",
    "description": "A post about a great R package to reach for when you need to calculate correlations on nested data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-18",
    "categories": [
      "correlation",
      "multilevel analysis",
      "hierarchical analysis",
      "r"
    ],
    "contents": "\r\nI am currently in the middle of a project where I am working with nested data and need to report multilevel correlations.\r\nTo my surprise, for quite a long time I couldn‚Äôt find any libraries in the R or Python ecosystems that provided an easy-to-use implementation of this type of analysis. I thought I would have to code it from scratch using Stan or JAGS.\r\nFortunately, I discovered a fantastic correlation package (part of the easystats universe) that can compute various types of correlations, including multilevel correlations, partial correlations, Bayesian correlations, polychoric correlations, biweight correlations, distance correlations, and more.\r\nSince nested data is (almost) everywhere, consider trying this package as it can make your life as an analyst a little bit easier üòâ Check out the code below to see it in action.\r\n\r\n\r\nShow code\r\n\r\n# Uploading libraries and creating custom functions\r\nlibrary(tidyverse)\r\nlibrary(correlation)\r\nlibrary(ggsci)\r\nlibrary(MASS) \r\n\r\n# Creating simulated dataset with nested data\r\n\r\n# Setting some basic parameters of the dataset\r\nnum_teams <- 7\r\nteam_ids <- LETTERS[1:num_teams]\r\nmin_rows <- 35\r\n\r\n# Defining function to generate data for a team with specified correlation\r\ngenerate_team_data <- function(team_id, correlation, job_sat_mean, agility_maturity_mean) {\r\n  \r\n  # Creating covariance matrix\r\n  covariance <- correlation * (20 * 50)\r\n  means <- c(job_sat_mean, agility_maturity_mean)\r\n  cov_matrix <- matrix(c(100, covariance, covariance, 2500), nrow = 2)\r\n  \r\n  # Generating correlated data\r\n  data <- MASS::mvrnorm(n = min_rows, mu = means, Sigma = cov_matrix)\r\n  \r\n  # Scaling the data\r\n  data[, 1] <- scale(data[, 1],center = FALSE,scale = TRUE)\r\n  data[, 2] <- scale(data[, 2],center = FALSE,scale = TRUE)\r\n  \r\n  # Putting data into dataframe\r\n  df <- data.frame(\r\n    TeamID = team_id,\r\n    JobSatisfaction = data[, 1],\r\n    AgilityMaturity = data[, 2])\r\n  \r\n  return(df)\r\n  \r\n}\r\n\r\n# Generating random means for job satisfaction and agility maturity for each of the teams within some range\r\nset.seed(42)\r\njob_sat_means <- runif(num_teams, min = -5, max = 5)\r\nagility_maturity_means <- runif(num_teams, min = 40, max = 60)\r\n\r\n# Generating random correlations for each of the teams team within some range\r\nset.seed(421)\r\ncorrelations <- runif(num_teams, min = -0.3, max = 0.4)\r\n\r\n# Generating data for each team and store in a list\r\nset.seed(123)\r\nteam_data <- mapply(generate_team_data, team_id = team_ids, correlation = correlations, job_sat_mean = job_sat_means, agility_maturity_mean = agility_maturity_means, SIMPLIFY = FALSE)\r\n\r\n# Combining team data into a single data frame\r\nsimulated_data <- do.call(rbind, team_data)\r\n\r\n# Computing multilevel Bayesian Pearson  correlation analysis\r\nc <- correlation::correlation(\r\n  simulated_data,\r\n  method = \"pearson\", \r\n  multilevel = TRUE, \r\n  bayesian = TRUE\r\n)\r\n\r\n# Extracting results of the analysis to be included in the chart defined below\r\nPearson_r = c[1,3]\r\nCI95L = c[1,4]\r\nCI95H = c[1,5]\r\n\r\n# Plotting the chart\r\nggplot2::ggplot(simulated_data, aes(y = JobSatisfaction, x = AgilityMaturity, color = TeamID)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE, size = 1.5) + \r\n  ggsci::scale_color_tron() +\r\n  ggplot2::labs(\r\n    y = \"JOB SATISFACTION\",\r\n    x = \"PERCEIVED ORGANIZATIONAL AGILITY MATURITY\",\r\n    title = \"Is organizational agility related to job satisfaction?\",\r\n    subtitle = stringr::str_glue(\"Bayesian Pearson r = {round(Pearson_r,2)}; 95% CrI: [{round(CI95L,2)}, {round(CI95H,2)}]\")\r\n  ) +\r\n \r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-18-multilevel-correlation/./plot.png",
    "last_modified": "2023-04-25T09:23:35+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2023-04-17-time-management/",
    "title": "Consequences of time management in the workplace",
    "description": "Some interesting insights from a meta-analytic review of the consequences of time management behaviors in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-17",
    "categories": [
      "time management",
      "meta-analysis",
      "job satisfaction",
      "job performance",
      "stress",
      "burnout"
    ],
    "contents": "\r\nBedi & Sass (2022) conducted a meta-analytic review of the consequences of employee time management behaviors on several employee outcomes. What are the main insights?\r\nIt may not come as a big surprise, but it is encouraging that data support the association between time management and various beneficial employee outcomes, such as increased job satisfaction, job performance, and lower levels of stress and burnout. Unfortunately, the ‚Äúproven‚Äù association is not causal, as the majority of studies were cross-sectional. In fact, there are not many studies on the causal effects of time management. The exception to this is procrastination, for which there is evidence that time management can help - see, for example, the meta-analysis by Van Eerde & Klingsieck (2018) on this topic.\r\nThe relationship between time management and employee outcomes is not only direct but also partially mediated by work-family conflict. This finding underscores the importance of work-life balance and highlights the need for organizations to help employees better address this specific issue, as it may positively affect a variety of employee outcomes.\r\n\r\nPerceived control over time, achieved through the use of time management, shows incremental validity in predicting job satisfaction, job performance, and stress with respect to the personality trait of conscientiousness. This suggests that regardless of an individual‚Äôs innate level of prudence, they may benefit from adopting time management in their professional lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-17-time-management/./tm.jpg",
    "last_modified": "2023-04-17T20:06:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-13-interpretable-ml/",
    "title": "Interpretable machine learning with modelStudio",
    "description": "There's a new kid on the block in the R ecosystem that can help analysts understand the behavior of their ML models.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-13",
    "categories": [
      "interpretability",
      "explainability",
      "machine learning",
      "predictive models",
      "r"
    ],
    "contents": "\r\nThere‚Äôs a great new R package, modelStudio, that makes it much easier for analysts to create both global and local interpretations of predictive models using an interactive interface.\r\nOnce you‚Äôve trained the model, you just get the DALEX explainer object ready and start up modelStudio that will run the following analyses (among others) and show the corresponding plots:\r\nFeature Importance: A visual representation that ranks and displays the significance of each input variable in a predictive model, helping to identify the most influential features for model predictions.\r\nPartial Dependence: A visualization that shows the relationship between a feature and the predicted outcome while averaging out the effects of all other features, to understand the marginal impact of a specific feature on the model‚Äôs predictions.\r\nAccumulated Dependence: Similar to the previous method, but reducing the influence of the assumption of uncorrelated features, providing a more robust and reliable representation of the feature‚Äôs impact on the model‚Äôs predictions.\r\nBreak Down Plot: A graphical explanation tool that demonstrates the contribution of each feature to a specific instance‚Äôs prediction, allowing for individual-level interpretation of model outcomes.\r\nShapley Values: A cooperative game theory-based approach for fairly attributing each feature‚Äôs contribution to a specific prediction, providing interpretable and consistent explanations for machine learning models.\r\nCeteris Paribus: A method that helps with understanding the influence of individual features on specific predictions by isolating the effect of a single variable while holding all other variables constant.\r\nIf you use ML in HR or any other field where it‚Äôs crucial to explain why you make specific predictions, classifications, and the resulting recommendations or decisions, definitely give it a try.\r\nWhat follows is a short demonstration of the tool using the well-known artificial IBM attrition dataset. First, let‚Äôs import the attrition dataset from the modeldata library and change the coding of the criterion variable, which will later make it easier to set up the DALEX explainer, which requires numerical data type for criterion variable even in classification tasks.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n\r\nWe now split the data into training, test, and validation datasets so that we can tune the prediction model, fit the model, and test its performance on new, previously unseen data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\n\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\nNow let‚Äôs define the whole model training workflow, which includes the data adjustment pipeline and the specification of the model used. We will use XGBoost, presumably the best ML algorithm for tabular type of data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(recipes)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\n\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors())\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\nAlthough the XGBoost algorithm works quite well with the default hyper-parameters, we will use the validation dataset to tune some of its hyper-parameters to get the best performance out of it. As can be seen below, after tuning the best model performs quite well in terms of AUC, which has a value of around 0.8.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n    )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n# A tibble: 1 √ó 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.802\r\n\r\nShow code\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n\r\nNow we can set up the final model training workflow and fit the model to the entire training dataset and check its performance on out-of-sample data using k-fold cross-validation and testing dataset. As we see below, in both cases the model performance as measured by AUC is around the value of 0.8.\r\n\r\n\r\nShow code\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  fit(data_train)\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# A tibble: 1 √ó 6\r\n  .metric .estimator  mean     n std_err .config             \r\n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 roc_auc binary     0.799    10  0.0166 Preprocessor1_Model1\r\n\r\nShow code\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n  yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# A tibble: 1 √ó 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.871\r\n\r\nNow we‚Äôre ready to explore the inner workings of our trained model. After setting up the explainer from the DALEX package, we simply insert this object into the modelStudio function and run it. We then get an interactive interface in our browser that we can use to easily check what our model is doing to make its predictions. You can try it out for yourself using the interactive interface below.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(DALEX)\r\nlibrary(modelStudio)\r\n\r\n# setting up the DALEX explainer object\r\n# creating predict function\r\npred <- function(model, newdata)  {\r\n  results <- (predict(model, newdata, type = \"prob\")[[1]])\r\n  return(results)\r\n}\r\n\r\nexplainer <- DALEX::explain(\r\n  model = xgb_fit,\r\n  data = data_test %>% mutate(Attrition = as.integer(Attrition)),\r\n  y = data_test %>% mutate(Attrition = as.integer(Attrition)) %>% pull(Attrition),\r\n  predict_function = pred,\r\n  type = \"classification\",\r\n  verbose = FALSE \r\n)\r\n\r\n# running ModelStudio\r\nmodelStudio::modelStudio(\r\n  explainer,\r\n  max_features = 100, # Maximum number of features to be included in BD, SV, and FI plots\r\n  N = 300, # Number of observations used for the calculation of PD and AD\r\n  new_observation_n = 3, #Number of observations to be taken from the data\r\n  show_info = TRUE,\r\n  viewer = \"browser\",\r\n  facet_dim = c(2,2) # layout of the resultiing charts\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-13-interpretable-ml/./miracle.jpg",
    "last_modified": "2023-04-25T09:27:11+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-openai-personality-interpretation/",
    "title": "Ask your personality using GPT",
    "description": "Can Generative AI like GPT meaningfully interpret personality profiles?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-11",
    "categories": [
      "gpt",
      "ai",
      "personality",
      "self-awareness",
      "hogan personality inventory",
      "shiny app",
      "r"
    ],
    "contents": "\r\nOne of my friends recently asked me if I could provide him with an interpretation of a free personality test he took on the internet. As a joke, I asked him if he had already tried using GPT for this.\r\nThis sparked my interest in how GPT would actually handle this kind of task. So, I provided it with my Hogan Personality Inventory (HPI) profile (in percentile scores, as requested), and to my surprise, it performed quite well - even when asked about more complex questions like interactions between my scores on selected scales or my strengths and weaknesses for specific jobs and tasks.\r\nBased on this experience, I created a simple POC app where users can input their HPI profile and some contextual information, such as their current or desired job, and ask GPT predefined or their own questions about their personality.\r\nLink to the app: https://aanalytics.shinyapps.io/ask_your_personality/\r\n\r\nI‚Äôve only tested it on a few profiles, so if you know your HPI profile (or your Big Five traits that are behind HPI), I would be happy to hear how you perceive the face validity and potential usefulness of the generated interpretations. Perhaps I have just become a victim of the well-known Barnum effect üòâ\r\nI am well aware that there are clear risks associated with using a generic GPT for such a task. However, I believe that with proper fine-tuning, an explicit disclaimer, and access to a qualified professional for possible consultation, it could be a useful tool for helping people gain self-awareness more easily - in many situations a crucial prerequisite for high-quality decisions. What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-openai-personality-interpretation/./introPic2.jfif",
    "last_modified": "2023-04-11T20:05:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-engagement-interventions/",
    "title": "Effectiveness of interventions for encreasing employee engagement",
    "description": "What evidence do we have for the effectiveness of interventions for increasing employee engagement? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-27",
    "categories": [
      "employee engagement",
      "work engagement",
      "interventions",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is some evidence that engagement of employees has positive causal impact on the bottom line of organizations (see, for example, the meta-analysis by Harter et al.¬†(2010), however, be aware of the specific definition of engagement used there, which focuses more on the contextual factors and conditions enabling engagement and less on the psychological states of engagement). Consequently, we might be naturally interested in whether we can positively influence engagement of employees through the use of certain interventions.\r\nBased on a systematic review and meta-analysis of studies with controlled workplace interventions by Knight, Patterson, and Dawson (2017), it seems the answer might be yes. The authors found a small positive effect on work engagement and each of its three sub-components: vigor, dedication, and absorption, as measured by the Utrecht Work Engagement Scale (UWES) from Bakker and Schaufeli.\r\nWhen it comes to the types of intervention (personal resource building, job resource building, leadership training, and health promotion), a moderator analysis did not find evidence for their differing effectiveness. However, there was evidence for a medium to strong effect of intervention style in favor of group interventions (vs.¬†individual), with the possible explanation being that group interventions effectively influence certain work engagement antecedents, such as social support and influence in decision-making.\r\n\r\nRegarding the sustainability of effects, there was no significant effect of time in the case of overall work engagement. However, for the vigor sub-component, there were stronger effects immediately post-intervention than at follow-up, with the opposite being true for dedication and absorption sub-components.\r\nHave you implemented any interventions to boost engagement of employees in your organization? Did you measure their effectiveness? What were your experiences and results? Feel free to share your thoughts in the comments below.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-engagement-interventions/./training.jpg",
    "last_modified": "2023-04-11T19:33:41+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-managers-overconfidence/",
    "title": "Where do managers put on their rose-tinted glasses the most?",
    "description": "In which areas are managers and leaders prone to overconfidence, and how can this overconfidence potentially impact team functioning? Let's check some data to address this question.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-23",
    "categories": [
      "team management",
      "blind spots",
      "rocket model",
      "team assessment survey"
    ],
    "contents": "\r\nThe ‚Äúillusory superiority‚Äù or ‚Äúbetter-than-average effect‚Äù causes individuals to overestimate their abilities compared to others. Unsurprisingly, even managers and leaders fall for this trap. After all, they‚Äôre human too, right? üòâ For a review of studies on this topic, see, for example, the systematic review by Heavey et al.¬†(2022).\r\nTo pinpoint particular areas where managers and leaders are prone to overconfidence, data from the Team Assessment Survey (TAS), a team assessment tool rooted in the Rocket Model of team performance, can be valuable as it enables comparisons between how team leaders and team members perceive their team‚Äôs functioning. For example, based on data from a Slovak sample of 85 teams with 835 members, it seems that team leaders rate their team‚Äôs effectiveness way better than team members in the following five areas in descending order:\r\nResources: Does the team have the budget, equipment, authority, and political capital it needs to accomplish its goals?\r\nTalent: Is the team sized correctly? Are team members‚Äô roles clear? Does the team have the right skills to succeed? Are people developing new skills? Do rewards encourage or discourage teamwork?\r\nContext: Are team members in agreement about the team‚Äôs political and economic realities, customers, competitors, suppliers, and key assumptions and challenges?\r\nMission: What is the team‚Äôs purpose? What are its goals? How does the team define winning? What are its strategies and plans for accomplishing its goals?\r\nCourage: Do team members trust each other? Is there an optimal level of tension and collaboration on this team? Do team members challenge each other in a constructive manner?\r\n\r\nWhen managers get overconfident in these areas, it can lead to all sorts of issues like underinvestment in critical resources, inefficient resource allocation, ignoring skill gaps, insufficient role clarity, making bad decisions, disjointed strategic planning, confusion, misaligned priorities, uncoordinated efforts, blocking innovation, etc.\r\nWhat‚Äôs the fix? Among other things, managers need to be aware of their own biases and work on open communication, feedback, and collaboration with team members. Easier said than done, but it‚Äôs crucial to avoid hurting the performance of the team.\r\nObviously, the size and demographics of the sample used are limited and conclusions are therefore difficult to generalize, but I may try to check with Dr.¬†Gordon Curphy, the author of the Rocket Model of team performance and TAS, to see if this pattern is also replicated in a larger, international sample.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-managers-overconfidence/./pinkGlasses.jpg",
    "last_modified": "2023-04-11T15:58:18+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-app-piloting-and-dif/",
    "title": "Estimating the impact of a new business app by piloting & method of difference-in-differences",
    "description": "What is the benefit of using the difference-in-differences method in combination with piloting a new business app, and how can this help estimate the app's effectiveness on key outcomes like time spent with prospects or closed deals?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-20",
    "categories": [
      "pilot program",
      "difference in differences",
      "data-driven decision-making"
    ],
    "contents": "\r\nWhen considering the introduction of a new business app, one of the benefits of piloting it is that it provides a great opportunity to test its causal impact on business processes or outcomes of interest. For example, in our pilots, apps are typically implemented for 3 or 6 months only in some teams and not in others. Such a situation creates ideal conditions for applying the difference-in-differences (DiD) method, which is used to approximate an experimental research design with observational data only.\r\nTo use one specific example, one of the problems addressed by our Sales Analytics app is that sales reps spend more time collaborating internally instead of communicating with prospects. The premise may be that the better visibility into time spent that the app enables will help sales reps and their managers better plan activities during their regular weekly 1-on-1 meetings, all with (hopefully) a positive impact on time spent with prospects.\r\nBy piloting the app in just one team and finding another team with a similar, parallel trend in the selected criterion, we can try to estimate its effectiveness. As shown in the attached chart, the fitted DiD model in this particular case slightly supports the effectiveness of the app, at least in terms of the amount of time spent with prospects, but can easily be switched to another criterion that is closer to the company‚Äôs bottom line, e.g., the number of closed deals. Moreover, if we are aware of certain systematic differences between the teams, such as the experience level of the sales reps, we can include relevant control variables in the model to achieve a more accurate estimation of the app‚Äôs effectiveness.\r\n\r\nSo, next time you consider introducing a new business app, consider piloting it in combination with the DiD method to better understand its impact on your organization‚Äôs goals. Happy piloting! ‚úåÔ∏è\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-app-piloting-and-dif/./impact.jpg",
    "last_modified": "2023-04-11T15:04:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-meeting-yes-men/",
    "title": "Are there meeting ‚Äúyes-men‚Äù?",
    "description": "One of our clients was struggling with meeting overload and wanted to know if the people who attend too many meetings are the kind of \"yes-men\" who just can't say no to meeting invites. You know the type - always saying \"yes\" and never protecting their precious time. What did they find?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-17",
    "categories": [
      "meeting culture",
      "meeting overload"
    ],
    "contents": "\r\nTo test this hypothesis, they looked at the monthly number of meetings people attended and the relative frequency of their responses to meeting invitations. Here‚Äôs what they found:\r\nPeople seemed to be similarly explicit in signaling their intentions about the meetings they were invited to, regardless of how many meetings they had on their plate.\r\nPeople accepted fewer meeting invites the more meetings they attended.\r\nThose who went to a bunch of meetings were more likely to say they‚Äôre not sure if they can make it or not.\r\nPeople who were busy with meetings declined more meeting invites than those who had fewer meetings.\r\n\r\nThus, contrary to initial expectations, the data showed that people who attended more meetings, on average, tended to accept fewer invites, were more likely to be unsure about their availability, and actually declined more invites than those with fewer meetings.\r\nWhile the client couldn‚Äôt rule out that there might be some individuals fitting ‚Äúyes-men‚Äù description in their company (in fact, one can easily spot a few people in the corresponding chart who attended many meetings and at the same time underutilized the option of declining the meeting invites), these results suggested that there isn‚Äôt a systematic problem in this specific area. Time for our client to explore other avenues through which the problem with meeting overload could be addressed. More on that in some of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-meeting-yes-men/./yes_man_yes.gif",
    "last_modified": "2023-04-11T14:44:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/",
    "title": "Impact of employee satisfaction at work on a company's bottom line",
    "description": "While there is evidence supporting the connection between employee satisfaction and a company's bottom line, it's essential to determine whether higher satisfaction directly causes better performance. Is there some evidence for that? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-14",
    "categories": [
      "performance",
      "employee satisfaction",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is now little doubt that employee attitudes, such as employee satisfaction or employee engagement, are to some extent related to performance - for some evidence in support of this claim, see, for example, the meta-analysis of the relationships between employee satisfaction, employee engagement and business unit-level performance conducted by Harter, Schmidt, & Hayes (2002).\r\nBut does it mean that higher employee satisfaction causes higher performance? As any statistically savvy person knows, not necessarily.\r\nFortunately, there are research designs that can help us untangle this conundrum a bit. One of these designs is path analysis of longitudinal/time-series data. This approach was also taken by Harter et al.¬†(2010) in their meta-analysis ‚ÄúCausal Impact of Employee Work Perceptions on the Bottom Line of Organizations‚Äù with the following results:\r\n‚ÄúUsing a massive longitudinal database that included 2,178 business units in 10 large organizations, we found evidence supporting the causal impact of employee perceptions on [‚Ä¶] bottom-line measures [customer loyalty, employee retention, revenue, and profit]; reverse causality of bottom-line measures on employee perceptions existed but was weaker.‚Äù\r\nThe attached figure with the two alternative path models clearly shows that the causal path from employee perceptions to outcomes is stronger than the other way around, especially when it comes to theoretically more proximal outcomes such as employee retention and customer loyalty.\r\n\r\nDespite not being free of potential biases, these results add more weight to the argument that investing in improving employee job satisfaction also makes sense for improving a company‚Äôs bottom line, rather than ‚Äúonly‚Äù improving employee well-being. The question now is what interventions to improve employee satisfaction might work. Let‚Äôs review the available evidence on this issue in one of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/./successfulEmployees.jpg",
    "last_modified": "2023-04-11T13:39:56+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-excel-and-python/",
    "title": "Excel + Python = Word Document",
    "description": "Using combination of Excel and Python for semi-automatic Word document generation.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-10",
    "categories": [
      "excel",
      "python",
      "document",
      "automation"
    ],
    "contents": "\r\nIn the spirit of ‚ÄúExcel isn‚Äôt dead and is actually doing well‚Äù posts, I‚Äôm sharing one practical example of combining Excel and Python to do one specific job. It‚Äôs not as exciting and sexy as ChatGPT, but it may still come in handy for someone, as it did for one of my friends.\r\nA friend of mine who works in psychological counselling has to write a large number of reports which, among other things, contain many recommendations for various compensations and interventions depending on established diagnosis.\r\nTo make his job easier, he created a simple Excel spreadsheet with a list of diagnoses and corresponding recommendations. He needed to generate a simple Word document listing and describing the appropriate compensations and interventions after he had marked the appropriate diagnoses for the client in Excel.\r\nI originally wanted to do it all in Excel, but since I‚Äôm not the best friend with VBA, I couldn‚Äôt get rid of the various text formatting issues. So I reached for Python and one of its document-related libraries and linked it via macro to Excel, which acts only as a source of input data, based on which Python generates a simple report when a button is pressed in Excel. Once generated, the document is ready for further editing and tuning.\r\nIf you are interested in technical details, you can check my GitHub page.\r\nMay the Excel be with you üôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-excel-and-python/./comboPic.png",
    "last_modified": "2023-04-11T13:06:43+02:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2023-04-11-job-attitudes-and-employee-outcomes/",
    "title": "Employee outcomes & employees' job attitudes",
    "description": "What employee outcomes are predicted by what employees' job attitudes? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-06",
    "categories": [
      "job attitudes",
      "job performance",
      "employee turnover",
      "organizational citizenship behavior"
    ],
    "contents": "\r\nIf you‚Äôve ever measured employee job attitudes (i.e.¬†constructs such as organizational commitment, procedural justice, distributive justice, job involvement, job engagement, job satisfaction, etc.), you probably won‚Äôt be surprised to learn that job attitudes are usually quite strongly correlated.\r\nThis is also the conclusion of a meta-analytic review of job attitudes by Woznyj et al.¬†(2022), which showed that job attitudes are moderately to strongly correlated with each other, with most relations falling between œÅ = .50 and .69.\r\nDespite this, relative weights and incremental validity analyses revealed that some attitudes have greater validity in predicting key employee outcomes. As shown in the table attached,\r\nperformance is most strongly predicted by job satisfaction, job engagement, and distributive justice (an employee‚Äôs perceived fairness of outcomes),\r\nturnover intentions is most strongly predicted by job satisfaction, perceived organizational support (a general evaluation regarding the extent to which employees feel their organization values their contribution and cares about their well-being), and distributive justice, and\r\norganizational citizenship behaviors is most strongly predicted by job engagement, procedural justice (perceived fairness of the means, or procedures, used to determine outcomes), and job involvement (the degree to which a person identifies psychologically with his or her work, or the importance of work in his or her total self-image).\r\n\r\nIMO, knowing this can be quite useful in planning what candidate constructs to measure in your company in an effort to support specific employee outcomes.\r\nWhat job attitudes do you regularly measure in your company? And does it pay off in any way, i.e.¬†does it have any tangible impact?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-job-attitudes-and-employee-outcomes/./employees.jpg",
    "last_modified": "2023-04-11T12:51:00+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-05-micro-breaks/",
    "title": "Effectiveness of micro-breaks at work",
    "description": "Quite satisfying news from a meta-analysis on the efficacy of micro-breaks for increasing well-being and performance in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "micro-break",
      "workplace",
      "well-being",
      "performance",
      "meta-analysis"
    ],
    "contents": "\r\nIf you feel you need a short break, take one - even if it is no longer than 10 minutes, it can increase your perceived well-being and some types of performance.\r\nAs the authors of the study, Albulescu et al.¬†(2022), conclude: ‚ÄúOur results revealed that micro-breaks are efficient in preserving high levels of vigor and alleviating fatigue. It seems that the effects are univocal and generalizable for the well-being outcomes. These were relatively homogeneous, and none of the included moderators were significant. Hence, the data suggest that micro-breaks may be a panacea for fostering well-being during worktime.\r\nWhen it comes to performance, the data revealed some nuances. The break duration was a significant covariate of the effect of micro-breaks: the longer the break, the better the performance. Moreover, the type of task from which participants were taking the break also emerged as a significant moderator. Micro-breaks could significantly increase performance for clerical work or creative exercises and not for a cognitively demanding task.‚Äù\r\nThere is nothing like having a meta-analysis to back up your habits, or better yet, initially bad habits üòâ\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-05-micro-breaks/./break.jpg",
    "last_modified": "2023-03-05T19:50:59+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-01-distracted-time/",
    "title": "Not-so-hidden cost of working in an office",
    "description": "Just a few data-backed thoughts on why many of us may often feel more distracted when working in an office.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-01",
    "categories": [
      "remote work",
      "in-office work",
      "collaboration",
      "focus time",
      "distraction"
    ],
    "contents": "\r\nWe have only recently started to measure the allocation of work time to collaborative and non-collaborative activities at Time is Ltd..\r\n\r\nWhen checking the resulting numbers in the context of where people work from, it wasn‚Äôt that much of a surprise that people spend less time (on average ~37 minutes less per person per day) on collaborative activities when working remotely vs.¬†in the office. Such a pattern is probably a good thing in many cases as it means that people have more time for focused work when working remotely, and use their time for intensive collaboration when in the office.\r\nWhat was quite surprising to me, however, was the rather large difference in the amount of distracted time. When working in the office there was much more distracted time, i.e.¬†time when people were working on their tasks while being distracted by various collaborative activities. Therefore, they didn‚Äôt have enough time to get into the flow. On average, the difference was a staggering ~69 minutes per person per day.\r\nHowever, when I thought about it a bit more (and also after I realized how we actually calculate distraction timeÔ∏è), it started to make more sense to me. After all, collaborative activities don‚Äôt just ‚Äúrob‚Äù us of time per se, they also fragment our available time into smaller chunks, which carries a cost in the form of cognitive overhead associated with task switching.\r\nI suppose it‚Äôs a trade-off that can‚Äôt be completely solved in principle, but can only be mitigated with tools like batching or timeboxing. What is your own experience with this phenomenon and what tools do you use to deal with it?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-01-distracted-time/./distraction.jpg",
    "last_modified": "2023-03-01T19:41:32+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-chatgpt-and-employee-feedback/",
    "title": "Using ChatGPT to summarize and explore employee feedback?",
    "description": "What's the potential use of tools like ChatGPT in analyzing open-ended feedback from employee engagement and satisfaction surveys? Let's take a look at the result of my little experiment in this area.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "chatgpt",
      "ai",
      "hrm",
      "people analytics",
      "employee engagement",
      "surveys"
    ],
    "contents": "\r\nIMHO, ChatGPT and similar tools may be quite useful in the near future (and probably to some extent even today) to quickly summarize and explore the feedback provided by employees through open-ended questions in employee engagement and satisfaction surveys.\r\nI just experimented with a small sample of anonymized employee feedback (just dozens of lines of text from a question about what employees would change in the company) and asked ChatGPT to summarize the feedback, identify the main topics covered in the feedback, and provide me with details about the selected topics - see the attached image of the conversation for illustration.\r\n\r\nJust based on my very limited sample, I found that there was a pretty good balance between information compression and accuracy, while the interaction was very natural, similar to asking HRBP what the results of the last ESS were for my department/team. Certainly, there are still too many known and unknown risks associated with these tools to rely blindly on them alone, but I can imagine that in the foreseeable future, when many of these risks are successfully mitigated, this will be one of the ways managers will listen to the voice of their people.\r\nHas anyone experimented with ChatGPT on similar kinds of HR data?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-chatgpt-and-employee-feedback/./chatgpt_listening.jpg",
    "last_modified": "2023-02-26T19:30:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-large-and-recurring-meetings/",
    "title": "Where to look first when considering meeting reset?",
    "description": "Let's briefly discuss the potential benefits of focusing on optimizing large recurring meetings to save time in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-19",
    "categories": [
      "meeting overload",
      "meeting culture",
      "meeting reset"
    ],
    "contents": "\r\nWhen thinking about the meeting reset like they did in Asana, IMHO, it‚Äôs not a bad idea to focus on one specific category of meetings first.\r\nI am thinking of large recurring meetings that combine two big time wasters, and thus hide a greater opportunity for time savings:\r\nLarge meetings are quite often a waste of people‚Äôs time as they don‚Äôt allow everyone to meaningfully contribute. These meetings often serve only to disseminate information and can therefore be safely replaced by less intrusive asynchronous collaboration tools such as email, instant messaging or some kind of knowledge management tool.\r\nRecurring meetings have their place, but often tend to outlive their purpose over time and waste the time of everyone involved. One should therefore regularly ask oneself the following questions in this context and act accordingly: Are meetings being planned automatically, rather than out of necessity? Are attendees invited as a formality, or will they bring value? Is there still clear agenda for these meetings? Are we already sufficiently aligned or do we need some additional platform for doing so? Does the current frequency of our meetings meet our needs? etc.\r\nAs illustrated in the attached chart, large and recurring meetings can represent a relatively large chunk of time in an employee‚Äôs work month. Large meetings here account for approximately 37% of all meeting time and more than 80% of large meetings are recurring. This represents a relatively large room for optimizing the time spent in meetings.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-large-and-recurring-meetings/./fatigue.jpg",
    "last_modified": "2023-02-26T19:09:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-hrm-value-chain-and-sem/",
    "title": "HRM value chain and structural equation modeling - Moneyball case",
    "description": "What's the link between the HRM value chain and structural equation modeling? Let‚Äôs check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "hrm value chain",
      "people analytics",
      "path analysis",
      "structural equation modeling",
      "bayesian statistics",
      "r"
    ],
    "contents": "\r\nAs the economy cooled, more posts started appearing in my social\r\nbubble about how people analytics can prove its value, justify its\r\nexistence, and increase its resilience to layoffs.\r\nOne of the often-mentioned conceptual tools in this context is the\r\nHRM value chain which shows how people processes lead\r\nto achieving companies‚Äô desired business outcomes. There are more\r\nversions of this tool, e.g.¬†one from Jaap\r\nPaauwe and Ray Richardson or another one from Max\r\nBlumberg, but what they have in common is a kind of causal flow from\r\nHRM activities to HRM outcomes to business outcomes.\r\nExample of the HRM value chain\r\nmodel from Paauwe\r\n& Richardson (1997).\r\n\r\nExample of the HRM value chain\r\nmodel from Blumberg\r\n(2018).\r\nHowever, as potentially useful as this metaphor of the organization\r\nas a kind of ‚Äúmachine‚Äù with certain inputs, processes, and outputs is,\r\nit is still only a conceptual tool that may or may not correspond to the\r\nreality of a particular organization.\r\nIn this regard, it may help if we try to operationalize this\r\nmetaphor. In this effort, structural\r\nequation modeling can be very handy, as it allows us to\r\nformalize our ideas about the relationships between several different\r\nvariables and to assess the extent to which these ideas are consistent\r\nwith the available data. After all, no one wants to make decisions based\r\non false assumptions.\r\nTo illustrate this with a more tangible example, let‚Äôs use sabermetric data\r\nfrom the famous Moneyball\r\ncase and let‚Äôs try to formally model the Oakland A‚Äôs\r\n(OAK) as a ‚Äúmachine‚Äù that produces playoffs by trying to win more games\r\nor score more points than opposing teams, using inputs in the form of\r\nplayers‚Äô ability to play well at bat and in the field.\r\nBased on our expert knowledge of the game of baseball, our working\r\nhypotheses, and the results of some previous analyses, we can construct\r\na conceptual model of how a baseball team functions, as outlined\r\nbelow.\r\nConceptual model of how a\r\nbaseball team works.\r\nThe diagram shows clearly how this ‚Äúmachine‚Äù works: Its outputs are\r\nqualifications for the playoffs, which it achieves by trying to win more\r\ngames or score more points than the opposing teams; to do this, it uses\r\ninputs in the form of the players‚Äô ability to play well at bat and in\r\nthe field; the inputs affecting the ‚Äúmachine‚Äôs‚Äù operation are also the\r\nsimilar abilities of the opposing teams‚Äô players. This is, of course, a\r\nvery simplistic causal model of how the OAK team operates, but as the\r\nfamous statistical aphorism states, all models are\r\nwrong, but some are useful.\r\nHowever incomplete our models of how an organization works will\r\nalways be, it is essential to check that these models sufficiently\r\nreflect reality as conveyed by the available data. Only after such an\r\nassessment of the plausibility of the model it is reasonable to base\r\nfurther, e.g.¬†hiring or L&D decisions on it. And, as hinted at the\r\nbeginning of this post, we will use the statistical method of structural\r\nequation modeling to do this. So let‚Äôs apply this method to our model of\r\nthe OAK and test its plausibility.\r\nWe will use data from a publicly available database\r\nof MLB historical statistics. Specifically, we will use the\r\nfollowing variables:\r\nqualification for the playoffs in a given season\r\n(Playoffs),\r\nnumber of wins in a given season (W),\r\nnumber of points won in a given season (RS),\r\nnumber of points lost in a given season (RA),\r\naverage frequency with which a player reaches base per plate\r\nappearance in a given season (OBP - On-Base Percentage) and\r\nanalogous statistics for opponent teams (OOBP),\r\naverage number of bases players earn per at bat in a given season\r\n(SLG - Slugging Percentage) and analogous statistics for\r\nopponent teams (OSLG).\r\nIn terms of time, we will work with data from the years 1996-2001,\r\nwhich precede 2002, when the story of Moneyball mostly takes place.\r\n\r\n\r\nShow code\r\n\r\n# uploading set of libraries for data manipulation and visualization\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\nbaseball <- readr::read_csv(\"./baseball.csv\")\r\n\r\n# filtering data used for the analysis\r\nmyData <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995\r\n    ) %>%\r\n  dplyr::select(Team, OBP, SLG, OOBP, OSLG, RS, RA, W, Playoffs)\r\n\r\n# user-friendly table with data used for the analysis\r\nDT::datatable(\r\n  myData,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\nNow let‚Äôs define a formal model of the OAK and fit it to the data\r\nusing the brms R\r\npackage which allows us to make inferences about the model\r\nparameters within a Bayesian\r\ninferential framework. Specifically, we will build and fit a\r\nso-called path\r\nanalysis model, which is a special type of structural equation\r\nmodeling used to describe directed dependencies among a set of\r\nvariables. Given that we have data for a group of the same teams over\r\nseveral seasons, we must also incorporate the hierarchical nature of the\r\ndata into the model.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for fitting Bayesian generalized (non-)linear multivariate multilevel models using Stan for full Bayesian inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# specifying individual parts of the SEM (without modeling the correlation between response variables and using Student's t distribution instead of the Gaussian distribution to make the model more robust) \r\na <- brms::bf(W ~ 1 + RS + RA + (1 + RS + RA | Team), family = student())\r\nb <- brms::bf(RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team), family = student())\r\nc <- brms::bf(RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team), family = student())\r\nd <- brms::bf(Playoffs ~ 1 + W + (1 + W | Team), family = bernoulli())\r\n\r\n# fitting the model\r\nfit <- brms::brm(\r\n  a + b + c + d + set_rescor(FALSE), \r\n  data = myData,\r\n  iter = 3000,\r\n  chains = 3,\r\n  warmup = 500,\r\n  thin = 1,\r\n  seed = 123,\r\n  cores = 5,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n  )\r\n\r\n# checking MCMC convergence\r\n# plot(fit)\r\n# summary(fit)\r\n\r\n\r\n\r\nAfter verifying that the mechanics of the MCMC algorithm work well\r\n(not shown here for brevity reasons), we should also verify how well the\r\nmodel fits the data by using the posterior predictive check for each of\r\nthe observed response variables in our model.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for arbitrarily complex composition of ggplot plots\r\nlibrary(patchwork)\r\n\r\n# posterior predictive check for all predicted observed variables \r\nplayoffsPpc <- brms::pp_check(fit, resp = \"Playoffs\", ndraws = 100) + ggplot2::labs(title = \"Playoffs variable\")\r\nwPpc <- brms::pp_check(fit, resp = \"W\", ndraws = 100) + ggplot2::labs(title = \"W variable\")\r\nrsPpc <- brms::pp_check(fit, resp = \"RS\", ndraws = 100) + ggplot2::labs(title = \"RS variable\")\r\nraPpc <- brms::pp_check(fit, resp = \"RA\", ndraws = 100) + ggplot2::labs(title = \"RA variable\")\r\n\r\nppc <- (playoffsPpc + wPpc) / (rsPpc + raPpc)\r\n\r\nprint(ppc)\r\n\r\n\r\n\r\n\r\nAs you can see, the model fits the data pretty well. Now we can look\r\nat the coefficients in the individual parts of the model. All of them\r\nshow non-zero values and are in directions that are consistent with our\r\nexpectations embodied in our conceptual model, i.e., all are positive\r\nexcept for the coefficient of the RA variable (number of points\r\nlost) as a predictor of the number of games won (W). These\r\nresults thus give us greater confidence in following our conceptual\r\nmodel of team functioning when making decisions about allocating our\r\nlimited resources.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting functions for posterior analysis, MCMC diagnostics, prior and posterior predictive checks, and other visualizations to support the applied Bayesian workflow\r\nlibrary(bayesplot)\r\n\r\n# getting overview of all parameters \r\n# get_variables(fit)\r\n# relevant parameters: \"b_RS_OBP\", \"b_RS_SLG\", \"b_RA_OOBP\", \"b_RA_OSLG\", \"b_W_RS\", \"b_W_RA\", \"b_Playoffs_W\"\r\n\r\nb_RS_OBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_OBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OBP variable in the RS ~ OBP part\"\r\n    ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n    )\r\n\r\n\r\nb_RS_SLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_SLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"SLG variable in the RS ~ SLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OOBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OOBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OOBP variable in the RA ~ OOBP part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OSLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OSLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OSLG variable in the RA ~ OSLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RS <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RS\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RS variable in the W ~ RS part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RA <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RA\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RA variable in the W ~ RA part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_Playoffs_W <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_Playoffs_W\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"W variable in the Playoffs ~ W part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\ncoefCharts <- (b_RS_OBP + b_RS_SLG + b_RA_OOBP) / (b_RA_OSLG + b_W_RS + b_W_RA) / (b_Playoffs_W + patchwork::plot_spacer() + patchwork::plot_spacer()) +\r\n  plot_annotation(\r\n    title = 'Posterior interval estimations',\r\n    caption = \"The solid vertical lines represent the point (median) estimates and the shaded areas represent the 95% credible interval.\",\r\n    theme = theme(\r\n      plot.title = element_text(size = 18),\r\n      plot.caption = element_text(size = 11)\r\n      )\r\n    )\r\n\r\nprint(coefCharts)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model  \r\nsummary(fit)\r\n\r\n\r\n Family: MV(student, student, student, bernoulli) \r\n  Links: mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = logit \r\nFormula: W ~ 1 + RS + RA + (1 + RS + RA | Team) \r\n         RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team) \r\n         RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team) \r\n         Playoffs ~ 1 + W + (1 + W | Team) \r\n   Data: myData (Number of observations: 90) \r\n  Draws: 3 chains, each with iter = 2500; warmup = 0; thin = 1;\r\n         total post-warmup draws = 7500\r\n\r\nGroup-Level Effects: \r\n~Team (Number of levels: 30) \r\n                                   Estimate Est.Error l-95% CI\r\nsd(W_Intercept)                        1.63      1.95     0.04\r\nsd(W_RS)                               0.00      0.00     0.00\r\nsd(W_RA)                               0.00      0.00     0.00\r\nsd(RA_Intercept)                      12.21     10.88     0.57\r\nsd(RA_OOBP)                           32.60     30.04     1.38\r\nsd(RA_OSLG)                           26.88     23.83     1.07\r\nsd(RS_Intercept)                      11.86     10.01     0.45\r\nsd(RS_OBP)                            33.58     27.10     1.55\r\nsd(RS_SLG)                            28.59     22.99     1.18\r\nsd(Playoffs_Intercept)                 2.20      2.16     0.09\r\nsd(Playoffs_W)                         0.22      0.47     0.00\r\ncor(W_Intercept,W_RS)                 -0.24      0.50    -0.96\r\ncor(W_Intercept,W_RA)                 -0.23      0.52    -0.97\r\ncor(W_RS,W_RA)                        -0.22      0.51    -0.95\r\ncor(RA_Intercept,RA_OOBP)             -0.20      0.51    -0.94\r\ncor(RA_Intercept,RA_OSLG)             -0.21      0.51    -0.95\r\ncor(RA_OOBP,RA_OSLG)                  -0.19      0.51    -0.95\r\ncor(RS_Intercept,RS_OBP)              -0.16      0.52    -0.94\r\ncor(RS_Intercept,RS_SLG)              -0.18      0.52    -0.95\r\ncor(RS_OBP,RS_SLG)                    -0.20      0.51    -0.94\r\ncor(Playoffs_Intercept,Playoffs_W)    -0.18      0.58    -0.98\r\n                                   u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsd(W_Intercept)                        7.45 1.01     1084     1547\r\nsd(W_RS)                               0.01 1.01      934     1938\r\nsd(W_RA)                               0.01 1.01     1012     3299\r\nsd(RA_Intercept)                      40.57 1.00     2271     3697\r\nsd(RA_OOBP)                          111.19 1.00     1464     2342\r\nsd(RA_OSLG)                           85.24 1.00      894     2025\r\nsd(RS_Intercept)                      38.03 1.00     1569     1794\r\nsd(RS_OBP)                           101.92 1.00      507      468\r\nsd(RS_SLG)                            86.47 1.00      654     2468\r\nsd(Playoffs_Intercept)                 7.42 1.00     2058     2623\r\nsd(Playoffs_W)                         1.80 1.07       33       16\r\ncor(W_Intercept,W_RS)                  0.79 1.00     4156     4013\r\ncor(W_Intercept,W_RA)                  0.82 1.00     1522     1654\r\ncor(W_RS,W_RA)                         0.80 1.00     3192     4270\r\ncor(RA_Intercept,RA_OOBP)              0.81 1.00     3376     3918\r\ncor(RA_Intercept,RA_OSLG)              0.80 1.00     3097     4358\r\ncor(RA_OOBP,RA_OSLG)                   0.84 1.01     1295     1547\r\ncor(RS_Intercept,RS_OBP)               0.85 1.00     1162     2565\r\ncor(RS_Intercept,RS_SLG)               0.84 1.00      983     1246\r\ncor(RS_OBP,RS_SLG)                     0.82 1.01      503     1922\r\ncor(Playoffs_Intercept,Playoffs_W)     0.93 1.01      187      219\r\n\r\nPopulation-Level Effects: \r\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nW_Intercept           85.97      5.78    74.51    97.16 1.00     4765\r\nRA_Intercept        -852.33     64.99  -982.24  -724.71 1.00     2989\r\nRS_Intercept        -949.77     67.88 -1084.15  -820.70 1.00     4559\r\nPlayoffs_Intercept  -471.68    923.11 -3529.81   -41.17 1.07       30\r\nW_RS                   0.10      0.01     0.09     0.11 1.00     3938\r\nW_RA                  -0.10      0.01    -0.11    -0.09 1.00     4087\r\nRA_OOBP             2850.08    297.29  2262.09  3420.05 1.00     2622\r\nRA_OSLG             1600.06    187.41  1241.97  1976.70 1.00     4134\r\nRS_OBP              3475.86    272.82  2955.81  4015.43 1.00     4594\r\nRS_SLG              1330.99    160.56  1014.09  1637.60 1.00     3936\r\nPlayoffs_W             5.30     10.37     0.46    39.82 1.07       30\r\n                   Tail_ESS\r\nW_Intercept            4672\r\nRA_Intercept           3771\r\nRS_Intercept           5211\r\nPlayoffs_Intercept       16\r\nW_RS                   5662\r\nW_RA                   4003\r\nRA_OOBP                2861\r\nRA_OSLG                4506\r\nRS_OBP                 3139\r\nRS_SLG                 4062\r\nPlayoffs_W               16\r\n\r\nFamily Specific Parameters: \r\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma_W      3.30      0.31     2.70     3.91 1.00     1995     1334\r\nsigma_RA    22.29      2.41    17.83    27.36 1.00     1130     1360\r\nsigma_RS    18.97      2.15    14.93    23.35 1.00     2979     3893\r\nnu_W        23.65     14.23     5.60    59.37 1.00     2419     1912\r\nnu_RA       21.98     14.02     4.93    57.35 1.00     1839     2379\r\nnu_RS       20.98     13.86     4.49    56.41 1.00     1853     2595\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nHowever, the main advantage of such a formalized model is that we can\r\nnow make specific predictions or simulations that will help us make more\r\ngranular decisions about where and how much to invest our limited\r\nresources, e.g.¬†in terms of hiring and/or L&D activities. For\r\nexample, we know from the available data that teams need to win\r\napproximately 95 games to have a high chance of making the playoffs -\r\nsee the two graphs below.\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 1\r\nset.seed(1234)\r\nmyData %>%\r\n  # creating random numbers for dispersing points across the y axis of the graph\r\n  dplyr::mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot2::ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs))) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5)) +\r\n  ggplot2::scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"The team has not made it to the playoffs\", \"The team has made it to the playoffs\")) +\r\n  ggplot2::labs(\r\n    title = \"Teams that didn't/made the playoffs from 1996-2001\",\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 2\r\n\r\nmyData %>%\r\n  # dividing W variable into intervals \r\n  dplyr::mutate(\r\n    WCat = cut(W, breaks = 21, right = TRUE, include.lowest = TRUE, ordered_result = TRUE)\r\n  ) %>% \r\n  dplyr::group_by(WCat) %>%\r\n  dplyr::summarise(\r\n    PlayoffsProb = mean(Playoffs)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = WCat, y = PlayoffsProb)) +\r\n  ggplot2::geom_bar(stat = \"identity\", color = NA, fill = \"lightblue\") +\r\n  ggplot2::scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1), labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"PROBABILITY OF MAKING IT TO THE PLAYOFFS\",\r\n    title = \"Relation between the number of wins in the regular season and the probability\\nof advancing to the playoffs (1996-2001)\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nConsidering the last known OAK performance statistics from 2001, what\r\nnumber of games can OAK expect to win and what is the likelihood of\r\nmaking the playoffs next year? Let‚Äôs plug the 2001 OAK numbers into the\r\nmodel and check the prediction including all uncertainties.\r\n\r\n\r\nShow code\r\n\r\n# summary prediction of the number of matches won by OAK in 2002 using OAK's statistics from 2001 as input\r\n# set.seed(123)\r\n# predict(\r\n#   fit, \r\n#   resp = \"W\", \r\n#   newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n# )\r\n\r\n# generating predictions using OAK's statistics from 2001 as input\r\nset.seed(123)\r\nwp <- brms::posterior_predict(\r\n  fit, \r\n  resp = \"W\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n  )\r\n\r\n# actual number of matches won by OAK in 2002\r\nrealNWins <- baseball %>%\r\n  filter(Year == 2002, Team == \"OAK\") %>%\r\n  pull(W)\r\n\r\n# uploading library for visualizations of distributions and uncertainty\r\nlibrary(ggdist)\r\n\r\n# creating the graph\r\nwp %>%\r\n  as.data.frame() %>% \r\n  ggplot2::ggplot(aes(x = V1)) +\r\n  ggdist::stat_halfeye(\r\n    fill = \"lightblue\",\r\n    .width = c(0.80, 0.95),\r\n      ) +\r\n  ggplot2::geom_vline(xintercept = realNWins, linetype = \"dashed\") +\r\n  ggplot2::geom_text(x = realNWins+8.3, y = 1, label = \"Actual number of matches won by OAK in 2002\") +\r\n  ggplot2::scale_x_continuous(breaks = seq(80,120,5)) +\r\n  ggplot2::labs(\r\n    x = \"PREDICTED NUMBER OF MATCHES WON (W)\",\r\n    y = \"DENSITY\",\r\n    linetype = \"\",\r\n    title = \"Predicted number of matches won by OAK in 2002\",\r\n    caption = \"\\nThe black horizontal lines at the bottom of the graph represent the 80% and 95% probability intervals, respectively.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"none\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see that the 95% probability interval of the predicted number\r\nof games won by the OAK in 2002 is safely above the 95-point threshold.\r\nAnd when we compare the prediction to the 2002 reality, we see that they\r\nare pretty close. Also, the projected probability of OAK advancing to\r\nthe playoffs is very high, which is consistent with the fact that OAK\r\nsuccessfully qualified for the playoffs in 2002.\r\n\r\n\r\nShow code\r\n\r\n# prediction of OAK making the playoffs in 2002 using OAK's statistics from 2001 as input \r\nset.seed(123)\r\npredict(\r\n  fit, \r\n  resp = \"Playoffs\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG),\r\n  probs = c(0.025, 0.5, 0.975)\r\n)\r\n\r\n\r\n      Estimate Est.Error Q2.5 Q50 Q97.5\r\n[1,] 0.9993333  0.025813    1   1     1\r\n\r\nBased on this information, we may conclude that there is no urgent\r\nneed to invest heavily now in increasing the current quality of the\r\nteam‚Äôs play at bat and in the field (variables OBP and\r\nSLG), assuming that the opposing teams do not significantly\r\nincrease the quality of their play next year (variables OOBP\r\nand OSLG). Using the model, we can also obtain a specific range\r\nof the quality of the OAK‚Äôs play at bat and in the field that would\r\nstill allow it to reach at least 95 games won. Such information could be\r\nuseful, for example, in deciding which players can be safely traded\r\nwithout having a detrimental effect on the likelihood of making the\r\nplayoffs.\r\n\r\n\r\nShow code\r\n\r\n# simulation of the effect of SLG and OBP on the number of OAK games won in 2002\r\n\r\n# getting plausible range of values of OBP and SLG variables\r\nstats <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995, \r\n    Team == \"OAK\"\r\n    ) %>%\r\n  dplyr::summarise(\r\n    dplyr::across(c(OBP, SLG), list(mean = mean, sd = sd))\r\n  )\r\n\r\n# creating a sequence of plausible values for OBP and SLG variables (3 SDs around the mean value)   \r\nOBPSeq <- seq(from = stats$OBP_mean + (3*stats$OBP_sd), to = stats$OBP_mean - (3*stats$OBP_sd), length.out = 20)\r\nSLGSeq <- seq(from = stats$SLG_mean + (3*stats$SLG_sd), to = stats$SLG_mean - (3*stats$SLG_sd), length.out = 20)\r\n\r\n# creating df with all possible combinations of plausible values of OBP and SLG variables\r\nsimDf <- expand.grid(OBP = OBPSeq, SLG = SLGSeq)\r\n\r\n# getting variables necessary for prediction (statistics of OAK's opponent teams from 2001)\r\nstatsO <- baseball %>%\r\n  dplyr::filter(Year == 2001, Team == \"OAK\") \r\n\r\n# adding Team and W variables\r\nsimDf <- simDf %>%\r\n  dplyr::mutate(\r\n    Team = \"OAK\",\r\n    W = NaN\r\n  )\r\n\r\n# running the simulation\r\nfor(i in 1:nrow(simDf)){\r\n  \r\n  #print(i)\r\n  \r\n  # prediction of the RS response variable\r\n  RSPred <- predict(\r\n    fit, \r\n    resp = \"RS\", \r\n    newdata = simDf[i,]\r\n  )[1]\r\n  \r\n  # prediction of the W response variable\r\n  WPred <- predict(\r\n    fit, \r\n    resp = \"W\", \r\n    newdata = data.frame(Team = \"OAK\", RS = RSPred, RA = statsO$RA)\r\n  )[1]\r\n  \r\n  simDf[i,\"W\"] <- WPred  \r\n  \r\n}\r\n\r\n# creating the graph\r\nsimDf %>%\r\n  ggplot2::ggplot(aes(x = OBP, y = SLG, color = W)) +\r\n  ggplot2::geom_point(alpha = 18, size = 3) +\r\n  ggplot2::scale_y_continuous(breaks = seq(0.36, 0.52, 0.02)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0.32, 0.38, 0.01)) +\r\n  ggplot2::scale_colour_gradient2(\r\n    low = \"blue\",\r\n    mid = \"white\",\r\n    high = \"red\",\r\n    midpoint = 95,\r\n    space = \"Lab\",\r\n    na.value = \"grey50\",\r\n    guide = \"colourbar\",\r\n    aesthetics = \"colour\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Simulation of the effect of SLG and OBP on the number of OAK games won in 2002\",\r\n    caption = \"The white color corresponds to the 95-point threshold to qualify for the playoffs.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nAs many of you know, thanks to similar analyses, the OAK management\r\nbegan to select players for their team who, although they did not meet\r\nthe traditional criteria by which scouts judged the quality of baseball\r\nplayers, exhibited exactly those characteristics that, according to\r\nconducted analyses, predicted the number of points won and lost, and\r\nthus the likelihood of advancing to the playoffs, which was the\r\nmanagement‚Äôs main goal. Because competing teams underestimated the\r\nimportance of these player statistics and overestimated other, less\r\nimportant variables (e.g., batting average), OAK management was able to\r\npurchase players relatively inexpensively who enabled them to achieve\r\ntheir goals. As a result, the OAK won 20 more games per season than\r\nsimilarly ‚Äúpoor‚Äù teams and about the same number of games as 2 to 3\r\ntimes richer competition. The power of data in practice!\r\n\r\n\r\nShow code\r\n\r\n# salaries vs number of matches won \r\n# uploading data from the Lahman's Baseball Database, which is publicly available at https://www.seanlahman.com/baseball-archive/statistics/\r\n\r\nplayersSalaries <- readr::read_csv(\"./salaries.csv\")\r\nteamsWins <- readr::read_csv(\"./teams.csv\")\r\n\r\n# computing average sum of salaries paid by each team to its players in 1998-2001\r\nplayersSalariesAvg <- playersSalaries %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n    ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(playersSalariesAvg = sum(salary)/length(unique(yearID)))\r\n\r\n# calculating the average number of wins per season for each team from 1998-2001\r\nteamsWinsAvg <- teamsWins %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n  ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(teamsWinsAvg = sum(W)/length(unique(yearID)))\r\n\r\n\r\n\r\nplayersSalariesAvg %>%\r\n  dplyr::left_join(teamsWinsAvg , \"teamID\") %>%\r\n  dplyr::mutate(OAK = ifelse(teamID == \"OAK\", \"yes\", \"no\")) %>%\r\n  ggplot2::ggplot(aes(x= playersSalariesAvg, y = teamsWinsAvg, fill = OAK)) +\r\n  ggplot2::geom_point()+\r\n  ggplot2::labs(\r\n    title = \"Player salaries and number of wins in 1998-2001\",\r\n    x = \"AVERAGE SUM OF PLAYERS' SALARIES (USD)\",\r\n    y = \"AVERAGE NUMBER OF WINS PER SEASON\"\r\n    ) +\r\n  ggrepel::geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50') +\r\n  ggplot2::scale_fill_manual(\r\n    values = c(\"#ffffff\", \"#ffd400\"), \r\n    labels = c(\"no\",\"yes\")\r\n    ) +\r\n  ggplot2::scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  ggplot2::scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07), labels = scales::number_format(scale = 0.000001, suffix = \"M\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"node\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nFinally, let‚Äôs breathe some life into the dry numbers by watching a\r\nshort clip from the Moneyball film, which\r\nnicely summarizes some of the ideas presented in this blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-hrm-value-chain-and-sem/./baseball.jpg",
    "last_modified": "2023-02-09T10:09:16+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-org-chart-and-collaboration/",
    "title": "Org chart and collaboration",
    "description": "How to effectively combine information about the formal organizational structure of a company and the actual collaborative activities of its employees?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-26",
    "categories": [
      "organizational structure",
      "org chart",
      "collaboration",
      "data visualization"
    ],
    "contents": "\r\nHow to effectively combine information about the formal\r\norganizational structure of a company and the actual collaborative\r\nactivities of its employees?\r\nAt Time is Ltd. we are\r\nprimarily focused on collaboration data analytics, whether it‚Äôs\r\nmeetings, emails, chat, CRM, project management or version control\r\nsystems, but besides that we also have a product that helps companies\r\nmap their formal organisational structure. Btw, you can give it a try\r\nbecause many of its features are available for free on the Google Workspace\r\nMarketplace.\r\nWe are currently trying to connect these two ‚Äúworlds‚Äù because in\r\nsituations of organizational transformation it can be very useful to\r\nhave information about the relationship between the current and/or\r\nintended formal structure of the organization on the one hand and the\r\nactual patterns of collaboration on the other.\r\nOne option we‚Äôre considering is using a kind of heatmap, which you\r\nmay be familiar with from eye-tracking studies used in marketing, to see\r\nwhere people focus their attention when interacting with products and\r\nmaking purchasing decisions. Now imagine if we overlayed a similar\r\nheatmap showing the intensity of collaboration between a selected unit\r\nand the rest of the organization over an org chart - see the figure\r\nbelow for illustration.\r\n\r\nWould you consider such a visualization useful if you were engaged in\r\norganizational transformation? Would you suggest any other kind of\r\nvisualization or specific metric related to collaboration? Thank you in\r\nadvance for any input you may have - it will help us broaden the range\r\nof perspectives we are currently considering.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-org-chart-and-collaboration/./collaborationOrgChartOverlay.png",
    "last_modified": "2023-02-08T22:12:38+01:00",
    "input_file": {},
    "preview_width": 1661,
    "preview_height": 970
  },
  {
    "path": "posts/2023-02-08-span-of-control/",
    "title": "Span of control and collaboration data",
    "description": "How can collaboration data be used to determine the \"optimal\" scope of control?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-24",
    "categories": [
      "span of control",
      "collaboration",
      "meetings"
    ],
    "contents": "\r\nIn determining the ‚Äúoptimal‚Äù span of control (SOC) for the company\r\nand specific departments and teams, it is always advisable to consider\r\nthe context and strategy of the company, the way in which individual\r\ndepartments and teams should perform their work, and the level of\r\ncompetence of individual managers.\r\nFor example, McKinsey\r\nsuggested the following four specific aspects of managerial complexity\r\nthat should be considered in this endeavor:\r\nThe time a manager spends doing her or his own work vs.¬†managing\r\nothers.\r\nThe extent to which the work process is not standardized and\r\nformally structured.\r\nThe variety of work of the manager‚Äôs direct reports.\r\nThe amount of experience and training that team members need to do\r\ntheir jobs.\r\nThe more of the above, the smaller the SOC should be.\r\nIn addition to these factors, we can use as other useful inputs some\r\nmeasures of collaboration that can reasonably be expected to be related\r\nboth to SOC and to the obligations that managers have to their direct\r\nreports. A good example is the metric of the number of 1-on-1 meetings\r\nthat managers have with their direct reports. As the attached chart\r\nillustrates, when pitted against each other, we can look for points on\r\nthe SOC scale where managers start to fall short of the goal of a\r\ncertain minimum number of 1:1s they have with their people, e.g.¬†2\r\nmeetings per month.\r\n\r\nWhat factors do you typically consider when determining the optimal\r\nspan of control in your company? And do you regularly reassess the\r\nadequacy of the current SOC in the context of your current\r\nsituation?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-span-of-control/./scheme.jpg",
    "last_modified": "2023-02-08T18:32:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-09-slack-best-practices/",
    "title": "Attaching numbers to best practices for instant messaging",
    "description": "Slack and other instant messaging platforms can be both a blessing and a curse. Can we attach numbers to some of the recommendations on how to use them effectively? Let's take a look.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-20",
    "categories": [
      "slack",
      "instant messaging",
      "best practices"
    ],
    "contents": "\r\nAs many of you can probably confess, Slack and other instant\r\nmessaging (IM) platforms can be both a blessing and a curse. To support\r\nthe former and suppress the latter, authors of these tools and their\r\nusers themselves have come up with several recommendations on how to use\r\nthem effectively.\r\nSpecific numbers can be attached to some of these best practices to\r\nhelp teams and entire companies systematically shape their behavior on\r\nIM platforms in the desired direction. At Time Is Ltd., we currently measure\r\nthe following seven best practices:\r\nThread use: It helps create organized discussions\r\naround specific messages, and they let users discuss a topic in more\r\ndetail without adding clutter to a channel or direct message\r\nconversation.\r\nMention use: Mentioning specific people in messages\r\nin both public and private channels is one effective way to avoid\r\noverwhelming users with a large number of messages that are not relevant\r\nto them.\r\nShort messages use: Shorter messages often mean\r\nmore messages, more messages mean more notifications, and more\r\nnotifications mean more distractions, more frequent context switching,\r\nand decreased productivity.\r\nEmoji use: People should use emojis instead of\r\nshort messages as they are less distracting and more friendly to other\r\npeople‚Äôs attention.\r\nBatching: Responding to chat messages round the\r\nclock can be detrimental to employees‚Äô productivity as it can distract\r\nthem from focused work. Better strategy is checking messages every one\r\nor two hours instead of continuous handling of all incoming\r\nmessages.\r\nInactive channels: Non-archived channels that show\r\nno activity are just clutter that makes it difficult to navigate and\r\ncollaborate on the chat platform.\r\nTransparency: Direct and group messages have their\r\nplace in chat, especially when discussing sensitive issues or when\r\ntrying to avoid spamming other employees. However, when majority of chat\r\ncommunication occurs in direct and group messages, there is a higher\r\nrisk that information important for task alignment, problem-solving, or\r\ndecision will be hidden in them and out of view from relevant\r\npeople.\r\n\r\nWould you add some other best practices for IM that have worked well\r\nfor you and that would make sense to measure?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-09-slack-best-practices/./im.jpg",
    "last_modified": "2023-02-09T10:41:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-slack-batching/",
    "title": "Always messaging",
    "description": "Let's take a look at two concepts from computer science that can be used in the workplace to improve people's focus and productivity, and expose two methods for measuring their related behaviors when collaborating on instant messaging platforms.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-17",
    "categories": [
      "instant messaging",
      "collaboration",
      "focus time",
      "distractions",
      "operationalization",
      "measurement"
    ],
    "contents": "\r\nWhen dealing with a large number of tasks and frequent task\r\nswitching, two related concepts originating from computer science can be\r\nused: batch\r\nprocessing and interrupt\r\ncoalescing.\r\nIn computing, these terms refer to a situation where computers wait\r\nuntil a fixed interval and check everything, rather than contextually\r\nswitching and processing separate, uncoordinated interrupts from their\r\nvarious sub-components.\r\nWhen transposed into the world of human workers, this design\r\nprinciple can manifest in checking emails or instant messages every one\r\nor two hours instead of continuously handling all incoming emails and\r\nmessages. Such an arrangement prevents fragmentation of people‚Äôs time\r\nand provides them with more focus time they need for deep work and\r\nexperiencing flow.\r\nBut how to measure this behavior so that a number can be put on it to\r\nenable people to better shape their behavior in this regard? At Time is\r\nLtd.¬†we are currently experimenting with two different approaches:\r\nClustering of sent emails/messages using K-Means or PAM and calculation\r\nof time gaps between start/end points of identified clusters, including\r\nthe start and end of the working day. The larger the gaps, the stronger\r\nthe signal of batch behavior.\r\nPercentage of emails/messages sent during the 3 busiest working\r\nhours (defined by the number of emails/messages sent) during a given\r\nday. The higher the proportion, the stronger the signal of batch\r\nbehavior. This approach is inspired by the 2016\r\nstudy by Mark et al. ‚ÄúEmail Duration, Batching and\r\nSelf-interruption: Patterns of Email Use on Productivity and Stress‚Äù,\r\nwhere the authors used similar approach in the domain of email\r\ncommunication.\r\n\r\nThere are advantages and disadvantages to both methods (face\r\nvalidity, accuracy, sensitivity to edge cases, computational complexity,\r\netc.), but irrespective of these, which one would you prefer to see in\r\nyour collaboration report? To get a better idea of what outputs both of\r\nthe above approaches generate, you can take a look at the attached\r\ngraphs showing the prevalence of batch behavior during one of my work\r\nweeks on Slack according to these two approaches.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-slack-batching/./slack.jpg",
    "last_modified": "2023-02-08T19:02:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-overloaded-employees/",
    "title": "Warning system for overloaded employees",
    "description": "What tools and/or signals can we use to identify employees at increased risk of overload? Let's take a look at some of the options we have in this regard.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-12",
    "categories": [
      "overload",
      "layoffs",
      "retention",
      "engagement"
    ],
    "contents": "\r\nOne of the negative side-effects of layoffs and efforts to achieve\r\nthe same (or ideally more) with fewer employees can be an increased\r\nworkload for those who stay because they have to do the work of those\r\nwho have left, which may lead to an increased risk of overload,\r\ndisengagement, and voluntary quits.\r\nTo prevent this from happening, it is useful to combine active\r\nlistening (through engagement surveys, pulse surveys, stay interviews,\r\nsimple chat, etc. ) with signals that can be obtained from the traces\r\nleft by people in various digital workplace tools, such as project\r\nmanagement systems (ClickUp, Jira, Asana, etc.), version control systems\r\n(GitLab, GitHub, etc.), calendars, instant messaging, or emails.\r\nAt Time is Ltd., we are\r\ncurrently focusing on the following metrics that could be useful in this\r\nrespect:\r\nNumber of assigned tasks\r\nTask close rate\r\nNumber of assigned tasks that other people‚Äôs tasks depend on\r\nNumber of commits\r\nNumber of code reviews\r\nResponse time to received messages and e-mails\r\nAmount of focus time available\r\nAmount of distracted time\r\nAmount of time spent working after hours or on weekends\r\nBy checking the distribution of these metrics across individual team\r\nmembers and their changes over time, it is possible to identify\r\nemployees at higher risk of overload, as well as opportunities for a\r\nmore even distribution of the workload.\r\nWhat tools and/or signals do you use in your company to identify\r\nemployees at increased risk of overload?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-overloaded-employees/./overload.jpg",
    "last_modified": "2023-02-08T22:31:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/",
    "title": "Evaluation of the results of the evidence-based HRM knowledge test",
    "description": "Have we made any progress in knowledge of evidence-based HRM practices in the last 20 years? Apparently not. But let's look at the details.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-08",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I shared an app\r\nthat tests knowledge of evidence-based HRM practices using items from Rynes, Colbert, and\r\nBrown‚Äôs 2002 study on HR practitioners‚Äô beliefs about effective HR\r\npractices. The fact that more than 140 people completed the test allowed\r\nme to compare our current results with those of the participants in the\r\noriginal study (959 HR practitioners, mostly HR managers, with an\r\naverage of 13.8 years of HR experience).\r\nSo how did we do overall?\r\nOn average, we had 19.4 items out of 35 correct, i.e., we had a 55%\r\nsuccess rate, which is very close to the results of the original study\r\nwhere respondents had an average 57% success rate (and also pretty close\r\nto the 50% success rate corresponding to random choice). So these\r\nresults suggest that we have not progressed much as a group over the\r\nlast 20 years, however, see the disclaimer at the very end of the\r\npost.\r\n\r\nIn which HRM area did we have the largest & smallest\r\nknowledge gaps?\r\nThe biggest gap was in the staffing area (44% success rate) and the\r\nsmallest was in the training & employee development area (67%\r\nsuccess rate).\r\nIn which items did we do best?\r\nLeadership training is effective because good leaders are made, not\r\nborn (95% success rate).\r\nLecture-based training is not generally superior to other forms of\r\ntraining delivery (92% success rate).\r\nWhen pay must be reduced or frozen, a company can do something to\r\nreduce employee dissatisfaction and dysfunctional behaviors (90% success\r\nrate).\r\nIn which items did we do the worst?\r\nScoring positive on drug tests doesn‚Äôt mean one will be any less\r\nreliable or productive employee (11.6% success rate).\r\nSetting performance goals is, on average, more effective for\r\nimproving organizational performance than encouraging employees to\r\nparticipate in decision-making (12.3% success rate).\r\nMost errors in performance appraisals cannot be eliminated by\r\nproviding training that describes the kinds of errors managers tend to\r\nmake and suggesting ways to avoid them (15% success rate).\r\nIn which items were we most unsure?\r\nOlder adults don‚Äôt learn more from training than younger adults (38%\r\nuncertain).\r\nIntegrity tests that try to predict whether someone will steal, be\r\nabsent, or otherwise take advantage of an employer work well in practice\r\n(34% uncertain).\r\nCompanies with merit pay systems tend to have higher performance\r\nthan companies without them (17% uncertain).\r\nPlease keep in mind that the comparison presented here is not\r\nentirely an apples-to-apples comparison due to several limitations of\r\nthe data collection method (e.g., we don‚Äôt know the sociodemographics of\r\nthe new participants, new evidence may have emerged that does not match\r\nthe correct answers in the original study, some people may have taken\r\nthe test multiple times, etc.).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/./testEval.jpg",
    "last_modified": "2023-01-08T21:07:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-meetings-improvement/",
    "title": "How to improve effectiveness of meetings?",
    "description": "What suggestions do people have for improving the effectiveness of meetings? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-05",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nIn my last\r\npost I outlined the reasons why people think the meetings they\r\nattend are in/effective. Let‚Äôs now look at what they suggest can be done\r\nto improve the effectiveness of meetings. Who knows, maybe in these\r\nrecommendations you‚Äôll find an alternative to canceling most internal\r\nmeetings like they did at Shopify.\r\nThe list below is again based on Geimer\r\net al.‚Äôs 2015 paper ‚ÄúMeetings at work: Perceived effectiveness and\r\nrecommended improvements‚Äù.\r\nPEOPLE SIDE: üëâ Come prepared üëâ Arrive on time üëâ\r\nOpen to change üëâ Actively listen to what others are saying üëâ Display\r\nprofessionalism during the meeting\r\nMEETING STRUCTURE & ORGANIZATION SIDE: üëâ\r\nDistribute appropriate information via e-mail instead of in meeting üëâ\r\nAllow time to prepare for meetings üëâ Provide meaningful agenda üëâ\r\nClarify plan of action üëâ Use or rotate a facilitator/chair üëâ Invite\r\nappropriate attendees üëâ Pay attention to timing limit, start/end on\r\ntime üëâ Shorten meetings üëâ Hold meetings at appropriate intervals &\r\nmeet only when necessary üëâ Make the meeting environment more\r\ncomfortable\r\nMEETING ACTIVITIES SIDE: üëâ Make meetings more\r\ninteractive & seek input from all attendees üëâ Stay focused on the\r\ntopic üëâ Prioritize items üëâ Break into smaller groups (brainstorming,\r\netc.) üëâ Delegate responsibilities and set deadlines for assigned\r\ntasks\r\nMEETING OUTCOMES SIDE: üëâ Record and distribute\r\nmeeting minutes üëâ Follow up with proposed solutions\r\nIs there anything that you think is missing in the list, especially\r\nin the context of the fact that since 2015 the workplaces have been\r\noperating more in remote/hybrid mode?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-meetings-improvement/./meetingsDalle.png",
    "last_modified": "2023-02-08T19:29:27+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-02-08-ineffective-meetings/",
    "title": "Why are meetings in/effective?",
    "description": "If you are a regular organiser or attendee of meetings, you may be interested in what people think about the reasons why the meetings they attend are in/effective, as this can give you a better chance of contributing to making your meetings more effective and meaningful for you and others.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-03",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nThe list below is based on Geimer\r\net al.‚Äôs 2015 paper ‚ÄúMeetings at work: Perceived effectiveness and\r\nrecommended improvements‚Äù.\r\nPEOPLE SIDE: üö´ Late arrive üö´ Unprepared attendees\r\nüö´ One-way (top-down) communication üö´ Lack of open-mindedness &\r\nempathy üö´ Self-promotion, people talk just to appear to add value, and\r\nhidden agenda üö´ People interrupt/talk during meeting üö´ Interpersonal\r\nconflicts, incivility, and disrespect\r\nMEETING ORGANIZATION SIDE: ‚úÖ Agenda ‚úÖ Distribution\r\nof agenda in advance üö´ Lack of direction/goals ‚úÖ Chaired effectively\r\nüö´ Meetings held just to have them, just a routine with no real purpose\r\nüö´ Appropriate parties are not invited and inappropriate parties are\r\ninvited üö´ Too many attendees üö´ Time conflicts üö´ Meet at inappropriate\r\nintervals üö´ Meetings take too long üö´ Takes time to travel to\r\nmeeting\r\nMEETING ACTIVITIES SIDE: üö´ Insufficient\r\ninteraction, meeting activities are monotonous and boring üö´ No new\r\ninformation üö´ Discussion gets off target üö´ Core issues not discussed\r\nüö´ Lack of clarity about what the attendee is supposed to do\r\nMEETING OUTCOMES SIDE: üö´ Inaction post-meeting üö´\r\nDecisions have already been made, just a rubber stamp\r\nDo you identify with the above reasons? Is there anything that you\r\nthink is missing from the list, particularly as the workplaces have been\r\noperating more remotely/hybrid way since 2015?\r\nP.S. In the next post let‚Äôs check what suggestions people have for\r\nimproving the effectiveness of meetings.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-ineffective-meetings/./meetingsDalle.png",
    "last_modified": "2023-02-08T19:29:53+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2022-12-17-good-manager/",
    "title": "Signals of a good manager",
    "description": "Do you think it's possible to find signals in collaboration (meta)data that someone is a good manager? Let's give it some thought.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-17",
    "categories": [
      "google",
      "oxygen project",
      "collaboration data",
      "performance management"
    ],
    "contents": "\r\nMany of you have probably already heard about Google‚Äôs\r\nOxygen project and its findings on the factors differentiating the\r\nhighest and lowest-rated managers based on performance reviews, employee\r\nengagement surveys, interviews, and other sources of employee feedback.\r\nThe final list included the following eight characteristics:\r\nIs a good coach.\r\nEmpowers the team and does not micromanage.\r\nExpresses interest in and concern for team members‚Äô success and\r\npersonal well-being.\r\nIs productive and results-oriented.\r\nIs a good communicator - listens and shares information.\r\nHelps with career development.\r\nHas a clear vision and strategy for the team.\r\nHas key technical skills that help him or her advise the team.\r\nDo you think it would be possible to find any signals, however weak,\r\nof the presence of some of these managers‚Äô characteristics in the\r\ncollaboration (meta)data?\r\nOf the collaboration metrics we currently work with at Time is Ltd., I would bet on the\r\nfollowing:\r\nNumber of 1-on-1 meetings managers have with their direct reports\r\nand new hires [concern for employees‚Äô success and personal\r\nwell-being]\r\nNumber of team meetings managers have with their teams [information\r\nsharing, alignment on vision, strategy, and tactics)\r\nAfter-hours or weekend work and workday length of managers‚Äô direct\r\nreports [well-being & work-life balance]\r\nManagers‚Äô presence at meetings of their direct reports, excluding\r\n1-on-1s and team meetings [micromanagement]\r\nManagers being in CC/BCC of emails sent by their direct reports\r\n[micromanagement]\r\nTime it takes managers to respond to emails from their direct\r\nreports [interest in and concern for team members]\r\nDirect reports having skip-level meetings [career development &\r\nnew career opportunities]\r\nWould you agree? Or what other signals of a good manager would you\r\nlook for in collaboration (meta)data?\r\nP.S. In a later\r\nupdate, the list of top manager characteristics at Google also\r\nincludes the characteristic ‚ÄúCollaborates across Google‚Äù, which is\r\nrelatively straightforward to measure using collaboration data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-good-manager/./worlds-best-boss-funny-the-office-micromanagement.gif",
    "last_modified": "2022-12-17T12:36:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-evidence-based-hrm-knowledge-test/",
    "title": "Evidence-based HRM knowledge test",
    "description": "Interested in testing your knowledge of evidence-based HRM practices? If so, click and get started.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-15",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management",
      "shiny app"
    ],
    "contents": "\r\n\r\n\r\n\r\nIf you are interested in testing your knowledge of evidence-based HRM practices in the following five areas‚Ä¶\r\nManagement practices\r\nGeneral employment practices\r\nTraining & Employee development\r\nStaffing\r\nCompensation & Benefits\r\n‚Ä¶ then give a try on the test, which is based on the items used in Rynes, Colbert, and Brown‚Äôs 2002 study ‚ÄúHR practitioners‚Äô beliefs about effective HR practices: a comparison of research and practice‚Äù.\r\nI built a simple shiny app that administers you the test, scores your answers, and compares your results to the results of the participants in the original study (959 HR professionals, mostly HR managers, with an average of 13.8 years of experience in HR). You can also use it to check the accuracy of your answers at the item level to fill in specific gaps in your knowledge.\r\nHere is the link to the app.\r\nFeel free to share your results in the comments. Think of it as a form of public commitment to making some progress in evidence-based HRM in the coming year üòâ To walk the talk, I attached my own results. As you can see, it‚Äôs not bad, but there is still room for improvement, especially in the general employment practices area üòÉ\r\n\r\nP.S. Keep in mind that the test is based on evidence available up to 2002, so it is possible that some correct answers or their contingencies may have changed in that time. For all of them, consider, for example, the adjustment of the estimate of the magnitude of the predictive validity of personnel selection procedures in their most recent meta-analysis by Sackett et al.¬†(2022). If you come across any such discrepancy, it would be great if you share it with others in the comments.\r\nUpdate: With more than 140 people completing the test, I was able to compare our current results with those of the participants in the original study. You can see the results of the comparison in the post Evaluation of the results of the evidence-based HRM knowledge test. Spoiler: It‚Äôs not very good üòØ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-evidence-based-hrm-knowledge-test/./manOnTheRoad.jpg",
    "last_modified": "2023-04-11T20:07:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-collaboration-during-vacations/",
    "title": "Can you really unplug?",
    "description": "With the Christmas holidays approaching, the following question is more relevant than ever, with the exception of the summer vacations: Can we really disconnect from work during the vacations? And what can collaboration data tell us about this?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-10",
    "categories": [
      "vacation",
      "well-being",
      "work-life balance",
      "collaboration data"
    ],
    "contents": "\r\nAlthough most people know they need time off to stay mentally sharp, productive, and resilient over the long term, many confess they work when they‚Äôre on vacation.\r\nThe data we collect and analyze at Time is Ltd. confirms this sad truth. As the first chart illustrates, on more than 75% of vacation days people show some collaborative activity, here measured by attending non-recurring meetings, sending emails, and editing shared files.\r\n\r\nHowever, the same data also suggests what could be part of the remedy for this unsatisfactory state of affairs. The second chart shows that there is a small but not insignificant positive relationship between the time that managers and their direct reports spend collaborating during vacation. The chart also shows that managers tend to collaborate more intensely during vacation than their direct reports, which is probably not too much of a surprise.\r\n\r\nThe data is thus in line with the quite often mentioned suggestion that managers should be better role models for their direct reports on how to behave during vacation. As in other areas of people management, it is not enough to lay down the rules - the ‚Äúplaying captain‚Äù must play by those rules as well.\r\nHow are you doing in this respect? And do you have any tricks that help you unplug from work during vacations?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-collaboration-during-vacations/./outOfOffice.jpg",
    "last_modified": "2023-11-18T16:54:07+01:00",
    "input_file": "collaboration-during-vacations.knit.md"
  },
  {
    "path": "posts/2022-12-07-mission-scale-from-tas/",
    "title": "One does not simply do a business without getting lost",
    "description": "Breaking down one weekend association.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-07",
    "categories": [
      "team assessment survey",
      "mission",
      "purpose",
      "goals",
      "metrics",
      "review"
    ],
    "contents": "\r\n\r\n\r\n\r\nAs I was scrolling through one of my feeds over the weekend, I came across a funny meme that resonated with my recent experience on a family trip that expressed the deep truth that ‚ÄúOne does not simply do a road trip without getting lost.‚Äù üòÑ\r\n\r\nBesides that, and that‚Äôs why I‚Äôm writing about it here on my blog, it also reminded me of the results of a study I did together with Rastislav Duris and Slavka Silberg, on the characteristics of more than 80 teams from different industries and composed of more than 800 people using the Team Assessment Survey, Dr.¬†Gordon Curphy‚Äôs survey that measures some of the basic factors that determine team performance.\r\nSpecifically, I was reminded of the results on the Mission scale, which consists of the following four items:\r\nPurpose: Team members are clear about the team‚Äôs purpose.\r\nGoals: The team has a set of overall goals.\r\nMetrics: Metrics and benchmarks have been identified for each team goal.\r\nReviews: The team regularly reviews progress on team goals.\r\nAs you can see in the attached chart, the results of the ‚ÄúHeartbeat analysis‚Äù can be briefly summarized as ‚ÄúWe know where we‚Äôre going, at least some of us know the points to get there, but we‚Äôre not sure if we‚Äôre on the right track and if we should change our original plans.‚Äù\r\n\r\nThat sounds a lot like the description of our last family trip. üòÖ Do you think you‚Äôre better off in this regard in your team or company? And if so, what tips would you give others on how to improve in this respect?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-07-mission-scale-from-tas/./lost_in_desert.jpg",
    "last_modified": "2023-04-11T20:17:08+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-02-change-detection/",
    "title": "How to quickly navigate dashboard users to what they need to know?",
    "description": "Let's take a look at some tips and tricks to make dasboards more useful for their users.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-01",
    "categories": [
      "dashboard",
      "contextualization",
      "time series",
      "change detection",
      "python"
    ],
    "contents": "\r\nSince the beginning of our efforts at Time is Ltd. to develop a\r\ncomprehensive collaboration analytics platform, we have created several\r\nhundred collaboration metrics. As you can probably imagine, with such a\r\nhuge amount of metrics, it was quite difficult for users to get\r\nsomething useful out of the platform.\r\nTo make the metrics provided more digestible, we‚Äôve enhanced the\r\nplatform using several approaches, from simply selecting metrics with\r\nthe most straightforward call to action and creating apps for very\r\nspecific use cases to providing users with comparative, historical,\r\nintuitively scaled, and equivalent information. Brent Dykes described\r\nmany of these sense-making methods very neatly in his article Contextualized\r\nInsights: Six Ways To Put Your Numbers In Context.\r\nHowever, even after several rounds of these improvements, there was\r\nstill a need to help users quickly find areas that might be worth their\r\nattention and deeper exploration. One approach we took was based on the\r\n(validated) assumption that collaboration metrics are relatively stable\r\nover time and that the intentional and unintentional behavioral changes\r\nbehind these metrics evolve rather slowly. We therefore decided to\r\ndetect the most significant changes over the last 3 months.\r\nFor this purpose, we chose a method that is related to the Bollinger Bands\r\nmethod used in time series analysis or ‚ÄúHeartbeat Analysis‚Äù used in\r\nsurvey response analysis. Specifically, we look at the standard\r\ndeviation and average of the month-to-month changes for each metric,\r\nscale the changes to z-scores, and then identify the metrics with the\r\nhighest absolute value of average change over the last 3 months. To\r\nillustrate, see the attached chart where the metrics are sorted by\r\nmagnitude of their change in descending order from left to right and top\r\nto bottom.\r\n\r\nIf you want to try this method on your own data, you can use its\r\nPython implementation, which is on my\r\nGitHub page.\r\nWhat other methods do you find useful in identifying values or\r\nchanges that might be worth users‚Äô attention? Feel free to share them in\r\nthe comments for inspiration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-02-change-detection/./dashboard.jpg",
    "last_modified": "2022-12-02T19:10:27+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-26-meeting-matrix/",
    "title": "Eisenhower matrix for meetings",
    "description": "Meet the Eisenhower matrix for meetings ;)",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "meeting culture",
      "meeting overload",
      "focus time",
      "collaboration culture"
    ],
    "contents": "\r\nI assume many, if not all, of you are familiar with the\r\nEisenhower matrix, a simple tool for prioritizing tasks based on a\r\ncombination of their importance and urgency. It assumes 4 types of tasks\r\n- important & urgent, important & not urgent, not important\r\n& urgent, not important & not urgent - and for each of them\r\noffers a simple recommendation on what to do with them - do it, schedule\r\nit, delegate it, and delete it. By following these rules, people should\r\nbe able to more successfully combat the ‚Äúmere-urgency‚Äù effect, eliminate\r\ntime-wasters in their lives, and create more space to make progress\r\ntoward their goals.\r\n\r\nSomething similar can be created also for meetings, which are big\r\ntime and money guzzlers and deserve to be treated accordingly. Only\r\ninstead of the dimensions of importance and urgency, we will use the\r\nsize and length of meetings. The resulting matrix assumes 4 types of\r\nmeetings with corresponding recommendations on what to do with them:\r\nSmall & Short: These meetings are a bit tricky\r\nbecause they often involve useful meetings, e.g., short syncs of teams\r\nworking on some specific task or project or one-on-one meetings between\r\nmanagers and their direct reports, however, when there is a lot of them,\r\nthey may cause calendar fragmentation and drop in available focus time;\r\nto avoid this, one can think of batching such meetings into larger\r\nblocks of two or three meetings with appropriate small breaks in between\r\nto avoid meeting fatigue and late arrivals.\r\nSmall & Long: These meetings are great for\r\nbrainstorming, problem-solving, or decision-making, so make sure you use\r\nthem primarily for that purpose and don‚Äôt waste them on low-value-added\r\nactivities.\r\nLarge & Short: These meetings often serve only\r\nto disseminate information and can therefore be safely replaced by less\r\nintrusive asynchronous collaboration tools such as email, instant\r\nmessaging or some kind of knowledge management tool.\r\nLarge & Long: These meetings are quite often a\r\nwaste of people‚Äôs time by not allowing everyone to meaningfully\r\ncontribute and by making them both physically and mentally exhausted, so\r\ntry to move these meetings into the third quadrant if your goal is\r\nsimple information dissemination or into the second quadrant if the goal\r\nis to solve some problem or to make some important decision.\r\nThe meeting matrix is by no means a panacea for meeting overload, but\r\nby following the rules above, people should be able to better protect\r\ntheir time for focused work and make meetings more efficient,\r\nmeaningful, and valuable for themselves and the company. Just be aware\r\nthat unlike the Eisenhower matrix, in the case of the meeting matrix,\r\npeople need to rely more on others to follow these rules in order for\r\nits positive effect to materialize. So get ready to update your\r\n(hopefully already existing) team agreement on your collaboration\r\nculture.\r\nBtw, what is your guess as to how much time you spend in each cell of\r\nthe meeting matrix? For comparison I attach my monthly numbers from\r\nOctober.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-meeting-matrix/./meetingMatrix.png",
    "last_modified": "2022-11-26T13:48:43+01:00",
    "input_file": {},
    "preview_width": 850,
    "preview_height": 565
  },
  {
    "path": "posts/2022-11-26-resources-on-retention-and-downsizing/",
    "title": "Some resources on staff retention and downsizing",
    "description": "Probably due to the current situation in the talent market, where many companies are laying people off and at the same time are worried about losing their key employees, a few people have contacted me in recent weeks asking for some tips on evidence-based approaches to dealing with retention and downsizing.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "employee retention",
      "employee turnover",
      "layoffs",
      "downsizing",
      "people analytics",
      "evidence-based management"
    ],
    "contents": "\r\nI referred them to the following resources - maybe some of you will\r\nfind them useful as well:\r\nEmployment\r\nDownsizing and Its Alternatives guideline from the SHRM‚Äôs Science-to-Practice\r\nSeries.\r\nRetaining\r\nTalent guideline from the SHRM‚Äôs Science-to-Practice\r\nSeries.\r\nRubenstein et al.‚Äôs meta-analysis\r\nof voluntary turnover predictors; you can also use this\r\napp visually summarizing its results.\r\nSpeer et al.‚Äôs article Here\r\nto stay or go? Connecting turnover research to applied attrition\r\nmodeling.\r\nDemo\r\ndashboard for analysis of employee turnover.\r\nFeel free to share any other resources on this topic you think might\r\nbe useful to others.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-resources-on-retention-and-downsizing/./retentionDownsizing.png",
    "last_modified": "2022-11-26T14:17:06+01:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 720
  },
  {
    "path": "posts/2022-11-18-interventions-reducing-gender-pay-gap/",
    "title": "Evidence-based interventions that help reduce the gender pay gap",
    "description": "Pay inequality between men and women is not only an ethical and legal issue for companies, but also a marketing issue - it can have a negative impact on their \"employer brand\" and attractiveness as an employer. This means that if companies want to attract and retain talented employees, they must be able to ensure that they treat men and women equally in this respect. Let's look at what the existing evidence tells us about what might help us with this.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-18",
    "categories": [
      "gender pay gap",
      "evidence-based management"
    ],
    "contents": "\r\nIf you are responsible for DEI initiatives in your company, you might\r\nbe interested in a paper by The\r\nBehavioural Insights Team that lists several possible interventions\r\nto close the gender pay gap, divided into three categories based on the\r\nextent to which their effectiveness is supported by empirical\r\nevidence.\r\nü•á Actions with well-documented effectiveness:\r\nIncluding more women on shortlists in recruitment and promotion. *Ô∏è\r\nUse of tasks assessing job skill levels in the selection of new\r\nemployees.\r\nUse of structured interviews in recruitment and promotion.\r\nEncouraging salary negotiation through disclosure of existing salary\r\nranges.\r\nIntroducing transparent promotion and reward processes.\r\nAppointing a manager or establishing a corporate diversity task\r\nforce.\r\nü•à Potentially promising actions, but requiring further\r\nevidence of their effectiveness:\r\nIncreasing work flexibility for men and women.\r\nSupporting shared parental leave.\r\nRecruiting former employees who have had to interrupt their careers\r\nfor a prolonged period for various personal reasons.\r\nOffering mentoring and sponsorship.\r\nOffering networking programs.\r\nSetting internal targets.\r\nü•â Actions with mixed evidence of their\r\neffectiveness:\r\nTraining on the topic of unconscious bias.\r\nDiversity training.\r\nLeadership development training.\r\nDemographically diverse selection panels in external and internal\r\nrecruitment.\r\nFor those interested, here is the original document for a closer\r\nlook.\r\n\r\n\r\nThis browser does not support PDF files. Please download the PDF file to\r\nview it: Download PDF.\r\n\r\n\r\n\r\nA final note. As useful as it is to know which interventions have a\r\ndecent chance of reducing gender pay inequality, an integral part of\r\nthis fight is to regularly check the existence of this inequality across\r\nthe organisation and within different types of organisational processes.\r\nAs Alessandro\r\nLinari aptly noted in a discussion on Linkedin, ‚ÄúThe best way to\r\nkeep the gender pay gap under control is still to do a periodic pay\r\nequity analysis across the organisation and address any identified gap.\r\nI say unfortunately because there are a thousand different ways where\r\npay differences still manage to sneak into the workforce, through hiring\r\nand promotion practices, but also retention policies, bonus allocation,\r\nperformance management, and others.‚Äù If you want to learn more\r\nabout some of the technical details of such an analysis, check out one\r\nof my previous posts on this topic (unfortunately, it is now only\r\navailable in Czech, but you can use one of the online translators to get\r\naround this limitation üòâ).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-18-interventions-reducing-gender-pay-gap/./genderPayGap.jpg",
    "last_modified": "2022-11-18T12:26:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-15-people-related-metrics-distribution/",
    "title": "It's perfectly normal not to be normal",
    "description": "And it definitely applies to the shape of the distribution of many HR metrics. Let's look at this in a little more detail.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-15",
    "categories": [
      "hr metrics",
      "normal distribution",
      "collaboration",
      "performance",
      "r"
    ],
    "contents": "\r\nThe fact is that many HR practitioners overestimate the frequency\r\nwith which the phenomena they commonly encounter in their practice have\r\na normal, bell-shaped, symmetrical distribution.\r\nThis is very much the case, for example, in the area of communication\r\nand collaboration that we deal with at Time is Ltd., as illustrated in\r\nthe attached chart with some of our collaboration metrics, which show a\r\nwide range of distributions from log-normal and power law to\r\nexponential, gamma, Weibull and beta.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./collaborationMetrics.rds\")\r\n\r\n# External network size metric\r\nexternalNetworkSizeG <- mydata %>%\r\n    dplyr::filter(metric == \"externalNetworkSize\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(breaks = seq(0,120,20)) +\r\n    ggplot2::labs(\r\n      x = \"EXTERNAL NETWORK SIZE / PERSON / MONTH\",\r\n      y = \"FREQUENCY\",\r\n      title = \"External network size\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n  )\r\n  \r\n\r\n# Focus rate metric\r\nfocusRatePrctG <- mydata %>%\r\n    dplyr::filter(metric == \"focusRatePrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF FOCUS TIME / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Available focus time\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n\r\n# Multitasking in meetings metric\r\nmultitaskingInMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"multitaskingInMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"MULTITASKING RATE / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Meeting participations with multitasking\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Breaks between meetings metric\r\nbreaksBetweenMeetingsMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"breaksBetweenMeetingsMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,600,120)) +\r\n    ggplot2::labs(\r\n      x = \"LENGTH OF BREAK / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Lenght of breaks between meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Collaboration time metric\r\ncollaborationTimePersonHrsDayG <- mydata %>%\r\n    dplyr::filter(metric == \"collaborationTimePersonHrsDay\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" hrs\"), breaks = seq(0,16,2)) +\r\n    ggplot2::labs(\r\n      x = \"TIME SPENT COLLABORATING / PERSON / DAY\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Time spent collaborating\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Supervised meetings metric\r\nmicromngMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"micromngMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF SUPERVISED MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of supervised meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Recurring meetings metric\r\nrecurringMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"recurringMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"% OF RECURRING MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of recurring meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call ending metric\r\ncallEndingMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callEndingMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(-60,60,20)) +\r\n    ggplot2::labs(\r\n      x = \"PLANNED VS. ACTUAL END OF MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Planned vs. actual end of meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call delay metric\r\ncallDelayMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callDelayMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,40,5)) +\r\n    ggplot2::labs(\r\n      x = \"DELAY OF ONLINE MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Late arrivals to online meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\ng <- (externalNetworkSizeG + focusRatePrctG + multitaskingInMeetingsPrctG) / \r\n  (breaksBetweenMeetingsMinutesG + collaborationTimePersonHrsDayG + micromngMeetingsPrctG) / \r\n  (recurringMeetingsPrctG + callEndingMinutesG + callDelayMinutesG) +\r\n  patchwork::plot_annotation(\r\n    title = 'Distribution of selected collaboration metrics',\r\n    theme = theme(\r\n      plot.title = element_text(size = 26, margin=margin(20,0,12,0))\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBut it also applies to more traditional areas of HR concern such as\r\nindividual or team performance. For many job roles, especially\r\nknowledge-based ones, job performance has a power-law distribution,\r\ni.e., only a few percent of individuals or teams have disproportionately\r\nhigh performance and most have performance below the statistical average\r\n(many of you have probably heard of Pareto‚Äôs law, and the 80/20 rule in\r\nthis context). More detailed information on this particular topic can be\r\nfound, for example, in the following two articles - The\r\nBest and the Rest: Revisiting the Norm of Normality of Individual\r\nPerformance by O‚ÄôBoyle Jr.¬†& Aguinis (2012) and Team\r\nPerformance: Nature and Antecedents of Nonnormal Distributions by\r\nBradley & Aguinis (2022).\r\nWhy bother with that? Well, because, based on incorrect assumptions\r\nabout the distribution of specific phenomena, companies may make\r\ndecisions that ultimately harm them. For example, suppose a company\r\napplies the assumption of normal distribution in evaluating employees‚Äô\r\nperformance that actually has power law distribution. In that case, it\r\nwill result in underestimating the contribution of the best performers\r\nand overestimating the contribution of the worst performers, which may\r\nbe negatively reflected in various decisions regarding reward &\r\ncompensation, learning & development, promotions, succession\r\nplanning, etc.\r\nLessons learned? For frequent decisions or decisions with a\r\nsignificant expected impact, it is worth checking that our underlying\r\nassumptions match reality.\r\nBtw, this also applies to personal life. Here‚Äôs an example from mine:\r\nI thought I was in control of watching movies and TV shows on streaming\r\nplatforms, but I started getting signals from those around me that I was\r\nspending too much time there. So as a proper ‚ÄòQuantified Selfer‚Äô, I decided to\r\ntrack my daily screen time for a month. To my surprise, I found that I\r\nwas indeed spending a lot more time there than I thought and wished. I\r\nthen put in place simple solutions to prevent me from watching more than\r\nI should - I have started to pay extra fees into the family budget for\r\nwatching movies (loss aversion), I\r\nhave pre-selected time slots for watching movies (implementation\r\nintention), I‚Äôve removed streaming apps from my phone (lowering salience),\r\nI‚Äôve stopped watching movies alone (social control), and I‚Äôm also\r\nplaying with the idea of asking my wife to change the PINs (preventing\r\nimpulsive\r\nwatching).\r\nP.S. If you ever need to check the shape distribution of any of your\r\nmetrics, you should definitely try the amazing fitdistrplus\r\nR package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-15-people-related-metrics-distribution/./distributions.png",
    "last_modified": "2022-11-17T14:57:23+01:00",
    "input_file": {},
    "preview_width": 962,
    "preview_height": 670
  },
  {
    "path": "posts/2022-10-27-bayesian-belief-updating/",
    "title": "A visual introduction to Bayesian belief updating",
    "description": "Teacher: \"Bayesian belief updating involves combining existing or prior beliefs with an assessment of the strength of new evidence.\" Student: \"And could I please see this in action?\"",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-27",
    "categories": [
      "bayesian belief updating",
      "bayesian inference",
      "judgment",
      "forecasting",
      "critical thinking",
      "r"
    ],
    "contents": "\r\nWhen trying to reduce uncertainty, even very small pieces of\r\ninformation count if you are patient and have some tool to combine them\r\neffectively. I recently re-read Philip\r\nTetlock‚Äôs book Superforecasting\r\nand came across an excellent illustration of such a tool: Bayesian belief\r\nupdating.\r\n‚ÄúImagine you are sitting with your back to a billiards table. A\r\nfriend rolls a ball onto the table and it stops at a random spot. You\r\nwant to locate the ball without looking. How? Your friend rolls a second\r\nball, which stops at another random spot. You ask, ‚ÄúIs the second ball\r\nto the left or the right of the first?‚Äù Your friend says, ‚ÄúTo the left.‚Äù\r\nThat‚Äôs an almost trivial scrap of information. But it‚Äôs not nothing. It\r\ntells you that the first ball is not on the extreme left edge of the\r\ntable. And it makes it just a tad more likely that the first ball is on\r\nthe right side of the table. If your friend rolls another ball on the\r\ntable and the procedure is repeated, you get another scrap of\r\ninformation. If he says, ‚ÄúIt‚Äôs to the left,‚Äù the likelihood of the first\r\nball being on the right side of the table increases a little more. Keep\r\nrepeating the process and you slowly narrow the range of the possible\r\nlocations, zeroing in on the truth‚Äîalthough you will never eliminate\r\nuncertainty entirely.‚Äù\r\nAfter reading this paragraph, I thought it would be much more\r\npedagogically compelling (and fun üòâ) to see this update process live\r\nand in action. What a great opportunity to learn how to work with the gganimate\r\nR package.\r\nHere is the code I put together.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # data manipulation and visualization\r\nlibrary(gganimate) # animation of the charts\r\nlibrary(bayestestR) # Highest Density Interval computation\r\n\r\n# specifying the width of the billiards table  \r\nfield <- seq(0,178,1)\r\n\r\n# likelihoood function for the situation when the second ball stops to the right of the first one \r\nrightLikelihood <- (178-field)/178\r\n# likelihoood function for the situation when the second ball stops to the left of the first one \r\nleftLikelihood <- 1-rightLikelihood\r\n# flat prior for the very beginning of the updating process  \r\nfirstPrior <- rep(1/179, 179)\r\n\r\n# position where the first ball stopped\r\npoint <- 88\r\n\r\n# Bayesian belief updating\r\n# creating shell dataframe for final results\r\nposteriors <- data.frame()\r\n\r\n# setting random seed for ensuring reproducibility\r\nset.seed(1234)\r\n\r\n# specifying the number of trials\r\nfor(i in 1:500){\r\n  \r\n  # throwing the second ball\r\n  pointTrial <- runif(n = 1, min = 0, max = 178)\r\n  \r\n  # determining whether the second ball stopped to the left or to the right of the first one \r\n  side <- ifelse(pointTrial > point, \"right\", \"left\")\r\n  \r\n  # selecting appropriate prior\r\n  if(i==1){\r\n    \r\n    prior <- firstPrior\r\n    \r\n  } else{\r\n    \r\n    prior <- posteriors %>% \r\n      dplyr::filter(trial == i-1) %>% \r\n      dplyr::pull(posterior)\r\n    \r\n  }\r\n  \r\n  # combining prior and likelihood (evidence) and transforming the result into probabilities\r\n  if(side == \"right\"){\r\n    \r\n    likelihood <- rightLikelihood * prior \r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  } else{\r\n    \r\n    likelihood <- leftLikelihood * prior\r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  }\r\n  \r\n  # putting results into the dataframe\r\n  suppDf <- data.frame(\r\n    posterior = probability,\r\n    trial = i,\r\n    side = side,\r\n    pointTrial = pointTrial\r\n  )\r\n  \r\n  # computing Highest Density Interval\r\n  sampling <- sample(x = field, size = 10000, replace = TRUE, prob = probability)\r\n  \r\n  hdi <- bayestestR::hdi(sampling, ci = 0.95)\r\n  \r\n  suppDf <- suppDf %>%\r\n    dplyr::mutate(\r\n      lhdi = hdi$CI_low,\r\n      hhdi = hdi$CI_high\r\n    )\r\n  \r\n  # putting results into the shell dataframe \r\n  posteriors <- dplyr::bind_rows(posteriors, suppDf)\r\n\r\n}\r\n\r\n# adjusting data for visualization purposes\r\nposteriorsDf <- posteriors %>%\r\n  dplyr::group_by(trial) %>%\r\n  dplyr::mutate(place = dplyr::row_number()-1) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    hdi = ifelse(place >= lhdi & place <= hhdi, \"yes\", \"no\")\r\n  )\r\n\r\n# creating charts\r\nmyAnimation <- ggplot2::ggplot(data = posteriorsDf, aes(x = place, y = posterior)) +\r\n  ggplot2::geom_area(data = posteriorsDf %>% dplyr::filter(hdi == 'yes'), fill = 'light blue') +\r\n  ggplot2::geom_line(size = 1) +\r\n  ggplot2::geom_point(aes(x = pointTrial, y = 0), size = 4) +\r\n  ggplot2::geom_point(aes(x = point, y = 0), size = 4, color = \"red\") +\r\n  ggplot2::geom_text(aes(x = 8, y = 0.1, label = stringr::str_glue(\"Trial: {trial}\\n95% HDI: [{lhdi}, {hhdi}]\")), color = \"grey\") +\r\n  ggplot2::scale_y_continuous(limits = c(0,0.105)) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" cm\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Bayesian belief updating in the billiards table thought experiment\",\r\n    subtitle = \"Target place: 88 cm\",\r\n    x = 'PLACE ON THE BILLIARDS TABLE', \r\n    y = 'POSTERIOR PROBABILITY',\r\n    caption = \"\\n\\nThe static red point corresponds to a random and unknown location of the first ball. The moving black point then corresponds to the location where the second,\\nrepeatedly rolled ball randomly ended up. The area in blue corresponds to the 95% Highest Density Interval of the posterior distribution. All points inside this\\ninterval have a higher probability density than points outside this interval.\"\r\n    ) +\r\n  gganimate::transition_time(trial) +\r\n  gganimate::ease_aes('linear')\r\n\r\n# animating chart\r\ngganimate::animate(myAnimation, nframes = 125, fps = 5, height = 6, width = 11, units = \"in\", res = 125)\r\n\r\n# saving animated chart as a .gif file\r\ngganimate::anim_save(filename = \"./bayesianBelifUpdating.gif\", animation = last_animation())\r\n\r\n\r\n\r\nAnd here is the resulting animation of the Bayesian belief updating\r\nprocess across 500 trials.\r\n\r\nFor those who would like to incorporate Bayesian reasoning into their\r\nmanagerial decision-making under uncertainty, I can recommend this\r\narticle by Brian T.\r\nMcCann.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-27-bayesian-belief-updating/./bayesian-belief-updating.jpg",
    "last_modified": "2022-11-17T14:59:19+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-24-police-cadet-evaluation-dataset/",
    "title": "Police cadet evaluation dataset",
    "description": "A \"new\" real-world dataset useful for training in people analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-24",
    "categories": [
      "people analytics",
      "data",
      "recruitment",
      "hiring",
      "training"
    ],
    "contents": "\r\nWhile cleaning out my (very) old computer, I came across a hidden\r\ngem: a dataset with real-world data about police cadet evaluation. It\r\nwas part of a tutorial from Peltarion, an AI\r\nsoftware company providing specialized software (Synapse) for\r\ncreating, training, evaluating, and deploying artificial neural networks\r\nand other adaptive systems (recently acquired by King). AFAIK,\r\nthis dataset is not part of any publicly available database with\r\ntraining datasets, so it may add a bit to the portfolio of possibilities\r\nfor those involved and interested in people analytics.\r\nThe data were collected as part of an effort by the National\r\nPolice Services Agency and the Dutch\r\nMinistry of Justice and Security to objectively examine whether the\r\ndata collected at the time of graduation of police cadets can be used to\r\npredict the requirements for passing the standard five-year evaluation.\r\nThe main reason for the study was to find the key indicators for the\r\nthen 20% failure rate, which was considered unacceptable (data were\r\ncollected in the late 1990s), and to study the effects of lowering\r\nadmission standards (accepting cadets with past criminal records and\r\nlowering the minimum grade from 5.5 to 4.0).\r\nThe dataset has the following characteristics:\r\n2000 observations\r\n9 attributes:\r\nAge: the age at which the cadet started studying to\r\nbecome a police officer.\r\nAvG: average grade at the time of graduation (scale\r\n1-10).\r\nChdn: number of children at the time of\r\ngraduation.\r\nExEd: extra university-level or equivalent\r\neducation (years).\r\nCR: criminal record (0=No, 1=Yes).\r\nSex: sex of the cadet (0 = Male, 1 = Female).\r\nSecE: other experience in the security sector (0 =\r\nNo, 1 = Yes).\r\nAvgE: average yearly evaluation score (The average\r\nof five years. The evaluation is performed by a committee of 10 senior\r\npolice officers. Scale 1-5). This is a help attribute and not for use as\r\ninput.\r\nFinalE: final evaluation. Fail if average yearly\r\nevaluation score (Avg) < 2.0 otherwise pass. (1610 Pass / 390 Fail).\r\nThis is the target attribute.\r\n\r\nHere is a table you can use to check and download the data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and making user-friendly data table\r\nlibrary(tidyverse)\r\nlibrary(DT)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./policeCadetEvaluation.csv\")\r\n\r\n# adjusting the data type for some variables for tabulation and visualization purposes\r\ndata <- data %>%\r\n  mutate(\r\n    CR = as.factor(CR),\r\n    Sex = as.factor(Sex),\r\n    SecE = as.factor(SecE),\r\n    FinalE = as.factor(FinalE)\r\n  )\r\n\r\n# defining the table\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy', 'csv', 'excel'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nAnd here is a pairplot showing the distribution and relationships\r\nbetween variables in the dataset.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for the pairplot data visualization\r\nlibrary(GGally)\r\n\r\n# defining custom function for diagonal continuous variable chart  \r\nmy_dens <- function(data, mapping) {\r\n  ggplot(data = data, mapping = mapping) +\r\n    geom_density(alpha = 0.6, color = NA) \r\n}\r\n\r\n# pairplot\r\nGGally::ggpairs(\r\n  data = data,\r\n  title = \"Police cadet evaluation dataset\",\r\n  mapping=ggplot2::aes(fill = FinalE),\r\n  lower=list(\r\n    combo = wrap(\"facethist\", binwidth=1, alpha = 0.6),\r\n    continuous = wrap(\"points\", alpha = 0.3, size = 0.7),\r\n    discrete = wrap(\"facetbar\", alpha = 0.6)\r\n    ),\r\n  upper=list(\r\n    discrete = wrap(\"box\", alpha = 0.6),\r\n    combo = wrap(\"box\", alpha = 0.6)\r\n  ),\r\n  diag = list(\r\n    continuous = my_dens,\r\n    discrete = wrap(\"barDiag\", alpha = 0.6)\r\n    )\r\n  ) +\r\n  ggplot2::scale_fill_manual(values=c(\"Fail\" = \"#e53935\", \"Pass\" = \"#00897b\")) +\r\n  labs(caption = \"\\nThe color indicates the pass/fail result of the final evaluation, the target attribute.\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 12, hjust = 0),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you want to download the dataset, you can do so here via the table\r\nabove or via my\r\nGitHub page where you can also find more information about the\r\ndataset. Happy analysis üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-24-police-cadet-evaluation-dataset/./Politie_Nederland_nieuw_uniform.jpg",
    "last_modified": "2022-11-17T15:01:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-conditional-inference-tree/",
    "title": "Divide and... understand",
    "description": "Finding the breakpoint when people start to score significantly higher/lower on a given criterion - the use case for the Conditional Inference Tree algorithm.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-18",
    "categories": [
      "conditional inference tree",
      "decision tree",
      "machine learning",
      "statistics",
      "interpretability",
      "prediction",
      "r"
    ],
    "contents": "\r\nWhen correlating collaboration metrics with business criteria that\r\nour clients are interested in, such as the size of the internal network\r\nof salespeople vs.¬†their sales performance, we often encounter the\r\nquestion of where the breakpoint is when people start to score\r\nsignificantly higher/lower on a given criterion.\r\nTo answer this question, I find very handy the Conditional\r\nInference Tree algorithm - a non-parametric class of decision trees\r\nthat, unlike traditional decision trees, use a significance/permutation\r\ntest (corrected for multiple testing) to select covariates to split and\r\nrecurse the variable.\r\nWhen applied to just one numerical predictor, it will provide a set\r\nof partitions that allow you to split that predictor into bins in such a\r\nway that you end up with statistically significant differences between\r\nsome of the identified bins. With this information in hand, it is much\r\neasier for you to find the ‚Äúsweet spots‚Äù (there may be more than one)\r\nwhere the criterion starts to behave differently in relation to the\r\npredictor values. See charts below for illustration.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and visualuzation \r\nlibrary(tidyverse)\r\n\r\n# defining normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\n# creating artificial dataset with internal network size and sales performance variables\r\ninternalNetworkSize = seq(-6, 6, 0.1)\r\nsalesPerf = 1*(internalNetworkSize**3) + 2*(internalNetworkSize**2) + 1*internalNetworkSize + 3\r\nsalesPerf_noise = 70 * rnorm(mean = 0, sd = 1, n=length(salesPerf))\r\nsalesPerformance = salesPerf + salesPerf_noise\r\n\r\n# putting data into dataframe and making some transformations of the variables\r\ndata <- data.frame(\r\n  internalNetworkSize = internalNetworkSize,\r\n  salesPerformance = salesPerformance\r\n) %>%\r\n  dplyr::mutate(\r\n    internalNetworkSize = normalize(internalNetworkSize),\r\n    salesPerformance = normalize(salesPerformance),\r\n    internalNetworkSize = internalNetworkSize*189,\r\n    salesPerformance = salesPerformance*100\r\n  )\r\n\r\n# visualizing relationship between internal network size and sales performance\r\nggplot2::ggplot(data = data, aes(x = internalNetworkSize, y = salesPerformance)) +\r\n  ggplot2::geom_point(color = \"#4d009d\", size = 3, alpha = 0.8) +\r\n  ggplot2::labs(\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE\",\r\n    y = \"SALES PERFORMANCE\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,200, 20), limits = c(0,200)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for ctree algorithm and visualization of the results of statistical tests\r\nlibrary(partykit)\r\nlibrary(ggstatsplot)\r\n\r\n# defining formula\r\nfmla <- as.formula(\"salesPerformance ~ internalNetworkSize\")\r\n\r\n# binning internal network size variiablle using ctree algorithm\r\nctree <- partykit::ctree(\r\n  fmla,\r\n  data = data,\r\n  na.action = na.exclude,\r\n  control = partykit::ctree_control(minbucket = ceiling(round(0.05*nrow(data))))\r\n)\r\n\r\n# plotting resulting tree\r\n#plot(ctree)\r\n\r\n# number of identified bins\r\n#bins = partykit::width(ctree)\r\n\r\n# extracting bin borders\r\ncutvct = data.frame(matrix(ncol=0,nrow=0)) # Shell\r\nn = length(ctree) # Number of nodes\r\nfor (i in 1:n) {\r\n  cutvct = rbind(cutvct, ctree[i]$node$split$breaks)\r\n}\r\ncutvct = cutvct[order(cutvct[,1]),] # sorting / converting to an ordered vector (asc)\r\ncutvct = ifelse(cutvct<0,trunc(10000*cutvct)/10000,ceiling(10000*cutvct)/10000) # rounding to 4th decimal place to avoid borderline cases\r\n\r\n# adding minimum and maximum values\r\ncutvct <- append(cutvct, min(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct <- append(cutvct, max(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct = cutvct[order(cutvct)]\r\n\r\n# creating bin categories\r\nvalueCat <- cut(x = data %>% dplyr::pull(\"internalNetworkSize\"), breaks = cutvct, include.lowest = TRUE)\r\n\r\n# creating supplementary dataframe for visualization purposes \r\nsuppDf <- data %>%\r\n  dplyr::select(internalNetworkSize, salesPerformance) %>%\r\n  dplyr::mutate(category = valueCat) %>%\r\n  dplyr::filter(category != \"NA\")\r\n\r\n# visualizing relationship between internal network size and sales performance using ggbetweenstats from ggstatsplot package\r\nggstatsplot::ggbetweenstats(\r\n  data = suppDf %>% as.data.frame(),\r\n  x = category,\r\n  y = salesPerformance,\r\n  type = \"robust\"\r\n) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\"), breaks = seq(0,100,20)) +\r\n  ggplot2::labs(\r\n    y = \"SALES PERFORMANCE\",\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE (BINNED)\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n    ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you are dealing with similar use cases, give it a try. And if you\r\nuse any other tools/approaches for this, feel free to share them in\r\nreturn.\r\nP.S. Thanks to Filip\r\nTrojan, my former boss and colleague from the Deloitte Advanced\r\nAnalytics team, who introduced me to this tool. I still benefit from it\r\nto this day üôèüí™\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-18-conditional-inference-tree/./decision-tree-analysis.jpg",
    "last_modified": "2022-10-25T10:37:02+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-11-timeboxing/",
    "title": "Timeboxing. Does it really work?",
    "description": "Checking with real-world collaboration data whether timeboxing has a protective function in terms of time available for focused work.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-11",
    "categories": [
      "timeboxing",
      "timeblocking",
      "regression analysis",
      "control variables"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I addressed the question of why people don‚Äôt make more use of\r\ntimeboxing, a time\r\nmanagement tool relying on a well-researched self-regulatory technique\r\ncalled implementation\r\nintention - planning what you will do, when, and how.\r\nQuite surprisingly, I found in our collaborative data that there is\r\nnot a positive but a slightly negative relationship between the amount\r\nof time for focused work and the amount of blocked working time in the\r\ncalendar, which I interpreted to mean that people who have plenty of\r\ntime for focused work usually do not have a strong need to block time\r\nfor focused work in their calendars.\r\nHowever, based on these results, one colleague wondered whether this\r\nresult actually speaks against the usefulness of this tool. To answer\r\nher question properly, we should avoid comparing apples with pears and\r\ncontrol for the effect of the number of collaborative activities people\r\nparticipate in, as it can be assumed that those who spend more time\r\ncollaborating with others have less time for focused work and also use\r\nthe timeboxing technique more.\r\nUsing this approach and our clients‚Äô collaborative data, I looked at\r\nthe relationship between the proportion of work time blocked on the\r\ncalendar and the time available for focused work (i.e.¬†no meetings, no\r\nad-hoc calls, no email or instant messaging), and found that the\r\nmarginal effect of timeboxing is in line with the positive effect of the\r\ntimeboxing technique on the time available for focused work. The effect\r\nis not huge (each percentage point of working time blocked in the\r\ncalendar yields on average .19% of focus rate), however, timeboxing\r\nseems to be saved, phew üòâ\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-11-timeboxing/./time-blocking.png",
    "last_modified": "2022-10-25T06:50:42+02:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-09-17-multilevel-modeling/",
    "title": "Multilevel modeling in people analytics",
    "description": "Don't chase (statistical) ghosts and use multilevel models instead!",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-17",
    "categories": [
      "multilevel modeling",
      "hierarchical modeling",
      "mixed models",
      "nested data",
      "bayesian inference",
      "collaboration",
      "employee engagement",
      "r"
    ],
    "contents": "\r\nIn one of our projects,\r\nwhere we were trying to find out how collaborative behavior relates to\r\nemployee engagement, we repeatedly came across patterns that reminded\r\nthe client of internally well-known differences between the behavior of\r\nteams from different parts of the company. For example, we found that\r\nmore frequent participation in short and small meetings was related to\r\nlower employee engagement. This pattern matched well the client‚Äôs\r\nobservation that one particular part of the company had regular daily\r\nstand-up meetings and also lower engagement scores compared to the rest\r\nof the company due to some other aspects of their work. As further\r\nanalysis confirmed, this pattern was really just a statistical artifact\r\ncaused by the coincidence of these two facts.\r\nOne way analysts can protect themselves from this type of misleading\r\nconclusions is by using multilevel or hierarchical\r\nmodels that take into account the fact that the data\r\nhave a nested structure, i.e.¬†that some observations are not\r\nindependent of each other because they belong to the same higher-order\r\ngroup, e.g.¬†to an organizational unit (one of the basic assumptions of\r\nmost statistical models in use).\r\nThis is well illustrated in the graphs below. They show that the\r\nrelationship between the number of monthly 1:1s that employees have with\r\ntheir line manager and their subjectively perceived support from their\r\nline manager is slightly positive across most groups of teams (shown by\r\ncolored dots and lines), but when all teams are analyzed together, the\r\nrelationship is rather negative (shown by black dots and lines).\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(scales)\r\nlibrary(patchwork)\r\nlibrary(ggtext)\r\n\r\ndata <- readr::read_csv(\"./data.csv\")\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support while taking into account differences between organizational units \r\ng1 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(aes(col = Unit), size = 2.5, alpha = 0.5) + \r\n  ggplot2::geom_smooth(aes(col = Unit), method = 'lm', alpha=0.2, se = F) +\r\n  labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Unit A\" = \"#20066b\", \"Unit B\" = \"#e56b61\", \"Unit C\" = \"#b4ba0d\", \"Unit D\" = \"#32b2c7\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support without taking into account differences between organizational units \r\ng2 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(size = 2.5, alpha = 0.5) +\r\n  ggplot2::geom_smooth(method = 'lm', alpha=0.2, linetype = \"solid\", color = \"black\", se = F) +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- g2 + g1\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Relationship between the number of 1:1s and perceived managerial support across**\r\n  <br>\r\n  **and within organizational units**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,12,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# fitting Bayesian hierarchical linear regression model\r\nmodel <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones + (1 + oneonones | Unit)),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(model)\r\n# plot(model)\r\n# brms::pp_check(model, ndraws = 100)\r\n\r\n\r\n# fitting Bayesian non-hierarchical linear regression model\r\nmodelNonHierarchical <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(modelNonHierarchical)\r\n# plot(modelNonHierarchical)\r\n# brms::pp_check(modelNonHierarchical, ndraws = 100)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(emmeans)\r\nlibrary(tidybayes)\r\n\r\n# marginal effect of 1:1s in the Bayesian hierarchical linear regression model\r\navg_marginal_effect <- model %>% \r\n  emmeans::emmeans(~ oneonones,\r\n                   at = list(oneonones = seq(0, 6, by = 0.1)),\r\n                   epred = TRUE,\r\n                   re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf1 <- ggplot2::ggplot(avg_marginal_effect, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Hierarchical linear regression model\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n# marginal effect of 1:1s in the Bayesian non-hierarchical linear regression model\r\navg_marginal_effect_nonHierarchical <- modelNonHierarchical %>% \r\n  emmeans::emmeans(\r\n    ~ oneonones,\r\n    at = list(oneonones = seq(0, 6, by = 0.1)),\r\n    epred = TRUE,\r\n    re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf2 <- ggplot2::ggplot(avg_marginal_effect_nonHierarchical, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Non-hierarchical linear regression model\"\r\n  ) +\r\n    ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ngf <- gf2 + gf1\r\ngf <- gf + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Posterior average marginal effect of 1:1 meetings on perceived managerial support**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,0,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(gf)\r\n\r\n\r\n\r\n\r\nWithout the use of the hierarchical model (and/or a careful post-hoc\r\nvisual check of alternative explanations), we would reach a completely\r\nopposite (and incorrect) conclusion about the relationship between the\r\nnumber of 1:1s and perceived support from the line manager (a phenomenon\r\nknown as Simpson‚Äôs\r\nparadox). In this particular case, it is relatively easy to\r\nrecognize that something may be wrong, but the situation is not always\r\nso obvious and intuitive. In these other cases, it is advantageous to\r\nhave some tool at hand to compensate for our imperfect intuition and\r\nlimited knowledge and imagination. Hierarchical models are one such\r\ntool.\r\nIf you don‚Äôt already have it in your analytics toolbox, be sure to\r\ngive it a try. If you work with R, you can use the lme4\r\nor brms\r\npackages to implement it. In a Python\r\nenvironment, you can use the statsmodels or\r\nPyMC3 libraries to\r\ndo this. And if you‚Äôre more used to drag-and-drop tools, then JASP or jamovi (both open-source alternatives\r\nto SPSS)\r\nwill give you access to various mixed models through an easy-to-use\r\ngraphical interface.\r\nFor an accessible discussion of this topic in the context of people\r\nanalytics, including other useful tools for working with hierarchical\r\ndata, see also the excellent articles by Paul van der\r\nLaken, John\r\nLipinski, and Max\r\nBlumberg:\r\nSimpson‚Äôs\r\nParadox: Two HR examples with R code.\r\nHow\r\nto Avoid Aggregation Errors and Simpson‚Äôs Paradox in HR Analytics: Part\r\n1\r\nHow\r\nto Avoid Aggregation Errors and Simpson‚Äôs Paradox in HR Analytics: Part\r\n2\r\nWhy\r\nPeople Analytics should NOT be using regression to predict team\r\noutcomes\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-multilevel-modeling/./groupsofpeople.jpg",
    "last_modified": "2022-09-17T20:03:50+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-and-personality/",
    "title": "Collaboration and personality",
    "description": "Personality is not fate, at least when it comes to the level of engagement in corporate communication and collaboration.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-15",
    "categories": [
      "collaboration",
      "communication",
      "networking behavior",
      "personality",
      "big five"
    ],
    "contents": "\r\nOne of our clients once\r\nasked us to what extent employees‚Äô level of engagement in corporate\r\ncommunication and collaboration is driven by their personality and to\r\nwhat extent by their job role, the conditions in which they work, and\r\nother factors outside their personality.\r\nOur first answer was that the latter probably plays a more\r\nsignificant role than the former, but it was difficult to answer more\r\nspecifically because we did not yet have all the data we needed to\r\nquantify the tightness of this relationship. This motivated me to look\r\nat existing research on this topic to help us better set our apriori\r\nexpectations on this issue.\r\nWith the help of metaBus, an\r\namazing platform for curating, searching, and summarizing research\r\nfindings from the social and organizational sciences, I was able to get\r\nthe results of over 100 studies on the relationship between employees‚Äô\r\nBig\r\n5 personality traits and the amount of interaction and networking\r\nbehavior they engage in. Among the criteria for the amount of\r\ninteraction were variables like contact frequency, frequency of\r\nparticipation, communication frequency, meeting frequency, hours of\r\ninteraction, interaction duration, etc. For networking behavior there\r\nwere criteria as liaison, building networks, relationship building,\r\nnetwork activity, maintaining contacts, increasing internal visibility,\r\nnetwork ability, and informal network.\r\n\r\n\r\nShow code\r\n\r\n# The following concepts were used to search for relevant studies on the metaBus platform (their respective codes are given in brackets):\r\n# Big 5 (20443)  \r\n# Amount of interaction (20287) \r\n# Networking behavior (80017)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # for data manipulation and visualization \r\nlibrary(ggridges) # for data visualization\r\nlibrary(ggtext) # for enabling markdown in ggplots\r\nlibrary(patchwork) #  for combining ggplots\r\n\r\n# data preparation\r\n# uploading data\r\ninteraction <- readr::read_csv(\"./interactionAmount.csv\")\r\nnetworking <- readr::read_csv(\"./networkingBehavior.csv\")\r\n\r\n# preparing dataset for amount of interaction concept\r\ninteractionPrep <- interaction %>%\r\n  dplyr::filter(\r\n    # removing non-relevant personality characteristics \r\n    !Var1 %in% c(\"Empathic concern\"),\r\n    # limiting to studies conducted at the individual level\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Extraversion\") | stringr::str_detect(Var1, \"Extroversion\") | stringr::str_detect(Var1, \"extraversion\") ~ \"Extraversion\",\r\n      stringr::str_detect(Var1, \"Openness to experience\") | stringr::str_detect(Var1, \"openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"agreeableness\") ~ \"Agreeableness\",\r\n      stringr::str_detect(Var1, \"emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# preparing dataset for networking behavior concept\r\nnetworkingPrep <- networking %>%\r\n  dplyr::filter(\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"Emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# data visualization\r\n# creating chart for the amount of interaction concept\r\ninteractionChart <- interactionPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#e56b61\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.text.x = element_text(),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n)\r\n\r\n# creating chart for the networking behavior concept\r\nnetworkingChart <- networkingPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#32b2c7\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text.y = element_blank(),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts together\r\ng <- interactionChart + networkingChart \r\n\r\n# adding title and caption\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:20pt;font-weight:bold;'>**Do Big 5 traits predict** \r\n    <span style='color:#e56b61;'>**the amount of interaction**<\/span> **&**\r\n    <span style='color:#32b2c7;'>**networking behavior**<\/span> **of employees?**\r\n    <\/span>\",\r\n  \r\n  caption = \"The solid vertical lines represent quartile values.\\nBased on studies found on the metaBus platform using the concepts 'Big 5' (code: 20443), 'Amount of interaction' (code: 20287), and 'Networking behavior' (code: 80017).\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(10,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 9, hjust = 0)\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nAs can be seen from the graphs above, the relationship between\r\npersonality and the amount of interaction and networking behavior goes\r\nin the expected direction. Agreeable, open, and extraverted employees\r\nand to some extent also conscientious and emotionally stable employees\r\ntend to engage more in interactions with others and in networking.\r\nHowever, the relationships found are relatively weak. The middle 80% of\r\nobserved effects range from an absolute value of .02 to .24, so across\r\nthe studies shown, small effects prevail. And even in the case of the\r\nstrongest effect (r = .38), personality ‚Äúexplains‚Äù only 14% of\r\nthe variability in the networking behavior. There is therefore ample\r\nscope for the influence of a range of other factors.\r\nHow about you? Are you able to engage in interactions and networking\r\nin a way that supports your career, work performance, or other positive\r\noutcomes, perhaps despite your natural tendencies due to your\r\npersonality setup? Feel free to share your experience and thoughts in\r\nthe comments. Btw, you can find interesting information on this topic in\r\nthe excellent book 8\r\nSteps to High Performance by Marc Effron, specifically\r\nin chapters 4 and 6.\r\nCaveat: The graphs represent only a simple summary\r\nof the effects observed in the selected studies, not a proper\r\nmeta-analysis. If you‚Äôre interested in the specific analysis steps and\r\nchoices behind the graphs shown, you can check the code above or go to\r\nmy GitHub\r\npage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-and-personality/./personalityandcollaboration.jpeg",
    "last_modified": "2022-09-17T14:19:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-overload-and-bottlenecks/",
    "title": "Hot spots of collaboration overload and collaboration bottlenecks and how to find them",
    "description": "One of the most useful insights that can be gleaned from collaboration data is where hot spots of potential collaboration overload and/or collaboration bottlenecks may exist in a company. Such insight can be especially valuable these days, when many companies are trying to fight the upcoming economic downturn by achieving more with less.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-08",
    "categories": [
      "collaboration overload",
      "collaboration bottlenecks",
      "organizational network analysis",
      "r"
    ],
    "contents": "\r\nOne simple way to identify such hot spots is to compare the\r\noutbound and inbound collaborative activities in which\r\nteams or individuals participate. The greater the difference between the\r\ntwo in favor of inbound collaboration activities, the stronger the\r\nsignal that collaboration overload and/or bottlenecks may be a problem\r\nfor that team or individual.\r\nTo illustrate, take a look at the attached charts that show the\r\npatterns of collaboration between several teams via Slack. The distances\r\nbetween teams, the thickness, and the direction of the arrows between\r\nthem tell us who is collaborating with whom and how much. The size of\r\nthe nodes then represents the amount of inbound and outbound\r\ncollaborative activities that the teams participate in, respectively.\r\nBased on the differences between them, the bar chart below shows us\r\nwhere the inbound collaboration activities outweigh the outbound ones\r\nthe most, and therefore where the risk of collaboration overload and/or\r\nbottlenecks is greatest.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nlibrary(patchwork)\r\n\r\n# uploading data with nodes/ties based on the current frequency of communication via Slack\r\nnodesR <- readr::read_csv(\"./nodes.csv\") \r\ntiesR <- readr::read_csv(\"./ties.csv\")\r\n\r\n# specifying cut-off value for showing only the x% of strongest edges\r\nprob = 1\r\n\r\n# changing coding of individual nodes in the network\r\nties <- tiesR %>%\r\n  dplyr::left_join(nodesR, by = c(\"from\" = \"id\")) %>%\r\n  dplyr::select(-from) %>% \r\n  dplyr::rename(from = name) %>%\r\n  dplyr::left_join(nodesR, by = c(\"to\" = \"id\")) %>%\r\n  dplyr::select(-to) %>%\r\n  dplyr::rename(to = name) %>%\r\n  dplyr::select(from, to, weight) %>%\r\n  dplyr::mutate(\r\n    from = stringr::str_to_title(from),\r\n    to = stringr::str_to_title(to),\r\n    from = stringr::str_replace(from, \"Development\", \"Dev\"),\r\n    to = stringr::str_replace(to, \"Development\", \"Dev\"),\r\n    from = stringr::str_replace(from, \"Dev Management\", \"Dev - Management\"),\r\n    to = stringr::str_replace(to, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n  \r\nnodes <- nodesR %>%\r\n  dplyr::select(-id) %>%\r\n  dplyr::mutate(\r\n    name = stringr::str_to_title(name),\r\n    name = stringr::str_replace(name, \"Development\", \"Dev\"),\r\n    name = stringr::str_replace(name, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n\r\n\r\n# making the network from the data frame \r\ng <- igraph::graph_from_data_frame(d = ties, vertices = nodes, directed = TRUE)\r\n\r\n# setting name of the network\r\ng$name <- \"Collaboration via Slack\"\r\n\r\n# assigning ids to nodes\r\nV(g)$id <- seq_len(vcount(g))\r\n\r\n# cutoff value for showing only the x% of strongest edges\r\ncutoff <- quantile(ties$weight, probs = prob)[[1]]\r\n\r\n# visualizing the inbound network\r\nset.seed(1234)\r\ninG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = inbound), alpha = 0.5, fill = \"#32b2c7\", color = \"#32b2c7\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n    ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"INBOUND COLLABORATION ACTIVITIES\")\r\n    )\r\n\r\n\r\n\r\n# visualizing the outbound network\r\nset.seed(1234)\r\noutG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = outbound), alpha = 0.5, fill = \"#46c8ae\", color = \"#46c8ae\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"OUTBOUND COLLABORATION ACTIVITIES\")\r\n  )\r\n\r\n\r\n# bar chart with info about difference between inbound and outbound collaboration activities\r\ninoutDiffG <- nodes %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(name, inoutDiff), y = inoutDiff)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = ifelse(nodes$inoutDiff > 0, \"#e56b61\", \"#20066b\"), alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(breaks = seq(-200,200,50)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"DIFFERENCE BETWEEN IN/OUTBOUND COLLABORATION ACTIVITIES\",\r\n    y = \"DIFFERENCE BETWEEN THE NUMBER OF IN/OUTBOUND INSTANT MESSAGES\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 30, margin=margin(0,0,12,0), hjust = 0.5),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 24, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 22, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- (inG + outG) / inoutDiffG\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nWe see that the ‚Äúhottest‚Äù hot spots are in teams Dev-Management and\r\nDev-Frontend. While this is not definitive proof that we have real\r\nproblems in these two specific teams, it should be a strong enough\r\nsignal to take notice and try to verify our suspicion with additional\r\ninformation, such as checking some relevant business metrics or simply\r\nasking a few people we know should be affected, if there is a problem.\r\nIf the initial suspicion is confirmed, appropriate action should be\r\ntaken, e.g.¬†consider the relevance of some requests, possibly redirect\r\nthem to other teams, automate some tasks, expand the team and recruit\r\nnew people, etc.\r\nFor more tips on how to leverage collaboration data in the current\r\nuncertain economic times, I recommend reading the articles Top\r\n7 Collaboration Metrics to Utilize in an Economic Crisis by Jan Rezab and Top\r\n6 Metrics to measure during an economic downturn by Shwetha Pai.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-overload-and-bottlenecks/./Organizational-network.jpg",
    "last_modified": "2022-09-17T18:33:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-back-to-back-meetings/",
    "title": "Are back-to-back meetings for good or bad?",
    "description": "A short post about the practice of back-to-back meetings and how to determine when it's for bad and when it's rather for good.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-01",
    "categories": [
      "meeting habits",
      "back-to-back meetings"
    ],
    "contents": "\r\n‚ÄúContext, Context, Context‚Äù could be the headline of this post.\r\nWhen we address the issue of good meeting habits with our clients, the length of breaks\r\nbetween successive meetings is one of the first metrics we focus on.\r\nAs many of you probably know from your firsthand experience,\r\nconsecutive meetings with no breaks, a.k.a. back-to-back\r\nmeetings, have many detrimental effects, from overload\r\nand exhaustion to not being adequately prepared for subsequent\r\nmeetings and arriving\r\nlate to them.\r\nAs useful as the above metric is, it does not tell the whole story\r\nand can lead to invalid conclusions and see a problem where there is\r\nnone. Data from one of our teams illustrates this well.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading datasets from the platform\r\ndata1 <- readr::read_delim(\"./timeisltd-chart1.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata2 <- readr::read_delim(\"./timeisltd-chart2.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata3 <- readr::read_delim(\"./timeisltd-chart3.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\n\r\n# Time between successive meetings\r\ng1 <- data1 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#20066b\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time between successive meetings\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Number of back-to-back meetings in a row\r\ng2 <- data2 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = as.numeric(cat)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#e56b61\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" mtgs\", accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Number of back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Time spent in back-to-back meetings in a row\r\ng3 <- data3 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = factor(cat, levels = c(\"31-60 Min\", \"61-90 Min\", \"91-120 Min\", \"121-150 Min\", \"151-180 Min\", \"181+ Min\"), ordered = TRUE)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#46c8ae\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time spent in back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# putting graphs together\r\ng <- g1/g2/g3\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBased purely on the time between successive meetings, we could\r\nconclude that a given team suffers from an unhealthy frequency of\r\nback-to-back meetings, as in more than a third of cases, there is no\r\nbreak between meetings. However, if we look at how long the series of\r\nback-to-back meetings tend to be (in 75% of cases it‚Äôs only 2 meetings\r\nin a row) and how much time people spend in them (in 42% of cases it‚Äôs\r\nbetween 31-60 minutes and in 30% of cases it‚Äôs between 61-90 minutes),\r\nthen the resulting picture is less pessimistic and more indicative of a\r\nrather healthy level of effort to protect time for focused\r\nwork by batching meetings into short blocks\r\nthat do not come at the cost of exhausting people and making meetings\r\nless effective.\r\nWhat is your approach to back-to-back meetings? Do you try to always\r\nhave at least a 5-minute buffer between two consecutive meetings? And\r\nhow successful are you at this? Are you aware of situations where it is\r\nappropriate to batch meetings into tight blocks without breaks? And do\r\nyou have a limit on how many meetings to put in a row? Feel free to\r\nshare your thoughts and experiences in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-back-to-back-meetings/./backtobackmeetings.jpg",
    "last_modified": "2022-09-17T17:37:26+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-20-people-analytics-popularity-after-covid/",
    "title": "The impact of the COVID pandemic on the popularity of people analytics",
    "description": "Many people analytics professionals think that after the COVID pandemic, organizations are more willing to listen to their insights and recommendations. Can we find any empirical support for their hunch? Let's check it out with data provided by Google Trends and segmented regression analysis of interrupted time series.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-29",
    "categories": [
      "people analytics",
      "hr analytics",
      "covid pandemic",
      "segmented regression analysis",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nPeople analytics popularity after pandemic\r\nData preparation\r\nModeling\r\nWhat do the model and data tell us?\r\n\r\nPeople analytics popularity after pandemic\r\nThere is a fairly common perception among the people analytics professionals with whom I am in contact that after the COVID pandemic, companies are much more willing to use the insights provided by people analytics teams and incorporate them into their business-related decision-making processes.\r\nI wondered if I could find any empirical support for this feeling in the surge of global web search interest in ‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù terms on Google after the pandemic outbreak, assuming the pandemic broke out in March 2020.\r\nLet‚Äôs start our quest with a simple visual inspection of a line chart showing the trend of worldwide web search interest in ‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù terms on Google from January 2007 to July 2022. (You can replicate this chart using the Google Trends website and the search terms, time range, location, and source for searches described above. If you are familiar with R, you can use the code below.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for getting data from Google Trends\r\nlibrary(gtrendsR)\r\n# uploading libraries for data manipulation\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\n# setting parameters for Google Trends\r\nsearchTerms   <- c(\"people analytics\", \"hr analytics\")\r\nlocation      <- \"\" # global\r\ntime          <- \"2007-01-01 2022-07-31\"\r\nsource         <- \"web\"\r\n\r\n# getting Google Trends data\r\ngtrendsResult <- gtrendsR::gtrends(\r\n    keyword = searchTerms, \r\n    geo     = location, \r\n    time    = time,\r\n    gprop   = source\r\n    )\r\n\r\n# cleaning data\r\ngtrendsResultDf <- gtrendsResult %>%\r\n    purrr::pluck(\"interest_over_time\") %>%\r\n    dplyr::select(date, hits, keyword) %>%\r\n    dplyr::mutate(date = lubridate::ymd(date))\r\n\r\n\r\n\r\nTowards the end of the time series, somewhere between September 2019 and March 2020, it seems that the trend stops increasing and starts to stagnate, except for the very last part of the graph, which shows a sharp increase in searches for both terms, but this may just be the result of the improved data collection system from January 2022 onwards (as indicated by the vertical line in the graph with a note). Thus, the data seems to suggest the opposite of what we would expect if a pandemic were to have a positive effect on interest in people analytics.\r\nHowever, after combining the results for the two search terms and plotting them on a graph together with the unadjusted regression trend lines, the resulting picture gives a slightly different impression. There appears to be little to no decline in interest in people analytics immediately after the pandemic outbreak, but a steeper slope of change after the pandemic.\r\n\r\n\r\nShow code\r\n\r\n# normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\ngtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100\r\n  ) %>%\r\n    # creating new pandemic variable \r\n  dplyr::mutate(\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ \"After the pandemic outbreak\",\r\n      TRUE ~ \"Before the pandemic outbreak\"\r\n    )\r\n  )%>%\r\n  ggplot2::ggplot(aes(x = date, y = interestInPeopleAnalytics, color = pandemic)) +\r\n  ggplot2::geom_point() +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggplot2::scale_x_date(breaks = \"1 year\", date_labels = \"%Y\") +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"Interest in people analytics\",\r\n    title = \"Interest in people analytics before and after the pandemic outbreak\",\r\n    caption = \"The solid lines represent unadjusted regression model trend lines before and after the pandemic outbreak.\"\r\n  ) +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nTo make the picture a little bit clearer, let us take the help of inferential statistics to answer the question we are interested in. The ideal analytical tool for our use case is a segmented regression analysis of interrupted time series that enables estimation of the changes in levels and trends of search interest before and after after a known ‚Äòintervention‚Äô or ‚Äòinterruption‚Äô (i.e., a change that could potentially affect the outcome variable). It does so by segmenting the time series data into different periods based on the known interruption points and modeling these segments separately. The model used has the following general structure:\r\n\\[Y_{t} = Œ≤_{0} + Œ≤_{1}*time_{t} + Œ≤_{2}*intervention_{t} + Œ≤_{3}*time after intervention_{t} + e_{t}\\]\r\nThe Œ≤0 coefficient estimates the baseline level of the outcome variable at time zero; Œ≤1 coefficient estimates the change in the mean of the outcome variable that occurs with each unit of time before the intervention (i.e.¬†the baseline trend); Œ≤2 coefficient estimates the level change in the mean of the outcome variable immediately after the intervention (i.e.¬†from the end of the preceding segment); and Œ≤3 estimates the change in the trend in the mean of the outcome variable per unit of time after the intervention, compared with the trend before the intervention (thus, the sum of Œ≤1 and Œ≤3 equals to the post-intervention slope). For a better understanding of the model, take a look at the illustrative chart taken from Turner et al.¬†(2021) below.\r\n\r\nSince we are dealing with correlated and truncated data, we should also include two additional terms in our model, an autocorrelation term and a truncation term, to handle these specific properties of our data.\r\nData preparation\r\nBut first, let‚Äôs prepare the data we will need for this type of analysis. Specifically, we will need the following five variables:\r\nsearch interest in people analytics ‚Äì numerical variable representing search interest in people analytics relative to the highest point on the chart for the given region and time; this variable is truncated within the interval between values of 0 and 100; a value of 100 is the peak popularity for the term; a value of 50 means that the term is half as popular; a score of 0 means that there was not enough data for this term; search interest for two monitored terms (‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù) was combined by simple summation and then normalized to a range of 0 to 100; this variable serves as a dependent (criterion) variable;\r\nelapsed time ‚Äì numerical variable representing the number of months that elapsed from the beginning of the time series; this variable enables estimation of the size and direction of the overall trend in the data before the intervention;\r\npandemic ‚Äì dichotomic variable indicating the presence/absence of pandemic; as already mentioned above, for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020; this variable enables estimation of the level change in the interest in people analytics immediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak ‚Äì numerical variable representing the number of months that elapsed from the beginning of pandemic; this variable enables estimation of the change in the trend in the interest in people analytics after the outbreak of pandemic;\r\nmonth ‚Äì categorical variable representing specific month within a year; this variable enables controlling for the effect of seasonality.\r\n\r\n\r\nShow code\r\n\r\n# munging data\r\nmydata <- gtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100,\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), ordered = FALSE)\r\n  ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(date) %>%\r\n    # creating new variables elapsed time, pandemic, and time elapsed after pandemic outbreak\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ 1,\r\n      TRUE ~ 0\r\n    ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic),\r\n    pandemic = as.factor(ifelse(pandemic == 1, \"After the pandemic outbreak\", \"Before the pandemic outbreak\"))\r\n  ) %>%\r\n  # final selection of variables\r\n  dplyr::select(\r\n    date, interestInPeopleAnalytics, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n  )\r\n\r\n\r\nHere is a table with the resulting data we will use for our analysis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making more user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nModeling\r\nWe can now fit the model to the data and test what it tells us about the impact of the pandemic on people‚Äôs search interest in people analytics. We will use the brms r package for this, which allows us to make inferences about the model parameters within a Bayesian inferential framework. For this reason, we must also specify some additional parameters (e.g.¬†chains, iter or warmup) of the Markov Chain Monte Carlo (MCMC) algorithm that generates posterior samples of our model‚Äôs parameters.\r\nThe Bayesian framework also allows us to specify priors for the estimated parameters and use them to incorporate our domain knowledge into the analysis. The specified priors are important for both parameter estimation and hypothesis testing because they define our initial information state before we consider our data. Here, we will use relatively broad, uninformative, and only slightly regularizing priors (that is, the inference results will be very close to the results of standard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors \r\npriors <- c(brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model\r\nmodel <- brms::brm(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  seed = 1234,\r\n  sample_prior = TRUE, \r\n  control = list(adapt_delta = 0.9),\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n\r\nBefore making any inferences, we should perform several validation checks to ensure that the mechanics of the MCMC algorithm worked well and that we can use the generated posterior samples to make inferences about the parameters of our model. There are many ways to do this, but here we will only use a visual check of the MCMC chains. We want the plots of these chains to look like a hairy caterpillar, indicating the convergence of the underlying Markov chain to stationarity and the convergence of the Monte Carlo estimates to population quantities, respectively. As can be seen in the graph below, we can observe the characteristics we are looking for in the MCMC chains described above. (For additional MCMC diagnostics procedures, see for example Bayesian Notes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains\r\nbayesplot::mcmc_trace(\r\n  model,\r\n  facet_args = list(nrow = 6)\r\n) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the model parameters\"\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nIt is also important to check how well the model fits the data. To do this, we can use the posterior predictive check, which uses a specified number of selected posterior values of the model parameters to show how well the fitted model predicts the observed data. In the graph below we see that the model fits the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model fit\r\n# specifying the number of samples\r\nndraws = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  ndraws = ndraws\r\n) +\r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive check (using {ndraws} samples)\")\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nWhat do the model and data tell us?\r\nWe can now use the parameters of our model to obtain information about our main question. Specifically, we are interested in the value of the coefficient of the pandemic and the time after pandemic terms in our model. They represent how much and in what direction the search interest in people analytics changed after the pandemic outbreak.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the fitted model\r\nsummary(model)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: mydata (Number of observations: 187) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.67      0.07     0.53     0.82 1.00    36988    34009\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                           -19.75      7.34   -35.53\r\nelapsedTime                           0.47      0.04     0.42\r\npandemicBeforethepandemicoutbreak    10.96      4.99     1.62\r\nelapsedTimeAfterPandemic              0.65      0.29     0.10\r\nmonthFeb                              1.74      1.43    -1.08\r\nmonthMar                              2.77      1.87    -0.93\r\nmonthApr                              4.42      2.09     0.30\r\nmonthMay                              2.61      2.19    -1.69\r\nmonthJun                              0.60      2.25    -3.80\r\nmonthJul                              0.35      2.28    -4.16\r\nmonthAug                             -2.33      2.28    -6.83\r\nmonthSep                              1.67      2.22    -2.69\r\nmonthOct                             -0.56      2.10    -4.67\r\nmonthNov                             -1.01      1.88    -4.71\r\nmonthDec                             -6.14      1.44    -8.96\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            -6.98 1.00    30645    29199\r\nelapsedTime                           0.56 1.00    44619    28942\r\npandemicBeforethepandemicoutbreak    21.28 1.00    44289    44331\r\nelapsedTimeAfterPandemic              1.26 1.00    54010    46306\r\nmonthFeb                              4.55 1.00    37661    48486\r\nmonthMar                              6.43 1.00    26135    39337\r\nmonthApr                              8.51 1.00    22840    37029\r\nmonthMay                              6.91 1.00    21100    35358\r\nmonthJun                              5.03 1.00    20495    36016\r\nmonthJul                              4.81 1.00    20712    37600\r\nmonthAug                              2.16 1.00    21240    37079\r\nmonthSep                              6.07 1.00    22312    36687\r\nmonthOct                              3.56 1.00    24241    39183\r\nmonthNov                              2.68 1.00    27803    43248\r\nmonthDec                             -3.31 1.00    40223    50422\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     5.02      0.29     4.49     5.63 1.00    66571    52402\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nThe following graph shows the posterior distribution of the pandemic parameter. We can see that it is on the right-hand side of the zero value, which supports the claim that there is an effect of the pandemic on people‚Äôs interest in people analytics immediately after the pandemic outbreak; however, it is on the opposite side of the zero value than we would expect if the pandemic were to have a positive effect on people‚Äôs interest in people analytics. Thus, this suggests that immediately after the pandemic outbreak, interest in people analysis declined slightly (somewhere between ~1 and ~20 points, as indicated by the 95% credible interval).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(tidybayes)\r\n\r\nparamVizBeforethepandemicoutbreak <- model %>%\r\n  tidybayes::gather_draws(b_pandemicBeforethepandemicoutbreak) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizBeforethepandemicoutbreak$value)\r\n\r\nparamVizBeforethepandemicoutbreak <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_pandemicBeforethepandemicoutbreak parameter \r\nggplot2::ggplot(paramVizBeforethepandemicoutbreak, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\",\r\n    y = \"Density\",\r\n    x = \"pandemicBeforethepandemicoutbreak\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(posterior)\r\n\r\n# extracting posterior samples\r\nsamplesBeforethepandemicoutbreak <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being higher than 0\r\nsum(samplesBeforethepandemicoutbreak$b_pandemicBeforethepandemicoutbreak > 0) / nrow(samplesBeforethepandemicoutbreak)\r\n\r\n[1] 0.9902778\r\n\r\nNow let‚Äôs check the second key parameter of our model, the time after the pandemic term. In this case, the posterior distribution is again on the right-hand side of zero value, but now this result is consistent with the claim that there is a positive effect of the pandemic on people‚Äôs interest in people analytics, specifically in terms of the change in slope after the pandemic. Compared to the pre-pandemic trend, the post-pandemic trend is steeper by ~0 to ~1 point per month (as indicated by the 95% confidence interval). We should bear in mind, however, that this effect may in fact only be an artifact caused by the improved data collection system from January 2022, as mentioned at the very beginning of this post.\r\n\r\n\r\nShow code\r\n\r\nparamVizElapsedTimeAfterPandemic <- model %>%\r\n  tidybayes::gather_draws(b_elapsedTimeAfterPandemic) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizElapsedTimeAfterPandemic$value)\r\n\r\nparamVizElapsedTimeAfterPandemic <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_elapsedTimeAfterPandemic parameter \r\nggplot2::ggplot(paramVizElapsedTimeAfterPandemic, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\",\r\n    y = \"Density\",\r\n    x = \"elapsedTimeAfterPandemic\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamplesElapsedTimeAfterPandemic <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_elapsedTimeAfterPandemic coefficient being higher than 0\r\nsum(samplesElapsedTimeAfterPandemic$b_elapsedTimeAfterPandemic > 0) / nrow(samplesElapsedTimeAfterPandemic)\r\n\r\n[1] 0.9897361\r\n\r\nThe overall resulting picture thus partially supports the impression of many of my people analytics fellows about the growing importance of people analytics in HR and business leaders‚Äô decision making. However, given that the Google search interest in people analytics is a fairly distant proxy for its actual use in HR and business practice, we should take these results with a grain of salt and try to find other data sources that would support our results. For example, Frank Corrigan, head of analytics at Ponder, came up with the idea of analyzing changes in postings for people analytics job positions over time. A good inspiration for anyone willing to spend some time getting at and analyzing such data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-popularity-after-covid/./people-analytics.jpg",
    "last_modified": "2023-07-03T21:41:53+02:00",
    "input_file": "people-analytics-popularity-after-covid.knit.md"
  },
  {
    "path": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/",
    "title": "People Analytics Challenge from Orgnostic: Plan for high growth",
    "description": "A brief summary of my participation in Orgnostic's People Analytics Challenge.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-19",
    "categories": [
      "recruitment channels",
      "workforce planning",
      "descriptive statistics",
      "r"
    ],
    "contents": "\r\nDuring the last few nights, I had the opportunity to participate in\r\nan interesting People Analytics Challenge from Orgnostic, a company providing people\r\nanalytics platform that links scattered HR and finance data, run surveys\r\non top, analyses the data, and get answers to the critical questions\r\nabout organizations and their employees.\r\nOut of three possible challenges, I chose one that was quite far from\r\nwhat I‚Äôm currently usually working on or what I‚Äôve worked on in the\r\npast. The chosen task was to analyze the effectiveness of\r\nrecruiting sources for a company that plans to double in size\r\nfrom its current 750+ employees.\r\nAlthough the dummy data provided was quite limited for obvious\r\nreasons and did not allow to answer all relevant questions (but you\r\ncould also use your own data which would not suffer from this\r\nshortcoming), after combining them and enriching them slightly based on\r\nrealistic assumptions, it was possible to arrive at quite interesting\r\ninsights and recommendations. See for yourself - the resulting\r\npresentation is attached to this post below.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it:\r\nDownload\r\nPDF.\r\n\r\n\r\n\r\nIf you‚Äôd like to look more into the guts of the analyses conducted,\r\nyou can find both the data provided by Orgnostic and the R script used\r\nto analyze it on my\r\nGitHub page.\r\nP.S. Oh, I almost forgot - there are exciting prizes in the form of\r\ntickets to HRtechX Copenhagen or HR Technology Conference in Las Vegas.\r\nSo please keep your fingers crossed for me ü§ûüòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/./recruitmentChannels.png",
    "last_modified": "2022-08-29T18:41:35+02:00",
    "input_file": {},
    "preview_width": 864,
    "preview_height": 515
  },
  {
    "path": "posts/2022-06-11-visual-inference-statistics/",
    "title": "Visual statistical inference",
    "description": "Visual statistical inference represents a valid alternative to standard statistical inference, and as a by-product it also helps with building intuition about the difference between signal and noise. Give it a try.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-06-13",
    "categories": [
      "statistical inference",
      "visual statistical inference",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nDo you have any experience with visual\r\nstatistical inference? Not only is it a valid alternative\r\nto standard statistical inference, but as a by-product it helps with\r\nbuilding intuition about the\r\ndifference between signal and noise.\r\nThe basis of the method is a so-called lineup\r\nprotocol that places a graph of the actual data between arrays\r\nof graphs of null data that are generated by a method consistent with\r\nthe null hypothesis. The lineup is shown to one or more observers who\r\nare asked to identify the graph that differs. If an observer can pick\r\nout the actual data as different from the others, this gives weight to\r\nthe statistical significance of the pattern in the graph.\r\nBecause people usually have a hard time recognizing randomness and\r\ntend to see patterns even where there are none, there is also a\r\nso-called Rorschach protocol that only displays graphs\r\ncreated with null datasets and which is used by observers to calibrate\r\ntheir eyes for variation due to sampling.\r\nYou can try it for yourself in the graphs below, which show the\r\nrelationship between two variables from my current area of work\r\n(collaboration\r\nanalytics), namely between time\r\navailable for focused work and the use of timeboxing\r\n(productivity enhancing technique of assigning a fixed unit of time to\r\nan activity within which a planned activity takes place).\r\nUse the first array of graphs (the Rorschach protocol) to calibrate\r\nyour eye for randomness and then try to identify the actual data in the\r\nsecond array of graphs (the lineup protocol). What‚Äôs your guess? Which\r\ngraph matches the actual data (1-20) and what relationship do you see in\r\nthe data? You can give your guess in the comments.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(nullabor)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./visualInferenceData.RDS\")\r\n\r\n# lineup protocol\r\nset.seed(1234)\r\nd <- lineup(null_permute(\"propBlockedTime\"), mydata)\r\n\r\nlplot <- ggplot(data=d, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"LINEUP PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n\r\n# Rorschach protocol\r\ndr <- rorschach(null_permute(\"propBlockedTime\"), mydata, n = 20, p = 0)\r\n\r\nrplot <- ggplot(data=dr, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"RORSCHACH PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n# combining plots\r\nfplot <- rplot / lplot\r\n\r\nprint(fplot)\r\n\r\n\r\n\r\n\r\nHere‚Äôs a check on your guess. The actual data are shown in chart #\r\n12. As you can see in the chart below, the relationship between time for\r\nfocused work and the use of timeboxing is slightly negative, which makes\r\npretty good sense, because people who have enough time for focused work\r\nusually don‚Äôt have such a strong need to block out time for focused work\r\nin their calendar.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot(aes(x = focusRate, y = propBlockedTime)) +\r\n  geom_point(size = 2, alpha = 0.5) +\r\n  geom_smooth(method = \"lm\", se = F) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format()) +\r\n  labs(\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\",\r\n    title = \"RELATIONSHIP BETWEEN FOCUSED TIME AND TIMEBOXING USAGE\",\r\n    caption = \"\\nThe blue line in the graph represents the linear regression line.\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can also test the relationship between time for focused work and\r\nthe use of timeboxing more formally by fitting a Bayesian\r\nbeta regression model to the data. As you can see in the summary\r\ntables and charts below, the null value is safely outside the 95%\r\ncredible interval of the mean of the focus rate parameter, and the\r\nmarginal effect of focus rate clearly shows its negative relationship\r\nwith the predicted proportion of working time blocked in the calendar.\r\nNote also that the relationship is non-linear, i.e.¬†the marginal effect\r\nof the focus rate is different depending on its level.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(bayesplot)\r\nlibrary(tidybayes) \r\nlibrary(ggdist)       \r\n\r\n# fitting Bayesian beta regression model\r\nmodel <- brms::brm(\r\n  bf(\r\n    propBlockedTime ~ focusRate,\r\n    phi ~ focusRate\r\n    ),\r\n  data=mydata,\r\n  family= Beta(),\r\n  seed = 1234,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  chains = 4,\r\n  cores = 6,\r\n  control = list(\r\n    adapt_delta = 0.9,\r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model\r\nsummary(model)\r\n\r\n\r\n Family: beta \r\n  Links: mu = logit; phi = log \r\nFormula: propBlockedTime ~ focusRate \r\n         phi ~ focusRate\r\n   Data: mydata (Number of observations: 306) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nPopulation-Level Effects: \r\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept        -0.85      0.25    -1.33    -0.35 1.00    61653\r\nphi_Intercept     2.09      0.39     1.30     2.84 1.00    59323\r\nfocusRate        -0.02      0.00    -0.03    -0.02 1.00    57209\r\nphi_focusRate     0.01      0.01    -0.01     0.02 1.00    56423\r\n              Tail_ESS\r\nIntercept        54084\r\nphi_Intercept    51459\r\nfocusRate        55167\r\nphi_focusRate    51573\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the estimated parameters\r\nposterior_beta <- model %>% \r\n  gather_draws(`b_.*`, regex = TRUE) %>% \r\n  mutate(component = ifelse(str_detect(.variable, \"phi_\"), \"Precision\", \"Mean\"),\r\n         intercept = str_detect(.variable, \"Intercept\")) %>%\r\n  filter(intercept == FALSE)\r\n\r\nggplot(posterior_beta, aes(x = .value, y = fct_rev(.variable), fill = component)) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  stat_halfeye(aes(slab_alpha = intercept), \r\n               .width = c(0.8, 0.95), point_interval = \"median_hdi\") +\r\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\r\n  scale_slab_alpha_discrete(range = c(1, 0.4)) +\r\n  guides(fill = \"none\", slab_alpha = \"none\") +\r\n  labs(\r\n    x = \"COEFFICIENT\", \r\n    y = \"\",\r\n    title = \"POSTERIOR DISTRIBUTION OF THE ESTIMATED PARAMETERS\",\r\n    caption = \"\\n80% and 95% credible intervals shown in black\") +\r\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing marginal effect of the focus rate\r\nmodel_pred <- model %>% \r\n  epred_draws(newdata = expand_grid(focusRate = seq(30, 100, by = 1)))\r\n\r\nggplot(model_pred , aes(x = focusRate, y = .epred)) +\r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Purples\") +\r\n  labs(x = \"FOCUS RATE\", \r\n       y = \"PREDICTED PROPORTION OF BLOCKED WORKING TIME\",\r\n       fill = \"Credible interval\",\r\n       title = \"MARGINAL EFFECT OF THE FOCUS RATE\"\r\n       ) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format(accuracy = 1)) +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nIf you would like to apply the visual statistical inference approach\r\nto your own data, you can easily do so using the nullabor R\r\npackage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-11-visual-inference-statistics/./lineup.jpg",
    "last_modified": "2022-06-19T21:18:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-06-standard-and-trend-predictors/",
    "title": "Standard vs. trend predictors",
    "description": "When modeling a phenomenon, one usually can't get by with just raw data but must use one's domain knowledge to select and transform the most relevant variables from raw data to be able to successfully grasp regularities in the domain of one's interest. Let's look at one simple example of such feature engineering from the domain of collaboration analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-02-06",
    "categories": [
      "predictive analytics",
      "feature engineering",
      "employee experience",
      "employee engagement",
      "employee satisfaction",
      "employee attrition",
      "collaboration data"
    ],
    "contents": "\r\nAs predictive analytics practitioners know, trend variables can be more useful in many situations for predicting certain phenomena than standard variables that simply refer to the state of the world at a particular time point or period.\r\nFor example, when trying to predict employee attrition, a downward trend in the use of a piece of company equipment, such as a printer/copier, over the 6 months prior to the resignation may be more predictive than the absolute number of pages printed/copied over the same period.\r\nThis is also true for our domain we focus on at Time is Ltd. where, among other things, we try to use collaboration data to infer some aspects of employee experience.\r\nTo illustrate, the attached chart shows the distribution of the typical daily amount of time people spend by collaboration for two groups of employees - one with above-average scores and the other with below-average scores on the employee satisfaction survey. As you can see, there is little difference between the two groups in terms of the average daily amount of time people spend by collaboration over the last six months (see the density plots), but there is a fairly clear difference in the trend of this metric over the same period, suggesting that less satisfied employees may be suffering from increasing collaboration overload (see the line charts with trend lines for individual employees and the estimated overall linear trend).\r\n\r\n\r\n\r\nDo you have a similar experience with or just a strong hunch about other metrics in your area of expertise? Let me know in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-06-standard-and-trend-predictors/./trendGraph.jpg",
    "last_modified": "2022-02-09T09:23:54+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-30-meeting-planning/",
    "title": "Fighting meeting overload",
    "description": "One of the most effective ways to fight meeting overload is to better plan meetings in terms of the time we spend in them. Let's look at how data can tell us how much room for improvement we have in this area.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-30",
    "categories": [
      "meeting planning",
      "meeting effectiveness"
    ],
    "contents": "\r\nOne of the recommended ways to save time in meetings is to plan them better in terms of the time we allocate for them. As in other activities, even here the well-known Parkinson‚Äôs rule applies that ‚Äúwork expands so as to fill the time available for its completion.‚Äù When this is combined with the automatic use of default meeting lengths, it leads to spending more time in meetings than is necessary.\r\nFor this reason, Steven Rogelberg suggests in his book The Surprising Science of Meetings that all meeting times should be reduced by 5-10 percent by default.\r\nTo assess whether you have room for improvement in this regard, it is useful to compare actual and planned meeting lengths. For illustration, the attached chart shows the distribution of the typical differences between actual and planned meeting lengths for each of our teams organizing online meetings over the course of a year. It clearly shows that a large proportion of teams are organizing meetings longer than necessary, by an average of 4 minutes. So in the case of our company Time is Ltd., there definitely seems to be room for implementing Steven Rogelberg‚Äôs suggestion.\r\n\r\n\r\nShow code\r\n\r\n# uploading package\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata <- readRDS(\"./tardiness.rds\")\r\n\r\n# preparing data for density plot\r\nmydata <- with(density(data %>% pull(tardiness)), data.frame(x, y)) %>%\r\n  mutate(col = ifelse(x >= 0, \"A\", \"B\"))\r\n\r\n# visualizing data\r\nmydata %>%\r\n  ggplot() +\r\n  geom_rug(data = filter(data, tardiness >=0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 1, position = \"identity\") +\r\n  geom_rug(data = filter(data, tardiness <0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 0.5, position = \"identity\") +\r\n  \r\n  geom_area(data = filter(mydata, col == 'A'), aes(x = x, y = y), fill = '#4d009d', alpha = 1) + \r\n  geom_area(data = filter(mydata, col == 'B'), aes(x = x, y = y),  fill = '#4d009d', alpha = 0.5) +\r\n  \r\n  geom_label(aes( x=-15.25, y=0.06, label=\" Shorter than planned \"), fill = \"#a67fce\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n  geom_label(aes( x=10.5, y=0.04, label=\" Longer than planned \"), fill = \"#4d009d\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n\r\n  labs(\r\n    x = \"TYPICAL DIFFERENCE BETWEEN ACTUAL AND PLANNED LENGTHS OF MEETINGS\",\r\n    y = \"DENSITY\",\r\n    title = \"Do our online meetings end on time?\",\r\n    subtitle = str_glue(\"On average, our teams organize online meetings {round(abs(mean(data$tardiness)),1)} minutes longer than necessary.\"),\r\n    caption = \"\\nPositive values indicate that online meetings tend to overrun; negative values indicate that online meetings are planned longer than they need to be.\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::label_number(suffix = \" min\"), breaks =  seq(-40, 20, 10)) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.2,.98),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.size = unit(0, \"cm\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt is also worth noting the reverse situation where meetings take longer than planned, as a late end to one meeting becomes a late start to the next meeting.\r\nHow do you feel about finishing meetings too early or too late? Are both similarly unpleasant for you? And isn‚Äôt actually having a shorter meeting than planned something positive?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-30-meeting-planning/./meetingPlanning.jpg",
    "last_modified": "2022-01-30T18:27:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-18-probability-words/",
    "title": "How do we perceive probability words?",
    "description": "Have you ever wondered exactly how much chance of success people give a project when they say they believe in it? If so, then you may find this post useful, as it attempts to answer that question at least in part with data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-18",
    "categories": [
      "probability",
      "perception"
    ],
    "contents": "\r\nNowadays - probably also due to the Covid pandemic and the associated predictions - we are more and more frequently encountering various probabilistic statements, but these are often expressed not in terms of precise numerical probabilities, but in terms of relatively vague probability words such as ‚Äúprobably‚Äù, ‚Äúmaybe‚Äù, ‚Äúunlikely‚Äù, etc.\r\nSince people may imagine different probabilities under these words, it would be useful to have something like a glossary to help us decipher these words and indicate what people usually mean when they use them.\r\nFortunately, there are some studies that examine what numerical probabilities people typically associate with probability words.\r\nFor this purpose, I used a collection of 123 responses to the Wade Fagen-Ulmschneider‚Äôs internet survey and created two similar graphs based on them. The first shows the distribution of the numerical probabilities that people associate with each word, and these are sorted in the graph by the median value of the corresponding probability in descending order. The second graph then differs only in that the words are sorted by the size of the interquartile range in descending order.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggridges)\r\nlibrary(ggpubr)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./survey-results.csv\")\r\n\r\n# getting ordered list of words based on the median value of corresponding probabilities\r\nwordsMedian <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(median = median(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, median)\r\n      )\r\n  \r\nlevelsMedian <- levels(wordsMedian$word)\r\n\r\n# getting ordered list of words based on the IQR of corresponding probabilities\r\nwordsVariability <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(sd = IQR(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, sd)\r\n  )\r\n\r\nlevelsVariability <- levels(wordsVariability$word)\r\n\r\n# graph 1\r\ng1 <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  mutate(word = factor(word, levels = levelsMedian, ordered = TRUE)) %>%\r\n  ggplot(aes(x = probability, y = word)) + \r\n  geom_density_ridges(\r\n    fill = \"#4d009d\",\r\n    alpha = 0.85,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 1, height = 0),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n    quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n  labs(\r\n    fill = \"Trend size\",\r\n    x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n    y = \"\",\r\n    title = \"How do people perceive probability words?\",\r\n    caption = \"\\nThe words are sorted by the median value of the corresponding probability in descending order.\\nThe white dashed lines represent the median values.\\nSource: A collection of 123 responses to an internet survey by Wade Fagen-Ulmschneider.\"\r\n  ) +\r\n  scale_fill_gradient2(\r\n    low = \"red\",\r\n    mid = \"white\",\r\n    high = \"blue\",\r\n    midpoint = 0,\r\n    space = \"Lab\"\r\n  ) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.text.x = element_text(),\r\n        legend.position = \"right\",\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  ) +\r\n  guides(\r\n    fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n  )\r\n\r\n\r\n# graph 2\r\ng2 <- data %>%\r\n    select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n    pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n    mutate(word = factor(word, levels = levelsVariability, ordered = TRUE)) %>%\r\n    ggplot(aes(x = probability, y = word)) + \r\n    geom_density_ridges(\r\n      fill = \"#4d009d\",\r\n      alpha = 0.85,\r\n      scale = 1,\r\n      jittered_points = TRUE,\r\n      position = position_points_jitter(width = 1, height = 0),\r\n      point_shape = '|', point_size = 1, point_alpha = 1, \r\n      quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n      quantile_fun=function(x,...)median(x)\r\n    ) +\r\n    scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n    labs(\r\n      fill = \"Trend size\",\r\n      x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n      y = \"\",\r\n      title = \"What probability words are the most noisy?\",\r\n      caption = \"\\nThe words are sorted by the size of the interquartile range in descending order.\\n\\n\"\r\n    ) +\r\n    scale_fill_gradient2(\r\n      low = \"red\",\r\n      mid = \"white\",\r\n      high = \"blue\",\r\n      midpoint = 0,\r\n      space = \"Lab\"\r\n    ) +\r\n    theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n          plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n          plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n          axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n          axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n          legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n          legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n          axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n          axis.text.x = element_text(),\r\n          legend.position = \"right\",\r\n          axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n          axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n          panel.background = element_blank(),\r\n          panel.grid.major.y = element_blank(),\r\n          panel.grid.major.x = element_blank(),\r\n          panel.grid.minor = element_blank(),\r\n          axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n          axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n          plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n          plot.title.position = \"plot\",\r\n          plot.caption.position =  \"plot\"\r\n    ) +\r\n    guides(\r\n      fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n    )\r\n\r\n# combining graphs\r\nggarrange(g1, g2, ncol = 2, nrow = 1)\r\n\r\n\r\n\r\n\r\nThe first graph can thus help us to use the right word, which in the mind of the other person is most likely to evoke the same probability we want to express. The second graph can then help us to identify the most noisy probability words, for which we will know to ask for a more precise definition because we will be aware that people may imagine very different probabilities under these words.\r\nHow about your perception of probability words? Is there anything in the graphs that surprised you? Would you expect differences between cultures? And what about other demographics? Btw, the original dataset also includes some demographic variables such as age, gender, and education level, so I‚Äôll probably come back to this question in a future post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-18-probability-words/./Probability-Word-Cards.jpg",
    "last_modified": "2022-01-30T17:55:43+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/",
    "title": "Hofstede's theory of cultural dimensions",
    "description": "Cultural diversity brings both positive effects and some challenges. To deal with the latter, it is useful to have some kind of map to help people better navigate the cultural specificities of people from different societies. Hofstede's theory of cultural dimensions is useful for such a purpose. Let's check how dis/similar countries are on these cultural dimensions with a simple app that could help us better understand, manage and appreciate cultural differences a little better.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-03",
    "categories": [
      "cultural diversity",
      "cultural awareness",
      "international management",
      "crosscultural communication",
      "shiny app"
    ],
    "contents": "\r\nOne of the advantages of switch to remote working is that companies can expand the pool of talent from which they choose their employees, without being too constrained by country or even continental boundaries. However, the resulting cultural diversity can bring not only positive effects (e.g.¬†a broader set of perspectives, a more diverse skill base, local market knowledge and insight, better creativity and innovation, etc.) but also some challenges (e.g.¬†risk of prejudice or negative cultural stereotypes, misinterpretation of communication, conflicting work styles, different understanding of professional etiquette, etc.).\r\nBetter knowledge and awareness of the cultural specificities of the societies from which people come is one way of dealing with these challenges. In this respect, Hofstede‚Äôs theory of cultural dimensions may be useful to us. Just as Big-5 theory facilitates our understanding of other people‚Äôs personalities, Hofstede‚Äôs theory facilitates our understanding of their cultural background by describing their social values and releated behaviors through the following six cultural dimensions:\r\n\r\nImage source: https://corporatefinanceinstitute.com/resources/knowledge/other/hofstedes-cultural-dimensions-theory/\r\nYou can easily check how countries are doing on these six dimensions on the Hofstede Insights website. To make it easier to compare cultural differences/similarities between countries, I built a simple app that projects the cultural profiles of countries into 2D space using dimensionality reduction technique called UMAP (Uniform manifold approximation and projection). By selecting a specific cultural dimension, you can see how it is distributed across countries and continents. In addition, you can select some specific countries in the comparator and compare them across all six cultural dimensions.\r\nCheck it out here ‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/Hofstede_Cultural_Dimensions/\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/./six-dimensions-hofstedes-cultural-dimensions-theory.jpg",
    "last_modified": "2023-04-11T20:08:25+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-19-makers-and-managers-schedule/",
    "title": "Makers' schedule and managers' schedule in collaboration data",
    "description": "Many of us have probably already heard of Paul Graham's two types of schedules - one that meets the needs of makers and one that meets the needs of managers. But can these two types of schedules be found in any real collaborative data? Let's find out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-19",
    "categories": [
      "schedule types",
      "makers and managers",
      "collaboration data"
    ],
    "contents": "\r\nI am sure that many of you have heard of the two types of schedules\r\nas described by Paul Graham in his famous article Maker‚Äôs Schedule,\r\nManager‚Äôs Schedule:\r\nThe manager‚Äôs schedule [is] embodied in the traditional\r\nappointment book, with each day cut into one hour intervals. [By]\r\ndefault you change what you‚Äôre doing every hour. But [makers] generally\r\nprefer to use time in units of half a day at least. You can‚Äôt write or\r\nprogram well in units of an hour. That‚Äôs barely enough time to get\r\nstarted. When you‚Äôre operating on the maker‚Äôs schedule, meetings are a\r\ndisaster. A single meeting can blow a whole afternoon, by breaking it\r\ninto two pieces each too small to do anything hard in. Plus you have to\r\nremember to go to the meeting.\r\nI recently realized that I have only seen illustrative pictures on\r\nthis topic so far, but not any real data. This inspired me to look at\r\nour own collaboration data at Time\r\nIs Ltd. and see if these two schedule categories can be found\r\nthere.\r\nWhen I contrasted the data on the average number of meetings per day\r\nand the average time between meetings, there were indeed categories of\r\npeople who either have relatively more meetings with relatively shorter\r\nbreaks (managers), or have relatively fewer meetings with\r\nrelatively longer breaks (makers).\r\n\r\nBut beyond that, there was a third type, which I called\r\nbatchers - they have relatively fewer meetings with relatively\r\nshorter breaks, which is a good strategy when you have to be both\r\nmanager and creator, which may be the case for more and more people as\r\nwe move to remote working.\r\nIn the charts below you can see how typical monthly calendars of\r\nthese three types of schedulers look like.\r\n\r\n\r\nWhat we cannot see in our own data, but could theoretically be there,\r\nis a fourth category I call overtimers, who have relatively\r\nmore meetings but manage to keep relatively longer breaks in between.\r\nHowever, this can only be achieved by making the meetings more spread\r\nout over time, i.e.¬†at the cost of working after hours.\r\nHow about you? Where would you fit in? And is there anyone among you\r\nwho would fit into the fourth, missing category?\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-19-makers-and-managers-schedule/./maker-schedule-vs-manager-schedule.jpg",
    "last_modified": "2022-06-19T22:21:49+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-16-linkedin-connections-analysis/",
    "title": "R Shiny app for LinkedIn connections analysis",
    "description": "An introduction of a simple R Shiny application for analysing LinkedIn connections.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-16",
    "categories": [
      "linkedin",
      "external networks",
      "social network analysis",
      "shiny app"
    ],
    "contents": "\r\nIf you like to use the end of the year as an opportunity for deeper self-reflection, you might enjoy the following simple app I have put together over the past weekend.\r\n‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/linkedIn_connections_analysis/\r\nOnce you upload your LinkedIn connections data to the app (you can easily download the data by following the instructions in the app or in this video), it automatically generates basic descriptive statistics about your LinkedIn connections:\r\nCumulative number of connections over time\r\nNumber of established connections by years, months, and days of the week\r\nTop N companies by the number of established connections\r\nTop N positions by their frequency among your connections (based on whole position titles, bigrams and single words)\r\nProportion of connections by their gender (based on your connections‚Äô first name)\r\n\r\nUnfortunately, since there is no information about your connections‚Äô connections in the data, the app cannot perform more advanced SNA-type of analyses on it. Still, I think you may find some of the statistics useful, or at least interesting and entertaining.\r\nYou can take it as a kind of Christmas gift for my fellow LinkedIn users. Enjoy exploring your connections! And if you‚Äôd like to explore and better manage also your company‚Äôs internal collaboration networks, then check out what we do at Time is Ltd.\r\nP.S. The data you upload is not permanently stored anywhere. The app runs on the shinyapps.io server. If you don‚Äôt want to upload your own data, but would still like to see what the analysis output looks like, you can download and then upload ready-made sample data from the app.\r\nP.P.S. Big thanks to Sebastian Vorac for bringing me to this idea and for UX review. Any remaining errors are, of course, mine alone.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-16-linkedin-connections-analysis/./linkedinLogo.png",
    "last_modified": "2023-04-11T20:08:53+02:00",
    "input_file": {},
    "preview_width": 3753,
    "preview_height": 2352
  },
  {
    "path": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/",
    "title": "Overview of predictors of voluntary employee turnover",
    "description": "An introduction of a simple R Shiny application to facilitate extraction and digestion of information from meta-analysis of predictors of voluntary employee turnover.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-12",
    "categories": [
      "great resignation",
      "employee turnover",
      "turnover predictors",
      "meta-analysis",
      "shiny app"
    ],
    "contents": "\r\nAlthough the ‚ÄòGreat Resignation‚Äô in some parts of the world may be due in no small part to factors specific to the COVID-19 pandemic, it is still useful in this context to draw on the extensive research on employee turnover carried out in the run-up to the pandemic.\r\nA useful overview of such findings is provided, for example, by a 2017 meta-analysis by Rubenstein et al.¬†that summarizes the significance of 57 predictors of voluntary turnover from 9 different domains based on 316 studies from 1975 to 2016 involving more than 300,000 people.\r\nTo make it easier to assimilate these findings, I extracted them from the original article and visualized them in a simple shiny app that helps one to quickly explore and grasp the estimated magnitude, direction, and reliability of the effect of each factor, along with information on the degree of their actionability. The last feature is based purely on my own judgement, so please take it with a grain of salt, or adjust it in your mind using your own judgement. Try it out and let me know if you find it useful.\r\n‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/voluntary_turnover_predictors/\r\n\r\nAnd here is the original research paper on which the shiny app is based.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/./great_resignation.png",
    "last_modified": "2023-04-11T20:09:22+02:00",
    "input_file": {},
    "preview_width": 2700,
    "preview_height": 1800
  },
  {
    "path": "posts/2021-01-29-paygap/",
    "title": "Firemn√≠ audit rozd√≠lu mezi platy mu≈æ≈Ø a ≈æen",
    "description": "Platov√° nerovnost mezi mu≈æi a ≈æenami nen√≠ pro firmy jen z√°le≈æitost√≠ etickou a pr√°vn√≠, ale tak√© marketingovou - m≈Ø≈æe m√≠t toti≈æ negativn√≠ dopad na jejich \"employer brand\" a atraktivitu coby zamƒõstnavatele. To znamen√°, ≈æe pokud firmy chtƒõj√≠ p≈ôil√°kat a tak√© si udr≈æet talentovan√© zamƒõstnance, mus√≠ b√Ωt schopny zajistit, ≈æe se u nich s mu≈æi a ≈æenami bude v tomto ohledu zach√°zet stejnƒõ. Prvn√≠m krokem k tomu je zjistit, jak velk√Ω je rozd√≠l mezi platy mu≈æ≈Ø a ≈æen ve firmƒõ a do jak√© m√≠ry ho lze vysvƒõtlit jin√Ωmi faktory ne≈æ je samotn√© pohlav√≠ zamƒõstnance. V tomto ƒçl√°nku demonstruji, jak takovou anal√Ωzu prov√©st s pomoc√≠ analytick√©ho n√°stroje R a dat, kter√° m√° vƒõt≈°ina firem bƒõ≈ænƒõ k dispozici. Struƒçnƒõ se zmi≈àuji rovnƒõ≈æ o tom, jak√© mohou b√Ωt p≈ô√≠padn√© dal≈°√≠ kroky a doporuƒçen√≠ vypl√Ωvaj√≠c√≠ z v√Ωsledk≈Ø provedn√© anal√Ωzy.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-05-17",
    "categories": [
      "gender pay gap",
      "gender pay audit",
      "regression analysis",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nCo to je gender pay\r\ngap a jak ho mƒõ≈ôit?\r\nProƒç se\r\nzab√Ωvat platovou nerovnost√≠ ve Va≈°√≠ firmƒõ?\r\nAudit platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami\r\nPl√°n anal√Ωzy\r\nDostupn√°\r\ndata\r\nP≈ô√≠prava dat k anal√Ωze\r\nExploraƒçn√≠\r\nanal√Ωza\r\nStatistick√Ω model platov√©\r\nnerovnosti\r\nV√Ωsledky\r\nanal√Ωzy\r\nMo≈æn√© dal≈°√≠\r\nkroky\r\n\r\nCo to je gender pay\r\ngap a jak ho mƒõ≈ôit?\r\nGender pay gap (GPG), v p≈ôekladu genderov√° p≈ô√≠jmov√°\r\nnerovnost nebo p≈ô√≠jmov√° propast mezi mu≈æi a ≈æenami, oznaƒçuje\r\ntypick√Ω rozd√≠l mezi platov√Ωm ohodnocen√≠m pracuj√≠c√≠ch ≈æen a\r\nmu≈æ≈Ø. Obvykle je GPG vyjad≈ôov√°na procenty, pomƒõrem typick√©\r\nhrub√© hodinov√© (ƒçi roƒçn√≠) mzdy ≈æeny k typick√© mzdƒõ mu≈æe nebo pomƒõrem\r\nrozd√≠lu mezi typickou mzdou mu≈æ≈Ø a ≈æen v≈Øƒçi typick√© mzdƒõ mu≈æ≈Ø.\r\nBez ohledu na zp≈Øsob mƒõ≈ôen√≠ GPG, je dob≈ôe dolo≈æen√Ωm faktem, ≈æe ≈æeny\r\njsou obecnƒõ h≈Ø≈ôe placeny ne≈æ mu≈æi, jakkoli se tento rozd√≠l\r\npostupem ƒçasu zmen≈°uje. Rozd√≠ly v platech se p≈ôitom mohou v\r\njednotliv√Ωch zem√≠ch pomƒõrnƒõ dost li≈°it. N√°zornƒõ to ilustruje n√≠≈æe\r\nuveden√Ω graf, kter√Ω ukazuje v√Ωvoj (neadjustovan√©) GPG (definovan√© jako\r\npomƒõr rozd√≠lu medi√°nov√© mzdy zamƒõstnan√Ωch mu≈æ≈Ø a ≈æen a medi√°nov√© mzdy\r\nzamƒõstnan√Ωch mu≈æ≈Ø) v pr≈Øbƒõhu nƒõkolika minul√Ωch let v zem√≠ch OECD.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ngpgoecd <- readr::read_csv(\"./DP_LIVE_29012021212234147.csv\")\r\n\r\n# creating color palette\r\n# list of R color Brewer's palettes: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\r\nnbCols <- length(unique(gpgoecd$LOCATION))\r\nmyColors <- colorRampPalette(brewer.pal(8, \"Set1\"))(nbCols)\r\n\r\n# creating a graph\r\ng <- gpgoecd %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(LOCATION, Value), y = Value, fill = LOCATION,\r\n                      text = paste('Zemƒõ: ', LOCATION,\r\n                                 '<\/br><\/br>GPG: ', round(Value))))+\r\n  ggplot2::geom_col() +\r\n  ggplot2::facet_wrap(~ TIME, nrow = 4) +\r\n  ggplot2::labs(x = \"\",\r\n                y = \"GPG\",\r\n                title = \"Genderov√° p≈ô√≠jmov√° nerovnost v zem√≠ch OECD v letech 2016-2019\") +\r\n  ggthemes::theme_few() +\r\n  ggplot2::scale_fill_manual(values = myColors) +\r\n  ggplot2::theme(legend.position = \"\",\r\n                 legend.title = element_blank(),\r\n                 axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n\r\n\r\n# making the graph interactive\r\nplotly::ggplotly(\r\n  g, \r\n  width = 800,\r\n  height = 700,\r\n  tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nD≈Øvod≈Ø pro nevyv√°≈æenost p≈ô√≠jm≈Ø ≈æen a mu≈æ≈Ø pravdƒõpodobnƒõ existuje\r\nvƒõt≈°√≠ mno≈æstv√≠. Mezi nejƒçastƒõji uv√°dƒõn√© d≈Øvody pat≈ô√≠:\r\nDiskriminace na pracovi≈°ti. Stejn√© pr√°ce je\r\nodmƒõ≈àov√°na rozd√≠lnƒõ ƒçistƒõ na z√°kladƒõ pohlav√≠ pracovn√≠ka.\r\nGenderov√© stereotypy. P≈ôedsudky ohlednƒõ v√Ωkonnosti,\r\nschopnost√≠ a vlastnost√≠ ≈æen maj√≠ za n√°sledek oslaben√≠ jejich pozic a\r\nvytv√°≈ôen√≠ tzv. ‚Äûsklenƒõn√©ho stropu‚Äú, tj. neviditeln√© bari√©ry, na kterou\r\n≈æeny nar√°≈æ√≠ p≈ôi snaze o kari√©rn√≠ postup na l√©pe placen√© pozice.\r\nSegregace trhu. Odvƒõtv√≠, v nich≈æ je tradiƒçnƒõ\r\nzamƒõstn√°v√°no v√≠ce ≈æen ne≈æ mu≈æ≈Ø jako je zdravotnictv√≠, ≈°kolstv√≠ nebo\r\nve≈ôejn√° spr√°va, jsou spoleƒçnost√≠ vn√≠m√°na jako m√©nƒõ presti≈æn√≠, a tedy i\r\nh≈Ø≈ôe odmƒõ≈àov√°na.\r\nRodinn√Ω ≈æivot. ≈Ωeny vƒõt≈°inou nesou vƒõt≈°√≠ ƒç√°st\r\nz√°tƒõ≈æe spojen√© s rodinn√Ωm ≈æivotem (nap≈ô. p≈ôi odchodu na mate≈ôskou\r\ndovolenou, p≈ôi p√©ƒçi o nemocn√© dƒõti ƒçi jin√© ƒçleny dom√°cnosti), co≈æ jim\r\nv√Ωznamnƒõ stƒõ≈æuje jejich snahu o kari√©rn√≠ r≈Øst.\r\nV situaci, kdy p≈ôi reportov√°n√≠ GPG nerozli≈°ujeme mezi r≈Øzn√Ωmi d≈Øvody\r\npro platovou nerovnost, hovo≈ô√≠me o tzv. neadjustovan√©\r\nGPG. Pro pot≈ôeby firemn√≠ho auditu platov√© nerovnosti je v≈°ak\r\nd≈Øle≈æit√© zjistit rovnƒõ≈æ tzv. adjustovanou GPG, kter√° se\r\nsna≈æ√≠ vyj√°d≈ôit m√≠ru platov√© nerovnosti, kter√° je zp≈Øsobena ƒçistƒõ\r\npohlav√≠m zamƒõstnance. Zat√≠mco adjustovan√° GPG umo≈æ≈àuje firmƒõ\r\nidentifikovat mo≈ænou diskriminaci na pracovi≈°ti, neadjustovan√° GPG (p≈ôi\r\nneprok√°zan√© adjustovan√© GPG) m≈Ø≈æe poukazovat na existenci probl√©m≈Ø jako\r\njsou genderov√© stereotypy ƒçi nedostateƒçn√° podpora ≈æen p≈ôi snaze skloubit\r\nsv≈Øj osobn√≠ a profesn√≠ ≈æivot. Pro firmy je tak u≈æiteƒçn√© sledovat oba\r\nukazatele.\r\nProƒç se\r\nzab√Ωvat platovou nerovnost√≠ ve Va≈°√≠ firmƒõ?\r\nI kdybychom odhl√©dli od etick√Ωch ƒçi pr√°vn√≠ch aspekt≈Ø platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami, je ve velice pragmatick√©m z√°jmu ka≈æd√©\r\nfirmy, aby se tento druh nespravedlnosti v jej√≠m syst√©mu odmƒõ≈àov√°n√≠\r\nnevyskytoval. V dobƒõ soci√°ln√≠ch s√≠t√≠ a platforem na hodnocen√≠ firem\r\njejich souƒçasn√Ωmi i b√Ωval√Ωmi zamƒõstnanci (za v≈°echny zmi≈àme nap≈ô. Glassdoor nebo\r\nƒçesk√Ω Atmoskop) se toti≈æ\r\ninformace o nerovn√©m p≈ô√≠stupu m≈Ø≈æe velice snadno roz≈°√≠≈ôit mezi\r\npotenci√°ln√≠ i st√°vaj√≠c√≠ zamƒõstnance, kte≈ô√≠ ji mohou zohlednit p≈ôi sv√©m\r\nrozhodov√°n√≠, zda se v dan√© firmƒõ uch√°zet o pr√°ci, resp. zda v n√≠ i\r\nnad√°le z≈Østat.\r\nTuto skuteƒçnost dokl√°daj√≠ nap≈ô. v√Ωsledky pr≈Øzkumu\r\nproveden√©ho spoleƒçnost√≠ Glassdoor, podle kter√©ho cca 67 % (U.S.)\r\nzamƒõstnanc≈Ø by se neuch√°zelo o pr√°ci tam, kde by si myslelo, ≈æe mu≈æi a\r\n≈æeny maj√≠ nerovn√© platov√© podm√≠nky.\r\nAudit platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami\r\nStejnƒõ jako p≈ôi ≈ôe≈°en√≠ jak√©hokoli jin√©ho probl√©mu, i v tomto p≈ô√≠padƒõ\r\nplat√≠, ≈æe v prvn√≠ ≈ôadƒõ je p≈ôedev≈°√≠m pot≈ôeba ovƒõ≈ôit, ≈æe nƒõjak√Ω\r\nprobl√©m k ≈ôe≈°en√≠ v≈Øbec existuje. K tomu poslou≈æ√≠\r\nfiremn√≠ audit platov√© nerovnosti mezi mu≈æi a ≈æenami.\r\nTen prost≈ôednictv√≠m anal√Ωzy platov√Ωch, demografick√Ωch a organizaƒçn√≠ch\r\ndat ovƒõ≈ô√≠, zda m√°me nƒõjak√© doklady pro to, ≈æe v dan√© spoleƒçnosti\r\nexistuj√≠ platov√© rozd√≠ly mezi zamƒõstnanci spojen√© s jejich pohlav√≠m.\r\nTeprve na z√°kladƒõ v√Ωsledk≈Ø takov√© anal√Ωzy je mo≈æn√© se zaƒç√≠t poohl√≠≈æet po\r\nmo≈æn√Ωch opat≈ôen√≠ch v oblastech n√°boru, odmƒõ≈àov√°n√≠ a/nebo povy≈°ov√°n√≠,\r\nkter√° by mohla pomoct nespravedliv√© platov√© nerovnosti odstranit nebo\r\nalespo≈à zm√≠rnit.\r\nN√≠≈æe uveden√Ω p≈ô√≠klad takov√©ho auditu vych√°z√≠ z ƒçl√°nku How\r\nto Analyze Your Gender Pay Gap: An Employer‚Äôs Guide od Andrew\r\nChamberlaina, Ph.D., hlavn√≠ho ekonoma a vedouc√≠ho v√Ωzkumu ve\r\nspoleƒçnosti Glassdoor.\r\nPl√°n anal√Ωzy\r\nAnal√Ωzu platov√© nerovnosti mezi mu≈æi a ≈æenami provedeme v\r\nn√°sleduj√≠c√≠ch nƒõkolika kroc√≠ch:\r\nNaƒçteme si data, kter√° obsahuj√≠ informace o platech vzorku\r\nzamƒõstnanc≈Ø, jejich pohlav√≠, demografick√Ωch a organizaƒçn√≠ch\r\ncharakteristik√°ch, na kter√Ωch budeme testovat na≈°e hypot√©zy. Za t√≠mto\r\n√∫ƒçelem pou≈æijeme ilustraƒçn√≠ data\r\nposkytnut√° spoleƒçnost√≠ Glassdoor.\r\nV p≈ô√≠padƒõ pot≈ôeby si uprav√≠me data tak, aby l√©pe vyhovovala pot≈ôeb√°m\r\nna≈°√≠ anal√Ωzy.\r\nProvedeme exploraƒçn√≠ anal√Ωzu, kter√° n√°m poskytne z√°kladn√≠ p≈ôedstavu\r\no na≈°ich datech.\r\nSpoƒç√≠t√°me si neadjustovanou GPG.\r\nS pomoc√≠ hierarchick√© regresn√≠ anal√Ωzy vytvo≈ô√≠me statistick√Ω model\r\nGPG, kter√Ω n√°m umo≈æn√≠ l√©pe rozli≈°it ‚Äúvliv‚Äù r≈Øzn√Ωch faktor≈Ø, vƒçetnƒõ\r\njejich interakc√≠, na pozorovan√© rozd√≠ly v platech mu≈æ≈Ø a ≈æen.\r\nOvƒõ≈ô√≠me, zda samotn√© pohlav√≠ zamƒõstance - p≈ôi zohlednƒõn√≠ ‚Äúvlivu‚Äù\r\nostatn√≠ch faktor≈Ø, ke kter√Ωm m√°me k dispozici nƒõjak√° data - hraje\r\nnƒõjakou v√Ωznamnƒõj≈°√≠ roli ve v√Ω≈°i platu, kter√Ω zamƒõstnanec dost√°v√°.\r\nOvƒõ≈ô√≠me, zda pohlav√≠ zamƒõstnance neinteraguje s nƒõkter√Ωmi dal≈°√≠mi\r\nfaktory p≈ôi predikci v√Ω≈°e jejich mzdy.\r\nDostupn√° data\r\n\r\n\r\nShow code\r\n\r\ndata <- readr::read_csv(\"./GenderPay_Data.csv\")\r\n\r\n\r\n\r\nK dispozici m√°me n√°sleduj√≠c√≠ data ke vzorku 1000 zamƒõstnanc≈Ø:\r\nTyp pozice, na kter√© zamƒõstnanec pracuje (jobTitle)\r\nPohlav√≠ zamƒõstnance (gender)\r\nVƒõk zamƒõstnance (age)\r\nHodnocen√≠ pracovn√≠ho v√Ωkonu zamƒõstnance (perfEval)\r\n√örove≈à vzdƒõl√°n√≠ zamƒõstnance (edu)\r\nOddƒõlen√≠, ve kter√©m zamƒõstnanec pracuje (dpt)\r\nM√≠ra seniority zamƒõstnance (seniority)\r\nZ√°kladn√≠ mzda zamƒõstnance (basePay)\r\nBonusov√° slo≈æka platu zamƒõstnance (bonus)\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames = FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nP≈ô√≠prava dat k anal√Ωze\r\nZe zbƒõ≈æn√© kontroly povahy na≈°ich dat je patrn√©, ≈æe ne ka≈æd√° z\r\npromƒõnn√Ωch je v na≈°em datasetu reprezentov√°na pomoc√≠ adekv√°tn√≠ho\r\ndatov√©ho typu. P≈ôed samotnou anal√Ωzou si tedy budeme muset na≈°e data\r\nje≈°tƒõ trochu upravit.\r\n\r\n\r\nShow code\r\n\r\ndplyr::glimpse(data)\r\n\r\n\r\nRows: 1,000\r\nColumns: 9\r\n$ jobTitle  <chr> \"Graphic Designer\", \"Software Engineer\", \"Warehous~\r\n$ gender    <chr> \"Female\", \"Male\", \"Female\", \"Male\", \"Male\", \"Femal~\r\n$ age       <dbl> 18, 21, 19, 20, 26, 20, 20, 18, 33, 35, 24, 18, 19~\r\n$ perfEval  <dbl> 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,~\r\n$ edu       <chr> \"College\", \"College\", \"PhD\", \"Masters\", \"Masters\",~\r\n$ dept      <chr> \"Operations\", \"Management\", \"Administration\", \"Sal~\r\n$ seniority <dbl> 2, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 3, 3, 5, 4, 3, 5,~\r\n$ basePay   <dbl> 42363, 108476, 90208, 108080, 99464, 70890, 67585,~\r\n$ bonus     <dbl> 9938, 11128, 9268, 10154, 9319, 10126, 10541, 1024~\r\n\r\nKonkr√©tnƒõ budeme cht√≠t upravit v≈°echny textov√© promƒõnn√© (pracovn√≠\r\npozice, pohlav√≠, √∫rove≈à vzdƒõl√°n√≠ a pracovn√≠ oddƒõlen√≠) a dvƒõ numerick√©\r\npromƒõnn√© (hodnocen√≠ pracovn√≠ho v√Ωkonu a m√≠ru seniority) na faktorov√©\r\npromƒõnn√©. Ke t≈ôem z tƒõchto novƒõ vytvo≈ôen√Ωch faktorov√Ωch promƒõnn√Ωch\r\n(√∫rove≈à vzdƒõl√°n√≠, hodnocen√≠ pracovn√≠ho v√Ωkonu a m√≠ra senirotity) je\r\npotom pot≈ôeba p≈ôidat informaci o spr√°vn√©m po≈ôad√≠ jejich jednotliv√Ωch\r\nkategori√≠, proto≈æe reprezentuj√≠ ordin√°ln√≠ promƒõnn√©, u kter√Ωch lze\r\nsmysluplnƒõ hovo≈ôit o relativn√≠m po≈ôad√≠ kategori√≠ ve smyslu vy≈°≈°√≠/ni≈æ≈°√≠,\r\nresp. vƒõt≈°√≠/men≈°√≠. Takto upraven√° data ji≈æ odpov√≠daj√≠ typu informac√≠,\r\nkter√© reprezentuj√≠, a m≈Ø≈æeme je tedy zaƒç√≠t pou≈æ√≠vat pro anal√Ωzu na≈°eho\r\nprobl√©mu.\r\n\r\n\r\nShow code\r\n\r\nmydata <- data %>%\r\n  dplyr::mutate_if(is.character, as.factor) %>%\r\n  dplyr::mutate(edu = factor(edu, ordered = TRUE, levels = c(\"High School\", \"College\", \"Masters\", \"PhD\")),\r\n                perfEval = factor(as.character(perfEval), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")),\r\n                seniority = factor(as.character(seniority), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")))\r\n\r\n\r\n\r\nExploraƒçn√≠ anal√Ωza\r\nV n√≠≈æe uveden√Ωch tabulk√°ch jsou uvedeny z√°kladn√≠ popisn√© statistiky k\r\njednotliv√Ωm promƒõnn√Ωm. M≈Ø≈æeme z nich vyƒç√≠st nap≈ô. to, ≈æe na≈°ich 1000\r\nzamƒõstnanc≈Ø je relativnƒõ rovnomƒõnƒõ rozdƒõlen√Ωch do jednotliv√Ωch kategori√≠\r\nz hlediska pracovn√≠ pozice, pohlav√≠, hodnocen√≠ pracovn√≠ho v√Ωkonu, √∫rovnƒõ\r\nvzdƒõl√°n√≠, oddƒõlen√≠, ve kter√©m pracuj√≠, i m√≠ry jejich seniority. D√°le se\r\nz nich m≈Ø≈æeme dozvƒõdƒõt, ≈æe prost≈ôedn√≠ch 50 % zamƒõstnanc≈Ø je ve vƒõku mezi\r\n29 a 54 lety, jejich roƒçn√≠ z√°kladn√≠ mzda se pohybuje od 76 850 do 111\r\n558 USD a jejich bonusy za rok ƒçin√≠ 4 849 a≈æ 8 026 USD.\r\n\r\n\r\nShow code\r\n\r\nskimr::skim(mydata)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nmydata\r\nNumber of rows\r\n1000\r\nNumber of columns\r\n9\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n6\r\nnumeric\r\n3\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\njobTitle\r\n0\r\n1\r\nFALSE\r\n10\r\nMar: 118, Sof: 109, Dat: 107, Fin:\r\n107\r\ngender\r\n0\r\n1\r\nFALSE\r\n2\r\nMal: 532, Fem: 468\r\nperfEval\r\n0\r\n1\r\nTRUE\r\n5\r\n5: 209, 4: 207, 1: 198, 3: 194\r\nedu\r\n0\r\n1\r\nTRUE\r\n4\r\nHig: 265, Mas: 256, Col: 241, PhD:\r\n238\r\ndept\r\n0\r\n1\r\nFALSE\r\n5\r\nOpe: 210, Sal: 207, Man: 198, Adm:\r\n193\r\nseniority\r\n0\r\n1\r\nTRUE\r\n5\r\n3: 219, 2: 209, 1: 195, 5: 193\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n41.39\r\n14.29\r\n18\r\n29.00\r\n41.0\r\n54.25\r\n65\r\n‚ñá‚ñá‚ñÜ‚ñÜ‚ñá\r\nbasePay\r\n0\r\n1\r\n94472.65\r\n25337.49\r\n34208\r\n76850.25\r\n93327.5\r\n111558.00\r\n179726\r\n‚ñÇ‚ñá‚ñá‚ñÉ‚ñÅ\r\nbonus\r\n0\r\n1\r\n6467.16\r\n2004.38\r\n1703\r\n4849.50\r\n6507.0\r\n8026.00\r\n11293\r\n‚ñÇ‚ñá‚ñá‚ñÜ‚ñÇ\r\n\r\n Z hlediska n√°mi analyzovan√©ho probl√©mu jsou pro n√°s ale\r\nd≈Øle≈æitƒõj≈°√≠ vztahy mezi jednotliv√Ωmi promƒõnn√Ωmi, zejm√©na mezi pohlav√≠m a\r\nostatn√≠mi promƒõnn√Ωmi a jejich r≈Øzn√Ωmi kombinacemi. Rychl√Ω p≈ôehled o\r\nnƒõkter√Ωch tƒõchto vztaz√≠ch n√°m m≈Ø≈æe poskytnout n√≠≈æe uveden√Ω graf, kter√Ω\r\nzobrazuje souvislosti mezi jednotliv√Ωmi dvojicemi promƒõnn√Ωch a s pomoc√≠\r\nbarevn√©ho k√≥dov√°n√≠ nav√≠c nese informaci o tom, jak se tyto souvislosti\r\nli≈°√≠ mezi pohlav√≠mi. V grafu m≈Ø≈æeme nap≈ô. vidƒõt, ≈æe se v p≈ô√≠padƒõ\r\nnƒõkter√Ωch pracovn√≠ch pozic v√Ωznamnƒõ li≈°√≠ relativn√≠ zastoupen√≠ mu≈æ≈Ø a\r\n≈æen. V men≈°√≠ m√≠≈ôe se zd√° tento rozd√≠l platit i v p≈ô√≠padƒõ √∫rovnƒõ\r\nvzdƒõl√°n√≠. Urƒçit√Ω rozd√≠l mezi mu≈æi a ≈æenami se zd√° existovat rovnƒõ≈æ ve\r\nv√Ω≈°i jejich z√°kladn√≠ mzdy (narozd√≠l od bonusov√© slo≈æky, kter√° se zd√° b√Ωt\r\nu mu≈æ≈Ø a ≈æen obdobnƒõ vysok√°).\r\n\r\n\r\nShow code\r\n\r\nGGally::ggpairs(mydata, aes(color = gender, alpha = 0.4)) +\r\n  ggplot2::theme(\r\n      strip.text.x = element_text(\r\n        size = 22),\r\n      strip.text.y = element_text(\r\n        size = 22)\r\n      ) +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\")\r\n\r\n\r\n\r\n\r\nVizu√°ln√≠ dojem o rozd√≠ln√© v√Ω≈°i z√°kladn√≠ mzdy u mu≈æ≈Ø a ≈æen potvrzuje i\r\ndetailnƒõj≈°√≠ anal√Ωza tohoto rozd√≠lu. Ta ukazuje, ≈æe v na≈°em vzorku\r\nmedi√°nov√° mzda ≈æen ƒçin√≠ 89913.5 USD a medi√°nov√° mzda mu≈æ≈Ø 98223 USD. To\r\nodpov√≠d√° rozd√≠lu 8309.5 USD, resp. neadjustovan√© GPG (definovan√© jako\r\npomƒõr rozd√≠lu medi√°nov√© mzdy mu≈æ≈Ø a ≈æen a medi√°nov√© mzdy mu≈æ≈Ø) 8.5 %.\r\nM√≠ra platov√© nerovnosti se tak v n√°mi sledovan√© firmƒõ zd√° b√Ωt sp√≠≈°e\r\nni≈æ≈°√≠, srovnateln√° s celkovou hodnotou tohoto ukazatele v zem√≠ch jako je\r\nnap≈ô. ≈†v√©dsko nebo Nov√Ω Z√©land (viz graf z √∫vodu tohoto ƒçl√°nku).\r\nPokud bychom chtƒõli zohlednit m√≠ru na≈°√≠ nejistoty p≈ôi odhadu\r\nvelikosti rozd√≠lu mezi typick√Ωm platem mu≈æ≈Ø a ≈æen, kter√° je dan√° t√≠m, ≈æe\r\npracujeme pouze se vzorkem zamƒõstnanc≈Ø a nikoli s celou firmou, mƒõli\r\nbychom s√°hnout po inferenƒçn√≠ statistice. P≈ôi pou≈æit√≠ bayesovsk√©ho\r\nekvivalentu t-testu pro dva nez√°visl√© v√Ωbƒõry z√≠sk√°me takto informaci o\r\nposteriorn√≠ distribuci velikosti tohoto rozd√≠lu. Na grafu n√≠≈æe m≈Ø≈æeme\r\nvidƒõt, ≈æe 95% interval kredibility se nach√°z√≠ v rozmez√≠ od 5511 do 11615\r\nUSD, s medi√°novou hodnotou 8392 USD. Z grafu tak√© m≈Ø≈æeme vyƒç√≠st, ≈æe\r\ndostupn√° data mluv√≠ silnƒõ v neprospƒõch nulov√© hypot√©zy o neexistenci\r\nrozd√≠lu mezi pr≈Ømƒõrn√Ωm platem mu≈æ≈Ø a ≈æen - viz velmi n√≠zk√° hodnota\r\nlogaritmu Bayesova\r\nfaktoru ve prospƒõch nulov√© hypot√©zu BF01.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nggstatsplot::ggbetweenstats(\r\n  data = mydata,\r\n  x = gender,\r\n  y = basePay,\r\n  type = \"bayes\",\r\n  title = \"Rozd√≠l v z√°kladn√≠ mzdƒõ mezi mu≈æi a ≈æenami\",\r\n  palette = \"Dark2\"\r\n) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::labs(x = \"\")\r\n\r\n\r\n\r\n\r\nSamotn√Ω fakt rozd√≠ln√© v√Ω≈°e z√°kladn√≠ mzdy u mu≈æ≈Ø a ≈æen ale\r\nje≈°tƒõ nemus√≠ automaticky znamenat, ≈æe by se za n√≠m skr√Ωvala diskriminace\r\n≈æen. Pozorovan√Ω rozd√≠l m≈Ø≈æe b√Ωt toti≈æ nap≈ô. zp≈Øsoben√Ω t√≠m, ≈æe\r\n≈æeny zamƒõstnan√© v n√°mi sledovan√© firmƒõ maj√≠ typicky ni≈æ≈°√≠ vzdƒõl√°n√≠ ne≈æ\r\nve stejn√© firmƒõ zamƒõstnan√≠ mu≈æi. A vzhledem k tomu, ≈æe v√Ω≈°e vzdƒõl√°n√≠ (z\r\nhlediska ‚Äúmeritokratick√© spravedlnosti‚Äù zcela neproblematicky) pozitivnƒõ\r\nkoreluje s v√Ω≈°√≠ platu, projev√≠ se tato souvislost v ni≈æ≈°√≠ typick√© mzdƒõ\r\n≈æen (ponechme nyn√≠ stranou ot√°zku, v jak√© m√≠≈ôe maj√≠ ≈æeny obecnƒõ p≈ô√≠stup\r\nk vy≈°≈°√≠mu vzdƒõl√°n√≠ ve spoleƒçnosti, kde dan√° firma p≈Øsob√≠). Tuto hypot√©zu\r\nse zdaj√≠ podporovat i dva n√≠≈æe uveden√© grafy, kter√© vizualizuj√≠ vztah\r\nmezi √∫rovn√≠ vzdƒõl√°n√≠ zamƒõstnance a v√Ω≈°√≠ jeho z√°kladn√≠ mzdy, resp.\r\nsouvislost mezi pohlav√≠m zamƒõstnance a √∫rovn√≠ jeho vzdƒõl√°n√≠.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, y = basePay)) +\r\n  PupillometryR::geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8, fill = \"#a9b2d1\") +\r\n  ggplot2::geom_point(aes(y = basePay), position = position_jitter(width = .15), size = .5, alpha = 0.8, color = \"#a9b2d1\") +\r\n  ggplot2::geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5, fill = \"#a9b2d1\") +\r\n  ggplot2::expand_limits(x = 5.25) +\r\n  ggplot2::guides(fill = FALSE) +\r\n  ggplot2::guides(color = FALSE) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      ),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::theme(panel.border = element_blank()) +\r\n  ggplot2::labs(title = \"Vztah mezi √∫rovn√≠ vzdƒõl√°n√≠ a v√Ω≈°√≠ z√°kladn√≠ mzdy\",\r\n       x = \"\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, fill = gender)) +\r\n  ggplot2::geom_bar(position = \"fill\") +\r\n  ggplot2::scale_fill_hue() +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"M√≠ra zastoupen√≠ m≈Ø≈æ≈Ø a ≈æen v jednotliv√Ωch kategori√≠ch √∫rovnƒõ vzdƒõl√°n√≠\",\r\n                x = \"\",\r\n                y = \"\",\r\n                fill = \"\") +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\") +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nPodobn√Ωch kombinovan√Ωch souvislost√≠ m≈Ø≈æe v na≈°ich datech (a v\r\nrealitƒõ, kterou reprezentuj√≠) existovat vƒõt≈°√≠ mno≈æstv√≠. Pokud by ƒçten√°≈ô\r\nchtƒõl vztahy mezi r≈Øzn√Ωmi kombinacemi promƒõnn√Ωch prozkoumat s√°m a\r\ndetailnƒõji, m≈Ø≈æe za t√≠mto √∫ƒçelem vyu≈æ√≠t tuto\r\ninteraktivn√≠ aplikaci, kde jsou nahran√° na≈°e data a kde lze snadno\r\nr≈Øzn√Ωm zp≈Øsobem vizualizovat zadan√© kombinace promƒõnn√Ωch. Viz n√≠≈æe\r\nuveden√° uk√°zka vyu≈æit√≠ t√©to aplikace p≈ôi vizualizaci vztahu mezi v√Ω≈°√≠\r\nplatu, pohlav√≠m a pracovn√≠ pozic√≠, vƒçetnƒõ poƒçtu zamƒõstnanc≈Ø v\r\njednotliv√Ωch kombinovan√Ωch kategori√≠ch. Z tohoto konkr√©tn√≠ho grafu je\r\ndob≈ôe patrn√©, ≈æe ≈æeny jsou ve srovn√°n√≠ s mu≈æi disproporƒçnƒõ m√©nƒõ\r\nzastoupeny na dvou nadpr≈Ømƒõrnƒõ odmƒõ≈àovan√Ωch pozic√≠ch Manager a\r\nSoftware Engineer a naopak disproporƒçnƒõ v√≠ce jsou zastoupeny na\r\npodpr≈Ømƒõrnƒõ platovƒõ ohodnocen√© pozici Marketing Associate.\r\n\r\n D≈Øle≈æitou kategori√≠ vztah≈Ø mezi promƒõnn√Ωmi, kterou bychom mƒõli\r\nprozkoumat, pokud se chceme co nejbl√≠≈æe dostat k p≈ô√≠ƒçin√°m pozorovan√Ωch\r\nnerovnost√≠ v platech mu≈æ≈Ø a ≈æen a dob≈ôe zac√≠lit p≈ô√≠padn√© intervence,\r\njsou tzv. interakce. Ty popisuj√≠ situace, kdy vztah\r\nmezi dvƒõma promƒõnn√Ωmi z√°vis√≠ na hodnotƒõ nƒõjak√© t≈ôet√≠ promƒõnn√©. N√°s zde\r\nbude konkr√©tnƒõ zaj√≠mat interakce mezi na≈°√≠ hlavn√≠ nez√°vislou promƒõnnou\r\n(prediktorem), tj. pohlav√≠m zamƒõstnance, a dal≈°√≠mi nez√°visl√Ωmi\r\npromƒõnn√Ωmi (nap≈ô. vƒõkem, √∫rovn√≠ vzdƒõl√°n√≠, hodnocen√≠m pracovn√≠ho v√Ωkonu,\r\npracovn√≠ pozic√≠ nebo oddƒõlen√≠m) ve vztahu k na≈°√≠ z√°visl√© promƒõnn√©\r\n(krit√©riu), tedy z√°kladn√≠ mzdƒõ.\r\nP≈ô√≠kladem vizualizace tohoto druhu vztahu mezi promƒõnn√Ωmi je n√≠≈æe\r\nuveden√Ω graf, ze kter√©ho m≈Ø≈æeme vyƒç√≠st, ≈æe ≈æeny maj√≠ sice v pr≈Ømƒõru\r\nni≈æ≈°√≠ z√°kladn√≠ mzdu ne≈æ mu≈æi nap≈ô√≠ƒç cel√Ωm vƒõkov√Ωm spektrem (viz n√≠≈æe\r\npolo≈æen√° regresn√≠ p≈ô√≠mka pro skupinu ≈æen), ale fakt, ≈æe zobrazen√©\r\nregresn√≠ p≈ô√≠mky jsou rovnobƒõ≈æn√©, svƒõdƒç√≠ pro to, ≈æe v r√°mci obou skupin\r\nplat√≠ stejn√Ω typ vztahu mezi vƒõkem a v√Ω≈°√≠ platu, a tedy ≈æe mezi pohlav√≠m\r\na vƒõkem ve vztahu k v√Ω≈°i mzdy nedoch√°z√≠ k ≈æ√°dn√© interakci. Pokud by se\r\nexistence takov√© interakce potvrdila i p≈ôi zohlednƒõn√≠ dal≈°√≠ch\r\nrelevantn√≠ch faktor≈Ø, mƒõlo by to pro n√°s b√Ωt podnƒõtem k dal≈°√≠ exploraci\r\ntoho, co se pozorovan√Ωm rozd√≠lem skr√Ωv√°.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = age, y = basePay, fill = gender, colour = gender, group = gender)) +\r\n  ggplot2::geom_point(size = 1L, position = \"jitter\", alpha = 0.5) +\r\n  ggplot2::geom_smooth(span = 1L, method = \"lm\") +\r\n  ggplot2::scale_fill_brewer(palette = \"Dark2\") +\r\n  ggplot2::scale_color_brewer(palette = \"Dark2\") +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"Vztah mezi vƒõkem zamƒõstnanc≈Ø a v√Ω≈°√≠ jejich z√°kladn√≠ mzdy\",\r\n                fill = \"\",\r\n                color = \"\") +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nStatistick√Ω model platov√©\r\nnerovnosti\r\nAbychom dok√°zali izolovat vliv samotn√©ho pohlav√≠ zamƒõstnanc≈Ø na v√Ω≈°i\r\nplatu a zohlednit p≈ôitom z√°rove≈à vliv v≈°ech ostatn√≠ch relevantn√≠ch\r\nfaktor≈Ø, vƒçetnƒõ nƒõkter√Ωch jejich interakc√≠, mus√≠me s√°hnout po\r\nkomplexnƒõj≈°√≠m n√°stroji ne≈æ je popisn√° statistika. A t√≠mto n√°strojem je\r\nstatistick√© modelov√°n√≠.\r\nStatistick√© modelov√°n√≠, podobnƒõ jako jak√©koli jin√© modelov√°n√≠ ve\r\nvƒõdƒõ, ale i v bƒõ≈æn√©m ≈æivotƒõ, nen√≠ niƒç√≠m jin√Ωm ne≈æ snahou\r\nvytvo≈ôit men≈°√≠ a zjednodu≈°en√Ω model na≈°eho svƒõta, kter√Ω v≈°ak\r\njeho chov√°n√≠ odr√°≈æ√≠ dostateƒçnƒõ vƒõrnƒõ na to, abychom s jeho pomoc√≠ mohli\r\nƒçinit √∫sudky a p≈ôedpovƒõdi o skuteƒçn√©m svƒõtƒõ a zakl√°dat na nƒõm sv√°\r\nrozhodnut√≠ (k tomuto t√©matu viz srozumitelnƒõ napsan√Ω\r\npopularizuj√≠c√≠ ƒçl√°nek Modeluji,\r\ntedy jsem od Josefa ≈†lerky).\r\nStatistick√© modelov√°n√≠ se potom od jin√Ωch druh≈Ø modelov√°n√≠ li≈°√≠ v tom,\r\n≈æe se ve vƒõt≈°√≠ m√≠≈ôe op√≠r√° o n√°stroje matematick√© statistiky a teorie\r\npravdƒõpodobnosti.\r\nP≈ôekvapivƒõ mnoho jev≈Ø na≈°eho svƒõta se d√° √∫spƒõ≈°nƒõ modelovat a\r\np≈ôedpov√≠dat pomoc√≠ relativnƒõ jednoduch√Ωch statistick√Ωch model≈Ø\r\nzobecnƒõn√© line√°rn√≠ regrese (Generalized Linear\r\nModels, GLM). Ty p≈ôedpokl√°daj√≠, ≈æe z√°visl√° promƒõnn√°, transformovan√°\r\nprost≈ôednictv√≠m tzv. linkovac√≠ funkce (link\r\nfunction), je funkc√≠ line√°rn√≠ kombinace nez√°visl√Ωch promƒõnn√Ωch.\r\nNejzn√°mƒõj≈°√≠ z t√©to rodiny statistick√Ωch model≈Ø je klasick√Ω\r\nline√°rn√≠ model, kter√Ω p≈ôedpokl√°d√° norm√°ln√≠ rozdƒõlen√≠ z√°visl√©\r\npromƒõnn√©, resp. rezidu√≠ (chyb) okolo predikovan√©/ oƒçek√°van√© st≈ôedn√≠\r\nhodnoty z√°visl√© promƒõnn√© (viz ilustrativn√≠ obr√°zek n√≠≈æe).\r\n\r\nVzhledem k tomu, ≈æe n√°mi modelovan√° promƒõnn√° z√°kladn√≠ mzdy se zd√° m√≠t\r\nnorm√°ln√≠, nebo t√©mƒõ≈ô norm√°ln√≠ rozdƒõlen√≠ (viz nƒõkter√© grafy v ƒç√°sti\r\nvƒõnovan√© exploraƒçn√≠ anal√Ωze), m≈Ø≈æeme i my s√°hnout po tomto statistick√©m\r\nmodelu. Jako nez√°visl√© promƒõnn√© v na≈°em modelu pou≈æijeme v≈°echny n√°m\r\ndostupn√© prediktory, spolu s interakcemi mezi promƒõnnou pohlav√≠ na\r\nstranƒõ jedn√© a promƒõnn√Ωmi √∫rovnƒõ vzdƒõl√°n√≠, seniority, vƒõku a hodnocen√≠\r\npracovn√≠ho v√Ωkonu na stranƒõ druh√©. Proto≈æe zamƒõstnanci tvo≈ô√≠ p≈ôirozen√©\r\nshluky v r√°mci oddƒõlen√≠, nap≈ô√≠ƒç kter√Ωmi se li≈°√≠ v√Ω≈°e mzdy a tak√© by se\r\nmohla li≈°it povaha vztahu mezi pohlav√≠m zamƒõstnance a v√Ω≈°√≠ jeho mzdy,\r\npou≈æijeme hierarchickou/v√≠ce√∫rov≈àovou variantu modelu line√°rn√≠\r\nregrese, kter√° umo≈æ≈àuje, aby hodnoty vybran√Ωch parametr≈Ø modelu\r\nvariovaly v z√°vilosti na p≈ô√≠slu≈°nosti zamƒõstnanc≈Ø do konkr√©tn√≠ho\r\noddƒõlen√≠.\r\nK odhadu hodnot parametr≈Ø na≈°eho modelu pou≈æijeme inferenƒçn√≠\r\nr√°mec bayesovsk√© statistiky, kter√° ve srovn√°n√≠ s\r\nfrekventistickou statistikou nab√≠z√≠ bohat≈°√≠ a intuitivnƒõ sn√°ze\r\nuchopiteln√© v√Ωstupy. Pro apriorn√≠ distribuci parametr≈Ø modelu pou≈æijeme\r\ndefaultn√≠, ≈°irok√© a neinformativn√≠ hodnoty, tak≈æe v√Ωsledky anal√Ωzy budou\r\nnomin√°lnƒõ podobn√© tƒõm, kter√© bychom z√≠skali p≈ôi pou≈æit√≠ tradiƒçnƒõj≈°√≠\r\nfrekventistick√© inferenƒçn√≠ statistiky.\r\n\r\n\r\nShow code\r\n\r\n# defining and running the model\r\n\r\nmodel <- brms::brm(\r\n  basePay | trunc(lb = 0) \r\n  ~ 1 \r\n  + jobTitle \r\n  + gender \r\n  + age \r\n  + perfEval \r\n  + edu \r\n  + seniority \r\n  + gender:edu \r\n  + gender:seniority \r\n  + gender:age \r\n  + gender:perfEval \r\n  + (1 + gender | dept),  \r\n  data = mydata %>% dplyr::mutate_if(is.factor, as.character),\r\n  family = gaussian(link = \"identity\"),\r\n  iter = 3000,\r\n  chains = 3,\r\n  cores = 6,\r\n  warmup = 1000,\r\n  seed = 2809,\r\n  control = list(\r\n    adapt_delta = 0.99, \r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\nV√Ωsledky anal√Ωzy\r\nD≈ô√≠ve ne≈æ p≈ôistoup√≠me k interpretaci v√Ωsledk≈Ø anal√Ωzy je dobr√© si\r\novƒõ≈ôit, ≈æe n√°≈° statistick√Ω model dok√°≈æe dostateƒçnƒõ vƒõrnƒõ napodobit ƒçi\r\nsimulovat data reprezentuj√≠c√≠ firemn√≠ realitu, na jej√≠≈æ vlastnosti\r\nchceme s pomoc√≠ tohoto modelu usuzovat. Za t√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t\r\nn√°stroj posteriorn√≠ prediktivn√≠ kontroly (posterior predictive\r\ncheck), kter√Ω ovƒõ≈ôuje, jak moc dob≈ôe n√°mi zvolen√Ω a odhadnut√Ω model\r\npredikuje pozorovan√° data na z√°kladƒõ vzorku posteriorn√≠ch hodnot jeho\r\nparametr≈Ø. Z n√≠≈æe uveden√©ho grafu je dob≈ôe patrn√©, ≈æe n√°≈° model si z\r\ntohoto hlediska nevede v≈Øbec ≈°patnƒõ.\r\nPo t√©to kontrole (a tak√© po ovƒõ≈ôen√≠ dal≈°√≠ch technick√Ωch\r\nn√°le≈æitost√≠, jako je nap≈ô. konvergence MCMC\r\n≈ôetƒõzc≈Ø, kter√© umo≈æ≈àuj√≠ odhadnout posteriorne√≠ distribuci parametr≈Ø i\r\nkomplexnƒõj≈°√≠ch statistick√Ωch model≈Ø jako je ten n√°≈°) m≈Ø≈æeme zaƒç√≠t\r\nvyu≈æ√≠vat parametry na≈°eho modelu k usuzov√°n√≠ na pravdƒõpodobn√© vlastnosti\r\nn√°mi studovan√© firemn√≠ reality.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model's fit\r\n\r\n# specifying the number of samples\r\nnsamples = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posteriorn√≠ prediktivn√≠ kontrola modelu za pou≈æit√≠ vzorku o velikoti n = {nsamples}\")\r\n    )\r\n\r\n\r\n\r\n\r\nN√≠≈æe je uveden souhrn informac√≠ o na≈°em odhadnut√©m modelu. Prim√°rnƒõ\r\nn√°s zaj√≠m√° hodnota parametru pohlav√≠ (genderMale) v sekci\r\nvƒõnovan√© efekt≈Øm na √∫rovni cel√© populace (Population-Level\r\nEffects). 95% interval kredibility (Credible Interval),\r\nkter√Ω ud√°v√° kam v posteriorn√≠m rozdƒõlen√≠ spad√° hodnota nepozorovan√©ho\r\nparametru s 95% pravdƒõpodobnost√≠, se nach√°z√≠ v rozmez√≠ od -3750.04 USD\r\ndo 9081.92 USD, se st≈ôedn√≠ hodnotou 2717.57. Tzn., ≈æe podle na≈°eho\r\nmodelu m√° mu≈æ - p≈ôi zohlednƒõn√≠ ostatn√≠ch faktor≈Ø a jejich vybran√Ωch\r\ninterakc√≠ - typicky o cca 2700 USD vy≈°≈°√≠ z√°kladn√≠ mzdu ne≈æ jej√≠ ≈æensk√Ω\r\nprotƒõj≈°ek. Anal√Ωza na≈°ich dat tak do urƒçit√© m√≠ry podporuje hypot√©zu o\r\nexistenci platov√© diskriminace na z√°kladƒõ pohlav√≠ zamƒõstnance v n√°mi\r\nstudovan√© firmƒõ. S√≠la d≈Økazu ve prospƒõch t√©to hypot√©zy v≈°ak nen√≠ nijak\r\nv√Ωrazn√°, co≈æ vypl√Ωv√° z toho, ≈æe 95% interval kredibility zahrnuje vedle\r\nkladn√Ωch hodnot i nulovou hodnotu a z√°porn√© hodnoty parametru pohlav√≠\r\njako jeho plauzibiln√≠ hodnoty.\r\n\r\n\r\nShow code\r\n\r\nsummary(model)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: basePay | trunc(lb = 0) ~ 1 + jobTitle + gender + age + perfEval + edu + seniority + gender:edu + gender:seniority + gender:age + gender:perfEval + (1 + gender | dept) \r\n   Data: mydata %>% dplyr::mutate_if(is.factor, as.characte (Number of observations: 1000) \r\n  Draws: 3 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 6000\r\n\r\nGroup-Level Effects: \r\n~dept (Number of levels: 5) \r\n                          Estimate Est.Error l-95% CI u-95% CI Rhat\r\nsd(Intercept)              3659.02   2392.77  1181.83  9771.07 1.00\r\nsd(genderMale)             1808.55   1724.07    78.57  6119.45 1.00\r\ncor(Intercept,genderMale)     0.30      0.53    -0.83     0.98 1.00\r\n                          Bulk_ESS Tail_ESS\r\nsd(Intercept)                 2653     2634\r\nsd(genderMale)                3466     3552\r\ncor(Intercept,genderMale)     6333     4308\r\n\r\nPopulation-Level Effects: \r\n                            Estimate Est.Error  l-95% CI  u-95% CI\r\nIntercept                   29216.96   3175.03  23007.14  35319.81\r\njobTitleDriver              -3633.29   1488.22  -6452.10   -760.74\r\njobTitleFinancialAnalyst     3749.02   1419.10   1008.10   6517.61\r\njobTitleGraphicDesigner     -2832.43   1454.75  -5687.49     37.72\r\njobTitleIT                  -1869.36   1438.48  -4666.71    969.94\r\njobTitleManager             31411.39   1495.39  28444.26  34352.28\r\njobTitleMarketingAssociate -16475.88   1390.53 -19181.98 -13758.33\r\njobTitleSalesAssociate        316.91   1428.70  -2488.94   3110.02\r\njobTitleSoftwareEngineer    13286.51   1416.67  10487.13  16055.90\r\njobTitleWarehouseAssociate  -1040.96   1491.96  -3955.62   1848.02\r\ngenderMale                   2717.57   3238.37  -3750.04   9081.92\r\nage                           995.31     33.84    927.62   1061.68\r\nperfEval2                     246.70   1444.36  -2609.56   3094.73\r\nperfEval3                   -1515.48   1463.94  -4327.32   1361.02\r\nperfEval4                     183.45   1438.20  -2567.99   3065.99\r\nperfEval5                    1433.01   1470.70  -1405.12   4405.58\r\neduHighSchool                -417.81   1302.59  -2969.86   2140.64\r\neduMasters                   4149.49   1321.31   1548.67   6728.00\r\neduPhD                       7627.28   1342.16   5025.37  10270.97\r\nseniority2                   8000.31   1527.46   4989.21  10992.21\r\nseniority3                  17954.08   1486.71  15132.57  20868.40\r\nseniority4                  30596.32   1613.05  27406.56  33680.11\r\nseniority5                  39640.70   1530.88  36626.89  42651.21\r\ngenderMale:eduHighSchool    -1926.51   1868.39  -5590.15   1674.20\r\ngenderMale:eduMasters         780.52   1829.37  -2755.04   4346.52\r\ngenderMale:eduPhD           -3154.41   1842.55  -6757.49    436.31\r\ngenderMale:seniority2         898.79   2074.27  -3179.87   4874.13\r\ngenderMale:seniority3        -354.32   2018.99  -4315.71   3647.04\r\ngenderMale:seniority4       -2847.26   2095.05  -7044.18   1157.86\r\ngenderMale:seniority5       -3538.92   2098.61  -7564.69    565.50\r\ngenderMale:age                 16.69     44.85    -71.88    104.73\r\ngenderMale:perfEval2         -603.35   2084.98  -4768.78   3466.38\r\ngenderMale:perfEval3         1601.12   2062.41  -2375.63   5640.84\r\ngenderMale:perfEval4         -471.22   2020.88  -4447.73   3466.87\r\ngenderMale:perfEval5        -2795.97   2013.44  -6768.31   1080.21\r\n                           Rhat Bulk_ESS Tail_ESS\r\nIntercept                  1.00     2311     3592\r\njobTitleDriver             1.00     3819     4183\r\njobTitleFinancialAnalyst   1.00     3294     4284\r\njobTitleGraphicDesigner    1.00     3372     3844\r\njobTitleIT                 1.00     4040     4704\r\njobTitleManager            1.00     3617     4213\r\njobTitleMarketingAssociate 1.00     3495     4556\r\njobTitleSalesAssociate     1.00     3885     4347\r\njobTitleSoftwareEngineer   1.00     3327     4760\r\njobTitleWarehouseAssociate 1.00     3601     4249\r\ngenderMale                 1.00     3031     4168\r\nage                        1.00     5997     4475\r\nperfEval2                  1.00     4400     3950\r\nperfEval3                  1.00     4457     4393\r\nperfEval4                  1.00     4276     4384\r\nperfEval5                  1.00     4239     4453\r\neduHighSchool              1.00     4654     4274\r\neduMasters                 1.00     4692     4883\r\neduPhD                     1.00     4686     4832\r\nseniority2                 1.00     3890     4236\r\nseniority3                 1.00     3995     4891\r\nseniority4                 1.00     4234     4872\r\nseniority5                 1.00     4092     4550\r\ngenderMale:eduHighSchool   1.00     4494     4650\r\ngenderMale:eduMasters      1.00     4323     4943\r\ngenderMale:eduPhD          1.00     4369     4810\r\ngenderMale:seniority2      1.00     3919     4684\r\ngenderMale:seniority3      1.00     4535     5116\r\ngenderMale:seniority4      1.00     4379     4976\r\ngenderMale:seniority5      1.00     4594     4807\r\ngenderMale:age             1.00     5968     4280\r\ngenderMale:perfEval2       1.00     4211     4414\r\ngenderMale:perfEval3       1.00     4106     4449\r\ngenderMale:perfEval4       1.00     3960     4438\r\ngenderMale:perfEval5       1.00     3865     4221\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma 10095.58    236.20  9649.25 10569.40 1.00    10053     4303\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nPokud bychom chtƒõli p≈ôesnƒõji vyjad≈ôit m√≠ru, s n√≠≈æ na≈°e data v r√°mci\r\nna≈°eho modelu favorizuj√≠ hodnoty parametru pohlav√≠ vƒõt≈°√≠ ne≈æ nula (tj.\r\nhodnoty, kter√© jsou v souladu s hypot√©zou o existenci platov√©\r\ndiskriminace na z√°kladƒõ pohlav√≠ v neprospƒõch ≈æen), m≈Ø≈æeme se pod√≠vat na\r\nposteriorn√≠ distribuci tohoto parametru a jednodu≈°e na nƒõm spoƒç√≠tat, s\r\njakou pravdƒõpodobnost√≠ nab√Ωv√° kladn√Ωch hodnot.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the model's b_genderMale parameter \r\n\r\nparamViz <- model %>%\r\n  tidybayes::gather_draws(\r\n    b_genderMale\r\n    ) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramViz$value)\r\n\r\nparamViz <- tibble(x = dens$x, y = dens$y)\r\n\r\n\r\nggplot2::ggplot(\r\n  paramViz,\r\n  aes(x,y)\r\n    ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x > 0),\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x <= 0),\r\n    fill = \"grey\"\r\n  ) +\r\n  ggplot2::geom_line(\r\n  ) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-15000, 15000, 5000)) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(\r\n    title = \"Posteriorn√≠ distribuce parametru pohlav√≠ zamƒõstnance\",\r\n    y = \"Density\",\r\n    x = \"genderMale\"\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(model)\r\n\r\n# probability of b_genderMale coefficient being higher\r\nprop <- sum(samples$b_genderMale > 0) / nrow(samples)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Bayesian hypothesis test\r\nthe_test <- brms::hypothesis(model, \"genderMale > 0\")\r\n\r\n\r\n\r\nPo proveden√≠ tohoto v√Ωpoƒçtu n√°m vych√°z√≠ hodnota 80 %. To je v souladu\r\ns p≈ôedchoz√≠m tvrzen√≠m, ≈æe d≈Økaz ve prospƒõch testovan√© hypot√©zy nen√≠\r\np≈ô√≠li≈° siln√Ω. Dal≈°√≠ mo≈ænost√≠ by bylo pou≈æit√≠ tzv. Bayesova faktoru,\r\nkter√Ω vyjad≈ôuje m√≠ru s n√≠≈æ dostupn√° data favorizuj√≠ testovanou hypot√©zu\r\nve srovn√°n√≠ s modelem odpov√≠daj√≠c√≠m nulov√© hypot√©ze. Ten m√° pro na≈°i\r\nhypot√©zu hodnotu 4, co≈æ odpov√≠d√° v√Ωznamn√©mu, ale zdaleka nikoli siln√©mu\r\nƒçi rozhodn√©mu d≈Økazu ve prospƒõch na≈°√≠ hypot√©zy.\r\nVedle parametru pohlav√≠ m≈Ø≈æe b√Ωt pro n√°s potenci√°lnƒõ u≈æiteƒçn√© pod√≠vat\r\nse tak√© na vztah z√°kladn√≠ mzdy a ostatn√≠ch prediktor≈Ø pou≈æit√Ωch v na≈°em\r\nmodelu. Za t√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t vizualizaci margin√°ln√≠ch efekt≈Ø\r\njednotliv√Ωch prediktor≈Ø, kter√© vyjad≈ôuj√≠ vztah mezi prediktorem a\r\nkrit√©riem p≈ôi zohlednƒõn√≠ vlivu ostatn√≠ch prediktor≈Ø. Takto nap≈ô. m≈Ø≈æeme\r\nna jednom z graf≈Ø vidƒõt, ≈æe vztah mezi √∫rovn√≠ vzdƒõl√°n√≠ a v√Ω≈°√≠ z√°kladn√≠ho\r\nplatu se m√° tendenci u mu≈æ≈Ø a ≈æen li≈°it. Na jin√©m grafu si m≈Ø≈æeme zase\r\nv≈°imnout toho, ≈æe rozd√≠l mezi z√°kladn√≠ mzdou mu≈æ≈Ø a ≈æen m√° tendenci\r\nnar≈Østat s t√≠m, jak kles√° seniorita zamƒõstnanc≈Ø. Tyto a dal≈°√≠ podobn√©\r\nvhledy n√°m mohou pomoct p≈ôibl√≠≈æit se k d≈Øvod≈Øm za pozorovan√Ωmi\r\nnerovnostmi v platech mu≈æ≈Ø a ≈æen.\r\n\r\n\r\nShow code\r\n\r\n# plotting marginal effects of predictors used \r\n# Note: Conditional vs. Marginal Relationships: The regression coefficients in generalized linear mixed models represent conditional effects in the sense that they express comparisons holding the cluster-specific random effects (and covariates) constant. For this reason, conditional effects are sometimes referred to as cluster-specific effects. In contrast, marginal effects can be obtained by averaging the conditional expectation Œºij over the random effects distribution. Marginal effects express comparisons of entire sub-population strata defined by covariate values and are sometimes referred to as population-averaged effects.In linear mixed models (identity link), the regression coefficents can be interpreted as either conditional or marginal effects. However, conditional and marginal effects differ for most other link functions.\r\n\r\nmarginalEffplots <- plot(\r\n  brms::marginal_effects(\r\n    model, \r\n    effects = c(\"jobTitle\", \"age\", \"perfEval\", \"edu\", \"seniority\", \"gender:edu\", \"gender:seniority\", \"gender:age\", \"gender:perfEval\"),\r\n    probs = c(0.025, 0.975)),\r\n  ask = FALSE\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# putting all graphs with marginal effects together  \r\nggpubr::ggarrange(\r\n  plotlist = marginalEffplots, \r\n  nrow = 9,\r\n  ncol = 1\r\n)\r\n\r\n\r\n\r\n\r\nMo≈æn√© dal≈°√≠ kroky\r\nI v situaci, kdy anal√Ωza dat nepodpo≈ô√≠ na≈°e podez≈ôen√≠ na existenci\r\nplatov√© diskriminace na z√°kladƒõ pohlav√≠ zamƒõstnance, je st√°le mo≈æn√©, ≈æe\r\nza pozorovan√Ωm rozd√≠lem v platech mu≈æ≈Ø a ≈æen jsou jin√© faktory, kter√© s\r\npohlav√≠m zamƒõstance nƒõjak souvis√≠. Nap≈ô. skuteƒçnost, ≈æe jsou ≈æeny m√©nƒõ\r\nreprezentovan√© na l√©pe placen√Ωch seniornƒõj≈°√≠ch pozic√≠ch, by mohla\r\nsvƒõdƒçit o tom, ≈æe se ≈æeny na pracovi≈°ti mohou pot√Ωkat s genderov√Ωmi\r\nstereotypy a ≈æe p≈ôi snaze o kari√©rn√≠ postup na l√©pe placen√© pozice\r\nnar√°≈æej√≠ na tzv. ‚Äúsklenƒõn√Ω strop‚Äú. Pro uƒçinƒõn√≠ takov√©ho z√°vƒõru je v≈°ak\r\nzapot≈ôeb√≠ z√≠skat dal≈°√≠ data, a to sp√≠≈°e kvalitativn√≠ povahy, takov√°,\r\nkter√° sb√≠r√° a analyzuje nap≈ô. organizaƒçn√≠\r\nƒçi firemn√≠\r\nantropologie.\r\nV situaci, kdy m√°me dostateƒçnƒõ siln√© d≈Økazy pro to, ≈æe se za\r\npozorovanou platovou nerovnost√≠ mezi mu≈æi a ≈æenami skr√Ωvaj√≠ faktory\r\nsouvisej√≠c√≠ s pohlav√≠m zamƒõstnance, je mo≈æn√© zaƒç√≠t se poohl√≠≈æet po\r\nmo≈æn√Ωch ≈ôe≈°en√≠ch. Stejnƒõ jako p≈ôi identifikaci probl√©mu, i p≈ôi hled√°n√≠\r\nzp≈Øsobu jeho ≈ôe≈°en√≠ je dobr√© dr≈æet se z√°sad na\r\nd≈Økazech zalo≈æen√©ho managementu a volit pouze ≈ôe≈°en√≠ s dostateƒçnƒõ\r\nempiricky dolo≈æenou √∫ƒçinnost√≠, kter√° z√°rove≈à d√°vaj√≠ smysl ve specifick√©m\r\nkontextu dan√© firmy.\r\nU≈æiteƒçn√Ω p≈ôehled mo≈æn√Ωch akc√≠, kter√© zamƒõstnavatel√© mohou podniknout\r\ns c√≠lem sn√≠≈æit GPG ve sv√© organizaci, vytvo≈ôila zn√°m√° skupina odborn√≠k≈Ø\r\nna behavior√°ln√≠ vƒõdy v r√°mci tzv. The\r\nBehavioral Insights Team, kter√° sv√©ho ƒçasu vznikla pro to, aby\r\nbritsk√© vl√°dƒõ pom√°hala realizovat √∫ƒçinnou politiku zalo≈æenou na\r\nd≈Økazech. V dokumentu s n√°zvem Reducing the gender pay gap and\r\nimproving gender equality in organisations: Evidence-based actions for\r\nemployers tato skupina odborn√≠k≈Ø uv√°d√≠ nƒõkolik mo≈æn√Ωch intervenc√≠,\r\nkter√© ≈ôad√≠ do t≈ô√≠ kategori√≠ podle toho, jak dob≈ôe je jejich √∫ƒçinnost\r\npodlo≈æen√° empirick√Ωmi d≈Økazy.\r\nMezi akce s dob≈ôe dolo≈æenou √∫ƒçinnost√≠ ≈ôad√≠\r\nn√°sleduj√≠c√≠ intervence:\r\nZahrnut√≠ vƒõt≈°√≠ho poƒçtu ≈æen do u≈æ≈°√≠ch seznam≈Ø v r√°mci v√Ωbƒõru nov√Ωch\r\nzamƒõstnanc≈Ø a povy≈°ov√°n√≠.\r\nPou≈æ√≠v√°n√≠ √∫loh posuzuj√≠c√≠ch √∫rove≈à pracovn√≠ch dovednost√≠ v r√°mci\r\nv√Ωbƒõru nov√Ωch zamƒõstnanc≈Ø.\r\nPou≈æ√≠v√°n√≠ strukturovan√©ho interview v r√°mci v√Ωbƒõru nov√Ωch\r\nzamƒõstnanc≈Ø a povy≈°ov√°n√≠.\r\nPodpora vyjedn√°v√°n√≠ o v√Ω≈°i platu pomoc√≠ zvƒõ≈ôejnƒõn√≠ existuj√≠c√≠ho\r\nplatov√©ho rozmez√≠.\r\nZaveden√≠ transparentn√≠ch proces≈Ø povy≈°ov√°n√≠ a odmƒõ≈àov√°n√≠.\r\nJmenov√°n√≠ mana≈æera ƒçi z≈ô√≠zen√≠ pracovn√≠ skupiny pro firemn√≠\r\ndiverzitu.\r\nMezi potenci√°lnƒõ slibn√© akce, kter√© ale vy≈æaduj√≠ dal≈°√≠ d≈Økazy\r\no sv√© √∫ƒçinnosti, ≈ôad√≠ n√°sleduj√≠c√≠ postupy:\r\nZv√Ω≈°en√≠ pracovn√≠ flexibility pro mu≈æe a pro ≈æeny.\r\nPodporu sd√≠len√© rodiƒçovsk√© dovolen√©.\r\nN√°bor b√Ωval√Ωch zamƒõstnanc≈Ø, kte≈ô√≠ museli z r≈Øzn√Ωch osobn√≠ch d≈Øvod≈Ø\r\nna del≈°√≠ dobu p≈ôeru≈°it svou kari√©ru.\r\nNab√≠dku mentoringu and sponsorshipu.\r\nNab√≠dku networkingov√Ωch program≈Ø.\r\nNastaven√≠ intern√≠ch c√≠l≈Ø.\r\nA mezi akce se sm√≠≈°en√Ωmi doklady o jejich √∫ƒçinnosti\r\npotom ≈ôad√≠ n√°sleduj√≠c√≠ opat≈ôen√≠:\r\n≈†kolen√≠ vƒõnovan√© t√©matu nevƒõdom√Ωch p≈ôedsudk≈Ø.\r\n≈†kolen√≠ v oblasti diverzity.\r\n≈†kolen√≠ vƒõnovan√© rozvoji leadershipu.\r\nDemograficky r≈Øznorod√© v√Ωbƒõrov√© panely v r√°mci extern√≠ho i intern√≠ho\r\nn√°boru.\r\nZde je pro z√°jemce origin√°ln√≠ dokument k bli≈æ≈°√≠mu prostudov√°n√≠.\r\n\r\n\r\nTento prohl√≠≈æeƒç nepodporuje soubory PDF. Pro zobrazen√≠ si, pros√≠m, PDF\r\nsoubor st√°hnƒõte: St√°hnout PDF.\r\n\r\n\r\n\r\nSkript k anal√Ωze je k dispozici ke sta≈æen√≠ v podobƒõ Jupyter Notebooku\r\nna m√Ωch GitHub\r\nstr√°nk√°ch.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-29-paygap/./PayGap.png",
    "last_modified": "2022-08-30T17:05:43+02:00",
    "input_file": {},
    "preview_width": 1438,
    "preview_height": 897
  },
  {
    "path": "posts/2020-12-31-segmentedregression/",
    "title": "Modeling impact of the COVID-19 pandemic on people‚Äôs interest in work-life balance and well-being",
    "description": "Illustration of Bayesian segmented regression analysis of interrupted time series data with a testing hypothesis about the impact of the COVID-19 pandemic on increase in people's search interest in work-life balance and well-being.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "well-being",
      "work-life balance",
      "covid pandemic",
      "segmented regression",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nSearch interest in work-life balance and well-being\r\nBayesian segmented regression\r\nSome necessary sanity checks\r\nResults of the analysis\r\n\r\nThe turn of the year, which is full of all sorts of resolutions to change for the better in our private lives and in our organizations, is a good time to remind ourselves that analytic tools can be very helpful in our efforts to make these resolutions come true. One way they can help us is by verifying that we have really achieved our stated goals and that we are not just fooling ourselves into believing so. We need to keep in mind Richard Feynman‚Äôs famous principle of critical thinking‚Ä¶\r\n\r\n\r\nOne of the tools that can help us with that is segmented regression analysis of interrupted time series data (thanks to Masatake Hirono for pointing me to its existence). It allows us to model changes in various processes and outcomes that follow interventions, while controlling for other types of changes (e.g.¬†trends and seasonality) that may have occurred regardless of the interventions. It is thus very useful for data analysis conducted within studies with a quasi experimental study design that are often in the organizational context the best alternative to the ‚Äúgold standard‚Äù of randomized controlled trials (RCTs) that are not always realizable or politically acceptable.\r\nSearch interest in work-life balance and well-being\r\nFor illustration, let‚Äôs use this tool for testing hypothesis about people‚Äôs increased interest in topics related to work-life balance and well-being due to the COVID-19 pandemic and subsequent changes in the way people work. As a proxy measure of this interest we will use worldwide search interest data over the last 10 years from Google Trends using search terms work-life balance and well-being (see Fig. 1 and 2 below).\r\nFig. 1: Interest in ‚Äúwork-life balance‚Äù topic over the last 10 years measured as a search interest by Google Trends. The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\r\n\r\nFig. 2: Interest in ‚Äúwell-being‚Äù topic over the last 10 years measured as a search interest by Google Trends. The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\r\n\r\nBased solely on the visual inspection of the graphs, it is pretty difficult to tell whether there was some effect of the COVID-19 pandemic or not, especially in the case of work-life balance (for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020). For sure it‚Äôs not a job for ‚Äúinter-ocular trauma test‚Äù when the existence of the effect hits you directly between the eyes. We need to rely here on inferential statistics and its ability to help us with distinguishing signal from noise.\r\nBefore conducting the analysis itself, we need to wrangle the data from Google Trends a little bit using the recipe presented in the Wagner, Zhang, and Ross-Degnan‚Äôs paper. Specifically, we need the following five variables (or six, given that we have two dependent variables):\r\nsearch interest ‚Äì numerical variable representing search interest relative to the highest point on the chart for the given region and time; this variable is truncated within the interval between values of 0 and 100; a value of 100 is the peak popularity for the term; a value of 50 means that the term is half as popular; a score of 0 means that there was not enough data for this term; this variable serves as a dependent (criterion) variable;\r\nelapsed time ‚Äì numerical variable representing the number of months that elapsed from the beginning of the time series; this variable enables estimation of the size and direction of the overall trend in the data;\r\npandemic ‚Äì dichotomic variable indicating the presence/absence of pandemic; as already mentioned above, for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020; this variable enables estimation of the level change in the interest in work-life balance and well-being immediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak ‚Äì numerical variable representing the number of months that elapsed from the beginning of pandemic; this variable enables estimation of the change in the trend in the interest in work-life balance and well-being after the outbreak of pandemic;\r\nmonth ‚Äì categorical variable representing specific month within a year; this variable enables controlling for the effect of seasonality.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for data manipulation\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndfWorkLifeBalance <- readr::read_csv(\"./workLifeBalanceGoogleTrendData.csv\")\r\ndfWellBeing <- readr::read_csv(\"./wellBeingGoogleTrendData.csv\")\r\n\r\ndfAll <- dfWorkLifeBalance %>%\r\n  # joining both datasets\r\n  dplyr::left_join(\r\n    dfWellBeing, by = \"Month\"\r\n    ) %>%\r\n  # changing the format and name of Month variable\r\n  dplyr::mutate(\r\n    Month = stringr::str_glue(\"{Month}-01\"),\r\n    Month = lubridate::ymd(Month)\r\n    ) %>%\r\n  dplyr::rename(\r\n    date = Month\r\n    ) %>%\r\n  # creating new variable month\r\n  dplyr::mutate(\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, \r\n                   levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   ordered = FALSE)\r\n    ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(\r\n    date\r\n    ) %>%\r\n  # creating new variables\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= \"2020-03-01\" ~ 1,\r\n      TRUE ~ 0\r\n      ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic)\r\n  ) %>%\r\n  dplyr::mutate(\r\n    pandemic = as.factor(case_when(\r\n        pandemic == 1 ~ \"After the pandemic outbreak\",\r\n        TRUE ~ \"Before the pandemic outbreak\"\r\n        ))\r\n  ) %>%\r\n  # changing order of variables in df\r\n  dplyr::select(\r\n    date, workLifeBalance, wellBeing, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n    )\r\n\r\n\r\n\r\nHere is a table with the resulting data we will use for testing our hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  dfAll,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nTable 1: Final dataset used for testing hypothesis about impact of the COVID-19 pandemic on people‚Äôs interest in work-life balance and well-being.\r\n\r\nBayesian segmented regression\r\nWe will model our data using common segmented regression models that have following general structure:\r\n\\[Y_{t} = Œ≤_{0} + Œ≤_{1}*time_{t} + Œ≤_{2}*intervention_{t} + Œ≤_{3}*time after intervention_{t} + e_{t}\\]\r\nThe Œ≤0 coefficient estimates the baseline level of the outcome variable at time zero; Œ≤1 coefficient estimates the change in the mean of the outcome variable that occurs with each unit of time before the intervention (i.e.¬†the baseline trend); Œ≤2 coefficient estimates the level change in the mean of the outcome variable immediately after the intervention (i.e.¬†from the end of the preceding segment); and Œ≤3 estimates the change in the trend in the mean of the outcome variable per unit of time after the intervention, compared with the trend before the intervention (thus, the sum of Œ≤1 and Œ≤3 equals to the post-intervention slope). For a better understanding of the model, take a look at the illustrative chart below.\r\n\r\nSince we are dealing with correlated and truncated data, we should also include two additional terms in our model, an autocorrelation term and a truncation term, to handle these specific properties of our data.\r\nNow let‚Äôs fit the models to the data and check what they tell us about the effect of pandemic on people‚Äôs search interest in work-life balance and well-being. We will use brms r package that enables making inferences about statistical models‚Äô parameters within Bayesian inferential framework. Because of that, we also need to specify some additional parameters (e.g.¬†chains, iter or warmup) of the Markov Chain Monte Carlo (MCMC) algorithm that will generate posterior samples of our models‚Äô parameters.\r\nBayesian framework also enables us to specify priors for estimated parameter and through them include our domain knowledge in the analysis. The specified priors are important for both parameter estimation and hypothesis testing as they define our starting information state before we take into account our data. Here we will use rather wide, uninformative, and only mildly regularizing priors (it means that the results of the inference will be very close to the results of standard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\nbrms::get_prior(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors for predictors in both models \r\npriors <- c(set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model for work-life balance\r\nmodelWorkLifeBalance <- brms::brm(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 12345,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n# defining the statistical model for well-being\r\nmodelWellBeing <- brms::brm(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 678910,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n\r\n\r\nSome necessary sanity checks\r\nBefore making any inferences, we should make some sanity checks to be sure that the mechanics of the MCMC algorithm worked well and that we can use generated posterior samples for making inferences about our models‚Äô parameters. There are many ways for doing that, but here we will use only visual check of the MCMC chains. We want plots of these chains look like hairy caterpillar which would indicate convergence of the underlying Markov chain to stationarity and convergence of Monte Carlo estimators to population quantities, respectively. As can be seen in Graph 1 and 2 below, in case of both models we can observe wanted characteristics of the MCMC chains described above. (For additional MCMC diagnostics procedures, see for example Bayesian Notes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains for the modelWorkLifeBalance \r\nbayesplot::mcmc_trace(\r\n  modelWorkLifeBalance,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWorkLifeBalance's parameters\"\r\n    )\r\n\r\n\r\n\r\nGraph 1: Trace plots of Markov chains for individual parameters of the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting the MCMC chains for the modelWellBeing \r\nbayesplot::mcmc_trace(\r\n  modelWellBeing,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWellBeing's parameters\"\r\n    )\r\n\r\n\r\n\r\nGraph 2: Trace plots of Markov chains for individual parameters of the modelWellBeing.\r\n\r\nIt is also important to check how well the models fit the data. We can use for this purpose posterior predictive checks that use specified number of sampled posterior values of models‚Äô parameters and show how well the fitted models predict observed data. We can see in Graphs 3 and 4 that both models fit the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWorkLifeBalance fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWorkLifeBalance, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWorkLifeBalance (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\nGraph 3: Posterior predictive checks comparing simulated/replicated data under the fitted modelWorkLifeBalance with the observed data.\r\n\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWellBeing fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWellBeing, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWellBeing (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\nGraph 4: Posterior predictive checks comparing simulated/replicated data under the fitted modelWellBeing with the observed data.\r\n\r\nResults of the analysis\r\nNow, after having sufficient confidence that - using terminology from the Richard McElreath‚Äôs book Statistical Rethinking - our ‚Äúsmall worlds‚Äù can pretty accurately mimic the data coming from our real,‚Äúbig world‚Äù, we can use our models‚Äô parameters to learn something about our research questions. Our primary interest is in the coefficient value of the pandemicBeforethepandemicoutbreak and elapsedTimeAfterPandemic terms in our models. It expresses how much and in what direction people‚Äôs search interest in work-life balance and well-being changed immediately after the outbreak of pandemic, and how slope of the trend changed after the pandemic, respectively.\r\nIn Graph 5 and 6 we can see posterior distribution of the pandemicBeforethepandemicoutbreak parameter in our two models. In both cases the posterior distribution of the pandemic term is (predominantly or completely) on the left side of the zero value, which supports the claim about existence of the effect of pandemic on people‚Äôs increased search interest in work-life balance and well-being immediately after the outbreak of pandemic. As is apparent from the graphs, for well-being (Graph 6) this evidence is much stronger than for work-life balance (Graph 5), which corresponds to impression we might have when looking at the original Google Trends charts shown in Fig. 1 and 2.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for \r\nlibrary(tidybayes)\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\nGraph 5: Visualization of the posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\nGraph 6: Visualization of the posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing.\r\n\r\nTo generate more summary statistics about posterior distributions (and also some diagnostic information like Rhat or ESS), we can use summary() function.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWorkLifeBalance \r\nsummary(modelWorkLifeBalance)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.21      0.09     0.03     0.40 1.00     7232     5932\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            69.59     11.51    47.51\r\nelapsedTime                          -0.05      0.04    -0.13\r\npandemicBeforethepandemicoutbreak   -13.90     10.15   -34.52\r\nelapsedTimeAfterPandemic              0.40      1.59    -2.77\r\nmonthFeb                              3.50      4.47    -5.29\r\nmonthMar                              4.49      4.99    -5.30\r\nmonthApr                              6.45      5.10    -3.44\r\nmonthMay                              8.81      5.10    -1.18\r\nmonthJun                             -2.91      5.05   -12.82\r\nmonthJul                             -7.40      5.02   -17.18\r\nmonthAug                             -2.51      5.01   -12.52\r\nmonthSep                             -2.20      5.01   -11.87\r\nmonthOct                              5.35      5.01    -4.68\r\nmonthNov                             12.31      4.91     2.56\r\nmonthDec                             -0.64      4.51    -9.37\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            92.59 1.00     4930     4892\r\nelapsedTime                           0.03 1.00     8111     5233\r\npandemicBeforethepandemicoutbreak     5.76 1.00     5569     5338\r\nelapsedTimeAfterPandemic              3.63 1.00     5567     5543\r\nmonthFeb                             12.29 1.00     3805     4101\r\nmonthMar                             14.36 1.00     3316     4648\r\nmonthApr                             16.44 1.00     3225     4565\r\nmonthMay                             18.82 1.00     3148     4520\r\nmonthJun                              7.14 1.00     3008     4287\r\nmonthJul                              2.42 1.00     3148     4652\r\nmonthAug                              7.42 1.00     3206     4379\r\nmonthSep                              7.58 1.00     3234     4922\r\nmonthOct                             14.99 1.00     3173     4504\r\nmonthNov                             22.21 1.00     2945     4709\r\nmonthDec                              8.24 1.00     3662     5175\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma    11.54      0.77    10.17    13.16 1.00     6603     5274\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWellBeing \r\nsummary(modelWellBeing)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.24      0.10     0.05     0.44 1.00     6461     5967\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            56.14      4.63    46.81\r\nelapsedTime                           0.11      0.02     0.08\r\npandemicBeforethepandemicoutbreak   -21.29      4.11   -29.26\r\nelapsedTimeAfterPandemic              0.50      0.64    -0.78\r\nmonthFeb                              8.61      1.80     5.09\r\nmonthMar                             10.63      2.02     6.70\r\nmonthApr                              9.57      2.02     5.66\r\nmonthMay                              3.52      2.07    -0.52\r\nmonthJun                             -4.35      2.05    -8.43\r\nmonthJul                             -8.05      2.06   -12.10\r\nmonthAug                             -5.76      2.05    -9.68\r\nmonthSep                              4.56      2.05     0.57\r\nmonthOct                              8.13      2.01     4.24\r\nmonthNov                              6.74      2.01     2.83\r\nmonthDec                             -4.95      1.80    -8.39\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            65.13 1.00     4316     5151\r\nelapsedTime                           0.14 1.00     8987     5597\r\npandemicBeforethepandemicoutbreak   -13.07 1.00     5825     5557\r\nelapsedTimeAfterPandemic              1.78 1.00     6106     5784\r\nmonthFeb                             12.19 1.00     3387     4626\r\nmonthMar                             14.59 1.00     2868     4601\r\nmonthApr                             13.59 1.00     2724     4233\r\nmonthMay                              7.60 1.00     2938     4027\r\nmonthJun                             -0.34 1.00     2815     4633\r\nmonthJul                             -4.01 1.00     2749     4036\r\nmonthAug                             -1.77 1.00     2863     4364\r\nmonthSep                              8.55 1.00     2969     3748\r\nmonthOct                             12.22 1.00     2907     3914\r\nmonthNov                             10.73 1.00     2962     4142\r\nmonthDec                             -1.35 1.00     3525     4630\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     4.63      0.31     4.07     5.28 1.00     7355     6173\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\nGiven that for work-life balance model the posterior distribution of pandemic term crosses the zero value, it would be useful to know how strong is the evidence in the favor of hypothesis that pandemic term is lower than zero. For that purpose we can extract posterior samples and use them for calculation of the proportion of values that are larger/smaller than zero. The resulting proportions show that the vast majority (around 92%) of posterior distribution lies below zero.\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(modelWorkLifeBalance, seed = 12345)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being lower than 0\r\nsum(samples$b_pandemicBeforethepandemicoutbreak < 0) / nrow(samples)\r\n\r\n[1] 0.9145\r\n\r\n\r\nNow let‚Äôs check the parameter elapsedTimeAfterPandemic. Its posterior distribution in both models ‚Äúsafely‚Äù includes zero value, which indicates that there is not huge support for positive change in trend after the outbreak of pandemic.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_elapsedTimeAfterPandemic\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_elapsedTimeAfterPandemic\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\nGraph 7: Visualization of the posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_elapsedTimeAfterPandemic\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_elapsedTimeAfterPandemic\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\nGraph 8: Visualization of the posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWellBeing.\r\n\r\nIn conclusion, we can say that there is some evidence that the COVID-19 pandemic has prompted people to be more interested in topics related to work-life balance and well-being. I wish us all to be able to transform our increased interest in these topics into truly increased quality of our personal and professional lives. It would be a shame not to use that extra incentive many of us have now for making significant change in our lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-31-segmentedregression/./wellBeingData.jpeg",
    "last_modified": "2023-07-03T22:33:50+02:00",
    "input_file": "segmentedregression.knit.md"
  },
  {
    "path": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/",
    "title": "HR analytika a odchodovost zamƒõstnanc≈Ø",
    "description": "Kter√© faktory p≈ôisp√≠vaj√≠ k odchodovosti zamƒõstnanc≈Ø a u kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø je zv√Ω≈°en√© riziko, ≈æe firmu bƒõhem nƒõkolika p≈ô√≠≈°t√≠ch mƒõs√≠c≈Ø opust√≠? Na tyto ot√°zky se ƒç√≠m d√°l t√≠m v√≠ce firem sna≈æ√≠ odpovƒõdƒõt pomoc√≠ anal√Ωzy dat o sv√Ωch vlastn√≠ch zamƒõstnanc√≠ch. V tomto ƒçl√°nku se prost≈ôednictv√≠m analytick√©ho n√°stroje R a vizualizaƒçn√≠ho n√°stroje Shiny pod√≠v√°me, jak m≈Ø≈æe b√Ωt tento druh HR analytick√©ho projektu pro firmy u≈æiteƒçn√Ω.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-14",
    "categories": [
      "employee turnover",
      "evidence-based hr",
      "shiny app"
    ],
    "contents": "\r\n\r\nContents\r\nCo je to HR analytika?\r\nOdchodovost zamƒõstnanc≈Ø a jej√≠ prediktory\r\nNa d≈Økazech zalo≈æen√° pro-retenƒçn√≠ opat≈ôen√≠\r\n\r\nCo je to HR analytika?\r\nHR analytika ve sv√© podstatƒõ vych√°z√≠ ze zn√°m√© z√°sady managementu, ≈æe co nelze mƒõ≈ôit, nelze ani (efektivnƒõ) ≈ô√≠dit a zlep≈°ovat, a aplikuje tuto z√°sadu na lidsk√© zdroje. V nƒõkolika posledn√≠ch letech potom k tomu nav√≠c p≈ôid√°v√° nadstavbu v podobƒõ pokroƒçilej≈°√≠ch analytick√Ωch postup≈Ø, kter√© maj√≠ vƒõt≈°√≠ potenci√°l p≈ôij√≠t s hlub≈°√≠mi vhledy a s doporuƒçen√≠mi s vƒõt≈°√≠m efektem. Ale a≈• u≈æ vyu≈æ√≠v√°te pouze z√°kladn√≠ reporting nebo nƒõjakou pokroƒçilej≈°√≠ analytiku, c√≠l je v≈ædy stejn√Ω ‚Äì sna≈æit se s pomoc√≠ dat a jejich anal√Ωzy ≈æ√°douc√≠m zp≈Øsobem ovlivnit jednotliv√© HR procesy, kter√© organizac√≠m pom√°haj√≠ dosahovat jejich strategick√Ωch c√≠l≈Ø. N√°zornƒõ to ilustruje n√≠≈æe uveden√© sch√©ma zachycuj√≠c√≠ mechanismus propojuj√≠c√≠ HR procesy s (nejen) finanƒçn√≠mi v√Ωsledky organizace (Paauwe & Richardson, 1997).\r\n\r\nHR analytika pom√°h√° optimalizovat nastaven√≠ tohoto mechanismu t√≠m, ≈æe umo≈æ≈àuje nal√©zat odpovƒõdi na nƒõkter√© kl√≠ƒçov√© ot√°zky, jako nap≈ô.:\r\nKter√Ωmi kan√°ly se k n√°m dost√°vaj√≠ ti nejlep≈°√≠ kandid√°ti?\r\nJak√© charakteristiky od sebe odli≈°uj√≠ √∫spƒõ≈°n√© a ne√∫spƒõ≈°n√© kandid√°ty?\r\nKter√© faktory p≈ôisp√≠vaj√≠ k √∫spƒõ≈°n√©mu onboardingu?\r\nKter√° ‚Äûk√°p√©√≠ƒçka‚Äú maj√≠ nejsilnƒõj≈°√≠ vazbu na finanƒçn√≠ v√Ωsledky firmy?\r\nJak√© tr√©ninky vedou s nejvy≈°≈°√≠ pravdƒõpodobnost√≠ ke zlep≈°en√≠ pracovn√≠ho v√Ωkonu?\r\nKter√© intervence maj√≠ nejvƒõt≈°√≠ dopad na zamƒõstnanci poci≈•ovan√Ω well-being nebo work-life balance?\r\nCo u zamƒõstnanc≈Ø zvy≈°uje, nebo naopak sni≈æuje m√≠ru jejich anga≈æovanosti?\r\nKde se v organizaci nach√°z√≠ izolovan√° sila a √∫zk√° hrdla znemo≈æ≈àuj√≠c√≠ efektivn√≠ komunikaci a spolupr√°ci mezi jednotliv√Ωmi zamƒõstnanci, t√Ωmy nebo i cel√Ωmi oddƒõlen√≠mi?\r\nKdo p≈ôedstavuje skryt√Ω talent, kter√Ω je pot≈ôeba podchytit a d√°le rozv√≠jet?\r\nKde lze oƒçek√°vat odpor v souvislosti s pl√°novan√Ωmi zmƒõnami ve firmƒõ a kdo naopak m≈Ø≈æe b√Ωt jejich ambasadorem a katalyz√°torem?\r\nKter√© faktory p≈ôisp√≠vaj√≠ k odchodovosti zamƒõstnanc≈Ø a u kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø je zv√Ω≈°en√© riziko, ≈æe firmu bƒõhem nƒõkolika p≈ô√≠≈°t√≠ch mƒõs√≠c≈Ø opust√≠?\r\nOdchodovost zamƒõstnanc≈Ø a jej√≠ prediktory\r\nPr√°vƒõ poslednƒõ jmenovan√Ω zp≈Øsob vyu≈æit√≠ HR analytiky ƒçasto p≈ôedstavuje jeden z prvn√≠ch druh≈Ø HR analytick√Ωch projekt≈Ø, kter√Ωmi se ve firm√°ch s HR analytikou zaƒç√≠n√°, a to z dob≈ôe pochopiteln√©ho d≈Øvodu. S ne≈æ√°douc√≠mi odchody zamƒõstnanc≈Ø jsou toti≈æ spojen√© vysok√© p≈ô√≠m√© i nep≈ô√≠m√© n√°klady, tak≈æe i pomƒõrnƒõ m√≠rn√© sn√≠≈æen√≠ odchodovosti zamƒõstnanc≈Ø m≈Ø≈æe p≈ôedstavovat znaƒçnou √∫sporu, kterou ocen√≠ management ka≈æd√© firmy. Nal√©havost tohoto probl√©mu nav√≠c je≈°tƒõ zvy≈°uje souƒçasn√° f√°ze ekonomick√©ho cyklu s rekordnƒõ n√≠zkou m√≠rou nezamƒõstnanosti, kter√° v kombinaci s r≈Øzn√Ωmi on-line platformami na zprost≈ôedkov√°n√≠ pr√°ce motivuje mnoho lid√≠ k hled√°n√≠ nov√©ho m√≠sta, kde, jak doufaj√≠, bude pr√°ce zaj√≠mavƒõj≈°√≠, smysluplnƒõj≈°√≠ a l√©pe placen√° a kde kolegov√© budou sympatiƒçtƒõj≈°√≠ a ≈°√©fov√© inspirativnƒõj≈°√≠. Viz tak√© graf n√≠≈æe, kter√Ω na datech z USA n√°zornƒõ dokl√°d√° tƒõsnost vztahu mezi m√≠rou nezamƒõstnanosti a m√≠rou dobrovoln√© odchodovosti zamƒõstnanc≈Ø (r = -0,95, p < 0,001 ).\r\n\r\n\r\n\r\nVzhledem k palƒçivosti tohoto probl√©mu, kter√Ω tr√°p√≠ nejednu firmu, nen√≠ ≈æ√°dn√Ωm velk√Ωm p≈ôekvapen√≠m, ≈æe se t√©matu odchodovosti zamƒõstnanc≈Ø vƒõnovalo a st√°le vƒõnuje velk√© mno≈æstv√≠ r≈Øzn√Ωch studi√≠. Takto nap≈ô. na konci roku 2017 vy≈°la rozs√°hl√° meta-anal√Ωza od autor≈Ø Rubensteina, Eberlyov√© a Leeho, kte≈ô√≠ syntetizovali v√Ωsledky v√≠ce ne≈æ 300 d√≠lƒç√≠ch v√Ωzkum≈Ø t√Ωkaj√≠c√≠ch se prediktor≈Ø odchodovosti. M≈Ø≈æeme se tak opr√°vnƒõnƒõ pt√°t, co nov√©ho n√°m m≈Ø≈æe p≈ôin√©st HR analytika zamƒõ≈ôen√° na odchodovost zamƒõstnanc≈Ø realizovan√° pouze v jedin√© organizaci. Nebylo v≈°e podstatn√© k tomuto t√©matu ji≈æ objeveno? (K t√©to ot√°zce viz nap≈ô. tento inspirativn√≠ a trochu provokativn√≠ ƒçl√°nek od Thomase Rasmussena.) Je pravda, ≈æe nen√≠ p≈ô√≠li≈° pravdƒõpodobn√©, ≈æe p≈ôi anal√Ωze va≈°ich vlastn√≠ch dat naraz√≠te na nƒõjak√Ω naprosto nov√Ω faktor souvisej√≠c√≠ s odchodovost√≠. Na druhou stranu je rovnƒõ≈æ pravda, ≈æe ka≈æd√° organizace je v nƒõƒçem jedineƒçn√°, tak≈æe nƒõkter√© z retenƒçn√≠ch faktor≈Ø pro danou organizaci budou pravdƒõpodobnƒõ v√≠ce a jin√© m√©nƒõ d≈Øle≈æit√©. Tato informace o relativn√≠ d≈Øle≈æitosti jednotliv√Ωch retenƒçn√≠ch faktor≈Ø je potom kl√≠ƒçov√° p≈ôi nastavov√°n√≠ retenƒçn√≠ho pl√°nu a HR analytika m≈Ø≈æe b√Ωt p≈ôi tomto velice n√°pomocn√°.\r\nNa d≈Økazech zalo≈æen√° pro-retenƒçn√≠ opat≈ôen√≠\r\nS pomoc√≠ tohoto dashboardu - vytvo≈ôen√©ho prost≈ôednictv√≠m analytick√©ho n√°stroje R a vizualizaƒçn√≠ho n√°stroje Shiny a za vyu≈æit√≠ uk√°zkov√Ωch dat od spoleƒçnosti IBM - si m≈Ø≈æete sami vyzkou≈°et, jak u≈æiteƒçn√© by pro V√°s mohly b√Ωt v√Ωstupy z takov√©ho HR analytick√©ho projektu zamƒõ≈ôen√©ho na odchodovost zamƒõstnanc≈Ø. Dashboard obsahuje informace, kter√© pom√°haj√≠ (nejen) managementu zodpovƒõdƒõt ≈ôadu kl√≠ƒçov√Ωch ot√°zek, kter√© stoj√≠ na poƒç√°tku ka≈æd√©ho √∫ƒçinn√©ho pl√°nu na retenci zamƒõstnanc≈Ø, jako nap≈ô.:\r\nKolik zamƒõstnanc≈Ø n√°s roƒçnƒõ opou≈°t√≠?\r\nKter√© skupiny zamƒõstnanc≈Ø odch√°zej√≠ nejƒçastƒõji?\r\nJak√Ω je extern√≠ benchmark? Jsme na tom podobƒõ jako konkurence v oboru?\r\nP≈ôedstavuje pro n√°s st√°vaj√≠c√≠ √∫rove≈à odchodovosti z√°va≈æn√Ω probl√©m, a vyplat√≠ se n√°m ho tedy ≈ôe≈°it?\r\nZ jak√Ωch d≈Øvod≈Ø lid√© obecnƒõ nejƒçastƒõji odch√°zej√≠ ze zamƒõstn√°n√≠?\r\nJak√© faktory p≈ôisp√≠vaj√≠ k odchodu specificky na≈°ich zamƒõstnanc≈Ø?\r\nJak√° pro-retenƒçn√≠ opat≈ôen√≠ jsou obecnƒõ k dispozici?\r\nJak√° pro-retenƒçn√≠ opat≈ôen√≠ bychom mƒõli zvolit vzhledem k pravdƒõpodobn√Ωm d≈Øvod≈Øm odchod≈Ø na≈°ich zamƒõstnanc≈Ø?\r\nNa jak√© skupiny zamƒõstnanc≈Ø se p≈ôedev≈°√≠m zamƒõ≈ôit z hlediska prevence jejich odchodovosti?\r\nU kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø existuje zv√Ω≈°en√© riziko, ≈æe odejdou, a na jak√© konkr√©tn√≠ retenƒçn√≠ faktory se u nich zamƒõ≈ôit v r√°mci pravideln√©ho stay interview?\r\nJak je z v√Ω≈°e uveden√©ho v√Ωƒçtu ot√°zek patrn√©, dashboard obsahuje informace, kter√© p≈ôi sv√©m rozhodov√°n√≠ mohou vyu≈æ√≠t nejen HR mana≈æe≈ôi, ale tak√© HR business partne≈ôi nebo p≈ô√≠mo team-leade≈ôi a liniov√≠ mana≈æe≈ôi jednotliv√Ωch t√Ωm≈Ø ƒçi oddƒõlen√≠. Kromƒõ toho dashboard obsahuje tak√© ≈ôadu technick√Ωch detail≈Ø o pou≈æit√©m predikƒçn√≠m modelu a samotn√° data, kter√© stoj√≠ v pozad√≠ v≈°ech prezentovan√Ωch vizualizac√≠ a anal√Ωz. S jejich pomoc√≠ tak HR/Business analytik m≈Ø≈æe nap≈ô. hledat optim√°ln√≠ zp≈Øsob, jak nastavit sk√≥rovac√≠ algoritmus, aby se maximalizoval pozitivn√≠ efekt pro-retenƒçn√≠ch opat≈ôen√≠, nebo m≈Ø≈æe v dostupn√Ωch datech s√°m hledat nƒõjak√© dal≈°√≠ u≈æiteƒçn√© informace. V√≠ce viz ji≈æ samotn√Ω dashboard, z nƒõho≈æ m≈Ø≈æete n√≠≈æe vidƒõt nƒõkolik screenshot≈Ø.\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje r≈Øzn√© ≈ôezy odchodovost√≠ zamƒõstnanc≈Ø, a d√°v√° tak dobr√Ω p≈ôehled o tom, kter√© skupiny zamƒõstnanc≈Ø jsou odchodovost√≠ nejv√≠ce ohro≈æen√©.\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje informace o pravdƒõpodobnosti odchodu jednotliv√Ωch zamƒõstnanc≈Ø spoleƒçnƒõ s dal≈°√≠mi informacemi, kter√© mohou poslou≈æit jako podklad pro individu√°ln√≠ intervence s c√≠lem p≈ôedej√≠t ne≈æ√°douc√≠m odchod≈Øm zamƒõstnanc≈Ø.\r\n\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje informace o v√Ωkonu/kvalitƒõ statistick√©ho modelu pou≈æit√©ho k identifikaci v√Ωznamn√Ωch prediktor≈Ø odchodovosti zamƒõstnanc≈Ø a k odhadu pravdƒõpodobnosti odchodu jednotliv√Ωch zamƒõstnanc≈Ø.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/./Types-of-Employee-Attrition.png",
    "last_modified": "2023-04-11T20:14:01+02:00",
    "input_file": {},
    "preview_width": 860,
    "preview_height": 396
  },
  {
    "path": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/",
    "title": "Moneyball v HR",
    "description": "P≈ôes popularitu t√©matu HR analytiky mezi HR profesion√°ly je st√°le relativnƒõ m√°lo spoleƒçnost√≠, kter√© HR analytiku re√°lnƒõ a systematicky vyu≈æ√≠vaj√≠. Jednou z mo≈æn√Ωch p≈ô√≠ƒçin je to, ≈æe tradiƒçn√≠ HR mnohdy postr√°d√° analytick√Ω mindset a nƒõkter√© z kompetenc√≠, kter√© jsou kl√≠ƒçov√© pro √∫spƒõ≈°nou realizaci HR analytick√Ωch projekt≈Ø. V takov√© situaci m≈Ø≈æe b√Ωt u≈æiteƒçn√© pod√≠vat se ve vƒõt≈°√≠m detailu na celkovou logiku i na konkr√©tn√≠ analytick√© kroky nƒõjak√©ho √∫spƒõ≈°n√©ho p≈ô√≠kladu vyu≈æit√≠ HR analytiky k optimalizaci nƒõkter√©ho z HR proces≈Ø s pozitivn√≠m dopadem na obchodn√≠ v√Ωsledky spoleƒçnosti. V tomto ƒçl√°nku se t√≠mto zp≈Øsobem pod√≠v√°me na zn√°m√Ω p≈ô√≠bƒõh oaklandsk√©ho baseballov√©ho t√Ωmu \"√Åƒçek\", jeho≈æ management pomƒõrnƒõ radik√°lnƒõ - a podle v≈°eho i √∫spƒõ≈°nƒõ - p≈ôehodnotil sv≈Øj dosavadn√≠ p≈ô√≠stup k v√Ωbƒõru nov√Ωch hr√°ƒç≈Ø na z√°kladƒõ v√Ωstup≈Ø statistick√© anal√Ωzy sabermetrick√Ωch dat o hern√≠m chov√°n√≠ hr√°ƒç≈Ø. Vyu≈æijeme p≈ôi tom volnƒõ dostupn√Ω statistick√Ω software R a ve≈ôejnƒõ dostupnou datab√°zi historick√Ωch √∫daj≈Ø o v√Ωsledc√≠ch v americk√© baseballov√© lize.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "employee selection",
      "correlation analysis",
      "multivariate regression analysis",
      "structural equation modeling",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\n1.\r\nkrok: Zaƒç√≠t od konce aneb strategick√Ω r√°mec HR analytiky\r\n2. krok: Definice\r\nprobl√©mu a kvantifikace c√≠le\r\n3. krok: Kladen√≠ ot√°zek a\r\nmƒõ≈ôen√≠\r\n4. krok: Kladen√≠\r\ndal≈°√≠ch ot√°zek a dal≈°√≠ mƒõ≈ôen√≠\r\n5.\r\nkrok: Propojen√≠ d√≠lƒç√≠ch vhled≈Ø aneb organizace jako stroj\r\n6. krok:\r\nIntervence\r\nOmezen√≠ HR analytiky\r\nZ√°vƒõr\r\n\r\nHR analytika u≈æ dnes nen√≠ ve svƒõtƒõ HR ≈æ√°dnou horkou novinkou. T√©mƒõ≈ô\r\nv≈°ichni z oboru u≈æ o HR analytice nƒõco sly≈°eli, nƒõco o n√≠ vƒõd√≠ a\r\np≈ô√≠padnƒõ se j√≠ u≈æ tak√© pokou≈°√≠ ve sv√Ωch organizac√≠ch v nƒõjak√© podobƒõ\r\nzav√°dƒõt. Z√°rove≈à vƒõt≈°inou uzn√°vaj√≠ jej√≠ d≈Øle≈æitost p≈ôi transformaci HR z\r\npodp≈Ørn√© a administrativn√≠ funkce na funkci, kter√° dok√°≈æe organizac√≠m\r\nbezprost≈ôednƒõ pom√°hat dosahovat jejich strategick√Ωch c√≠l≈Ø. Navzdory\r\ntomuto v≈°eobecn√©mu povƒõdom√≠ o HR analytice a navzdory ≈ôadƒõ √∫spƒõ≈°nƒõ\r\nrealizovan√Ωch HR analytick√Ωch projekt≈Ø (viz nap≈ô. s√©rie ƒçl√°nk≈Ø od Davida Greena - ƒçl√°nek\r\n1, ƒçl√°nek\r\n2, ƒçl√°nek\r\n3, ƒçl√°nek\r\n4) p≈ôekvapivƒõ m√°lo organizac√≠ HR analytiku re√°lnƒõ a systematicky\r\nvyu≈æ√≠v√°. Tento stav reflektuj√≠ i v√Ωsledky v√Ωzkumu 2018\r\nHuman Capital Trends od spoleƒçnosti Deloitte, ze kter√Ωch vypl√Ωv√°, ≈æe\r\norganizace si vƒõt≈°inou uvƒõdomuj√≠ strategickou d≈Øle≈æitost v√Ωzvy, kterou\r\np≈ôedstavuje datifikace HR, z√°rove≈à se ale nec√≠t√≠ b√Ωt na ƒçelen√≠ t√©to\r\nv√Ωzvƒõ p≈ô√≠li≈° dob≈ôe p≈ôipraveny. U≈æ nƒõjakou dobu plat√≠, ≈æe kdy≈æ u≈æ se v\r\norganizaci s HR daty nƒõjak pracuje, tak je to vƒõt≈°inou pouze na √∫rovni\r\nnƒõjak√©ho z√°kladn√≠ho reportingu vybran√Ωch HR metrik a KPIs typu n√°klady\r\nna n√°bor, d√©lka obdob√≠ neobsazenosti voln√© pracovn√≠ pozice, m√≠ra\r\nne/dobrovoln√© odchodovosti zamƒõstnanc≈Ø, poƒçet zamƒõstnanc≈Ø na jednoho HR\r\nbusiness partnera apod. Slabinou tohoto p≈ô√≠stupu je, ≈æe takto sledovan√©\r\nmetriky jsou ƒçasto relevantn√≠ pouze pro monitorov√°n√≠ a ≈ô√≠zen√≠\r\nefektivnosti HR coby n√°kladov√©ho st≈ôediska, ale ji≈æ m√©nƒõ pro dosahov√°n√≠\r\nstrategick√Ωch c√≠l≈Ø organizace. Sp√≠≈°e v√Ωjimeƒçnƒõ se potom v tomto kontextu\r\nvyu≈æ√≠vaj√≠ nƒõjak√© pokroƒçilej≈°√≠ analytiky, kter√© obecnƒõ maj√≠ vƒõt≈°√≠\r\npotenci√°l p≈ôich√°zet s doporuƒçen√≠mi s p≈ô√≠m√Ωm dopadem na schopnost\r\norganizac√≠ dosahovat sv√Ωch strategick√Ωch c√≠l≈Ø.\r\nV√Ωsledky v√Ωzkumu\r\nproveden√©ho spoleƒçnostmi MIT Sloan Management Review a SAS\r\nnaznaƒçuj√≠, ≈æe tento nevyu≈æit√Ω potenci√°l HR analytiky m√° dvƒõ hlavn√≠\r\np≈ô√≠ƒçiny. Prvn√≠ z nich je to, ≈æe tradiƒçn√≠ HR mnohdy postr√°d√° analytick√Ω\r\nmindset a nƒõkter√© z kompetenc√≠, kter√© jsou kl√≠ƒçov√© pro √∫spƒõ≈°nou\r\nrealizaci HR analytick√Ωch projekt≈Ø (p≈ôehled tƒõchto kompetenc√≠ a d≈Øsledk≈Ø\r\njejich absence ƒçi nedostateƒçn√© √∫rovnƒõ viz nap≈ô. tento\r\nƒçl√°nek od Mortena Kamp\r\nAndersena). Ve stejn√©m duchu Josh Bersin ve sv√© zpr√°vƒõ\r\nHR\r\nTechnology Disruptions for 2018 konstatuje, ≈æe zvl√°dnut√≠ z√°kladn√≠ch\r\nanalytick√Ωch dovednost√≠ pat≈ô√≠ mezi nejd≈Øle≈æitƒõj≈°√≠ prediktory efektivn√≠\r\nimplementace HR analytiky v organizac√≠ch: ‚ÄúEquip all HR staff with\r\nbasic data literacy skills. All HR practitioners should know basic\r\nstatistical concepts, where to find data, how to slice and dice it, how\r\nto read a dashboard, and how to bring data and analytics to bear on\r\nbusiness issues. Our research reveals that such basic skills are among\r\nthe most important predictors of high-performing people\r\nanalytics.‚Äù\r\nDruhou hlavn√≠ p≈ô√≠ƒçinou je potom to, ≈æe HR analytick√© projekty\r\nneb√Ωvaj√≠ ukotveny v r√°mci nƒõjak√© ≈°ir≈°√≠ strategie, jak data systematicky\r\nvyu≈æ√≠vat p≈ôi ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø, nav√≠c zp≈Øsobem, kter√Ω by byl\r\nsladƒõn√Ω se strategick√Ωmi c√≠li spoleƒçnosti. Zde plat√≠ prax√≠ osvƒõdƒçen√°\r\npravda projektov√©ho managementu, ≈æe p≈ôi implementaci projekt≈Ø je pot≈ôeba\r\nv≈ædy zaƒç√≠nat od konce. V kontextu HR analytick√Ωch projekt≈Ø to tedy\r\nznamen√° zaƒç√≠nat nikoli od dat, ale od toho, k ƒçemu maj√≠ b√Ωt HR\r\nanalytick√© v√Ωstupy pou≈æity. A oƒçek√°v√°n√≠ managementu je, ≈æe HR analytika\r\nbude v posledku hlavnƒõ pom√°hat zlep≈°ovat obchodn√≠ v√Ωsledky spoleƒçnosti.\r\nN√°zornƒõ to ilustruje n√≠≈æe uveden√© sch√©ma (p≈ôevzat√© z ƒçl√°nku\r\nMaxe Blumberga),\r\nkter√© zachycuje p≈ôedpokl√°dan√Ω kauz√°ln√≠ ≈ôetƒõzec spojuj√≠c√≠ HR procesy s\r\nobchodn√≠mi v√Ωsledky. √ökolem HR analytiky je potom s pomoc√≠ dat a\r\nanalytick√Ωch n√°stroj≈Ø tyto dvƒõ oblasti propojit a zjistit, jak\r\noptimalizac√≠ prvn√≠ho zajistit zlep≈°en√≠ toho druh√©ho.\r\n\r\n≈òadƒõ organizac√≠ by v tomto ohledu mohl b√Ωt inspirac√≠ zn√°m√Ω p≈ô√≠bƒõh oaklandsk√©ho\r\nbaseballov√©ho t√Ωmu ‚Äû√Åƒçek‚Äú, kter√Ω se stal p≈ôedlohou pro knihu Moneyball a z\r\nn√≠ vych√°zej√≠c√≠ stejnojmenn√Ω film.\r\nPr√°vƒõ tento p≈ô√≠bƒõh jako jeden z prvn√≠ch uk√°zal a mezi ≈°irokou ve≈ôejnost√≠\r\nzpopularizoval mo≈ænosti vyu≈æit√≠ statistick√© anal√Ωzy ve svƒõtƒõ sportu a\r\npota≈æmo tak√© v r√°mci ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø. D√≠ky radik√°ln√≠ zmƒõnƒõ\r\ndosavadn√≠ho p≈ô√≠stupu k v√Ωbƒõru nov√Ωch hr√°ƒç≈Ø, kter√Ω se zaƒçal v√≠ce op√≠rat o\r\nv√Ωstupy statistick√© anal√Ωzy sabermetrick√Ωch\r\ndat o hern√≠m chov√°n√≠ hr√°ƒç≈Ø, dok√°zal management oaklandsk√©ho\r\nbaseballov√©ho t√Ωmu ‚Äû√Åƒçek‚Äú p≈ôij√≠mat rozhodnut√≠, kter√° z jednoho z\r\nnejchud≈°√≠ch t√Ωm≈Ø americk√© baseballov√© ligy uƒçinila jeden z\r\nnej√∫spƒõ≈°nƒõj≈°√≠ch t√Ωm≈Ø soutƒõ≈æe (mƒõ≈ôeno poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti\r\nsoutƒõ≈æe a poƒçtem postup≈Ø do play-off). Abychom mohli tento p≈ô√≠bƒõh plnƒõ\r\nvytƒõ≈æit coby inspiraci, jak analyzovat sv√° vlastn√≠ zamƒõstnaneck√° data,\r\nbude u≈æiteƒçn√©, kdy≈æ se na jednotliv√© analytick√© kroky, kter√© st√°ly v\r\npozad√≠ √∫spƒõchu oklandsk√Ωch ‚Äú√Åƒçek‚Äù, pod√≠v√°me trochu podrobnƒõji. A uƒçin√≠me\r\ntak za vyu≈æit√≠ volnƒõ dostupn√©ho statistick√©ho softwaru R a ve≈ôejnƒõ\r\ndostupn√© datab√°ze\r\nhistorick√Ωch √∫daj≈Ø o v√Ωsledc√≠ch v americk√© baseballov√© lize.\r\n1.\r\nkrok: Zaƒç√≠t od konce aneb strategick√Ω r√°mec HR analytiky\r\nJak bylo uvedeno v√Ω≈°e, ƒçasto podce≈àovan√Ωm krokem p≈ôi zav√°dƒõn√≠ HR\r\nanalytiky do firem a organizac√≠ je zasazen√≠ HR analytiky do nƒõjak√©ho\r\n≈°ir≈°√≠ho strategick√©ho r√°mce, ze kter√©ho by jasnƒõ vypl√Ωvalo, ƒçemu m√°\r\nvlastnƒõ HR analytika slou≈æit. HR analytika je pouze n√°stroj, konkr√©tnƒõ\r\nn√°stroj na zodpov√≠d√°n√≠ ot√°zek, resp. na testov√°n√≠ r≈Øzn√Ωch hypot√©z. To,\r\nzda bude tento n√°stroj u≈æiteƒçn√Ω, z√°vis√≠ na tom, zda si dok√°≈æeme kl√°st ty\r\nspr√°vn√© ot√°zky. To je p≈ôitom z velk√© ƒç√°sti d√°no t√≠m, zda si jsme vƒõdomi,\r\njak√© jsou strategick√© c√≠le na≈°√≠ organizace. Jen ve svƒõtle tƒõchto c√≠l≈Ø\r\nd√°v√° smysl kl√°st si nƒõjak√© ot√°zky, sb√≠rat a analyzovat nƒõjak√° data za\r\n√∫ƒçelem nalezen√≠ odpovƒõd√≠ na polo≈æen√© ot√°zky a posl√©ze ƒçinit nƒõjak√°\r\nkonkr√©tn√≠ rozhodnut√≠ na z√°kladƒõ nalezen√Ωch odpovƒõd√≠. V p≈ô√≠padƒõ\r\noaklandsk√Ωch ‚Äû√Åƒçek‚Äú byl c√≠l jasn√Ω ‚Äì kvalifikovat se do play-off.\r\n2. krok: Definice\r\nprobl√©mu a kvantifikace c√≠le\r\nPaul\r\nDePodesta, kter√©ho gener√°ln√≠ mana≈æer oaklandsk√Ωch ‚Äû√Åƒçek‚Äú Billy Beane p≈ôijal\r\ndo t√Ωmu jako statistick√©ho analytika, redukoval tento c√≠l na celkem\r\njednoduch√Ω matematick√Ω probl√©m: Kolik z√°pas≈Ø mus√≠ t√Ωm vyhr√°t, aby se\r\nkvalifikoval do play-off? K zodpovƒõzen√≠ t√©to ot√°zky DePodesta pot≈ôeboval\r\nhistorick√° data o poƒçtu v√≠tƒõzstv√≠ jednotliv√Ωch t√Ωm≈Ø v minul√Ωch sez√≥n√°ch\r\na o tom, zda se jim poda≈ôilo postoupit do play-off, ƒçi nikoli.\r\n\r\n\r\nShow code\r\n\r\n# Naƒçtƒõme si knihovnu, kter√° n√°m umo≈æn√≠ si naƒç√≠st a p≈ôedp≈ôipravit data k anal√Ωze a tak√© je i vizualizovat. \r\nlibrary(tidyverse)\r\n\r\n# Naƒçteme si na≈°e data.\r\nbaseball <- read_csv(\"baseball.csv\")\r\n\r\n# Pro z√≠sk√°n√≠ lep≈°√≠ p≈ôedstavy o nich se pod√≠vejme na jejich prvn√≠ch deset ≈ô√°dk≈Ø.\r\nhead(baseball, 10)\r\n\r\n\r\n# A tibble: 10 x 15\r\n   Team  League  Year    RS    RA     W   OBP   SLG    BA Playoffs\r\n   <chr> <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>\r\n 1 ARI   NL      2012   734   688    81 0.328 0.418 0.259        0\r\n 2 ATL   NL      2012   700   600    94 0.32  0.389 0.247        1\r\n 3 BAL   AL      2012   712   705    93 0.311 0.417 0.247        1\r\n 4 BOS   AL      2012   734   806    69 0.315 0.415 0.26         0\r\n 5 CHC   NL      2012   613   759    61 0.302 0.378 0.24         0\r\n 6 CHW   AL      2012   748   676    85 0.318 0.422 0.255        0\r\n 7 CIN   NL      2012   669   588    97 0.315 0.411 0.251        1\r\n 8 CLE   AL      2012   667   845    68 0.324 0.381 0.251        0\r\n 9 COL   NL      2012   758   890    64 0.33  0.436 0.274        0\r\n10 DET   AL      2012   726   670    88 0.335 0.422 0.268        1\r\n# ... with 5 more variables: RankSeason <dbl>, RankPlayoffs <dbl>,\r\n#   G <dbl>, OOBP <dbl>, OSLG <dbl>\r\n\r\nShow code\r\n\r\n# Pro n√°sleduj√≠c√≠ anal√Ωzy si potom vytvo≈ôme podmno≈æinu dat, kter√° mƒõli k dispozici v Oaklandu v roce 2002, kdy se dƒõj Moneyballu p≈ôev√°≈ænƒõ odehr√°v√°.\r\nmoneyball <- baseball %>%\r\n  filter(Year < 2002)\r\n\r\n\r\n\r\nPod√≠v√°me-li se na data mezi lety 1996‚Äì2001, tj. na data z relativnƒõ\r\nned√°vn√© minulosti (vzta≈æeno k roku 2002, kdy se dƒõj Moneyballu p≈ôev√°≈ænƒõ\r\nodehr√°v√°), z grafick√©ho vyj√°d≈ôen√≠ vztahu mezi poƒçtem v√≠tƒõzstv√≠ v\r\nz√°kladn√≠ ƒç√°st√≠ soutƒõ≈æe a postupem do play-off je dob≈ôe patrn√©, ≈æe ƒç√≠m\r\nv√≠ce z√°pas≈Ø t√Ωm vyhraje v z√°kladn√≠ soutƒõ≈æi, t√≠m vƒõt≈°√≠ je ≈°ance, ≈æe se\r\ntak√© dostane do play-off.\r\n\r\n\r\nShow code\r\n\r\n# Vytvo≈ôme si graf zachycuj√≠c√≠ vztah mezi poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a postupem do play-off\r\nmoneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select (W, Playoffs) %>%\r\n  mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs)))+\r\n  geom_point(size = 2)+\r\n  scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5))+\r\n  scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"T√Ωm nepostoupil do play-off\",\"T√Ωm postoupil do play-off\"))+\r\n  ggtitle(\"Postupy t√Ωm≈Ø do play-off mezi lety 1996-2001\")+\r\n  ylab(\"\")+\r\n  xlab(\"Poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe\")+\r\n  theme(legend.position = \"bottom\",\r\n        axis.ticks.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.text.x = element_text(size=11),\r\n        axis.title.x = element_text(size=11),\r\n        legend.text = element_text(size=11),\r\n        legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nS daty, kter√° m√°me k dispozici, m√°me tu v√Ωhodu, ≈æe m≈Ø≈æeme vztah mezi\r\npoƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a ≈°anc√≠ na postup do play-off\r\np≈ôesnƒõ kvantifikovat. Provedeme-li podrobnƒõj≈°√≠ anal√Ωzu na≈°ich dat, uk√°≈æe\r\nse, ≈æe velkou (p≈ôibli≈ænƒõ 95%) ≈°anci na postup do play-off m√° t√Ωm tehdy,\r\nkdy≈æ v z√°kladn√≠ ƒç√°sti vyhraje minim√°lnƒõ 95 z√°pas≈Ø. Tƒõchto 95 v√≠tƒõzstv√≠\r\np≈ôedstavuje dob≈ôe definovan√Ω a kvantifikovan√Ω c√≠l, kter√©ho by se\r\noaklandsk√° ‚Äû√Åƒçka‚Äú mƒõla sna≈æit dos√°hnout.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si opƒõt data mezi lety 1996-2001. \r\nmoneyball2 <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995)\r\n\r\n# Vytvo≈ôme si seznam nƒõkolika r≈Øzn√Ωch hodnot poƒçtu v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe.\r\npocet_vitezstvi <- seq(60,115,5)\r\nucast_v_playoff <- vector(mode=\"numeric\", length=length(pocet_vitezstvi))\r\nplayoff_data <- data.frame(pocet_vitezstvi = pocet_vitezstvi, ucast_v_playoff = ucast_v_playoff)\r\n\r\n# Vypoƒçtƒõme si, jak√° je pravdƒõpodobnost postupu do play-off p≈ôi r≈Øzn√©m poƒçtu v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe.\r\nfor(i in 1:nrow(playoff_data)){\r\n playoff_data$ucast_v_playoff[i] <- length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i] & moneyball2$Playoffs == 1])/length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i]]) \r\n}\r\n\r\n# A nyn√≠ si vztah mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a pravdƒõpodobnost√≠ √∫ƒçasti v play-off vizualizujme.\r\nggplot(playoff_data, aes(x = pocet_vitezstvi, y = ucast_v_playoff))+\r\n  geom_point(size = 2)+\r\n  geom_line()+\r\n  ggtitle(\"Souvislost mezi poƒçtem v√Ωher v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a\\npravdƒõpodobnost√≠ postupu t√Ωmu do play-off (1996-2001)\")+\r\n  ylab(\"Pravdƒõpodobnost postupu t√Ωmu do play-off\")+\r\n  xlab(\"Poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe\")+\r\n  scale_x_continuous(limits=c(60,115), breaks = seq(60,115,5))+\r\n  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1))+\r\n  theme(axis.text = element_text(size=11),\r\n        axis.title = element_text(size=11))\r\n\r\n\r\n\r\n\r\n3. krok: Kladen√≠ ot√°zek a\r\nmƒõ≈ôen√≠\r\nS takto definovan√Ωm a kvantifikovan√Ωm c√≠lem si potom m≈Ø≈æeme kl√°st\r\ndal≈°√≠ch ot√°zky, na kter√© kdy≈æ si dok√°≈æeme odpovƒõdƒõt, zv√Ω≈°√≠me t√≠m na≈°e\r\n≈°ance na to, ≈æe tohoto c√≠le dos√°hneme. V p≈ô√≠padƒõ oaklandsk√Ωch ‚Äû√Åƒçek‚Äú se\r\nm≈Ø≈æeme pt√°t, d√≠ky ƒçemu t√Ωm dosahuje v z√°pasech v√≠tƒõzstv√≠? Celkem zjevn√°\r\nodpovƒõƒè zn√≠, ≈æe d√≠ky tomu, ≈æe dok√°≈æe z√≠skat v√≠ce bod≈Ø ne≈æ jeho soupe≈ôi.\r\nOt√°zkou ale je, p≈ôesnƒõ o kolik bod≈Ø nav√≠c mus√≠ t√Ωm z√≠skat, aby v\r\nz√°kladn√≠ ƒç√°sti soutƒõ≈æe dos√°hl na minim√°lnƒõ 95 v√≠tƒõzstv√≠. K zodpovƒõzen√≠\r\nt√©to ot√°zky opƒõt pot≈ôebujeme historick√° data (√∫daje o vyhran√Ωch a\r\nprohran√Ωch bodech) a relativnƒõ jednoduch√Ω statistick√Ω model zvan√Ω line√°rn√≠\r\nregrese, pomoc√≠ kter√©ho m≈Ø≈æeme popsat vztah mezi poƒçtem vyhran√Ωch\r\nz√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi\r\nbody. Z n√≠≈æe uveden√©ho grafu je z≈ôejm√©, ≈æe mezi tƒõmito dvƒõma promƒõnn√Ωmi\r\nje velice tƒõsn√Ω vztah a ≈æe spolu velice silnƒõ koreluj√≠.\r\n\r\n\r\nShow code\r\n\r\n# Vypoƒçtƒõme si rozd√≠l mezi vyhran√Ωmi a prohran√Ωmi body\r\nmoneyball <- moneyball %>%\r\n  mutate(RD = RS - RA)\r\n\r\n# Graficky si zn√°zornƒõme vztah mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body\r\nlibrary(ggpubr)\r\nggplot(moneyball, aes(x = RD , y = W))+\r\n  geom_point(alpha = 0.5, size = 2)+\r\n  geom_smooth(method = \"lm\", se = FALSE)+\r\n  ggtitle(\"Vztah mezi poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a\\nrozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body\")+\r\n  xlab(\"Rozd√≠l mezi poƒçtem vyhran√Ωch a prohran√Ωch bod≈Ø\")+\r\n  ylab(\"Poƒçet v√≠tƒõzstv√≠\")+\r\n  theme(axis.title = element_text(size = 11),\r\n        axis.text = element_text(size = 11))+\r\n  scale_x_continuous(limits = c(-350,350), breaks = seq(-350,350,50))+\r\n  scale_y_continuous(limits = c(40, 120), breaks = seq(40,120,10))+\r\n  stat_cor(method = \"pearson\", label.x = 175, label.y = 45)\r\n\r\n\r\n\r\n\r\nP≈ôi pou≈æit√≠ modelu line√°rn√≠ regrese m≈Ø≈æeme vztah mezi tƒõmito dvƒõma\r\npromƒõnn√Ωmi popsat trochu podrobnƒõji.\r\n\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body \r\nreg_model1 <- glm(W ~ RD, data = moneyball, family = \"gaussian\")\r\nsummary(reg_model1)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = W ~ RD, family = \"gaussian\", data = moneyball)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-14.2662   -2.6509    0.1234    2.9364   11.6570  \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 80.881375   0.131157  616.67   <2e-16 ***\r\nRD           0.105766   0.001297   81.55   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 15.51641)\r\n\r\n    Null deviance: 117164  on 901  degrees of freedom\r\nResidual deviance:  13965  on 900  degrees of freedom\r\nAIC: 5037\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nV√Ωsledn√° regresn√≠ rovnice n√°m ≈ô√≠k√°, ≈æe oƒçek√°van√Ω poƒçet v√≠tƒõzstv√≠\r\n= 80.88 + 0.106 x Rozd√≠lov√Ω sk√≥r. Tzn., ≈æe p≈ôi vyrovnan√©m pomƒõru\r\nvyhran√Ωch a prohran√Ωch bod≈Ø m≈Ø≈æeme oƒçek√°vat, ≈æe t√Ωm vyhraje p≈ôibli≈ænƒõ 80\r\nz√°pas≈Ø za sez√≥nu, a ≈æe kdy≈æ se rozd√≠lov√© sk√≥re nav√Ω≈°√≠ o deset bod≈Ø,\r\nm≈Ø≈æeme oƒçek√°vat, ≈æe t√Ωm vyhraje v pr≈Ømƒõru o jeden z√°pas za sez√≥nu nav√≠c.\r\nKl√≠ƒçov√© je ale pro n√°s to, ≈æe s pomoc√≠ t√©to rovnice a s trochou algebry\r\nsi m≈Ø≈æeme jednodu≈°e vypoƒç√≠tat, ≈æe k dosa≈æen√≠ minim√°lnƒõ 95 v√≠tƒõzstv√≠ za\r\nsez√≥nu pot≈ôebuje t√Ωm vyhr√°t p≈ôibli≈ænƒõ o 133 bod≈Ø v√≠ce, ne≈æ kolik jich se\r\nsoupe≈ôi prohraje ((95 - 80.88) / 0.106).\r\n4. krok: Kladen√≠\r\ndal≈°√≠ch ot√°zek a dal≈°√≠ mƒõ≈ôen√≠\r\nT√≠mto zji≈°tƒõn√≠m se n√°≈° c√≠l opƒõt trochu v√≠ce specifikuje a vyvol√°v√°\r\ndal≈°√≠ ot√°zky. Ot√°zka, kter√° se t√©mƒõ≈ô sama nab√≠z√≠, se t√Ωk√° charakteristik\r\nhr√°ƒç≈Ø, kter√© nejl√©pe p≈ôedpov√≠daj√≠ poƒçet vyhran√Ωch a prohran√Ωch bod≈Ø, a\r\nt√≠m tedy tak√© pravdƒõpodobnost postupu t√Ωmu do play-off. DePodesta na\r\nz√°kladƒõ sv√Ωch anal√Ωz zjistil, ≈æe poƒçet vyhran√Ωch bod≈Ø nejtƒõsnƒõji souvis√≠\r\ns procentem p≈ô√≠pad≈Ø, kdy se hr√°ƒç dostane na metu (tzv. On-Base\r\nPercentage - OBP), a to, jak daleko se hr√°ƒç dostane p≈ôi sv√©m odpalu\r\n(tzv. Slugging Percentage - SLG). Analogick√© statistiky pro\r\nt√Ωmy soupe≈ô≈Ø (OOBP a OSLG) potom stejnƒõ dob≈ôe p≈ôedpov√≠daj√≠ poƒçet\r\nprohran√Ωch bod≈Ø. Kdy≈æ vztah mezi tƒõmito promƒõnn√Ωmi pop√≠≈°eme opƒõt pomoc√≠\r\nmodelu line√°rn√≠ regrese, m≈Ø≈æeme se s jeho pomoc√≠ pokusit p≈ôedpovƒõdƒõt,\r\njak si t√Ωm povede p≈ô√≠≈°t√≠ sez√≥nu. Takov√° p≈ôedpovƒõƒè by p≈ôitom mohla b√Ωt\r\npotenci√°lnƒõ velice u≈æiteƒçn√°, proto≈æe na jej√≠m z√°kladƒõ bychom p≈ô√≠padnƒõ\r\nmohli upravit nƒõkter√° sv√° rozhodnut√≠ o koupi nebo prodeji vybran√Ωch\r\nhr√°ƒç≈Ø. Pojƒème tuto p≈ôedpovƒõƒè vytvo≈ôit pro t√Ωm oaklandsk√Ωch ‚Äû√Åƒçek‚Äú pro\r\nsez√≥nu 2002 na z√°kladƒõ dat z let 1962-2001. Z p≈ôedchoz√≠ anal√Ωzy ji≈æ\r\nv√≠me, ≈æe‚Ä¶\r\nPoƒçet v√≠tƒõzstv√≠ = 80.88 + 0.106 x (Poƒçet vyhran√Ωch bod≈Ø - Poƒçet\r\nprohran√Ωch bod≈Ø).\r\nNyn√≠ pot≈ôebujeme urƒçit, jak√Ω bude pravdƒõpodobn√Ω poƒçet vyhran√Ωch a\r\nprohran√Ωch bod≈Ø. Pom≈Ø≈æeme si opƒõt regresn√≠ anal√Ωzou.\r\n\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem vyhran√Ωch bod≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a dvƒõma vybran√Ωmi hr√°ƒçsk√Ωmi/t√Ωmov√Ωmi statistikami \r\nregModel2 = lm(RS ~ OBP + SLG, data=moneyball)\r\nsummary(regModel2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RS ~ OBP + SLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-70.838 -17.174  -1.108  16.770  90.036 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -804.63      18.92  -42.53   <2e-16 ***\r\nOBP          2737.77      90.68   30.19   <2e-16 ***\r\nSLG          1584.91      42.16   37.60   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 24.79 on 899 degrees of freedom\r\nMultiple R-squared:  0.9296,    Adjusted R-squared:  0.9294 \r\nF-statistic:  5934 on 2 and 899 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem prohran√Ωch bod≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a dvƒõma vybran√Ωmi hr√°ƒçsk√Ωmi/t√Ωmov√Ωmi statistikami \r\nregModel3 = lm(RA ~ OOBP + OSLG, data=moneyball)\r\nsummary(regModel3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RA ~ OOBP + OSLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-82.397 -15.178  -0.129  17.679  60.955 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -837.38      60.26 -13.897  < 2e-16 ***\r\nOOBP         2913.60     291.97   9.979 4.46e-16 ***\r\nOSLG         1514.29     175.43   8.632 2.55e-13 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.67 on 87 degrees of freedom\r\n  (812 observations deleted due to missingness)\r\nMultiple R-squared:  0.9073,    Adjusted R-squared:  0.9052 \r\nF-statistic: 425.8 on 2 and 87 DF,  p-value: < 2.2e-16\r\n\r\nS pomoc√≠ regresn√≠ anal√Ωzy jsme zjistili, ≈æe‚Ä¶\r\nPoƒçet vyhran√Ωch bod≈Ø = -804.63 + 2737.77 x OBP + 1584.91 x\r\nSLGPoƒçet prohran√Ωch bod≈Ø = -837.38 + 2913.60 x OOBP + 1514.29 x\r\nOSLG.\r\nSe znalost√≠ hr√°ƒçsk√Ωch/t√Ωmov√Ωch statistik oaklandsk√Ωch ‚Äû√Åƒçek‚Äú za rok\r\n2001 se nyn√≠ m≈Ø≈æeme pokusit p≈ôedpovƒõdƒõt nejd≈ô√≠ve poƒçet vyhran√Ωch a\r\nprohran√Ωch bod≈Ø a potom tak√© p≈ôedpokl√°dan√Ω poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠\r\nƒç√°sti soutƒõ≈æe. P≈ôi formulov√°n√≠ t√©to p≈ôedpovƒõdi vych√°z√≠me z p≈ôedpokladu,\r\n≈æe se slo≈æen√≠ t√Ωmu v pr≈Øbƒõhu sez√≥ny 2002 nebude (nap≈ô. z d≈Øvodu zranƒõn√≠\r\nhr√°ƒç≈Ø) p≈ô√≠li≈° li≈°it od jeho slo≈æen√≠ v roce 2001.\r\n\r\n\r\nShow code\r\n\r\n# Hr√°ƒçsk√©/t√Ωmov√© statistiky oaklandsk√Ωch ‚Äû√Åƒçek‚Äú za rok 2001\r\nOBP_OAK <- moneyball$OBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nSLG_OAK <- moneyball$SLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOOBP_OAK <- moneyball$OOBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOSLG_OAK <- moneyball$OSLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\n\r\n# Pravdƒõpodobn√© hodnoty vybran√Ωch statistik oaklandsk√Ωch \"√Åƒçek\" pro rok 2002 vypoƒç√≠tan√© s pomoc√≠ odhadnut√Ωch regresn√≠ch model≈Ø\r\npocet_vyhranych_bodu_pred <- round(-804.63 + 2737.77*OBP_OAK + 1584.91*SLG_OAK)\r\npocet_prohranych_bodu_pred <- round(-837.38 + 2913.60*OOBP_OAK + 1514.29*OSLG_OAK)\r\npocet_vitezstvi_pred <- round(80.88 + 0.106 * (pocet_vyhranych_bodu_pred - pocet_prohranych_bodu_pred), 0)\r\n\r\n# Skuteƒçn√© hodnoty vybran√Ωch statistik oaklandsk√Ωch \"√Åƒçek\" pro rok 2002\r\npocet_vyhranych_bodu_real <- baseball$RS[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_prohranych_bodu_real <- baseball$RA[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_vitezstvi_real <- baseball$W[which(baseball$Team==\"OAK\" & baseball$Year == 2002)]\r\n\r\n# Tabulka porovn√°vaj√≠c√≠ statistick√© p≈ôedpovƒõdi se skuteƒçnost√≠ \r\npred <- c(pocet_vyhranych_bodu_pred, pocet_prohranych_bodu_pred, pocet_vitezstvi_pred)\r\nreal <- c(pocet_vyhranych_bodu_real, pocet_prohranych_bodu_real, pocet_vitezstvi_real)\r\ntable <- data.frame(\"P≈ôedpovƒõd\" = pred, \"Skuteƒçnost\" = real)\r\nrow.names(table) <- c(\"Vyhran√© body\", \"Prohran√© body\", \"Poƒçet v√≠tƒõzstv√≠\")\r\ntable\r\n\r\n\r\n                P≈ôedpovƒõd Skuteƒçnost\r\nVyhran√© body          836        800\r\nProhran√© body         635        654\r\nPoƒçet v√≠tƒõzstv√≠       102        103\r\n\r\nPorovn√°n√≠ na≈°ich p≈ôedpovƒõd√≠ s re√°ln√Ωmi v√Ωsledky za sez√≥nu 2002\r\nukazuje, ≈æe se n√°m poda≈ôilo velice p≈ôesnƒõ p≈ôedpovƒõdƒõt v√Ωsledky v\r\nnadch√°zej√≠c√≠ ligov√© sez√≥nƒõ, a v√Ωznamnƒõ tak sn√≠≈æit m√≠ru na≈°√≠ nejistoty\r\np≈ôi jej√≠m pl√°nov√°n√≠.\r\n5.\r\nkrok: Propojen√≠ d√≠lƒç√≠ch vhled≈Ø aneb organizace jako stroj\r\nMatt Dancho ve\r\nsv√© metodice k datovƒõ-analytick√Ωm projekt≈Øm doporuƒçuje, abychom se p≈ôi\r\nsnaze o pochopen√≠ obchodn√≠ho probl√©mu organizace na danou organizaci\r\nd√≠vali jako na druh stroje, kter√Ω m√° urƒçit√© vstupy, procesy a v√Ωstupy.\r\nTuto metaforu stroje m≈Ø≈æeme nyn√≠ vyu≈æ√≠t k tomu, abychom v≈°echny v√Ω≈°e\r\nuveden√© d√≠lƒç√≠ vhledy spojili do jednotn√©ho r√°mce. V nƒõm budou m√≠t\r\noaklandsk√° ‚Äú√Åƒçka‚Äù podobu jednoduch√©ho stroje na v√Ωrobu postup≈Ø do\r\nplay-off - viz obr√°zek n√≠≈æe.\r\n\r\nZe sch√©matu je dob≈ôe patrn√©, jak tento stroj funguje: Jeho v√Ωstupy\r\njsou postupy do play-off, kter√Ωch dosahuje tak, ≈æe se sna≈æ√≠ vyhr√°t v√≠ce\r\nz√°pas≈Ø, resp. z√≠skat v√≠ce bod≈Ø ne≈æ soupe≈ô√≠c√≠ t√Ωmy; k tomu vyu≈æ√≠v√° vstupy\r\nv podobƒõ schopnosti hr√°ƒç≈Ø hr√°t dob≈ôe na p√°lce a v poli; vstupem\r\novliv≈àuj√≠c√≠m chod stroje jsou rovnƒõ≈æ obdobn√© schopnosti hr√°ƒç≈Ø\r\nsoupe≈ô√≠c√≠ch t√Ωm≈Ø. Jedn√° se samoz≈ôejmƒõ o velmi zjednodu≈°en√Ω kauz√°ln√≠\r\nmodel fungov√°n√≠ t√Ωmu oakladnsk√Ωch ‚Äú√Åƒçek‚Äù, ale jak konstatuje slavn√Ω\r\nstatistick√Ω aforismus, modely jsou\r\nv≈ædy nep≈ôesn√©, ale nƒõkter√© z nich jsou u≈æiteƒçn√©.\r\nJakkoli na≈°e modely fungov√°n√≠ organizace budou v≈ædy ne√∫pln√©, je\r\nd≈Øle≈æit√© ovƒõ≈ôit, zda tyto modely i p≈ôes svou omezenost v dostateƒçn√© m√≠≈ôe\r\nodr√°≈æej√≠ realitu tak, jak n√°m ji zprost≈ôedkov√°vaj√≠ dostupn√° data. Za\r\nt√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t statistickou metodu struktur√°ln√≠ho\r\nmodelov√°n√≠, kter√° umo≈æ≈àuje formalizovat na≈°e p≈ôedstavy o vz√°jemn√Ωch\r\nvztaz√≠ch mezi nƒõkolika r≈Øzn√Ωmi promƒõnn√Ωmi a zhodnotit m√≠ru souladu\r\ntƒõchto na≈°ich p≈ôedstav s dostupn√Ωmi daty. Teprve po takov√©m zhodnocen√≠\r\nvƒõrohodnosti modelu je rozumn√© na nƒõm zakl√°dat sv√° dal≈°√≠ rozhodnut√≠.\r\nPojƒème tedy tuto metodu pou≈æ√≠t rovnƒõ≈æ na n√°≈° novƒõ vytvo≈ôen√Ω model\r\nfungov√°n√≠ t√Ωmu oaklandsk√Ωch ‚Äú√Åƒçek‚Äù a ovƒõ≈ôit m√≠ru jeho vƒõrohodnosti.\r\n\r\n\r\nShow code\r\n\r\n# Data, kter√° budeme pot≈ôebovat pro ovƒõ≈ôen√≠ vƒõrohodnosti na≈°eho modelu fungov√°n√≠ oaklandsk√Ωch \"√Åƒçek\" \r\nsem_data <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select(RS, RA, RD, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n\r\n# Definice modelu, kter√° je v souladu s v√Ω≈°e uveden√Ωm sch√©matem\r\nlibrary(lavaan)\r\noak_model <- '\r\n     Playoffs ~ W\r\n     W ~ RS + RA\r\n     RA ~ OOBP + OSLG\r\n     RS ~ OBP + SLG \r\n'\r\n# Odhad parametr≈Ø modelu\r\nfit_oak_model <- sem(oak_model, data = sem_data, missing = \"pairwise\", estimator = \"WLSMV\", ordered = \"Playoffs\")\r\nsummary(fit_oak_model, standardized = T, fit.measures = T, rsq = T)\r\n\r\n\r\nlavaan 0.6-9 ended normally after 148 iterations\r\n\r\n  Estimator                                       DWLS\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        14\r\n                                                      \r\n                                                  Used       Total\r\n  Number of observations                            90         176\r\n  Number of missing patterns                         1            \r\n                                                                  \r\nModel Test User Model:\r\n                                              Standard      Robust\r\n  Test Statistic                                 8.354      10.470\r\n  Degrees of freedom                                15          15\r\n  P-value (Chi-square)                           0.909       0.789\r\n  Scaling correction factor                                  1.159\r\n  Shift parameter                                            3.260\r\n       simple second-order correction                             \r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                               167.943     150.692\r\n  Degrees of freedom                                 6           6\r\n  P-value                                        0.000       0.000\r\n  Scaling correction factor                                  1.119\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    1.000       1.000\r\n  Tucker-Lewis Index (TLI)                       1.016       1.013\r\n                                                                  \r\n  Robust Comparative Fit Index (CFI)                            NA\r\n  Robust Tucker-Lewis Index (TLI)                               NA\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.000       0.000\r\n  90 Percent confidence interval - lower         0.000       0.000\r\n  90 Percent confidence interval - upper         0.040       0.067\r\n  P-value RMSEA <= 0.05                          0.964       0.903\r\n                                                                  \r\n  Robust RMSEA                                                  NA\r\n  90 Percent confidence interval - lower                     0.000\r\n  90 Percent confidence interval - upper                        NA\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  SRMR                                           0.108       0.108\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                           Robust.sem\r\n  Information                                 Expected\r\n  Information saturated (h1) model        Unstructured\r\n\r\nRegressions:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n  Playoffs ~                                                     \r\n    W                  0.234    0.025    9.181    0.000     0.234\r\n  W ~                                                            \r\n    RS                 0.093    0.006   15.189    0.000     0.093\r\n    RA                -0.094    0.006  -16.031    0.000    -0.094\r\n  RA ~                                                           \r\n    OOBP            3158.695  360.178    8.770    0.000  3158.695\r\n    OSLG            1520.258  213.163    7.132    0.000  1520.258\r\n  RS ~                                                           \r\n    OBP             3621.290  258.284   14.021    0.000  3621.290\r\n    SLG             1418.260  144.885    9.789    0.000  1418.260\r\n  Std.all\r\n         \r\n    0.993\r\n         \r\n    0.682\r\n   -0.726\r\n         \r\n    0.564\r\n    0.452\r\n         \r\n    0.606\r\n    0.425\r\n\r\nIntercepts:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.000                                0.000\r\n   .W                 96.691   11.568    8.358    0.000    96.691\r\n   .RA              -808.808  116.566   -6.939    0.000  -808.808\r\n   .RS             -1041.496   73.943  -14.085    0.000 -1041.496\r\n  Std.all\r\n    0.000\r\n    8.629\r\n   -9.367\r\n  -12.661\r\n\r\nThresholds:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs|t1       22.653    6.998    3.237    0.001    22.653\r\n  Std.all\r\n    8.578\r\n\r\nVariances:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.101                                0.101\r\n   .W                  7.779    2.306    3.374    0.001     7.779\r\n   .RA               536.111   97.833    5.480    0.000   536.111\r\n   .RS               449.043   80.457    5.581    0.000   449.043\r\n  Std.all\r\n    0.014\r\n    0.062\r\n    0.072\r\n    0.066\r\n\r\nScales y*:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs           1.000                                1.000\r\n  Std.all\r\n    1.000\r\n\r\nR-Square:\r\n                   Estimate \r\n    Playoffs           0.986\r\n    W                  0.938\r\n    RA                 0.928\r\n    RS                 0.934\r\n\r\n\r\n\r\nShow code\r\n\r\n# Grafick√© zn√°zornƒõn√≠ modelu fungov√°n√≠ oaklandsk√Ωch \"√Åƒçek\" \r\nlibrary(semPlot)\r\nsemPaths(fit_oak_model, \r\n         whatLabels=\"std\", \r\n         intercepts=FALSE, \r\n         style=\"lisrel\",\r\n         nCharNodes=0,\r\n         nCharEdges=0,\r\n         curveAdjacent = TRUE,\r\n         title=TRUE,\r\n         layout=\"tree2\",\r\n         curvePivot=TRUE,\r\n         rotation =3)\r\n\r\n\r\n\r\n\r\nV√Ωstupy proveden√© tzv. pƒõ≈°inkov√©\r\nanal√Ωzy, kter√° je speci√°ln√≠m typem struktur√°ln√≠ho modelov√°n√≠,\r\nnaznaƒçuj√≠, ≈æe n√°mi navr≈æen√Ω model je v souladu s daty, kter√° m√°me k\r\ndispozici (viz ‚Äúp≈ô√≠zniv√©‚Äù hodnoty index≈Ø shody, resp. neshody jako je\r\nTLI a CFI, resp. RMSEA, a tak√© vysok√© hodnoty standardizovan√Ωch\r\nregresn√≠ch koeficient≈Ø). D√°vaj√≠ n√°m tak dobr√Ω d≈Øvod vƒõ≈ôit, ≈æe na≈°e dal≈°√≠\r\nkroky a rozhodnut√≠, kter√° zalo≈æ√≠me na tomto modelu, budou m√≠t ≈æ√°douc√≠\r\nefekt na po≈æadovan√© v√Ωstupy, tj. na postup oaklandsk√Ωch ‚Äú√Åƒçek‚Äù do\r\nplay-off.\r\n6. krok: Intervence\r\nNa z√°kladƒõ v√Ω≈°e uveden√Ωch zji≈°tƒõn√≠ zaƒçal management oaklandsk√Ωch\r\n‚Äû√Åƒçek‚Äú do sv√©ho t√Ωmu vyb√≠rat hr√°ƒçe, kte≈ô√≠ sice nevyhovovali tradiƒçn√≠m\r\nkrit√©ri√≠m, podle kter√Ωch hr√°ƒç≈°t√≠ skauti posuzovali kvalitu baseballov√Ωch\r\nhr√°ƒç≈Ø, ale za to vykazovali p≈ôesnƒõ ty charakteristiky, kter√© podle\r\nDePodestov√Ωch anal√Ωz p≈ôedpov√≠daly poƒçet vyhran√Ωch a prohran√Ωch bod≈Ø, a\r\npota≈æmo tedy tak√© pravdƒõpodobnost √∫ƒçasti v play-off, kter√° byla hlavn√≠m\r\nc√≠lem managementu. D√≠ky tomu, ≈æe konkurenƒçn√≠ t√Ωmy d≈Øle≈æitost tƒõchto\r\nhr√°ƒçsk√Ωch statistik podce≈àovaly a naopak p≈ôece≈àovaly jin√©, m√©nƒõ d≈Øle≈æit√©\r\npromƒõnn√© (nap≈ô. m√≠ru √∫spƒõ≈°nosti odpal≈Ø, tzv. Batting Average),\r\nmohl management oaklandsk√Ωch ‚Äû√Åƒçek‚Äú relativnƒõ levnƒõ skupovat hr√°ƒçe,\r\nkte≈ô√≠ jim umo≈æ≈àovali dosahovat stanoven√©ho c√≠le. V√Ωsledkem bylo to, ≈æe\r\noaklandsk√° ‚Äû√Åƒçka‚Äú vyhr√°vala zhruba o 20 z√°pas≈Ø za sez√≥nu v√≠ce ne≈æ stejnƒõ\r\n‚Äûchud√©‚Äú t√Ωmy a p≈ôibli≈ænƒõ stejnƒõ tolik z√°pas≈Ø jako 2kr√°t a≈æ 3kr√°t bohat≈°√≠\r\nkonkurence - viz graf n√≠≈æe.\r\n\r\n\r\nShow code\r\n\r\n# Naƒçtƒõme si pot≈ôebn√° data Lahmanovy baseballov√© datab√°ze, kter√° je ve≈ôejnƒõ p≈ô√≠stupn√° na adrese http://seanlahman.com/baseball-archive/statistics/\r\nmzdyHracu <- read_csv(\"salaries.csv\")\r\nvyhryTymu <- read_csv(\"teams.csv\")\r\n\r\n# Vypoƒçtƒõme si pr≈Ømƒõrnou sumu mezd vypl√°cen√Ωch jednotliv√Ωmi t√Ωmy sv√Ωm hr√°ƒç≈Øm v letech 1998-2001 \r\nprumerna_suma_MezdHracu <- mzdyHracu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypoƒçtƒõme si pro jednotliv√© t√Ωmy pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu v letech 1998-2001\r\nprumerny_pocet_vyher <- vyhryTymu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyj√°d≈ôeme si graficky vztah mezi poƒçtem v√Ωher a mno≈æstv√≠m penƒõz, kter√© t√Ωmy vynakl√°daj√≠ na mzdy sv√Ωch hr√°ƒç≈Ø \r\nlibrary(ggrepel)\r\nprumerna_suma_MezdHracu %>%\r\n  left_join(prumerny_pocet_vyher, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hr√°ƒç≈Ø a poƒçet v√≠tƒõzstv√≠ v letech 1998-2001\")+\r\n  xlab(\"Pr≈Ømƒõrn√° suma mezd hr√°ƒç≈Ø (USD)\")+\r\n  ylab(\"Pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\"))+\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07))\r\n\r\n\r\n\r\n\r\nOmezen√≠ HR analytiky\r\nP≈ôes ve≈°kerou p≈ôidanou hodnotu, kterou HR analytika pro organizaci\r\nm≈Ø≈æe m√≠t, je vhodn√© si v≈Øƒçi n√≠ zachovat zdravou m√≠ru skepse a b√Ωt si\r\nvƒõdom jej√≠ch omezen√≠. N√≠≈æe uv√°d√≠m p≈ôehled nƒõkolika z nich.\r\nKvalita a u≈æiteƒçnost v√Ωstup≈Ø HR analytiky je z√°visl√° na kvalitƒõ\r\ndat, kter√° do n√≠ vstupuj√≠. Jako kdekoli jinde i zde plat√≠ ok≈ô√≠dlen√©\r\nrƒçen√≠ ‚Äûrubbish in, rubbish out‚Äú. Schopnost z√≠skat pot≈ôebn√° data\r\nvƒças, v dostateƒçn√© kvalitƒõ a v dostateƒçn√©m mno≈æstv√≠ p≈ôitom p≈ôedstavuje\r\njedno z neju≈æ≈°√≠ch hrdel cel√©ho procesu zav√°dƒõn√≠ HR analytiky v\r\norganizac√≠ch.\r\nHR analytika pracuje s historick√Ωmi daty a vych√°z√≠ z p≈ôedpokladu,\r\n≈æe minulost je dobr√Ωm prediktorem budoucnosti. Ale jak n√°s na to\r\nopakovanƒõ upozor≈àuj√≠ odborn√≠ci jako Nassim\r\nTaleb nebo Philip\r\nTetlock, tento vztah mezi minulost√≠ a budoucnost√≠ plat√≠ pouze do\r\nurƒçit√© m√≠ry a pouze v relativnƒõ kr√°tk√©m ƒçasov√©m horizontu. Na ka≈æd√©m\r\nrohu na n√°s ƒç√≠h√° nƒõjak√° potenci√°ln√≠ ƒçern√°\r\nlabu≈•, kter√° m≈Ø≈æe postavit na hlavu v≈°echno, co jsme se na z√°kladƒõ\r\nna≈°ich minul√Ωch zku≈°enost√≠ nauƒçili br√°t jako samoz≈ôejmou\r\njistotu.\r\nNe ka≈æd√© prost≈ôed√≠ je stejnƒõ p≈ôedv√≠dateln√© jako svƒõt sportu.\r\nPomƒõr sign√°lu\r\na ≈°umu se m≈Ø≈æe nap≈ô√≠ƒç r≈Øzn√Ωmi oblastmi v√Ωznamnƒõ li≈°it a ƒç√≠m v√≠ce\r\np≈ôevl√°d√° n√°hodn√Ω ≈°um nad sign√°lem, t√≠m m√©nƒõ jsou v√Ωstupy z HR analytiky\r\nu≈æiteƒçn√©. P≈ô√≠kladem zde m≈Ø≈æe b√Ωt relativnƒõ ne√∫spƒõ≈°n√° snaha p≈ôedpov√≠dat\r\nto, jak si baseballov√© t√Ωmu povedou v play-off. Na rozd√≠l od z√°kladn√≠\r\nƒç√°sti soutƒõ≈æe, kde se hraje dostatek z√°pas≈Ø na to, aby se vyru≈°il vliv\r\nn√°hodn√©ho ≈°tƒõst√≠ a sm≈Øly, v pƒõtiz√°pasov√Ωch kolech play-off hraje n√°hoda\r\ntak v√Ωznamnou roli, ≈æe souvislost mezi celkov√Ωm poƒçtem v√≠tƒõzstv√≠ v\r\nz√°kladn√≠ ƒç√°sti a po≈ôad√≠m t√Ωmu v play-off je t√©mƒõ≈ô nulov√°.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si data mezi lety 1994-2011, kdy v play-off hraje 8 t√Ωm≈Ø.\r\nmoneyball3 <- moneyball %>%\r\n  filter(Year < 2012 & Year > 1993)\r\n  \r\n# V√Ωpoƒçtƒõme si Kendallovu po≈ôadovou korelaci mezi mezi celkov√Ωmm poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a po≈ôad√≠m t√Ωmu v play-off mezi lety 1994-2011. \r\nsuppressWarnings(cor.test(~ W + RankPlayoffs, data = moneyball3, method = \"kendall\"))\r\n\r\n\r\n\r\n    Kendall's rank correlation tau\r\n\r\ndata:  W and RankPlayoffs\r\nz = -0.48318, p-value = 0.629\r\nalternative hypothesis: true tau is not equal to 0\r\nsample estimates:\r\n        tau \r\n-0.05541167 \r\n\r\nƒå√≠sla maj√≠ tu zvl√°≈°tn√≠ moc, ≈æe dok√°≈æou v ƒçlovƒõku velice snadno\r\nvzbudit dojem, ≈æe toho v√≠me mnohem v√≠ce ne≈æ je tomu ve skuteƒçnosti. Je\r\nv≈°ak dobr√© si b√Ωt vƒõdom toho, ≈æe ka≈æd√° statistick√° p≈ôedpovƒõƒè je v≈ædy\r\nzat√≠≈æena nƒõjakou m√≠rou chyby, tu vƒõt≈°√≠, tu men≈°√≠. Velkou v√Ωhodou\r\nstatistick√Ωch model≈Ø je to, ≈æe tato chyba je u nich explicitnƒõ\r\nvyƒç√≠slena, tak≈æe s n√≠ lze dop≈ôedu poƒç√≠tat a zohlednit ji p≈ôi n√°sledn√©m\r\nrozhodov√°n√≠. Tato ‚Äûup≈ô√≠mnost‚Äú ohlednƒõ sv√© vlastn√≠ omylnosti paradoxnƒõ\r\nmnohdy stav√≠ statistick√© modely do hor≈°√≠ho svƒõtla ne≈æ jinak m√©nƒõ p≈ôesn√©\r\nintuitivn√≠ √∫sudky expert≈Ø, pro kter√© podobn√© √∫daje o m√≠≈ôe jejich\r\nomylnosti vƒõt≈°inou nejsou v≈Øbec k dispozici.\r\nVelikost v√Ωhody, kterou n√°m zaveden√≠ HR analytiky d√°v√°, m≈Ø≈æe b√Ωt\r\nz√°visl√° na tom, zda podobn√© postupy vyu≈æ√≠v√° tak√© na≈°e konkurence. Opƒõt\r\nto lze celkem dob≈ôe dolo≈æit na oaklandsk√Ωch ‚Äû√Åƒçk√°ch‚Äú. Jejich v√Ωsledky se\r\nmezi lety 2002 a≈æ 2012, tj. v dobƒõ po zve≈ôejnƒõn√≠ Moneyballu, kdy ji≈æ\r\nv≈°echny t√Ωmy mƒõly p≈ô√≠le≈æitost sezn√°mit se s principy prediktivn√≠\r\nanalytiky a zav√©st ji do sv√© praxe, zaƒçaly v√≠ce p≈ôibli≈æovat v√Ωsledk≈Øm\r\npodobnƒõ ‚Äûchud√Ωch‚Äú soupe≈ô≈Ø a naopak jejich bohat≈°√≠ soupe≈ôi jim sv√Ωm\r\nv√Ωkonem zase trochu odskoƒçili - viz graf n√≠≈æe. Z toho mimo jin√© vypl√Ωv√°,\r\n≈æe s t√≠m, jak se st√°le v√≠ce spoleƒçnost√≠ bude p≈ôi ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø\r\nspol√©hat na v√Ωstupy z HR analytiky, p≈ôestane b√Ωt HR analytika nƒõjakou\r\nz√°sadn√≠ konkurenƒçn√≠ v√Ωhodou a stane se z n√≠ nƒõco, co organizaci ‚Äúpouze‚Äù\r\numo≈æn√≠ dr≈æet krok s konkurenc√≠.\r\n\r\n\r\nShow code\r\n\r\n# Vypoƒçtƒõme si pr≈Ømƒõrnou sumu mezd vypl√°cen√Ωch jednotliv√Ωmi t√Ωmy sv√Ωm hr√°ƒç≈Øm v letech 2002-2012 \r\nprumerna_suma_MezdHracu2 <- mzdyHracu %>%\r\n  filter(yearID > 2001 & yearID <= 2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypoƒçtƒõme si pro jednotliv√© t√Ωmy pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu v letech 2002-2012\r\nprumerny_pocet_vyher2 <- vyhryTymu %>%\r\n  filter(yearID > 2001 & yearID <=2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyj√°d≈ôeme si graficky vztah mezi poƒçtem v√Ωher a mno≈æstv√≠m penƒõz, kter√© t√Ωmy vynakl√°daj√≠ na mzdy sv√Ωch hr√°ƒç≈Ø \r\nprumerna_suma_MezdHracu2 %>%\r\n  left_join(prumerny_pocet_vyher2, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hr√°ƒç≈Ø a poƒçet v√≠tƒõzstv√≠ v letech 2002-2012\")+\r\n  xlab(\"Pr≈Ømƒõrn√° suma mezd hr√°ƒç≈Ø (USD)\")+\r\n  ylab(\"Pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\")) +\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(3e+07,2e+08), breaks = seq(3e+07,2e+08,2e+07))\r\n\r\n\r\n\r\n\r\nZ√°vƒõr\r\nNa p≈ô√≠kladu oaklandsk√©ho baseballov√©ho mu≈æstva jsme takto mohli\r\nsledovat obvykl√Ω postup aplikace HR analytiky na urƒçit√Ω druh probl√©mu,\r\nkter√Ω se sna≈æ√≠ v dan√© organizaci vy≈ôe≈°it. Vzhledem ke specifick√©mu\r\np≈ôedmƒõtu podnik√°n√≠ oaklandsk√Ωch ‚Äû√Åƒçek‚Äú bylo t√≠mto c√≠lem dos√°hnout\r\npostupu do play-off a to v situaci, kdy management nemƒõl dostatek\r\nfinanƒçn√≠ch prost≈ôedk≈Ø na zaplacen√≠ hr√°ƒç≈Ø pova≈æovan√Ωch dle tradiƒçn√≠ch\r\nmƒõ≈ô√≠tek za kvalitn√≠ a perspektivn√≠. Od tohoto c√≠le se potom odv√≠jela\r\n≈ôada krok≈Ø, kter√© bl√≠≈æe specifikovaly jeho povahu a identifikovaly\r\nfaktory (mimo jin√© i ty person√°ln√≠), kter√© s jeho dosa≈æen√≠m souvis√≠. Na\r\nz√°kladƒõ t√©to znalosti potom bylo mo≈æn√© formulovat urƒçit√© p≈ôedpovƒõdi a\r\nuƒçinit jist√° rozhodnut√≠, kter√° zv√Ω≈°ila pravdƒõpodobnost toho, ≈æe se\r\npoda≈ô√≠ vytƒçen√©ho c√≠le dos√°hnout. P≈ôesto≈æe tento p≈ô√≠bƒõh o vyu≈æit√≠ HR\r\nanalytiky se odehr√°l ve svƒõtƒõ sportu, jeho logika je platn√° i v kontextu\r\ntradiƒçnƒõj≈°√≠ho typu organizac√≠. Ostatnƒõ ve v≈°ech typech\r\norganizac√≠ jde nakonec p≈ôedev≈°√≠m o to m√≠t na spr√°vn√©m m√≠stƒõ a ve spr√°vn√Ω\r\nƒças ty spr√°vn√© lidi - jedinƒõ tak tyto organizace mohou\r\nsystematicky dosahovat sv√Ωch strategick√Ωch c√≠l≈Ø.\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/./Pitcher-at-the-mound.jpg",
    "last_modified": "2022-09-17T18:41:49+02:00",
    "input_file": {}
  }
]
