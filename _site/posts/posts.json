[
  {
    "path": "posts/2025-02-13-causal-identification-and-feature-selection/",
    "title": "A way to make prediction models more precise and interpretable at the same time?",
    "description": "An interesting and potentially useful combination of causal identification algorithms and prediction modeling in the machine learning pipeline.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-02-13",
    "categories": [
      "causal identification",
      "causal inference",
      "feature selection",
      "prediction models",
      "ml interpretability",
      "machine learning"
    ],
    "contents": "\r\nI just came across an interesting paper by Ding, Zhang, & Bos (2018), who used automatic causal identification algorithms for feature selection and achieved better prediction performance with models based on these features than with models built in a more traditional way.\r\nThe authors specifically studied the experiences and choices of 3,293 visitors at a large theme park and attempted to use them to predict their Big Five personality characteristics. First, they applied two algorithms for causal identification‚Äîthe PC algorithm and the Fast Greedy Equivalence Search algorithm‚Äîto narrow down the candidate DAGs that explain the data. Then, they used the outputs from these algorithms to identify features that were causally impacted by the personality characteristic of interest. Finally, they built prediction models using only these features.\r\n\r\n\r\n\r\nThe authors found that their ML pipeline, which incorporated causal identification for feature selection, outperformed baseline models in predicting individual characteristics (specifically, they used LASSO linear regression) as the baseline model, which also performs automatic feature selection). Beyond that, these models, according to the authors, provided more human-interpretable results. A win-win.\r\n\r\n\r\n\r\nFor me, these results are quite surprising, as I would expect that if prediction performance were the only criterion, a purely ‚Äúcorrelational‚Äù approach without causal constraints would perform better. I‚Äôm curious how these alleged benefits generalize to other types of situations‚Äîsuch as cases with less noise in the data. Definitely worth trying in one of my future projects.\r\nDoes anyone have experience with this specific approach to feature selection? Feel free to share your insights.\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-02-13-causal-identification-and-feature-selection/./fig1.png",
    "last_modified": "2025-02-13T13:16:40+01:00",
    "input_file": "causal-identification-and-feature-selection.knit.md",
    "preview_width": 916,
    "preview_height": 850
  },
  {
    "path": "posts/2025-02-05-career-hurdles/",
    "title": "And what are your career hurdles?",
    "description": "Visualization of the results from a meta-analytic review of the hurdles to subjective career success.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-02-05",
    "categories": [
      "subjective career success",
      "io psychology",
      "meta-analysis"
    ],
    "contents": "\r\nI came across an interesting meta-analytic review by Ng & Feldman (2014) that might be useful for setting priors about the hurdles people face when it comes to their perception of subjective career success (SCS). The authors estimated the impact of 54 specific hurdles, grouped into six categories that can potentially lower people‚Äôs SCS:\r\nTrait-related\r\nSocial-network-related\r\nSkill-related\r\nOrganizational and job-related\r\nMotivational\r\nBackground-related hurdles\r\nOn average, the strongest negative effects were found in the social and organizational hurdle groups, followed by trait-related and motivational hurdles. Interestingly, background-related and skill-related hurdles weren‚Äôt strongly associated with SCS. I‚Äôd say that‚Äôs good news, considering how non/malleable some of these types of hurdles can be ü§î\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggtext)\r\nlibrary(readxl)\r\n\r\n# uploading the dataset with stats extracted from the research paper\r\ndata <- readxl::read_excel('career_hurdles_95CI.xlsx')\r\n\r\n# defining colors for individual groups of career hurdles\r\ncategory_colors <- c(\r\n  \"Background-related hurdles\" = \"#1B39A6\", \r\n  \"Motivational hurdles\" = \"#289ECC\", \r\n  \"Organizational and job hurdles\" = \"#B13AA0\",\r\n  \"Skill-related hurdles\" = \"#73A239\",\r\n  \"Social network hurdles\" = \"#895F22\",\r\n  \"Trait-related hurdles\" = \"#D08311\"\r\n  \r\n) \r\n\r\n# enriching the dataset for coloring the value labels\r\ndata <- data %>%\r\n  dplyr::arrange(Category, desc(`95% CI Upper`)) %>%\r\n  dplyr::mutate(\r\n    Hurdle_ordered = factor(Hurdle, levels = unique(Hurdle)),\r\n    Hurdle_colored = paste0(\"<span style='color:\", category_colors[Category], \"'>\", Hurdle, \"<\/span>\")\r\n  )\r\n\r\n# dataviz\r\ndata %>% \r\n  ggplot2::ggplot(aes(x = Hurdle_ordered, y = rc, color = Category)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_errorbar(aes(ymin = `95% CI Lower`, ymax = `95% CI Upper`), width = 0.2, position = position_dodge(0.05), linewidth = 1) +\r\n  ggplot2::geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey\") +\r\n  ggplot2::scale_color_manual(values = category_colors) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"SAMPLE-SIZE WEIGHTED CORRECTED CORRELATION\",\r\n    title = \"Relationships of career hurdles with subjective career success\",\r\n    caption = \"\\nThe bars around the point estimates represent the 2.5% lower and 97.5% upper limits of the 95% confidence interval.\\nThe hurdles are listed by groups and in ascending order based on the upper bound of the 95% confidence interval for effect size.\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin = margin(0, 0, 10, 0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin = margin(0, 0, 20, 0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 10, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.title.y.left = element_text(margin = margin(r = 15), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.text.y = element_markdown(size = 12, lineheight = 1.2),  # Apply markdown here\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position = \"top\", \r\n    legend.box = \"horizontal\",\r\n    legend.justification = \"right\",\r\n    legend.key = element_blank(),\r\n    legend.text = element_text(size = 12),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin = unit(c(5, 5, 5, 5), \"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\"\r\n  ) +\r\n  ggplot2::scale_x_discrete(labels = function(x) data$Hurdle_colored[match(x, data$Hurdle_ordered)])  +\r\n  ggplot2::guides(color = guide_legend(reverse = TRUE))\r\n\r\n\r\n\r\nP.S. As is evident from the code snippet above, the attached chart isn‚Äôt straight from the paper. I made it based on the results presented there, so any mistakes in the chart are on me!\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-02-05-career-hurdles/career-hurdles_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2025-02-05T14:19:21+01:00",
    "input_file": "career-hurdles.knit.md",
    "preview_width": 2112,
    "preview_height": 1920
  },
  {
    "path": "posts/2025-02-02-analytical-choices-and-variability/",
    "title": "How variations in analytic choices affect results?",
    "description": "It‚Äôs human nature to take things at face value, assuming they are exactly as they appear, without noticing the assumptions and choices shaping that appearance. Remember Kahneman‚Äôs WYSIATI principle‚ÄîWhat You See Is All There Is?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-02-02",
    "categories": [
      "reproducibility",
      "open science",
      "data science"
    ],
    "contents": "\r\nUnfortunately, the same bias can also creep into data analysis. Along the way, we make numerous decisions‚Äîsome small, some significant‚Äîthat impact our results. Yet, we often act as if none of these choices happened, believing we‚Äôve arrived at the one true, objective finding.\r\nA great example of this comes from Silberzahn et al.¬†(2018), who set out to expose these subjective analytical choices and their influence on results. They asked 29 teams, comprising 61 analysts, to analyze the same dataset and answer the same question: Are soccer referees more likely to give red cards to players with darker skin tones than to those with lighter skin tones?\r\nThey found that analytical approaches varied widely, leading to effect size estimates ranging from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, while nine teams (31%) did not. Interestingly, neither the analysts‚Äô prior beliefs nor their level of expertise explained the variation. Even peer ratings of analysis quality failed to account for the differences.\r\n\r\n\r\n\r\nThis study highlights an important reality: even defensible, well-intentioned analytical choices can lead to vastly different results. What should we take from this? Should sensitivity analysis be a standard practice? Should we crowdsource high-profile analyses? What do you think?\r\nP.S. If you want to see how your own approach would shape the results, you can download the original dataset from OSF and give it a try!\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-02-02-analytical-choices-and-variability/./results_chart.jpeg",
    "last_modified": "2025-02-02T18:35:00+01:00",
    "input_file": "analytical-choices-and-variability.knit.md"
  },
  {
    "path": "posts/2025-01-20-self-awareness-and-personality/",
    "title": "Does your personality interfere with your self-awareness?",
    "description": "A brief data-driven exploration of personality factors that might negatively affect self-awareness.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-01-20",
    "categories": [
      "personality",
      "self-awareness",
      "multi-rater feedback",
      "hpi"
    ],
    "contents": "\r\nMany wise people (starting with Plato and his ‚ÄòKnow thyself‚Äô idea?) suggest that self-awareness is one of the key ingredients for psychological well-being, career success, and effective leadership, among other things.\r\nIf that‚Äôs true (see, for example, this research paper on the impact of self-awareness on leadership behavior and effectiveness), it would be useful to understand what might get in the way of well-calibrated self-awareness. One potential obstacle could be our habitual patterns of feelings, thoughts, and behaviors, i.e., our personality.\r\nBut to explore this, we first need a way to measure self-awareness. One common (though imperfect) method in a business context is to compare self-evaluation with evaluations from others, such as through multi-rater feedback. The difference between these evaluations can then be analyzed alongside a valid personality measure.\r\nIn a small-scale project, I had the opportunity to conduct exactly this type of analysis. The individuals assessed‚Äîa selected group of 72 managers from a large CEE company‚Äîwere distributed relatively symmetrically around the zero difference between their self-evaluations and the average evaluations by others across several competencies, measured on a 1‚Äì5 Likert scale. For personality, I had access to their data from the Hogan Personality Inventory, a business-oriented personality measure based on the Five-Factor Model of personality.\r\n\r\n\r\n\r\nWhen modeling the relationship between the discrepancy in self-evaluation and evaluation by others and individual personality traits, I employed a generalized linear model with a Gaussian link function, and fitted it using the brms package for Bayesian modeling with its default priors. The results were as follows:\r\n\r\n\r\n\r\nPeople with higher ambition (assertive, competitive, goal-oriented) and higher sociability (outgoing, talkative, socially confident) tend to overrate their skills‚Äîand vice versa.\r\nPeople with a stronger learning approach (curious, improvement-focused, achievement-driven) tend to underrate their skills‚Äîand vice versa.\r\nThere‚Äôs some, though not very strong, evidence that people with high adjustment (calm, resilient, stress-tolerant, and less receptive to feedback) also tend to overrate their skills‚Äîand vice versa.\r\nTo better understand the effects found, take a look at the scatter plots with fitted regression lines below:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThese results are, IMO, pretty intuitive and not all that surprising‚Äîat least in hindsight üòâ‚Äîthough I personally expected a stronger effect of adjustment and inquisitive traits. What would be your take on it? Have you done a similar type of analysis? If so, what were your results?\r\nCaveat: The sample used was very small (just 72 people!) and highly specific‚Äîmanagers from a single company, industry, country, and region‚Äîso the results can‚Äôt be easily generalized to a broader population. Still, IMO, the findings offer some useful hints about when we should be more mindful of the possibility of misjudging our self-evaluation, whether by overestimating or underestimating ourselves. That said, we shouldn‚Äôt forget that overly optimistic self-deception can sometimes be useful, in the ‚Äúfake it till you make it‚Äù spirit üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-01-20-self-awareness-and-personality/./density_plots.png",
    "last_modified": "2025-01-21T09:10:04+01:00",
    "input_file": "self-awareness-and-personality.knit.md",
    "preview_width": 5500,
    "preview_height": 5000
  },
  {
    "path": "posts/2025-01-17-personality-and-cultural-values/",
    "title": "We are probably more similar - and different - than expected",
    "description": "A brief summary of the key findings from the research paper on the multilevel exploration of Big Five personality traits and cultural values.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-01-17",
    "categories": [
      "personality",
      "culture values",
      "multilevel modeling",
      "intercultural psychology"
    ],
    "contents": "\r\nAn interesting research paper by Stackhouse et al.¬†(2024) looked at the sources of variance in people‚Äôs Big Five personality and Hofstede cultural values (individual-level vs nation-level differences), and at the relationship between these two types of individual characteristics, while taking into account the multi-level nature of the data.\r\nThe authors used intraclass correlation (ICC1) to decompose the source of variance in these two constructs. Unsurprisingly, they found that personality only shows small variability that can be attributed to differences between nations (for individual traits, it ranged between 3.1% and 11%). However, surprisingly, similar results were found also for cultural values (from 3.6% to 9.7%)! We thus differ in our cultural values within nations more than we would have expected.\r\n\r\n\r\n\r\nTo assess the strength and direction of the association between personality and cultural values characteristics, the authors used multilevel correlation analysis and found that the correlations are much weaker than those suggested by previous studies that used only aggregated, nation-level data. The top three correlations - between uncertainty avoidance and agreeableness (r = 0.25), gender egalitarianism and agreeableness (r = ‚àí0.23), and teamwork preference and extraversion (r = 0.20) - made it only into the ‚Äòweak effect‚Äô area.\r\n\r\n\r\n\r\nMultilevel associations between the Five Factor Model and cultural values.\r\nAll this suggests that the next time you feel an urge to guess someone‚Äôs personality or cultural values from their nationality, or to infer one from another, think of this paper and maybe try to base your guess on other information about them üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-01-17-personality-and-cultural-values/./correlations.jpg",
    "last_modified": "2025-01-20T10:27:30+01:00",
    "input_file": "personality-and-cultural-values.knit.md"
  },
  {
    "path": "posts/2025-01-15-managerial-quality/",
    "title": "Unexpected protective effect of having a good manager?",
    "description": "Imagine that your direct manager recently left your company voluntarily, and she was among those who excelled in her managerial duties. Do you think that her above-average managerial skills would make you more or less likely to leave the company in the following months?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-01-15",
    "categories": [
      "people management",
      "employee attrition",
      "manager",
      "causal analysis",
      "heterogeneity"
    ],
    "contents": "\r\nUntil now, based on my personal experience and common sense, I would have been quite certain that it would increase the odds of a direct report‚Äôs departure. After all, we like good bosses so much that we might follow them ‚Äì even outside our current company, right?\r\nSo I was quite surprised when I examined a real-world dataset and checked the impact of managers‚Äô voluntary departure on employees‚Äô probability of leaving, in interaction with manager quality as measured by multi-rater feedback, while controlling for some typical organizational confounders.\r\nAs expected, a manager‚Äôs departure increased the odds of an employee leaving later. However, managerial quality actually had a protective effect and decreased the employee‚Äôs risk of leaving when their manager left. Interestingly, more ‚Äútechnical‚Äù skills ‚Äì such as communicating expectations, goal setting, or providing feedback ‚Äì seemed to be more important in this respect than ‚Äúsofter‚Äù skills ‚Äì like coaching, psychological safety, or wellbeing support.\r\nNow, besides cross-validating the results, I am trying to understand the potential reasons (including missed confounders) behind this pattern. What would be your favorite hypotheses to test?\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-01-15-managerial-quality/./mng_attrition_illustration.png",
    "last_modified": "2025-01-15T20:35:30+01:00",
    "input_file": "managerial-quality.knit.md",
    "preview_width": 1029,
    "preview_height": 618
  },
  {
    "path": "posts/2025-01-09-self-leadership/",
    "title": "Self-Leadership: A New Superpower?",
    "description": "In today‚Äôs world of hybrid work and freelancing, people can no longer rely solely on external supervision. Instead, they need to build and cultivate self-leadership ‚Äî the skill to guide themselves toward better performance and personal fulfillment by actively influencing their thoughts, emotions, and actions.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2025-01-09",
    "categories": [
      "self-leadership",
      "performance",
      "flexible work",
      "meta-analysis"
    ],
    "contents": "\r\nA recent meta-analysis by Knotts et al.¬†(2022), covering 57 studies with over 16K participants, put some numbers on just how important this skill may be when it comes to improving individual outcomes. Here are some of the key findings:\r\nPretty strong effect: The overall relationship between self-leadership and individual outcomes is œÅ = .38. It‚Äôs even stronger when it comes to creativity and innovation (œÅ = .49) compared to basic task performance (œÅ = .28).\r\nIt‚Äôs not just about actions ‚Äî it‚Äôs about mindset too: Behavioral strategies like goal-setting are helpful, but adding cognitive strategies like self-talk and mental imagery makes a big difference. People who use a combo of both have stronger results (œÅ = .42) than those who focus only on behaviors (œÅ = .27).\r\nSelf-efficacy is a key mediator: The meta-analysis also found that self-leadership boosts self-efficacy ‚Äî confidence in one‚Äôs ability to succeed. This, in turn, leads to better outcomes like job satisfaction, work engagement, and creativity.\r\n\r\n\r\n\r\nCaveat: Most of the studies included were correlational, meaning they show associations but don‚Äôt prove that self-leadership directly causes better performance.\r\nAnd here‚Äôs some food for thought to end on: With companies implementing flexible work policies, it may be valuable for them to consider self-leadership as part of employee selection criteria. Do you already take this into account? If so, how do you assess this skill in candidates? Asking for a friend üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-01-09-self-leadership/./path_model.png",
    "last_modified": "2025-01-09T17:19:35+01:00",
    "input_file": "self-leadership.knit.md",
    "preview_width": 856,
    "preview_height": 682
  },
  {
    "path": "posts/2024-12-30-new-years-resolution/",
    "title": "Stop. Start. Continue.",
    "description": "A brief, year-end reflection on the 'what,' 'why,' and 'how' behind our personal growth journeys.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-12-30",
    "categories": [
      "change",
      "resolution",
      "development",
      "google trends"
    ],
    "contents": "\r\nAs we enter this part of the year when the ‚Äòfresh start‚Äô effect can kick in and help us make some desirable changes in our lives, I began to wonder what types of changes people are most inclined to pursue. One way to approach this question is through the well-known Stop, Start, Continue framework, commonly used in employee feedback and development.\r\nLooking at Google Trends data and examining the frequency of search terms related to these three actions, it seems that people most often search for ways to stop doing things, followed by how to start doing things, and least often, how to continue doing things.\r\n\r\n\r\n\r\nAny hypotheses as to why this might be the case? Here are my ‚Äòthree cents‚Äô, based on my personal experience:\r\nThe things we want to stop doing may feel more urgent because we are already experiencing their negative impact in our lives.\r\nStopping certain behaviors can be perceived as more challenging, as it often requires overcoming deeply ingrained habits formed through repeated actions ‚Äì habits we tend to revert to, especially under stress or cognitive load. Or, as Mark Twain put it, ‚ÄúIt is easier to stay out than to get out.‚Äù\r\nThe availability heuristic might play a role, as behaviors we already engage in are more easily brought to mind for reflection.\r\nMore importantly, whatever stops, starts, and continues make it onto your New Year‚Äôs resolution list ‚Äì and for whatever reason ‚Äì I wish you success in achieving them! üññüôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-12-30-new-years-resolution/./google_trends.png",
    "last_modified": "2025-01-05T18:38:26+01:00",
    "input_file": "new-years-resolution.knit.md",
    "preview_width": 1065,
    "preview_height": 330
  },
  {
    "path": "posts/2024-12-15-brexit-uk-eurozone/",
    "title": "Divorce usually impacts both sides - but does that hold true for Brexit?",
    "description": "We can often come across analyses highlighting the negative effects of Brexit on the UK‚Äôs economic performance as measured by GDP. But what about the reverse perspective? How has Brexit impacted the economies of EU member countries?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-12-15",
    "categories": [
      "causal inference",
      "synthetic control",
      "causalpy",
      "brexit",
      "eurozone",
      "oecd",
      "gdp"
    ],
    "contents": "\r\nThis gap in the conversation provides a great opportunity to try out a causal inference method from the CausalPy package, specifically, the synthetic control method (with the caveat that its implementation is still a wip, so the outputs should be taken with a grain of salt). Generally, this technique constructs a weighted combination of control units to mimic the characteristics of a treated unit, enabling us to estimate the causal effect of an intervention without a traditional control group.\r\nFor the analysis, I looked at the combined GDP of 20 eurozone countries from 2007 to 2023 as the outcome of interest. To construct the synthetic control group for counterfactual comparison, I used GDP data from selected OECD countries (non-EU, non-eurozone, non-UK) over the same period.\r\n\r\n\r\n\r\nTo my surprise, the results for the eurozone countries contrast pretty starkly with those for the UK (see the charts above). There doesn‚Äôt seem to be strong evidence of a significant negative impact from Brexit on eurozone GDP. Of course, this is just one facet of Brexit‚Äôs potential effects. There may be many other consequences - beyond what GDP metrics reveal - that apply to the eurozone as well. Still, it‚Äôs interesting to observe such a stark difference using the same type of data and methodology.\r\nMaybe it reflects what we often see in personal relationships: divorce usually affects both sides, but the costs are only rarely distributed equally and/or in the same ways ü§î\r\nP.S. If interested, the data from the OECD Data Explorer and the code for replicating this analysis are provided below.\r\n\r\n\r\nShow code\r\n\r\n# reticulate library for running Python in .Rmd file\r\nlibrary(reticulate)\r\n# libraries for data manipulation and visualization\r\nlibrary(tidyverse)\r\nlibrary(DT)\r\n\r\n# uploading the data from the OECD Data Explorer at https://data-explorer.oecd.org/\r\ndf <- readr::read_csv(\"oecd_data.csv\")\r\n\r\n# table view of two rows of the data\r\nDT::datatable(\r\n  df,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 2, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy', 'csv', 'excel'),\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n# uploading libraries for data manipulation, visualization, and causal inference\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport arviz as az\r\nimport causalpy as cp\r\naz.style.use(\"arviz-white\")\r\n\r\n# uploading the data from the OECD Data Explorer at https://data-explorer.oecd.org/\r\ndata = pd.read_csv('oecd_data.csv')\r\ndata.info()\r\ndata.head()\r\n\r\n# keeping only relevant variables\r\ndata = data[[\r\n    'Reference area',\r\n    'TIME_PERIOD',\r\n    'OBS_VALUE'\r\n]]\r\n\r\n# cleaning the quarter column\r\n# function to calculate the last day of the quarter\r\ndef quarter_to_date(q):\r\n    year, quarter = q.split('-')\r\n    quarter_month_map = {\r\n        'Q1': '03-31',\r\n        'Q2': '06-30',\r\n        'Q3': '09-30',\r\n        'Q4': '12-31'\r\n    }\r\n    return f\"{year}-{quarter_month_map[quarter]}\"\r\n\r\ndata['TIME_PERIOD'] = data['TIME_PERIOD'].apply(quarter_to_date)\r\ndata['TIME_PERIOD'] = pd.to_datetime(data['TIME_PERIOD'], format='%Y-%m-%d')\r\n\r\n# cleaning the country names\r\ndata['Reference area'] = data['Reference area'].str.replace('- ', '', regex=True).str.replace(' ', '_', regex=True).str.replace(r'[()]', '', regex=True)\r\n\r\n# putting data on the trillion/billion (10^12) unit scale\r\ndata['OBS_VALUE'] = data['OBS_VALUE']/1000000\r\n\r\n# getting data from long to wide format\r\nmelted_df = data.melt(id_vars=['Reference area', 'TIME_PERIOD'], var_name='Variable', value_name='Value')\r\npivoted_df = melted_df.pivot(index='TIME_PERIOD', columns='Reference area', values='Value')\r\npivoted_df.info()\r\n\r\n# removing countries with missing values in the time series data\r\npivoted_df.drop(columns=['Saudi_Arabia', 'Russia'], inplace=True)\r\n\r\n# removing row for date where we have data for only one country\r\npivoted_df = pivoted_df[pivoted_df.index != pd.to_datetime('2024-09-30')]\r\n\r\n# identifying countries (that didn't left EU) GDP of which most strongly correlated with UK's GDP before actual Brexit (2020 January 31)\r\ncorrelation_df = pivoted_df[pivoted_df.index <= pd.to_datetime(\"2020 January 31\")].corr()\r\nprint(correlation_df[['United_Kingdom']].drop('United_Kingdom').sort_values(by='United_Kingdom', ascending=False))\r\n\r\n\r\n\r\n# Analysis 1: The impact of Brexit on UK GDP\r\n\r\n# target country of interest\r\ntarget_country = 'United_Kingdom'\r\n# control countries (top 15)\r\nother_countries = [\r\n    \"United_States\",\r\n    \"Costa_Rica\",\r\n    \"Germany\",\r\n    \"New_Zealand\",\r\n    \"Sweden\",\r\n    \"Austria\",\r\n    \"Luxembourg\",\r\n    \"Denmark\",\r\n    \"Belgium\",\r\n    \"Korea\",\r\n    \"Canada\",\r\n    \"Lithuania\",\r\n    \"France\",\r\n    \"India\",\r\n    \"Estonia\"\r\n]\r\n\r\n# formula\r\nformula = target_country + \" ~ \" + \"0 + \" + \" + \".join(other_countries)\r\nprint(formula)\r\n\r\n# modeling data preparation\r\ntreatment_time = pd.to_datetime(\"2020 January 31\")\r\nmodeling_data = pivoted_df.copy()\r\nmodeling_data = modeling_data[[target_country] + other_countries]\r\nmodeling_data.dropna(axis='rows', inplace=True, how='all')\r\n\r\n# fitting the model\r\nresult = cp.SyntheticControl(\r\n    modeling_data,\r\n    treatment_time,\r\n    formula=formula,\r\n    model=cp.pymc_models.WeightedSumFitter(\r\n        sample_kwargs={\r\n        \"random_seed\": 2024, \r\n        \"draws\": 10000,\r\n        \"tune\": 5000, \r\n        \"target_accept\": 0.99,\r\n        \"chains\": 4 \r\n      }\r\n    )\r\n)\r\n\r\n# results summary\r\naz.summary(result.idata, var_names=[\"~mu\"])\r\n\r\n# visualization of the posterior samples\r\naz.plot_trace(result.idata, var_names=\"~mu\", compact=False)\r\nplt.show()\r\n\r\n# visualization of the estimated causal effect\r\nfig, ax = result.plot(plot_predictors=False)\r\nfor i in [0, 1, 2]:\r\n    ax[i].set(ylabel=\"Trillion USD\")\r\nplt.show()\r\n\r\n\r\n\r\n\r\n# Analysis 2: The impact of Brexit on the GDP of eurozone countries\r\n\r\n# identifying countries (non-EU, non-eurozone, non-UK) GDP of which most strongly correlated with eurozone countries' GDP before actual Brexit (2020 January 31)\r\ncorrelation_df = pivoted_df[pivoted_df.index <= pd.to_datetime(\"2020 January 31\")].corr()\r\nprint(correlation_df[['Euro_area_20_countries']].drop('Euro_area_20_countries').sort_values(by='Euro_area_20_countries', ascending=False))\r\n\r\n# target country of interest\r\ntarget_country = 'Euro_area_20_countries'\r\n# control countries (top 12)\r\nother_countries = [\r\n    \"Costa_Rica\",\r\n    \"New_Zealand\",\r\n    \"United_States\",\r\n    \"India\",\r\n    \"Korea\",\r\n    \"Colombia\",\r\n    \"Canada\",\r\n    \"Israel\",\r\n    \"Iceland\",\r\n    \"Switzerland\",\r\n    \"Indonesia\",\r\n    \"Australia\"\r\n]\r\n\r\n# formula\r\nformula = target_country + \" ~ \" + \"0 + \" + \" + \".join(other_countries)\r\nprint(formula)\r\n\r\n# modeling data preparation\r\ntreatment_time = pd.to_datetime(\"2020 January 31\")\r\nmodeling_data = pivoted_df.copy()\r\nmodeling_data = modeling_data[[target_country] + other_countries]\r\nmodeling_data.dropna(axis='rows', inplace=True, how='all')\r\n\r\n# fitting the model\r\nresult = cp.SyntheticControl(\r\n    modeling_data,\r\n    treatment_time,\r\n    formula=formula,\r\n    model=cp.pymc_models.WeightedSumFitter(\r\n        sample_kwargs={\r\n        \"random_seed\": 2024, \r\n        \"draws\": 10000,\r\n        \"tune\": 5000, \r\n        \"target_accept\": 0.99,\r\n        \"chains\": 4 \r\n      }\r\n    )\r\n)\r\n\r\n# results summary\r\naz.summary(result.idata, var_names=[\"~mu\"])\r\n\r\n# visualization of the posterior samples\r\naz.plot_trace(result.idata, var_names=\"~mu\", compact=False)\r\nplt.show()\r\n\r\n# visualization of the estimated causal effect\r\nfig, ax = result.plot(plot_predictors=False)\r\nfor i in [0, 1, 2]:\r\n    ax[i].set(ylabel=\"Trillion USD\")\r\nplt.show()\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-12-15-brexit-uk-eurozone/./brexit.jpg",
    "last_modified": "2024-12-16T09:04:33+01:00",
    "input_file": "brexit-uk-eurozone.knit.md"
  },
  {
    "path": "posts/2024-12-12-managers-and-performance-evaluations/",
    "title": "\"A new broom sweeps clean\"",
    "description": "Does the saying above also apply to new managers and how they evaluate their direct reports?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-12-12",
    "categories": [
      "performance evaluation",
      "performance management",
      "employee feedback",
      "organizational behavior"
    ],
    "contents": "\r\nAs the year comes to a close, my LinkedIn feed is buzzing with posts about performance evaluations - some positive, but mostly calling out the negatives. This, combined with reflecting on one of my own past experiences as a people manager, got me wondering: do managers who are relatively new to their direct reports tend to be stricter (or perhaps more objective?) in their evaluations, on average?\r\nMy hypothesis is based on the assumption that a shorter history with their direct reports allows managers to focus more purely on performance, without the influence of relationship-building efforts, potentially leading to more objective assessments.\r\nOne straightforward way to explore this question is to analyze how employee evaluations vary based on the length of time they‚Äôve worked under their current manager - while controlling for other relevant factors like employees‚Äô and managers‚Äô age, company and position tenure, gender, etc.\r\nLooking at the available data, it seems to support my initial ‚Äúsuspicion.‚Äù The effect is there but quite small - specifically, there‚Äôs only a 2.5% increase in the odds of being in a higher performance category for each additional year spent working under the same manager (95% CI [1.4%, 3.5%]). However, when compounded over four years - a realistic scenario for the average employee - this results in a more noticeable 10.3% increase.\r\n\r\n\r\n\r\nWhile this sheds some light on my original question, it‚Äôs worth noting that this pattern could still be explained by alternative mechanisms. For instance, over time, direct reports‚Äô performance might better align with their manager‚Äôs expectations as they incorporate their feedback. Another possible explanation is a selection effect, where managers are more likely to retain those they consider to be high performers in their teams over time. So, the quest can happily continue üòâ\r\nCurious if anyone has practical experiences with systematically studying the mechanisms behind managers‚Äô evaluations of their direct reports‚Äô performance. If so, I‚Äôd love to hear your thoughts or suggestions - feel free to share them in the comments or drop me a message via email or LinkedIn.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-12-12-managers-and-performance-evaluations/./performance_eval_estimates.png",
    "last_modified": "2024-12-12T09:49:52+01:00",
    "input_file": "managers-and-performance-evaluations.knit.md",
    "preview_width": 5500,
    "preview_height": 3000
  },
  {
    "path": "posts/2024-11-21-instrumental-and-expressive-networks/",
    "title": "Not all workplace relationships are created equal when it comes to retaining talent",
    "description": "A brief sneak peek into an interesting and potentially useful meta-analysis that explored the specific mechanisms through which two types of workplace networks are related to employee turnover.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-11-21",
    "categories": [
      "ona",
      "networks",
      "employee turnover",
      "meta-analysis"
    ],
    "contents": "\r\nI recently came across an interesting and potentially useful meta-analysis by Porter et al.¬†(2018) that used meta-analytic path analysis to explore specific mechanisms through which two types of workplace networks are related to employee turnover:\r\nInstrumental networks: These are job-related connections that provide resources like advice, expertise, or work-related information, built through initiatives like knowledge-sharing programs, access to key stakeholders, and attending external conferences.\r\nExpressive networks: These are connections with focus on emotional support, friendships, and social bonding, developed through activities like company social events, cross-team job rotations, and peer mentorship programs.\r\n\r\n\r\n\r\nThe study found that both network types are connected with turnover but through different mechanisms. Instrumental ties reduce turnover by improving job performance (thanks to access to work-related resources) and strengthening organizational commitment. Meanwhile, expressive ties lower turnover by increasing job satisfaction (through emotional support and social bonds) and boosting organizational commitment.\r\nWhat also stood out: Expressive network ties appear to be nearly three times as ‚Äústicky‚Äù (95% CI: [-0.60, -0.37]) as instrumental ties (95% CI: [-0.23, -0.07]) based on simple bivariate correlations. Moreover, after accounting for turnover antecedents such as job performance and satisfaction, only expressive ties maintained a negative association with turnover (b = -0.47, SE = 0.02).\r\nWhile there are some limitations to consider - such as the relatively small number of studies, variability in how networks were measured, sampling bias, or missing contextual moderators - these estimates may still be useful for setting priors when considering the shape of retention strategies before conducting a similar analysis on your own data. Meanwhile, with only these results in hand, we might assume that, while instrumental ties help employees succeed in their jobs, they don‚Äôt generate the same level of ‚Äòstickiness‚Äô as expressive ties.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-11-21-instrumental-and-expressive-networks/./model_to_be_tested.png",
    "last_modified": "2024-11-21T11:05:11+01:00",
    "input_file": "instrumental-and-expressive-networks.knit.md",
    "preview_width": 1150,
    "preview_height": 739
  },
  {
    "path": "posts/2024-11-13-chatgpt-emails-and-causalpy/",
    "title": "ChatGPT as a new email writing coach?",
    "description": "Using CausalPy package to test the plausibility of the hypothesis that Stack Overflow may not be the only ‚Äúvictim‚Äù of ChatGPT and the GenAI likes.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-11-13",
    "categories": [
      "email communication",
      "genai",
      "chatgpt",
      "llm",
      "google trends",
      "causal inference",
      "causalpy",
      "python"
    ],
    "contents": "\r\nI wanted to try out the new CausalPy package for causal inference and was brainstorming interesting research questions to apply it to. My inspiration came from a recent wave of LI posts in my feed, all sharing a chart showing a drop in Stack Overflow traffic after ChatGPT‚Äôs release in November 2022.\r\n\r\n\r\n\r\nGiven that many studies - including one we conducted internally at Sanofi - show that one of the most frequent use cases for GenAI in the business world is helping with email writing (we‚Äôve probably all seen the hilarious meme on this topic shown below üòÅ), I got curious about whether a similar pattern might show up in Google Trends for searches like ‚ÄúHow to write an email.‚Äù\r\n\r\n\r\n\r\nFor the analysis, I used an interrupted time series method, which examines the effect of an intervention by comparing time series data before and after the intervention that happened at a known point in time, allowing us to assess any shifts in level or trend.\r\nAs the resulting charts below show, there does seem to be a noticeable drop in searches on how to write emails following ChatGPT‚Äôs official release. In fact, the shift is so pronounced that it‚Äôs clear even with a quick eyeballing analysis. Sure, we can speculate about other factors to be involved - maybe we‚Äôre sending fewer emails because we‚Äôre relying more on platforms like Slack or Teams - but as other available stats suggest (see, for example, this one), we can only wish that were the case üòâ\r\n\r\n\r\n\r\nEither way, CausalPy turned out to be a super easy-to-use and user-friendly tool. I‚Äôm looking forward to using it in my future projects. Kudos to the entire CausalPy team! üëè\r\nP.S. To replicate the analysis, you can use the Python code below.\r\n\r\n\r\nShow code\r\n\r\n# uploading the reticulate package for running Python in .Rmd file\r\nlibrary(reticulate)\r\n\r\n\r\n\r\n\r\nShow code\r\n# uploading libraries\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom pytrends.request import TrendReq\r\nimport matplotlib.pyplot as plt\r\nimport arviz as az\r\nimport causalpy as cp\r\n\r\n# getting global Google Trends data for a key phrase of interest\r\n# initializing the TrendReq object\r\npytrends = TrendReq(hl='en-US', tz=360, requests_args={'verify': False})\r\n\r\n# specifying keyword and time range\r\nkeywords = ['How to write email']\r\ntimeframe = '2020-01-01 2024-10-31'\r\n\r\n# creating payload and fetching data\r\npytrends.build_payload(keywords, cat=0, timeframe=timeframe, geo='', gprop='')\r\ndata = pytrends.interest_over_time()\r\n\r\n# data overview\r\n# data.head()\r\n# data.info()\r\n\r\n# creating month variable for capturing seasonality in the data\r\ndata['month'] = data.index.month\r\n\r\n# creating time variable\r\ndata['t'] = np.arange(len(data))\r\n\r\n# renaming target variable\r\ndata.rename(columns={'How to write email': 'y'}, inplace=True)\r\n\r\n# saving the data for later usage\r\n# data.to_csv('google_trends_data.csv', index=True)\r\n\r\n# specifying the date of intervention\r\ntreatment_time = pd.to_datetime(\"2022-10-30\")\r\n\r\n# specifying and fitting the model\r\nseed = 2024\r\nresult = cp.InterruptedTimeSeries(\r\n    data,\r\n    treatment_time,\r\n    formula=\"y ~ 1 + t + C(month)\",\r\n    model=cp.pymc_models.LinearRegression(\r\n      sample_kwargs={\r\n        \"random_seed\": seed, \r\n        \"draws\": 5000,\r\n        \"tune\": 1000, \r\n        \"chains\": 4 \r\n      }\r\n    ),\r\n)\r\n\r\n# summary of the fitted model\r\nresult.summary()\r\n\r\n# plotting the results\r\nfig, ax = result.plot()\r\nplt.show()\r\n\r\n# summary statistics of the causal impact over the entire post-intervention period\r\naz.summary(result.post_impact.mean(\"obs_ind\"))\r\n\r\n# summary statistics of the cumulative causal impact\r\n# getting index of the final time point\r\nindex = result.post_impact_cumulative.obs_ind.max()\r\n# grabbing the posterior distribution of the cumulative impact at this final time point\r\nlast_cumulative_estimate = result.post_impact_cumulative.sel({\"obs_ind\": index})\r\n# getting summary stats\r\naz.summary(last_cumulative_estimate)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-11-13-chatgpt-emails-and-causalpy/./output_plot.png",
    "last_modified": "2024-11-21T11:04:26+01:00",
    "input_file": "chatgpt-emails-and-causalpy.knit.md",
    "preview_width": 4558,
    "preview_height": 5058
  },
  {
    "path": "posts/2024-11-11-culture-500/",
    "title": "How does your company stack up in the Big Nine Cultural Values?",
    "description": "A quick intro to the Culture 500 - a tool for assessing company culture using Glassdoor reviews.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-11-11",
    "categories": [
      "culture",
      "employee listening",
      "glassdoor",
      "nlp"
    ],
    "contents": "\r\nI recently came across the Culture 500 - an interesting tool created by MIT Sloan Management Review and CultureX, which offers a data-driven evaluation of corporate culture across the Big Nine Cultural Values as assessed through analysis of over 1 million employee reviews on Glassdoor. The model used includes the following nine culture values:\r\nAgility: Employees can respond quickly and effectively to changes in the marketplace and seize new opportunities.\r\nCollaboration: Employees work well together within their team and across different parts of the organization.\r\nCustomer: Employees put customers at the center of everything they do, listening to them and prioritizing their needs.\r\nDiversity: The company promotes a diverse and inclusive workplace where no one is disadvantaged because of their gender, race, ethnicity, sexual orientation, religion, or nationality.\r\nExecution: Employees are empowered to act, have the resources they need, adhere to process discipline, and are held accountable for results.\r\nInnovation: The company pioneers novel products, services, technologies, or ways of working.\r\nIntegrity: Employees consistently act in an honest and ethical manner.\r\nPerformance: The company rewards results through compensation, informal recognition, and promotions, and deals effectively with underperforming employees.\r\nRespect: Employees demonstrate consideration and courtesy for others, and treat each other with dignity.\r\nThe tool‚Äôs primary chart visualizes each company‚Äôs results by showing, in standard deviations, the frequency of mentions and the sentiment toward each cultural value discussed in employee reviews. The resulting 2x2 matrix provides a clear dataviz that aids in identifying a company‚Äôs core strengths, critical weaknesses, emerging positives, and under-the-radar issues (see the example below).\r\nWith a standardized scale, one can quickly see how their company‚Äôs culture compares to that of other companies across individual values. The tool also allows side-by-side comparisons, helping leaders benchmark against both industry standards and standout peers (see the example below).\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis tool can be a nice complement to an internal employee listening program, giving an outside perspective. That said, if your listening program is on point, the results shouldn‚Äôt come as a big surprise üòâ Also could be a helpful resource for anyone weighing job offers and looking to understand a company‚Äôs culture before joining.\r\n‚ö†Ô∏è When using the tool, one should keep in mind its limits, like potential review bias, limited cross-industry comparability, constraints in capturing nuanced sentiments and emerging cultural dimensions, the freshness of data reflecting real-time changes, dependency on Glassdoor‚Äôs review policies, focus restricted to nine core values, and exclusion of global employee perspectives. And as its name suggests, it includes results for only around 500 leading companies.\r\nTo learn more about the methodology behind the tool, check out this site.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-11-11-culture-500/./chart2.png",
    "last_modified": "2024-11-12T10:36:40+01:00",
    "input_file": "culture-500.knit.md",
    "preview_width": 1309,
    "preview_height": 763
  },
  {
    "path": "posts/2024-10-30-xmr-charts-in-people-analytics/",
    "title": "Do you use XmR charts for People Analytics use cases?",
    "description": "Looking for tips from those who answered \"Yes\" to the question in the title of this post üôÇ",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-10-30",
    "categories": [
      "xmr chart",
      "signal and noise",
      "statistical inference",
      "data vizualization"
    ],
    "contents": "\r\nI‚Äôm curious if anyone in People Analytics space has experience - good, bad, or a bit of both - using XmR charts for tracking and interpreting HR-related metrics and guiding follow-up process improvements, especially among non-technical end users.\r\nI stumbled upon this interesting tool, originally rooted in statistical process control in industrial engineering and manufacturing, through posts by Frank Corrigan, and recently dug a bit deeper into it through a series of email lectures from Commoncog.\r\nFrom the get-go, it struck me as a potentially effective way for HR or business people - who may not have deep data expertise - to become more data-driven, as it simplifies the task of separating signal from noise and makes it straightforward for even laypeople to determine whether changes in metrics are due to real shifts or just random fluctuations. In effect, it helps users address the ‚ÄúSo what?‚Äù question by validating when data points signify something meaningful, like a trend, shift, or outlier that warrants further action or adjustment.\r\nAs the attached charts illustrate (based on a sample of dummy data tracking internal mobility), it is pretty intuitive to spot if a metric should (or should not) require further investigation into the process behind it, or whether a change introduced at the start of 2024 is actually impacting the numbers. All you really need to do is keep an eye on the limit lines and follow some basic rules, such as:\r\nInvestigate when a point is outside the limit lines.\r\nTake a closer look if three out of four successive points are near the limits rather than the center.\r\nIf eight consecutive points appear on one side of the average, that may signal a meaningful shift.\r\n\r\n\r\n\r\nWould love to hear any experiences you might‚Äôve had with XmR charts in the PA space! üôè\r\nP.S. The attached charts were created with Xmrit, a free online tool from Commoncog, whose website also has more useful information about this analytical tool.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-10-30-xmr-charts-in-people-analytics/./charts.png",
    "last_modified": "2024-10-30T11:18:09+01:00",
    "input_file": "xmr-charts-in-people-analytics.knit.md",
    "preview_width": 888,
    "preview_height": 652
  },
  {
    "path": "posts/2024-10-29-job-personality-fit/",
    "title": "Do people‚Äôs personalities vary across different jobs?",
    "description": "And if they do, are those differences big enough to be useful for career counseling and personal development?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-10-29",
    "categories": [
      "personality",
      "job-person fit",
      "career counseling"
    ],
    "contents": "\r\nOver the years, it seems I‚Äôve gravitated toward a job family that aligns well with my personality - or at least, that‚Äôs how it appears when you find job choices of other similarly-minded people as a validation of your own choices üòâ\r\nAttached, you can see one of my results from an online tool that measures the Big Five personality traits and compares them with the personality profiles of individuals across various occupations.\r\n\r\n\r\n\r\nIt‚Äôs based on research by Anni, Vainik, & M√µttus (2024), which examined personality trait patterns across 263 occupations and yielded following key insights:\r\nCertain personality traits align predictably with specific jobs, supporting the person‚Äìjob fit model. For instance, openness is more common in creative fields, while conscientiousness tends to be higher in managerial roles.\r\nOccupations account for 2%‚Äì7% of the variance in Big Five personality traits, suggesting modest but meaningful differences across roles.\r\nOccupations with high average levels of traits linked to job performance (e.g., conscientiousness) show greater homogeneity in these traits, indicating selective environments.\r\nThe results align with previous studies across different regions, suggesting that these occupational-personality trends are broadly applicable across cultures.\r\nDetailed personality traits beyond the Big Five (e.g., specific aspects of extraversion or conscientiousness) explain even more variance, illustrating nuanced differences among job incumbents.\r\nThese findings suggest that while personality explains only part of the variation among occupations, it may still offer valuable insights for career counseling and personal development.\r\nIf you‚Äôd like to try the tool yourself, here‚Äôs the link.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-10-29-job-personality-fit/./results_screenshot.jpg",
    "last_modified": "2024-10-29T16:36:09+01:00",
    "input_file": "job-personality-fit.knit.md"
  },
  {
    "path": "posts/2024-10-26-personality-and-non-linearities/",
    "title": "Nonlinear relationships between personality traits and business outcomes seem to be the norm rather than the exception",
    "description": "... at least in my projects üòâ",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-10-26",
    "categories": [
      "personality",
      "prediction",
      "nonlinearity",
      "machine learning",
      "interpretable ml",
      "hogan",
      "hpi"
    ],
    "contents": "\r\nI recently came across a post discussing research that highlights how the prediction power of personality traits in relation to business outcomes may be underestimated because most studies rely on linear analytical methods, which, unlike many ML algorithms like Random Forest or XGBoost, fail to capture and utilize nonlinear signals in the data.\r\nThis led me to reflect that maybe it‚Äôs not just the prediction power that suffers from this limitation. The true nature and shape of these relationships are likely misunderstood as well as a result of relying on linear methods. We‚Äôre used to reading about positive or negative correlations between personality traits and various outcomes, but the reality is probably more complex and nuanced.\r\nFor a quick visual, I‚Äôve attached charts from one of my recent projects, illustrating how the top three personality trait predictors, as measured by the HPI, relate to employee performance across two competencies, as modeled by XGBoost and visualized using Partial Dependence Plots - a global ML interpretation tool. Notice how these relationships aren‚Äôt simply positive or negative but show clear non-linearities.\r\n\r\n\r\n\r\nWith interpretable ML, exploring these nuances becomes pretty straightforward. If you haven‚Äôt dived into these waters yet, give it a try - you might be surprised (or not) by what you find üòâ\r\nP.S. Below is a snippet of R code if you‚Äôd like to start experimenting with your own data.\r\n\r\n\r\nShow code\r\n\r\n# libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(recipes)\r\nlibrary(tidymodels)\r\nlibrary(xgboost)\r\nlibrary(vip)\r\nlibrary(pdp)\r\n\r\n# uploading data\r\ndata <- read_xlsx(\"your_data.xlsx\")\r\n\r\n# setting the target and predictors\r\ntarget <- \"Agility_and_Initiative\"\r\npredictors <- c(\"Adjustment\", \"Ambition\", \"Sociability\", \"Interpersonal_Sensitivity\", \"Prudence\", \"Inquisitive\", \"Learning_Approach\")\r\n\r\n# specifying the formula\r\nfmla <- as.formula(paste(target, \" ~ .\"))\r\n\r\n# dropping rows without the target\r\npredDf <- mydata %>%\r\n  drop_na(target) %>%\r\n  select(all_of(c(target, predictors)))\r\n\r\n# defining recipe for data preparation\r\nrecipe_spec <- recipe(fmla, data = predDf) %>%\r\n  step_impute_knn(all_predictors(), neighbors = 3) %>%\r\n  step_dummy(all_nominal_predictors())\r\n\r\n# specifying the model and running the whole pipeline\r\nset.seed(1234)\r\nworkflow_fit_xgb <- workflow() %>%\r\n  add_model(boost_tree(mode = \"regression\") %>% set_engine(\"xgboost\")) %>%\r\n  add_recipe(recipe_spec) %>%\r\n  fit(predDf)\r\n\r\n# identifying and visualizing the most important predictors\r\nworkflow_fit_xgb$fit$fit$fit %>% \r\n  vi() %>%\r\n  ggplot(aes(x =  fct_reorder(Variable, Importance), y = Importance)) +\r\n  geom_bar(stat = \"identity\", fill = \"#003d73\", color = \"#003d73\") +\r\n  coord_flip() +\r\n  labs(\r\n    y = \"IMPORTANCE\",\r\n    x = \"PREDICTORS\",\r\n    title = str_glue(\"Strength of predictors of {target}\"),\r\n    caption = \"\\nXGBoost algorithm is used for identification of the most important predictors.\"\r\n  ) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 18, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 14, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 14, lineheight = 16, hjust = 1),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16),\r\n        panel.border = element_rect(color = \"#E0E1E6\", fill = NA),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position = \"top\",\r\n        legend.direction = \"horizontal\",\r\n        legend.justification = c(0, 1),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# exploring the shape of relationship between the target and its predictors using ICE & PDP\r\n\r\n# preparing pre-processed data for partial function \r\npreprocessedData <- recipe_spec %>%\r\n  prep() %>%\r\n  bake(new_data = predDf)\r\n\r\n# function for computation of ICE and PDP\r\npred <- function(object, newdata){\r\n  # Predict and return the vector of predictions, not the mean\r\n  results <- as.numeric(as.data.frame(predict(object, newdata))[[1]])\r\n  return(results)  # Return a vector, not a single number\r\n}\r\n\r\n# function for computation of PDP only\r\n# pred <- function(object, newdata){\r\n#   results <- mean(as.numeric(as.data.frame(predict(object, newdata))[[1]]))\r\n#   return(results)\r\n# }\r\n  \r\n# predictor of interest\r\npredictor <- \"Prudence\"\r\n\r\n# computing ICE\r\ng <- partial(\r\n  object = workflow_fit_xgb,\r\n  pred.var = predictor,\r\n  pred.fun = pred,\r\n  grid.resolution = 50,\r\n  ice=TRUE,\r\n  type = \"regression\",\r\n  train = preprocessedData %>% select(-target),\r\n  plot = FALSE\r\n)\r\n\r\n# computing PDP part (average yhat for individual predictor values)\r\ng2 <- g %>% \r\n  group_by(!!sym(predictor)) %>% \r\n  summarise(avg_yhat = mean(yhat)) %>% \r\n  ungroup()\r\n\r\n# ICE & PDP dataviz\r\nggplot()+\r\n  geom_line(mapping=aes_string(x = predictor, y = \"yhat\", group=\"yhat.id\"), data=g, size = 1.35, color = \"grey\", alpha=0.2) + \r\n  geom_line(mapping=aes_string(x = predictor, y = \"avg_yhat\", group=1), data=g2, size = 1.5, color = \"#340078\", alpha=1) +\r\n  labs(\r\n    x = str_to_upper(str_replace_all(predictor, '_', ' ')),\r\n    y = str_glue(\"{str_to_upper(str_replace_all(target, '_', ' '))} (yhat)\"),\r\n    title = str_glue(\"PDP plot for the {str_to_upper(str_replace_all(predictor, '_', ' '))} predictor\")\r\n  ) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 18, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 14, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 14, lineheight = 16, hjust = 1),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16),\r\n        panel.border = element_rect(color = \"#E0E1E6\", fill = NA),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position = \"top\",\r\n        legend.direction = \"horizontal\",\r\n        legend.justification = c(0, 1),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-10-26-personality-and-non-linearities/./plots.png",
    "last_modified": "2024-10-26T07:44:32+02:00",
    "input_file": "personality-and-non-linearities.knit.md",
    "preview_width": 1174,
    "preview_height": 694
  },
  {
    "path": "posts/2024-10-02-tenure-vs-satisfaction/",
    "title": "Simulating the \"survivorship\" effect in employee satisfaction data over time",
    "description": "How to align observations in organizational data with the results of one research on the relationship between tenure and employee satisfaction?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-10-02",
    "categories": [
      "tenure",
      "employee satisfaction",
      "simulation",
      "r"
    ],
    "contents": "\r\nIn a recent post on LinkedIn, I shared a research paper by Dobrow & Ganzach (2014) that used longitudinal data to disentangle the effects of age and job tenure on employee satisfaction and concluded that people are on average more satisfied with their jobs over their lifetimes, but their satisfaction declines over time within a given company (see the illustrative chart below).\r\n\r\n\r\n\r\nA reader of this post reached out to me and asked how this might relate to the U-shaped pattern they observe in their data: new hires tend to be the most satisfied, then there is a slight decline at ‚Äúmid-company‚Äù age, and then satisfaction rises again.\r\nMy general reply was that this pattern might be a kind of illusion caused by the fact that the more satisfied employees ‚Äúsurvive‚Äù and are more likely to stay longer, and the more dissatisfied ones leave after a few years, so they couldn‚Äôt lower the average satisfaction level of those who stayed longer and are, by definition, more satisfied.\r\nIn support of my point, I also created a simple simulation in R that takes a cohort of 2000 employees who joined the company at the same point in time, each of whom exhibits a more or less steep and noisy downward (or flat) slope of declining satisfaction levels over their years at the company, and a probability of leaving that is non-linearly (sigmoidally) related to their respective satisfaction levels. As can be seen in the attached chart, the resulting pattern closely resembles what might be observed in real data.\r\n\r\n\r\nShow code\r\n\r\n# loading necessary libraries\r\nlibrary(gganimate)\r\nlibrary(tidyverse)\r\n\r\n# setting basic params of the simulation\r\nset.seed(2024)\r\nn_employees <- 2000\r\nyears <- 20\r\ndesired_sd <- 5 \r\n\r\n# creating basic df\r\nemployee_data <- data.frame(\r\n  employee_id = 1:n_employees,\r\n  tenure = 0,\r\n  satisfaction = runif(n_employees, min = 80, max = 100),\r\n  status = \"Active\",\r\n  degree = abs(rnorm(n_employees, mean = 0, sd = desired_sd))\r\n)\r\n\r\n# function to compute leaving probability\r\ncompute_prob_leave <- function(satisfaction, k, midpoint) {\r\n  # lower satisfaction should correspond to higher probability of being cut\r\n  # non-linear/logistic probability\r\n  p_leave <- 1 / (1 + exp(-k * (satisfaction - midpoint)))\r\n  return(p_leave)\r\n}\r\n\r\nmidpoint <- 60  # satisfaction level where probability of cut is 50%\r\nk <- -0.15      # controlling the steepness of the logistic curve\r\n\r\n# just checking the shape of the function determining probability of leaving depending on satisfaction level\r\nvisualize_prob_leave <- function(k, midpoint, satisfaction_range = c(0, 100)) {\r\n  # generating a sequence of satisfaction values\r\n  satisfaction <- seq(from = satisfaction_range[1], to = satisfaction_range[2], length.out = 1000)\r\n  \r\n  # compute probabilities\r\n  prob_leave <- compute_prob_leave(satisfaction, k, midpoint)\r\n  \r\n  # creating a data frame for plotting\r\n  df <- data.frame(satisfaction = satisfaction, probability = prob_leave)\r\n  \r\n  # creating the plot\r\n  p <- ggplot(df, aes(x = satisfaction, y = probability)) +\r\n    geom_line(color = \"blue\", size = 1) +\r\n    geom_vline(xintercept = midpoint, linetype = \"dashed\", color = \"red\") +\r\n    geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\r\n    labs(title = \"Probability of Leaving vs. Satisfaction\",\r\n         x = \"Satisfaction\",\r\n         y = \"Probability of Leaving\") +\r\n    theme_minimal() +\r\n    coord_cartesian(ylim = c(0, 1))\r\n  \r\n  # displaying the plot\r\n  print(p)\r\n}\r\n\r\nvisualize_prob_leave(k, midpoint)\r\n\r\n\r\n# function to update satisfaction level of employees over time\r\nupdate_satisfaction <- function(data, year) {\r\n  data <- data %>%\r\n    mutate(\r\n      # increasing tenure\r\n      tenure = tenure + 1,\r\n      # decreasing satisfaction according to degree parameter and add some noise\r\n      satisfaction = satisfaction - degree + rnorm(n(), mean = 0, sd = 5),\r\n      # ensuring satisfaction is between 0 and 100\r\n      satisfaction = pmin(pmax(satisfaction, 0), 100),\r\n      # computing probability to leave\r\n      prob_leave = compute_prob_leave(satisfaction, k, midpoint),\r\n      # determining employee status\r\n      status = ifelse(runif(n()) <= prob_leave, \"Leave\", \"Active\")\r\n    ) %>%\r\n    # keeping only active employees\r\n    filter(status == \"Active\")\r\n  \r\n  if (nrow(data) > 0) {\r\n    data$year <- year  # assigning the year if there are remaining employees\r\n  } else {\r\n    # if there are no employees left after filtering, creating an empty data frame for consistency\r\n    data <- data.frame(employee_id = integer(0), tenure = integer(0), satisfaction = numeric(0), \r\n                       status = character(0), year = integer(0), degree = numeric(0))\r\n  }\r\n  \r\n  return(data)\r\n}\r\n\r\n# running simulation over 20 years \r\nset.seed(2023)\r\nall_years_data <- list()\r\nall_years_data[[1]] <- employee_data\r\nfor (i in 2:(years + 1)) {\r\n  all_years_data[[i]] <- update_satisfaction(all_years_data[[i - 1]], i - 1)\r\n}\r\n\r\n# combine all data\r\nsimulated_data <- bind_rows(all_years_data) %>% \r\n  drop_na()\r\n\r\n\r\n# plotting with gganimate\r\nplot <- ggplot(simulated_data, aes(x = as.factor(tenure), y = satisfaction)) +\r\n  geom_line(aes(group = employee_id), alpha = 0.25) +\r\n  geom_boxplot(alpha = 0.5, fill = \"grey\", color = \"black\", width = 0.45, outlier.shape = NA) +\r\n  stat_summary(\r\n    fun = mean, geom = \"point\", color = \"#00FFFF\", size = 2.5, shape = 21, fill = \"#00FFFF\"\r\n  ) +\r\n  labs(\r\n    title = 'Satisfaction and departures of 2000 simulated employees over time', \r\n    x = 'TENURE (YEARS)', \r\n    y = 'EMPLOYEE SATISFACTION'\r\n  ) +\r\n  scale_x_discrete() +\r\n  scale_y_continuous(limits = c(0, 100)) +\r\n  theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 16, margin = margin(0, 0, 12, 0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin = margin(0, 0, 0, 0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.line.x = element_line(color = \"lightgrey\"),\r\n    axis.line.y = element_line(color = \"lightgrey\"),\r\n    axis.title.x = element_text(size = 13, color = '#2C2F46', hjust = 0, margin = margin(5, 0, 0, 0)),  \r\n    axis.title.y = element_text(size = 13, color = '#2C2F46', margin = margin(0, 5, 0, 0), hjust = 1),\r\n    axis.text.x = element_text(size = 12, color = '#2C2F46'),\r\n    axis.text.y = element_text(size = 12, color = '#2C2F46'),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\",\r\n    legend.position = \"\",\r\n    plot.background = element_rect(fill = \"white\", color = NA),\r\n    panel.background = element_rect(fill = \"white\", color = NA)\r\n  ) +\r\n  transition_reveal(year)\r\n\r\n# animating the plot and saving it\r\nanim <- animate(plot, nframes = 125, fps = 10, height = 6, width = 11, units = \"in\", res = 200)\r\ngganimate::anim_save(filename = \"employee_tenure_satisfaction.gif\", animation = anim)\r\n\r\n\r\n\r\n\r\n\r\nAlthough this mechanism seems to provide a plausible answer to the reader‚Äôs question, perhaps I have missed something important or there are alternative or complementary explanations. Feel free to share your ideas and suggestions.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-10-02-tenure-vs-satisfaction/./employee_satisfaction_10_200_0.25.gif",
    "last_modified": "2024-10-02T11:56:09+02:00",
    "input_file": "tenure-vs-satisfaction.knit.md"
  },
  {
    "path": "posts/2024-09-20-performance-vs-motivation/",
    "title": "Motivation vs. Performance: What causes what?",
    "description": "An interesting research paper by Wang, Luan & Ma (2024) in Nature explores the causal relationship between work motivation and job performance using longitudinal data from 11 independent studies and meta-analytic structural equation modeling.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-09-20",
    "categories": [
      "performance",
      "motivation",
      "meta-analysis",
      "causality"
    ],
    "contents": "\r\nThe authors of the paper examined four competing hypotheses:\r\nMotivation causes performance.\r\nPerformance causes motivation.\r\nA reciprocal relationship exists.\r\nThey are causally unrelated.\r\nBelow are visual illustrations of arguments for these four hypotheses.\r\n\r\n\r\n\r\nTheir findings suggest that work motivation is more likely to cause job performance than vice versa. The results appear reasonably robust, as the finding that work motivation predicts job performance was consistent across various moderators, including the type of job performance measure, the type of motivation measure, and the length of the time lag.\r\n\r\n\r\n\r\nThe practical implications for people management are straightforward according to the authors: leveraging human performance practices (e.g., compensation management and performance management) and motivation-based leadership (e.g., empowering leadership) can positively influence employee motivation, which in turn enhances employee performance over the long term.\r\nAdditionally, there was one surprising finding: job performance does not predict work motivation. According to the authors, this suggests that HR practitioners should identify effective practices to strengthen feedback mechanisms between them, helping employees to continuously improve their performance.\r\nFor a deeper dive into this research, check out the full article here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-09-20-performance-vs-motivation/./performance_motivation.png",
    "last_modified": "2024-09-20T16:09:53+02:00",
    "input_file": "performance-vs-motivation.knit.md",
    "preview_width": 877,
    "preview_height": 454
  },
  {
    "path": "posts/2024-09-18-extraversion-vs-neuroticism/",
    "title": "Why are introverts often described in terms of negative emotionality, and should they be?",
    "description": "If interested, here are a few quick thoughts of mine along with some light data exploration on the topic.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-09-18",
    "categories": [
      "personality",
      "introversion",
      "extraversion",
      "neuroticism",
      "ipip"
    ],
    "contents": "\r\nIn public discourse, one often encounters defenses of the strengths and advantages of introverts in a world that generally prefers and celebrates more extroverted behaviors. While this advocacy can have many positive effects, it seems to me that those defending introverts often (mistakenly?) describe them in terms of negative emotionality, such as shyness, anxiety, fear, insecurity, etc., i.e.¬†in terms of qualities that fall under the umbrella of the trait of Neuroticism rather than Extraversion.\r\nI was curious about why this is the case. One obvious explanation is the strong correlation between these two traits. When examining data from Johnson‚Äôs IPIP-NEO data repository, the data indeed appear to support this hypothesis. Compared to all other traits, Neuroticism much more strongly (negatively) correlates with Extraversion (see the pairplot below).\r\n\r\n\r\nShow code\r\n\r\n# libraries for data manipulation and dataviz\r\nlibrary(foreign)\r\nlibrary(tidyverse)\r\nlibrary(GGally)\r\n\r\n# uplaoding .por file \r\n# data from Johnson, J. A. (2014). Measuring thirty facets of the five factor model with a 120-item public domain inventory: Development of the IPIP-NEO-120. J. of Research in Personality, 51, 78-89.\r\n# source: https://osf.io/tbmh5/ \r\ndata <- read.spss(\"IPIP300.por\", to.data.frame = TRUE)\r\n\r\n# assigning items to individual traits\r\no_items <- c(3, 8, 13, 18, 53, 28, 33, 68, 43, 138, 203, 58, 63, 188, 223, 168, 233, 148, 93, 218, 283, 288, 263, 268, 23, 38, 48, 73, 78, 83, 88, 98, 103, 108, 113, 118, 123, 128, 133, 143, 153, 158, 163, 173, 178, 183, 193, 198, 208, 213, 228, 238, 243, 248, 253, 258, 273, 278, 293, 298)\r\nc_items <- c(5, 40, 45, 50, 55, 120, 35, 160, 105, 140, 145, 150, 65, 190, 165, 260, 205, 210, 155, 220, 195, 290, 265, 270, 10, 15, 20, 25, 30, 60, 70, 75, 80, 85, 90, 95, 100, 110, 115, 125, 130, 135, 170, 175, 180, 185, 200, 215, 225, 230, 235, 240, 245, 250, 255, 275, 280, 285, 295, 300)\r\ne_items <- c(2, 7, 12, 17, 22, 27, 62, 37, 42, 47, 52, 57, 212, 157, 132, 77, 142, 147, 272, 247, 162, 167, 172, 177, 32, 67, 72, 82, 87, 92, 97, 102, 107, 112, 117, 122, 127, 137, 152, 182, 187, 192, 197, 202, 207, 217, 222, 227, 232, 237, 242, 252, 257, 262, 267, 277, 282, 287, 292, 297)\r\na_items <- c(4, 99, 74, 169, 144, 29, 34, 159, 104, 199, 174, 59, 64, 249, 194, 229, 204, 149, 184, 279, 284, 259, 264, 239, 9, 14, 19, 24, 39, 44, 49, 54, 69, 79, 84, 89, 94, 109, 114, 119, 124, 129, 134, 139, 154, 164, 179, 189, 209, 214, 219, 224, 234, 244, 254, 269, 274, 289, 294, 299)\r\nn_items <- c(1, 6, 11, 76, 111, 26, 31, 36, 41, 106, 171, 56, 61, 126, 71, 136, 201, 86, 91, 216, 251, 256, 231, 176, 16, 21, 46, 51, 66, 81, 96, 101, 116, 121, 131, 141, 146, 151, 156, 161, 166, 181, 186, 191, 196, 206, 211, 221, 226, 236, 241, 246, 261, 266, 271, 276, 281, 286, 291, 296)\r\n\r\n# adding 'I' prefix\r\no_items <- paste0(\"I\", o_items)\r\nc_items <- paste0(\"I\", c_items)\r\ne_items <- paste0(\"I\", e_items)\r\na_items <- paste0(\"I\", a_items)\r\nn_items <- paste0(\"I\", n_items)\r\n\r\n# selecting a random sample of 5000 respondents and computing scores for all five major scales  \r\nset.seed(2024)\r\nmydata_sample <- data %>% \r\n  dplyr::select(all_of(c(o_items, c_items, e_items , a_items, n_items))) %>% \r\n  dplyr::rename_with(~ str_replace_all(., \"I\", \"O\"), .cols = all_of(o_items)) %>% \r\n  dplyr::rename_with(~ str_replace_all(., \"I\", \"C\"), .cols = all_of(c_items)) %>% \r\n  dplyr::rename_with(~ str_replace_all(., \"I\", \"E\"), .cols = all_of(e_items)) %>% \r\n  dplyr::rename_with(~ str_replace_all(., \"I\", \"A\"), .cols = all_of(a_items)) %>% \r\n  dplyr::rename_with(~ str_replace_all(., \"I\", \"N\"), .cols = all_of(n_items)) %>% \r\n  dplyr::sample_n(5000,replace = FALSE) %>% \r\n  dplyr::mutate(\r\n    Openness = rowSums(select(., starts_with(\"O\"))),\r\n    Conscientiousness = rowSums(select(., starts_with(\"C\"))),\r\n    Extraversion = rowSums(select(., starts_with(\"E\"))),\r\n    Agreeableness = rowSums(select(., starts_with(\"A\"))),\r\n    Neuroticism = rowSums(select(., starts_with(\"N\")))\r\n  ) %>% \r\n  dplyr::select(Openness:Neuroticism)\r\n\r\n\r\n# pairplot\r\nGGally::ggpairs(\r\n  mydata_sample,\r\n    lower = list(\r\n      continuous = wrap(\"smooth\", method = \"loess\", se = FALSE, alpha = 0.1, color='#5c00ae')\r\n    ),\r\n    diag = list(\r\n      continuous = wrap(\"densityDiag\", fill = \"grey\", color=NA)\r\n    )\r\n  ) +\r\n  ggplot2::labs(\r\n    title = 'Relationships between Big Five traits as measured by the IPIP-NEO-300',\r\n    caption = \"\\nData Source: Johnson's IPIP-NEO data repository (only a random sample of 5,000 out of 307,313 observations was used).\\nNote: Smoothing lines were fitted using the LOESS method.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 22, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    panel.background = element_rect(fill = \"white\", color = NA), \r\n    panel.grid.major =  element_blank(),          \r\n    panel.grid.minor = element_blank(),                          \r\n    plot.background = element_rect(fill = \"white\", color = NA), \r\n    strip.text = element_text(size = 12, face = \"plain\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nAt the level of sub-facets of these two traits, we can observe in the correlation plot below - where individual items are ordered based on the first principal component, which groups together highly correlated variables - that the sub-facets of Extraversion are most strongly predicted by scores on the Neuroticism sub-facets of Self-Consciousness (sensitivity to others‚Äô judgments and fear of embarrassment) and Depression (inclination toward feelings of sadness, hopelessness, and low self-esteem). To some extent, Anxiety (tendency to worry, feel nervous, or experience fear) and Vulnerability (susceptibility to stress and inability to cope under pressure) also contribute.\r\n\r\n\r\nShow code\r\n\r\n# library for correlation plot\r\nlibrary(corrplot)\r\n\r\n# assigning items to individual sub-facets of Extraversion and Neuroticism\r\nE_Friendliness_items <- c(2, 62, 212, 272, 32, 92, 122, 152, 182, 242)\r\nE_Gregariousness_items <- c(7, 37, 157, 247, 67, 97, 127, 187, 217, 277)\r\nE_Assertiveness_items <- c(12, 42, 132, 162, 72, 102, 192, 222, 252, 282)\r\nE_Activity_Level_items <- c(17, 47, 77, 167, 107, 137, 197, 227, 257, 287)\r\nE_Excitement_Seeking_items <- c(22, 52, 142, 172, 82, 112, 202, 232, 262, 292)\r\nE_Cheerfulness_items <- c(27, 57, 147, 177, 87, 117, 207, 237, 267, 297)\r\n\r\nN_Anxiety_items <- c(1, 31, 61, 91, 121, 151, 181, 211, 241, 271)\r\nN_Anger_items <- c(6, 36, 126, 216, 66, 96, 156, 186, 246, 276)\r\nN_Depreesion_items <- c(11, 41, 71, 251, 101, 131, 161, 191, 221, 281)\r\nN_Self_Consciousness_items <- c(76, 106, 136, 256, 16, 46, 166, 196, 226, 286)\r\nN_Immoderation_items <- c(111, 171, 201, 231, 21, 51, 81, 141, 261, 291)\r\nN_Vulnerability_items <- c(26, 56, 86, 176, 116, 146, 206, 236, 266, 296)\r\n\r\n# adding 'I' prefix\r\nE_Friendliness_items <- paste0(\"I\", E_Friendliness_items)\r\nE_Gregariousness_items <- paste0(\"I\", E_Gregariousness_items)\r\nE_Assertiveness_items <- paste0(\"I\", E_Assertiveness_items)\r\nE_Activity_Level_items <- paste0(\"I\", E_Activity_Level_items)\r\nE_Excitement_Seeking_items <- paste0(\"I\", E_Excitement_Seeking_items)\r\nE_Cheerfulness_items <- paste0(\"I\", E_Cheerfulness_items)\r\n\r\nN_Anxiety_items <- paste0(\"I\", N_Anxiety_items)\r\nN_Anger_items <- paste0(\"I\", N_Anger_items)\r\nN_Depreesion_items <- paste0(\"I\", N_Depreesion_items)\r\nN_Self_Consciousness_items <- paste0(\"I\", N_Self_Consciousness_items)\r\nN_Immoderation_items <- paste0(\"I\", N_Immoderation_items)\r\nN_Vulnerability_items <- paste0(\"I\", N_Vulnerability_items)\r\n\r\n# selecting a random sample of 5000 respondents and computing scores for all sub-facets considered  \r\nset.seed(2024)\r\nmydata_sample <- data %>%\r\n  dplyr::select(all_of(c(\r\n    E_Friendliness_items, E_Gregariousness_items, E_Assertiveness_items, \r\n    E_Activity_Level_items, E_Excitement_Seeking_items, E_Cheerfulness_items, \r\n    N_Anxiety_items, N_Anger_items, N_Depreesion_items, \r\n    N_Self_Consciousness_items, N_Immoderation_items, N_Vulnerability_items\r\n  ))) %>% \r\n  dplyr::sample_n(5000,replace = FALSE) %>% \r\n  dplyr::mutate(\r\n    Friendliness = rowSums(select(., all_of(E_Friendliness_items))),\r\n    Gregariousness = rowSums(select(., all_of(E_Gregariousness_items))),\r\n    Assertiveness = rowSums(select(., all_of(E_Assertiveness_items))),\r\n    Activity_Level = rowSums(select(., all_of(E_Activity_Level_items))),\r\n    Excitement_Seeking = rowSums(select(., all_of(E_Excitement_Seeking_items))),\r\n    Cheerfulness = rowSums(select(., all_of(E_Cheerfulness_items))),\r\n    \r\n    Anxiety = rowSums(select(., all_of(N_Anxiety_items))),\r\n    Anger = rowSums(select(., all_of(N_Anger_items))),\r\n    Depression = rowSums(select(., all_of(N_Depreesion_items))),\r\n    Self_Consciousness = rowSums(select(., all_of(N_Self_Consciousness_items))),\r\n    Immoderation = rowSums(select(., all_of(N_Immoderation_items))),\r\n    Vulnerability = rowSums(select(., all_of(N_Vulnerability_items)))\r\n  ) %>% \r\n  dplyr::select(\r\n    Friendliness, Gregariousness, Assertiveness, Activity_Level, \r\n    Excitement_Seeking, Cheerfulness, Anxiety, Anger, Depression, \r\n    Self_Consciousness, Immoderation, Vulnerability\r\n  )\r\n  \r\n# correlation matrix\r\ncor_matrix <- cor(mydata_sample, use = \"complete.obs\")\r\n\r\n# correlation plot\r\ncorrplot::corrplot(\r\n  cor_matrix, \r\n  method = \"circle\", \r\n  type = \"full\", \r\n  order = \"FPC\", # ordering vars based on the first principal component which groups together highly correlated vars\r\n  tl.col = \"black\", tl.cex = 1.2, cl.cex=1.2,\r\n  mar=c(0,0,5,0), tl.offset = 1\r\n)\r\ntitle(\r\n  main = \"Correlation matrix with Extraversion and Neuroticism sub-facets\",\r\n  cex.main = 2, font.main = 2\r\n)\r\n\r\n\r\n\r\n\r\n\r\nThese patterns could indeed provide an answer to the question posed in the title of my post. However, the question now is whether, when discussing introverts, we should limit ourselves to behaviors that fall exclusively under the umbrella of Extraversion, as described in modern personality psychology, or whether we should also include other closely related behaviors. My personal stance is that we should include them, while explicitly noting that the picture we are describing is not a monolith but rather a blend of multiple traits. This approach helps prevent oversimplification, avoid stereotypes, and appreciate the complexity of introverts‚Äô strengths and challenges, ultimately leading to greater self-awareness and more tailored strategies in personal and professional contexts. What‚Äôs your take on this? ü§î\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-09-18-extraversion-vs-neuroticism/./corrplot2.png",
    "last_modified": "2024-09-18T12:48:11+02:00",
    "input_file": "extraversion-vs-neuroticism.knit.md",
    "preview_width": 3300,
    "preview_height": 3000
  },
  {
    "path": "posts/2024-09-12-honesty-in-engagement-vs-exit-surveys/",
    "title": "Are people during exit surveys more honest in their responses than in engagement surveys?",
    "description": "It is quite common to encounter the opinion that responses in exit surveys or interviews are more honest than those in regular engagement surveys, despite their anonymity and/or confidentiality, especially when it comes to more politically sensitive topics such as satisfaction with one's direct manager. It is usually argued that people no longer fear repercussions or negative consequences for speaking openly about their real experiences and opinions because they leave the organization and have less incentive to conceal critical feedback. Let's see if we can find any evidence of it in the data...",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-09-12",
    "categories": [
      "employee experience",
      "exit survey",
      "engagement survey",
      "response style"
    ],
    "contents": "\r\nOne way to test whether this ‚Äòpolitical distortion‚Äô exists, or to what extent, is to examine how consistent the responses of voluntarily departing employees are across the exit and engagement surveys. If such a distortion exists, one would expect to see in the data that the primary reason people gave for leaving is NOT reflected in lower scores on the corresponding items in the most recent engagement survey they completed before leaving the company, compared to leavers who gave a different primary reason. Another compatible observation would be that the primary reason for leaving DOES translate into lower scores, but this effect diminishes with the longer lag between the administration of the two surveys, because people further away from the actual date of departure may not have decided whether to leave and may therefore feel less free to express their true positions on politically sensitive topics.\r\nIn analyzing a sample dataset and focusing on ‚Äòdirect manager‚Äô as a primary reason for leaving, the results from an ordinal regression analysis appear to support the second scenario described above. The results are illustrated by the pair of heatmaps below, which show the negative effect of that primary reason (i.e., direct manager vs.¬†all other reasons) on ratings on the engagement survey, and the interaction between that primary reason and the distance between the two surveys in their effect on ratings of one particular engagement survey item related to the quality of the direct manager.\r\n\r\n\r\n\r\nHowever, this pattern could also be alternatively explained by the simple, non-problematic fact that people‚Äôs opinions change over time. The closer the exit survey is to the last engagement survey, the more it reflects what people answered in the last engagement survey. This suggests that the observed interaction/ ‚Äúdiminishing‚Äù effect may be actually common across all stated reasons and engagement survey items.\r\nTo differentiate between these two explanations, one could examine another primary reason for leaving that is assumed to be less politically sensitive, where employees may feel more comfortable expressing their true opinions before deciding to leave. An example of such a reason might be dissatisfaction with flexibility in work location and/or schedule. In this case, the data shows a negative (though statistically non-significant) effect of the stated primary reason (i.e., flexibility vs.¬†all other reasons) on engagement survey ratings, and almost zero interaction between the stated primary reason and the time between the two surveys. This finding seems to align more closely with the expectation of how honest responses should be reflected in the data.\r\n\r\n\r\n\r\nHowever, we can still speculate about alternative explanations. It could be that the dynamics of different reasons for leaving evolve differently over time. For example, dissatisfaction with flexibility might stay similarly high over the long term without any employee‚Äôs action till the moment when better opportunity occurs and they leave. In contrast, dissatisfaction with a direct manager may have a tendency to show a more sudden, abrupt increase before an employee makes a final decision to depart. To further investigate this, we can examine a reason for leaving that is less politically sensitive than dissatisfaction with a direct manager but has similar dynamics over time. One such reason could be dissatisfaction with career growth opportunities. The data for this specific reason shows a pattern very similar to what we observed already for the ‚Äòflexibility‚Äô reason.\r\n\r\n\r\n\r\nThis similarity, therefore, supports the conclusion that, in cases of dissatisfaction with a direct manager, there is some evidence in the data of a ‚Äúpolitical distortion‚Äù in people‚Äôs responses to the engagement survey on this topic. However, this conclusion should be taken with a big grain of salt, as it relies on several significant, and perhaps overly speculative, assumptions made throughout the analysis; and it is also crucial to consider the limitations of the data used, such as the shorter time span of the data and the self-selected nature of the group of people motivated enough to complete both the engagement and exit surveys. All this weakens the validity of the conclusion made above.\r\nDespite all the uncertainty, it was a very useful exercise to confront my thoughts and ideas with some evidence and refine them a bit. To build on this work and move closer to answering the question in the title of this post, I would welcome any suggestions for next steps, detours or completely alternative approaches that you think might help.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-09-12-honesty-in-engagement-vs-exit-surveys/./pic2.png",
    "last_modified": "2024-09-12T16:12:03+02:00",
    "input_file": "honesty-in-engagement-vs-exit-surveys.knit.md",
    "preview_width": 1000,
    "preview_height": 600
  },
  {
    "path": "posts/2024-09-04-sprint/",
    "title": "Unpacking surprises in the women‚Äôs 100m World Championships",
    "description": "How surprising was Sha‚ÄôCarri Richardson‚Äôs victory in the women‚Äôs 100m race at the World Athletics Championships in Budapest 2023? Let's check it out with some data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-09-04",
    "categories": [
      "sport",
      "sprint",
      "perfomance",
      "variability"
    ],
    "contents": "\r\nI just finished watching Netflix‚Äôs excellent sport documentary Sprint, which shows the ups and downs of several top male and female sprinters during their 2023 season.\r\nIn the final episode, while discussing Sha‚ÄôCarri Richardson‚Äôs victory in the women‚Äôs 100m race at the World Athletics Championships in Budapest 2023, the reporter highlighted how surprising and exceptional her win was, especially considering she did not qualify directly for the final (i.e., by being among the top two in one of the semifinals).\r\nThe data enthusiast in me felt compelled to investigate just how surprising this achievement truly was. So, I examined the data from the women‚Äôs 100m finals and semifinals at all World Athletics Championships from 2001 to 2023, and used a slopegraph and a multilevel Spearman‚Äôs rank correlation to analyze the consistency of finalists‚Äô rankings across the finals and semis.\r\nAs the plot below illustrates, the consistency is relatively high (r = 0.8), but it‚Äôs not absolute, which leaves room for some surprising wins, which are rare but not impossible. However, in Sha‚ÄôCarri‚Äôs case, imho, her victory doesn‚Äôt seem particularly surprising in light of the available data, as she moved ‚Äúonly‚Äù from third place in the semis to first in the final (which, of course, doesn‚Äôt make her victory any less remarkable).\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(correlation)\r\n\r\n# uploading data for female sprinters\r\ndata <- readxl::read_xlsx('sprint_data.xlsx', sheet = 'female_100m')\r\n\r\nmydata <- data %>% \r\n  dplyr::select(-name) %>% \r\n  dplyr::mutate(event = as.factor(event))\r\n\r\n# computing multilevel Bayesian Pearson  correlation analysis\r\nc <- correlation::correlation(\r\n  mydata,\r\n  include_factors = TRUE,\r\n  method = \"spearman\", \r\n  multilevel = TRUE, \r\n  bayesian = FALSE,\r\n  ci = 0.99\r\n)\r\n\r\n# extracting correlation estimates\r\nSpearman_r = c[1,3]\r\nCI95L = c[1,4]\r\nCI95H = c[1,5]\r\n\r\n# preparing data for dataviz via slopegraph\r\ndataviz_data <- data %>% \r\n  dplyr::mutate(event = as.factor(event)) %>% \r\n  dplyr::group_by(event) %>% \r\n  dplyr::mutate(\r\n    final_rank = rank(time_seconds_final, ties.method = \"min\", na.last = \"keep\"),\r\n    semifinal_rank = rank(time_seconds_semifinal, ties.method = \"min\", na.last = \"keep\"),\r\n  ) %>% \r\n  dplyr::ungroup() %>% \r\n  dplyr::mutate(\r\n    athlete = row_number(),\r\n    group = ifelse(final_rank < semifinal_rank, 'better', ifelse(final_rank > semifinal_rank, 'worse', 'same'))\r\n  ) %>% \r\n  tidyr::pivot_longer(\r\n    cols = c(final_rank, semifinal_rank), \r\n    names_to = \"stage\", \r\n    values_to = \"rank\"\r\n  ) %>% \r\n  dplyr::mutate(\r\n    highlight = ifelse(name==\"Sha'Carri RICHARDSON\" & event == 'World Athletics Championships, Budapest 2023', TRUE, FALSE) \r\n  )\r\n\r\n# jitter function for slopegraph\r\njitter_width <- 0.25 \r\njitter_fn <- function(x) x + runif(length(x), -jitter_width, jitter_width)\r\n\r\n# creating the slopegraph\r\nggplot2::ggplot(dataviz_data, aes(x = stage, y = rank, group = athlete, color = group)) +\r\n  # Plot non-highlighted lines first\r\n  ggplot2::geom_line(data = dataviz_data %>% filter(!highlight), \r\n            aes(y = jitter_fn(rank)), \r\n            alpha = 0.4, size = 1) + \r\n  # Plot highlighted line separately to ensure it is on top\r\n  ggplot2::geom_line(data = dataviz_data %>% filter(highlight), \r\n            aes(y = jitter_fn(rank)), \r\n            alpha = 1, size = 1.5, color = 'black', linetype='dashed') + \r\n  # Define custom x-axis settings\r\n  ggplot2::scale_x_discrete(\r\n    position = \"bottom\", \r\n    expand = c(0.03, 0.03),\r\n    limits = c(\"semifinal_rank\", \"final_rank\"),\r\n    labels = c(\"semifinal_rank\" = \"Semifinal Ranking\", \"final_rank\" = \"Final Ranking\")\r\n  ) + \r\n  ggplot2::scale_color_manual(\r\n    values = c('worse'='#F44336', 'same'='#757171', 'better'='#008080'),\r\n    labels = c('worse'='Worse Ranking', 'same'='Same Ranking', 'better'='Better Ranking')\r\n  ) +\r\n  # Define y-axis settings\r\n  ggplot2::scale_y_reverse(\r\n    breaks = seq(1, 10, by = 1),\r\n    sec.axis = dup_axis(name = NULL)  \r\n  ) + \r\n  # Add plot labels\r\n  ggplot2::labs(\r\n    x = NULL, \r\n    y = NULL, \r\n    color = NULL,\r\n    title = \"Stability of women's 100m sprinters' rankings across finals and semifinals\\nat the 2001-2023 World Athletics Championships\",\r\n    subtitle = stringr::str_glue(\"Multilevel Spearman rank correlation = {round(Spearman_r,2)}; 99% CI: [{round(CI95L,2)}, {round(CI95H,2)}]\"),\r\n    caption = \"\\nOnly those sprinters who made it to the finals are included, and the semifinal rankings are based on all semifinalists.\\nThe highlighted line corresponds to Sha'Carri Richardson's victory in the women's 100m at the 2023 World Championships in Budapest.\\nData Source: worldathletics.org\"\r\n  ) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin = margin(0, 0, 12, 0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 14, margin = margin(0, 0, 12, 0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    panel.grid = element_blank(),\r\n    axis.ticks = element_blank(),\r\n    axis.text.x = element_text(size = 14, color = '#2C2F46', hjust = c(0, 1)),\r\n    axis.text.y = element_text(size = 12, color = '#2C2F46'),\r\n    plot.margin = margin(t = 10, r = 30, b = 10, l = 30),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\",\r\n    legend.position = \"top\",\r\n    legend.margin = margin(0, 0, 0, 0),\r\n    legend.box.margin = margin(0, 0, -10, 0) ,\r\n    legend.text = element_text(size = 12, color = '#2C2F46'),\r\n    plot.background = element_rect(fill = \"white\", color = NA),\r\n    panel.background = element_rect(fill = \"white\", color = NA)\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(override.aes = list(linewidth = 5))) \r\n\r\n\r\n\r\nWe could also ask how those who improved their rankings achieved this. Was it because they ran faster, or was it that their competitors ran slower in the final compared to the semis? A comparison of time differences between the finals and semis for those who improved, worsened, or maintained their ranking suggests that it‚Äôs a combination of both. Those who improved or maintained their rankings tended to improve their times in the final, though the former group did so more significantly. Conversely, those who dropped in ranking generally ran slower in the final in comparison with the semi.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(ggdist)\r\n\r\n# preparing data for dataviz\r\ncompa_data <- data %>% \r\n  dplyr::mutate(event = as.factor(event)) %>% \r\n  dplyr::group_by(event) %>% \r\n  dplyr::mutate(\r\n    final_rank = rank(time_seconds_final, ties.method = \"min\", na.last = \"keep\"),\r\n    semifinal_rank = rank(time_seconds_semifinal, ties.method = \"min\", na.last = \"keep\"),\r\n  ) %>% \r\n  dplyr::ungroup() %>% \r\n  dplyr::mutate(\r\n    athlete = row_number(),\r\n    group = ifelse(final_rank < semifinal_rank, 'Better Ranking', ifelse(final_rank > semifinal_rank, 'Worse Ranking', 'Same Ranking')),\r\n    group = relevel(factor(group), ref = \"Same Ranking\"),\r\n    diff = time_seconds_final - time_seconds_semifinal\r\n  )\r\n\r\n# raincloud plot\r\nggplot2::ggplot(\r\n  compa_data %>% mutate(group = factor(group, levels = c('Better Ranking', 'Same Ranking', 'Worse Ranking'))), \r\n  aes(x = diff, y = fct_rev(group), color = group, fill = group)) +\r\n  ggdist::stat_halfeye(\r\n    adjust = 0.5,  \r\n    justification = -0.2,\r\n    .width = 0,  \r\n    point_interval = \"mean_qi\"\r\n  ) +\r\n  ggplot2::geom_jitter(\r\n    aes(color = group),  # Add jittered raw data points\r\n    width = 0.1, height = 0.1, alpha = 0.6\r\n  ) +\r\n  ggplot2::geom_boxplot(\r\n    aes(color = group),  # Add a boxplot\r\n    width = 0.1, outlier.shape = NA, alpha = 0.4, position = position_nudge(y = 0.12)\r\n  ) +\r\n  ggplot2::geom_vline(xintercept = 0, linetype='dashed') +\r\n  ggplot2::scale_color_manual(\r\n    values = c('Worse Ranking'='#F44336', 'Same Ranking'='#757171', 'Better Ranking'='#008080')\r\n  ) +\r\n  ggplot2::scale_fill_manual(\r\n    values = c('Worse Ranking'='#F44336', 'Same Ranking'='#757171', 'Better Ranking'='#008080')\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Difference in women's 100m sprinters' run time between finals and semifinals\\nat the 2001-2023 World Athletics Championships\",\r\n    x = \"DIFFERENCE IN RUN TIME BETWEEN FINAL AND SEMIFINAL (IN SECS)\",\r\n    y = \"\",\r\n    caption = \"\\nOnly those sprinters who made it to the finals are included.\\nData Source: worldathletics.org\"\r\n  ) +\r\n  ggplot2::theme_minimal() + \r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin = margin(0, 0, 12, 0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 14, margin = margin(0, 0, 12, 0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(color = '#2C2F46', face = \"plain\", size = 14, hjust = 0, margin = margin(15, 0, 0, 0)),\r\n    axis.text.x = element_text(size = 12, color = '#2C2F46'),\r\n    axis.text.y = element_text(size = 12, color = '#2C2F46'),\r\n    plot.margin = margin(t = 10, r = 30, b = 10, l = 30),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\",\r\n    plot.background = element_rect(fill = \"white\", color = NA),\r\n    panel.background = element_rect(fill = \"white\", color = NA),\r\n    legend.position = ''\r\n  )\r\n\r\n\r\n\r\nNot sure if there are any deeper lessons to be drawn from these results, perhaps just that while patterns and predictability exist, there is always room for surprises, the awareness of which can help nurture resilience and flexibility, key qualities for navigating uncertain environments. But maybe I just have a limited imagination, so feel free to share any suggestions for a better lesson üôÇ\r\nWhat I am 100% sure of, though, is that I enjoyed playing with the data a lot. If you‚Äôd like to play with them yourself, you can download them for both women and men from the two tables below.\r\n\r\nWomen‚Äôs 100m Data\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n      data,\r\n      class = 'cell-border stripe', \r\n      filter = 'top',\r\n      extensions = 'Buttons',\r\n      fillContainer = FALSE,\r\n      rownames= FALSE,\r\n      options = list(\r\n        pageLength = 5, \r\n        lengthMenu = c(5, 10, 15, 25),\r\n        autoWidth = TRUE,\r\n        dom = 'Blfrtip',\r\n        buttons = c('copy', 'excel', 'csv'), \r\n        scrollX = TRUE, \r\n        scrollY = TRUE\r\n      )\r\n    )\r\n\r\n\r\n\r\n\r\nMen‚Äôs 100m Data\r\n\r\n\r\n\r\nShow code\r\n\r\ndata_males <- readxl::read_xlsx('sprint_data.xlsx', sheet = 'male_100m')\r\n\r\nDT::datatable(\r\n      data_males,\r\n      class = 'cell-border stripe', \r\n      filter = 'top',\r\n      extensions = 'Buttons',\r\n      fillContainer = FALSE,\r\n      rownames= FALSE,\r\n      options = list(\r\n        pageLength = 5, \r\n        lengthMenu = c(5, 10, 15, 25),\r\n        autoWidth = TRUE,\r\n        dom = 'Blfrtip',\r\n        buttons = c('copy', 'excel', 'csv'), \r\n        scrollX = TRUE, \r\n        scrollY = TRUE\r\n      )\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-09-04-sprint/./sprint_foto.jpg",
    "last_modified": "2024-09-04T12:15:14+02:00",
    "input_file": "sprint.knit.md"
  },
  {
    "path": "posts/2024-08-29-survey-participation-and-attrition-prediction/",
    "title": "People may signal their exit intentions not only by their actions but also by their inactions",
    "description": "A brief reflection on one of the predictors of voluntary employee attrition.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-08-29",
    "categories": [
      "employee turnover",
      "employee survey",
      "turnover prediction",
      "employee withdrawal"
    ],
    "contents": "\r\nI recently came across a LI post discussing a more or less surprising fact about the relatively high predictive power of a single item from employee surveys in relation to employee turnover. The item in question was a simple question about the employee‚Äôs intention to stay with the company if they were offered the same job at another organization. An insight corresponding to my mom‚Äôs wise advice she gave me when I was a small child: If you don‚Äôt know, just ask, most people will be happy to answer üôÇ\r\nTo expand on this post, I‚Äôd like to add another related fact that I‚Äôve encountered in several real-world datasets. There is another aspect of employee surveys that is quite highly predictive of people‚Äôs intention to leave the company. However, this time, it‚Äôs not a specific survey item, but the team (or individual) survey participation rate. The lower the participation rate, the higher the probability of leaving, all other things being equal.\r\nWe can view it as one specific manifestation of employees‚Äô withdrawal that is highly predictive of the intention to leave, as illustrated in the attached chart showing the strength, direction, and estimation precision of voluntary turnover predictors, as assessed in Rubenstein et al.¬†(2017) meta-analysis.\r\n\r\n\r\n\r\n\r\nLink to the app with the interactive plot\r\n\r\nApparently, people may signal their exit intentions not only by their actions but also by their inactions. As Sherlock Holmes famously observed in one of his cases, what‚Äôs suspicious is not only the barking of the dog but also its silence.\r\nHave you noticed any other forms of inaction that might predict an intention to leave? For example, one might consider lateness and absenteeism as types of inaction as well (they are, btw, also included among the predictors in the chart and are performing pretty well).\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-08-29-survey-participation-and-attrition-prediction/./plot_predictors_screenshot.png",
    "last_modified": "2024-08-29T18:24:50+02:00",
    "input_file": {},
    "preview_width": 1577,
    "preview_height": 775
  },
  {
    "path": "posts/2024-08-29-tas-c-suit-teams/",
    "title": "Insights from the Team Assessment Survey results of C-suite teams",
    "description": "Results from the exploration of data on team effectiveness and efficiency across ~40 C-suite teams.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-08-29",
    "categories": [
      "team performance",
      "team effectiveness",
      "team efficiency",
      "rocket model",
      "network modeling",
      "partial correlation networks"
    ],
    "contents": "\r\nRecently, Dr.¬†Gordon (Gordy) Curphy posted an insightful article discussing lessons learned from working with nearly 40 C-suite teams, based on their results in the Team Assessment Survey (TAS), a tool designed to evaluate the efficiency and effectiveness of teams by assessing various factors that contribute to team performance.\r\nThe lessons could be briefly summarized as follows:\r\nNeed for Improvement: Many C-suite teams overestimate their performance and lack ‚Äúhow‚Äù feedback on team dynamics, revealing a need for benchmarking to improve efficiency and effectiveness.\r\nOptimal Team Size: Teams often struggle with inclusivity issues, becoming too large to be effective. Smaller, focused ‚Äútiger teams‚Äù are recommended for strategic decision-making, while larger groups can handle execution.\r\nGroup vs.¬†Team Dynamics: Most C-suite teams operate as hybrids, balancing individual and collective goals. Clarifying when to function as a group versus a team can enhance performance and reduce inefficiencies.\r\nAlpha Paralysis: Dominance by assertive members and avoidance of difficult topics often lead to ineffective meetings. Facilitators should help teams prioritize strategic discussions and decide on group or team approaches to minimize this issue.\r\nAdaptability Challenges: C-suite teams may need restructuring or personnel changes when performance lags, especially if team members‚Äô skills or attitudes are mismatched with evolving organizational needs.\r\nContinuous Improvement: Benchmarking feedback is essential but insufficient; ongoing efforts and accountability are crucial for sustained improvement in team dynamics and performance.\r\nThe article also included a screenshot of a data table with the C-suite teams‚Äô TAS results, along with information about their respective team size and the extent to which they operate as a team versus as a group.\r\n\r\n\r\n\r\nIt sparked my curiosity about whether further insights could be gleaned from this data using an analytical technique known as a regularized partial correlation network, a method that identifies direct relationships between variables by controlling for others while using regularization to enforce sparsity, ensuring clarity and reducing overfitting.\r\n\r\n\r\nShow code\r\n\r\n# libraries\r\nlibrary(tidyverse)\r\nlibrary(bootnet)\r\n\r\n# data extracted from the table\r\ndata <- readxl::read_xlsx('table_extracted.xlsx')\r\n\r\n# preparing data\r\nmydata <- data %>% \r\n  dplyr::select(-Team, -TQ) %>% \r\n  dplyr::rename(`Team x Group`=`Group vs Team`)\r\n\r\n\r\n# estimating a regularized partial correlation network\r\nnetwork <- bootnet::estimateNetwork(\r\n  mydata,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE # when TRUE, enforces higher specificity, at the cost of sensitivity\r\n)\r\n\r\n# plotting the estimated network \r\nplot(\r\n  network, \r\n  layout = \"spring\",\r\n  groups = NaN,\r\n  nodeNames = names(mydata),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend = FALSE,\r\n  font = 2,\r\n  theme = \"classic\"\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# computing centrality measures for individual variables\r\nqgraph::centralityPlot(network, include = \"all\", orderBy = \"ExpectedInfluence\")\r\n\r\n\r\n\r\nThe resulting network, IMHO, revealed two or three extra insights that are at least interesting, if not useful:\r\nThe Central Role of Norms and Buy-In: These factors emerged as central in the network, a conclusion supported not only by a visual inspection of the network but also by the expected influence centrality measure. This finding may suggest that interventions targeting these areas could have the greatest leverage when trying to improve team efficiency and effectiveness.\r\nTeam Dynamics and Courage: Operating more as a team than as a group appears to predict lower levels of Courage - a factor related to team trust and psychological safety. A plausible explanation for this surprising finding could be that teams, as opposed to groups, place a stronger emphasis on cohesion, interdependence, and collective identity, which may create pressures that suppress perceived psychological safety. If this is true, it underscores the need to strike a delicate balance between fostering team cohesion and encouraging individual courage, ensuring that members feel safe to speak up without fear of disrupting team harmony.\r\nThe Trade-offs of Team Size: While having more members can be detrimental to a team‚Äôs efficiency and effectiveness - particularly regarding Buy-In (motivation to achieve team goals) and Talent (right size, skills, roles, rewards, and followership) - larger teams may benefit from better access to resources such as budget, equipment, data, and the authority or autonomy needed to accomplish goals. As is often the case in real life, things are not black and white and one must navigate various trade-offs and tailor their approach to specific circumstances.\r\n‚ö†Ô∏è A small caveat: These findings are based on a very small sample of highly specific teams, which limits their reliability and validity. They should therefore be treated as such and used only as an inspiration for further reflection on team performance and potential improvement strategies.\r\nP.S. Kudos to ChatGPT for extracting the data from the screenshot of the data table - it made the analysis much easier. Thanks! ü§ñüòÄ\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-08-29-tas-c-suit-teams/tas-c-suit-teams_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-09-02T09:32:48+02:00",
    "input_file": {},
    "preview_width": 2112,
    "preview_height": 1728
  },
  {
    "path": "posts/2024-08-21-kohonen-self-organizing-maps/",
    "title": "Kohonen's Self-Organizing Maps",
    "description": "A brief demonstration of an awesome tool for exploring multidimensional data, using personality and work-life satisfaction data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-08-21",
    "categories": [
      "data visualization",
      "data exploration",
      "self-organizing maps",
      "artificial neural networks",
      "personality",
      "big five",
      "work-life satisfaction"
    ],
    "contents": "\r\nRecently, I needed to explore the interactions of several variables in relation to a variable of interest, and I was wondering what tool could help me, until I remembered that in the distant past I had used Kohonen‚Äôs Self-Organizing Maps (SOMs) in such situations - an artificial neural network designed for unsupervised learning that relies on competitive learning mechanism to preserve topological properties of high-dimensional data and map them onto a lower-dimensional grid for visualization and clustering.\r\nTo illustrate, I applied it to a piece of data collected by Clearer Thinking, which used it to compare the predictive performance of different popular personality test frameworks in relation to various life outcomes. Specifically, I tried to explore the relationship between Big Five traits and work-life satisfaction.\r\n\r\n\r\nShow code\r\n\r\n# loading necessary libraries\r\nlibrary(tidyverse)\r\nlibrary(kohonen)\r\nlibrary(gridExtra)\r\n\r\n# uploading data\r\ndata <- readr::read_csv('big5_mbti-stlye_astrologicalSigns_enneagramTypes_outcome_variables.csv')\r\n\r\n# data preparation\r\nmydata <- data %>% \r\n  dplyr::select(contains(\"Big5\"), outcome_SatisfiedWithWorkLife ) %>% \r\n  tidyr::drop_na() %>% \r\n  dplyr::rename(\r\n    Extraversion = Big5_E,\r\n    Openness = Big5_O,\r\n    Agreeableness = Big5_A,\r\n    Conscientiousness = Big5_C,\r\n    Neuroticism = Big5_N,\r\n    \"Work-Life Satisfaction\" = outcome_SatisfiedWithWorkLife\r\n  )\r\n\r\n# normalize the data\r\ndata_scaled <- mydata %>%\r\n  dplyr::mutate(across(everything(), ~scale(.)))\r\n\r\n# creating a matrix for SOM function\r\ndata_matrix <- as.matrix(data_scaled)\r\ndata_matrix <- apply(data_matrix, 2, as.numeric)\r\n\r\n# keeping the original names of the vars\r\ncolnames(data_matrix) <- colnames(data_scaled)\r\n\r\n# creating a SOM grid with hexagonal topology 20x20\r\nsom_grid <- kohonen::somgrid(xdim = 20, ydim = 20, topo = \"hexagonal\")\r\n\r\n# training the SOM \r\nset.seed(2024)\r\nsom_model <- kohonen::som(\r\n  data_matrix, \r\n  grid = som_grid, \r\n  rlen = 5000, \r\n  alpha = c(0.05, 0.01), \r\n  keep.data = TRUE\r\n)\r\n\r\n# plotting the SOM using ggplot\r\n# SOM with hexagonal cells using ggplot\r\n\r\n# function to create hexagonal coordinates\r\nhex_coords <- function(x, y) {\r\n  r <- 1 / sqrt(3)  # the radius of a hexagon\r\n  angles <- seq(0, 2*pi, length.out = 7)  # angles for hexagon vertices\r\n  data.frame(\r\n    x = x + r * cos(angles),\r\n    y = y + r * sin(angles)\r\n  )\r\n}\r\n\r\n# defining a blue-to-red color palette\r\nblue_to_red_palette <- colorRampPalette(c(\"blue\", \"white\", \"red\"))\r\n\r\n# creating a list to store ggplot objects\r\nplots <- list()\r\n\r\n# generating the component planes for each variable and store them as ggplot objects\r\nfor(i in 1:ncol(data_matrix)){\r\n  # extracting the component plane data\r\n  plane_data <- data.frame(\r\n    x = rep(1:som_grid$xdim, each = som_grid$ydim),\r\n    y = rep(1:som_grid$ydim, som_grid$xdim),\r\n    z = getCodes(som_model)[,i]\r\n  )\r\n  \r\n  # creating hexagon coordinates for each tile\r\n  hex_list <- lapply(1:nrow(plane_data), function(j) {\r\n    hex_coords(plane_data$x[j], plane_data$y[j])\r\n  })\r\n  \r\n  # combining all hexagons into one data frame\r\n  hex_data <- do.call(rbind, hex_list)\r\n  hex_data$z <- rep(plane_data$z, each = 7)\r\n  hex_data$group <- rep(1:nrow(plane_data), each = 7)\r\n  \r\n  # creating a ggplot object with hexagons\r\n  p <- ggplot2::ggplot(hex_data, aes(x = x, y = y, group = group, fill = z)) +\r\n    ggplot2::geom_polygon() +\r\n    ggplot2::scale_fill_gradientn(colors = blue_to_red_palette(100)) +\r\n    ggplot2::coord_fixed() +\r\n    ggplot2::labs(\r\n      title = colnames(data_matrix)[i], \r\n      fill = \"Value\"\r\n    ) +\r\n    ggplot2::theme_minimal() +\r\n    ggplot2::theme(\r\n      axis.text = element_blank(),\r\n      axis.title = element_blank(), \r\n      panel.grid = element_blank(),\r\n      plot.title = element_text(hjust = 0.5)\r\n    )\r\n  \r\n  plots[[i]] <- p\r\n}\r\n\r\n# arranging the plots in a 2x3 grid\r\ngridExtra::grid.arrange(grobs = plots, nrow = 2, ncol = 3)\r\n\r\n\r\nShow code\r\n\r\n# an alternative and easier SOM visualization\r\n# par(mfrow = c(2, 3))\r\n# par(cex.main = 1.5) \r\n# for(i in 1:ncol(data_matrix)){\r\n#   plot(som_model, type = \"property\", property = getCodes(som_model)[,i],\r\n#        main = colnames(data_matrix)[i], shape = \"straight\", palette.name = blue_to_red_palette)\r\n# }\r\n# dev.off()\r\n# # resetting the layout to default\r\n# par(mfrow = c(1, 1), cex.main = 1)\r\n\r\n\r\nFrom the maps, it is easy to see that satisfaction with work life is strongly related to Extraversion (positively) and Neuroticism (negatively) and weakly related to Conscientiousness and Openness (both positively). More importantly, however, we can also check various ‚Äúexceptions‚Äù to these bivariate patterns by looking at the same specific locations across all maps. For example, one can see a group of people with higher Neuroticism (lower left corner) who nevertheless show higher work-life satisfaction, perhaps due to higher Extraversion, Conscientiousness, and partly also Openness. That‚Äôs something that wouldn‚Äôt be possible to find out from the pairwise plots that are usually used for similar data exploration. Check it out for yourself below.\r\n\r\n\r\nShow code\r\n\r\n# exploring the data in the pairplot\r\nlibrary(GGally)\r\n\r\nGGally::ggpairs(\r\n  mydata,\r\n  lower = list(continuous = GGally::wrap(\"smooth\", alpha = 0.3)),\r\n  diag = list(continuous = GGally::wrap(\"densityDiag\", fill = \"grey\"))\r\n) +\r\nggplot2::theme_minimal()\r\n\r\n\r\n\r\nIf you find yourself in a similar situation, definitely give it a try. Happy exploration(s) üôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-08-21-kohonen-self-organizing-maps/./som_plots.png",
    "last_modified": "2024-08-21T13:47:04+02:00",
    "input_file": {},
    "preview_width": 5500,
    "preview_height": 3000
  },
  {
    "path": "posts/2024-08-15-linkedin-contacts-job-positions/",
    "title": "Analyzing LinkedIn connections' jobs using LLMs and the BERTopic package",
    "description": "If you're curious about what your LinkedIn connections are up to, keep reading...",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-08-15",
    "categories": [
      "nlp",
      "llm",
      "topic modeling",
      "linkedin",
      "networking"
    ],
    "contents": "\r\nI took advantage of one rainy day during my vacation to dive into some LI data and explore what my nearly 7K contacts are doing professionally.\r\nThe challenge was that the available LI data about my contacts‚Äô jobs was pretty limited - just their job titles. So, my first step was to enrich this information using a LLM to generate brief descriptions of their positions based on those titles. It‚Äôs not perfect and it‚Äôs a bit error-prone, but in a low-stakes situation like this, fortunately we can accept a bit of imperfection and some minor ‚Äúhallucinations‚Äù üßô\r\nNext, I fed these descriptions into a BERTopic workflow, which applies several NLP techniques to identify clusters of similar documents:\r\nGenerating sentence embeddings.\r\nReducing the dimensionality of those embeddings.\r\nClustering the reduced embeddings.\r\nVectorizing texts at the cluster level.\r\nExtracting key terms per cluster using class-based TF-IDF.\r\nFine-tuning the cluster representations with the help of an LLMs.\r\nAs you can see in the chart below, the range of roles is quite broad, but it seems that most of my contacts work in people analytics, data science, HR & people management, as executives, and as researchers in academia or in enterprises.\r\n\r\n\r\n\r\nBelow is an interactive version of the chart, where you can check the individual job titles behind the job categories.\r\n\r\n\r\nShow code\r\n# packages used\r\nimport numpy as np\r\nimport pickle\r\nimport pandas as pd\r\nimport plotly.graph_objects as go\r\nimport plotly.offline as py\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.colors as mcolors\r\n\r\n\r\n# data used\r\nwith open('all_labels_remapped.pkl', 'rb') as f:\r\n    all_labels_remapped = pickle.load(f)\r\nreduced_embeddings = np.load('reduced_embeddings.npy')\r\ndata = pd.read_csv('data.csv')\r\n\r\n# extracting job titles\r\ntitles = data['position']\r\n\r\n# defining the 'Uncategorized' label\r\nuncategorized_label = 'Uncategorized'\r\n\r\n# generating 20 distinct colors\r\ncmap = plt.get_cmap('tab20')\r\ncolors = [cmap(i) for i in range(20)]\r\n\r\n# convert matplotlib colors to hex format\r\ncolors_hex = [mcolors.to_hex(c) for c in colors]\r\n\r\n# defining color for 'Uncategorized'\r\nuncategorized_color = '#D3D3D3'  # Light gray\r\n\r\n# creating an empty figure\r\nfig = go.Figure()\r\n\r\n# plotting 'Uncategorized' points first\r\nuncategorized_indices = pd.Series(all_labels_remapped) == uncategorized_label\r\nfig.add_trace(go.Scatter(\r\n    x=reduced_embeddings[uncategorized_indices, 0],\r\n    y=reduced_embeddings[uncategorized_indices, 1],\r\n    mode='markers',\r\n    marker=dict(\r\n        size=5,\r\n        color=uncategorized_color,\r\n        opacity=0.5\r\n    ),\r\n    name=uncategorized_label,\r\n    text=[f'Title: {titles[j]}<br>Category: {all_labels_remapped[j]}' for j in range(len(uncategorized_indices)) if uncategorized_indices[j]],\r\n    hoverinfo='text',\r\n    showlegend=True\r\n));\r\n\r\n# plotting the rest of the groups on top\r\nunique_labels = pd.Series(all_labels_remapped).unique()\r\n\r\nfor i, label in enumerate(unique_labels):\r\n    if label == uncategorized_label:\r\n        continue  # skipping 'Uncategorized' since it was already plotted\r\n\r\n    indices = pd.Series(all_labels_remapped) == label\r\n    color = colors_hex[i % len(colors_hex)]\r\n    \r\n    fig.add_trace(go.Scatter(\r\n        x=reduced_embeddings[indices, 0],\r\n        y=reduced_embeddings[indices, 1],\r\n        mode='markers',\r\n        marker=dict(\r\n            size=5,\r\n            color=color,\r\n            opacity=0.5\r\n        ),\r\n        name=f'{label}',\r\n        text=[f'Title: {titles[j]}<br>Category: {all_labels_remapped[j]}' for j in range(len(indices)) if indices[j]],\r\n        hoverinfo='text',\r\n        showlegend=True\r\n    ));\r\n\r\n# updating layout\r\nfig.update_layout(\r\n    title='',\r\n    xaxis=dict(\r\n        showgrid=False, \r\n        zeroline=False,  \r\n        showline=False,  \r\n        showticklabels=False,  \r\n        title='',  \r\n    ),\r\n    yaxis=dict(\r\n        showgrid=False, \r\n        zeroline=False,  \r\n        showline=False,  \r\n        showticklabels=False, \r\n        title='',  \r\n    ),\r\n    plot_bgcolor='rgba(0,0,0,0)',\r\n    height=600,\r\n    width=850,\r\n    template='plotly_white'\r\n)\r\n                                    \r\n\r\nNo big surprises here, considering my career path and control over who I connect with. However, it can be more useful to view it from the perspective of who is missing and use it as a tool for intentional LI network building. After all, as some wise people say, we are the average of the people we spend the most time with. For example, I would appreciate having more artists among my contacts, but it‚Äôs a question of whether LI is the right network for finding such connections üòâ\r\nP.S. If interested, feel free to check out one of my earlier apps that automatically generates basic descriptive statistics about your LI connections.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-08-15-linkedin-contacts-job-positions/./li_contacts_jobs.png",
    "last_modified": "2024-08-15T17:42:06+02:00",
    "input_file": {},
    "preview_width": 1211,
    "preview_height": 1211
  },
  {
    "path": "posts/2024-07-01-job-insecurity-and-behavioral-outcomes/",
    "title": "Does a stick work?",
    "description": "Probably each of us has experienced, or at least heard of, the practice of using job insecurity as a motivational tool at some point in our careers. A well-known example of this is stack-ranking performance reviews, where bonuses are given to top performers and those at the bottom are let go. While we may not personally like this method of motivation, it would be beneficial to have some data on its effectiveness.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-07-01",
    "categories": [
      "motivation",
      "meta-analysis",
      "performance management"
    ],
    "contents": "\r\nSuch data seems to be provided by an interesting meta-analysis conducted by Jiang, Lawrence, and Xu (2022). The authors conclude that although job insecurity has curvilinear relationships with some employee workplace behaviors, such as task performance and organizational citizenship behavior (OCB-organization) (i.e., they first decrease and then increase with increasing job insecurity after reaching a certain inflection point), it is not a good motivational strategy for several reasons:\r\nThe positive relationships after the inflection points are relatively weak.\r\nThere is a linear, negative relationship of job insecurity with safety behavior, and a linear, positive relationship with counterproductive work behavior (CWB-organization).\r\nThe (negative) relationship of job insecurity with OCB-individual and creative performance becomes nonsignificant as job insecurity further increases, so if the goal is to increase these two specific behavioral outcomes, increasing employee job insecurity is not a way to achieve that.\r\nAs other studies have shown, such as meta-analysis by Jiang & Lavaysse (2018), increased job insecurity has unwanted negative impacts on employee well-being and job attitudes.\r\n\r\n\r\n\r\nA plot of the curvilinear relationship between (overall) job insecurity and task performance where the blue line is the fitted curve, whereas the red, straight lines are based on the interrupted regression results from the two-lines test.\r\nAll in all, low levels of job insecurity seem to be more beneficial as they are related to higher levels of task performance, OCB-I, OCB-O, creative performance, and safety behavior, as well as lower levels of CWB-O, without unwanted negative impacts on employee attitudes and well-being.\r\nFor more interesting details of the study, check the original paper here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-07-01-job-insecurity-and-behavioral-outcomes/./job_insecurity_plot.jpg",
    "last_modified": "2024-07-01T12:07:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-06-24-network-graph-employee-comments/",
    "title": "Using network graph modeling to capture overarching thematic clusters in employee comments",
    "description": "A showcase on how to use network analysis to display the co-occurrence of topics in employee comments.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-06-24",
    "categories": [
      "employee survey",
      "topic analysis",
      "network analysis",
      "llm",
      "genai",
      "ai",
      "python",
      "r"
    ],
    "contents": "\r\nI recently came across a qualitative study that used interviews with NHS Covid staff to explore the impact of working in COVID during a pandemic on their experience, ability to work effectively together and the impact of social dynamics (e.g.¬†cohesion, social support) on teamwork and mental health.\r\nWhat particularly caught my attention was usage of network graph modeling for capturing overarching thematic clusters based on the co-occurrences of themes in the thematically coded interview transcripts.\r\nIt inspired me to try to apply this approach to the open-ended comments from the employee survey after identifying the topics present in each comment using the LLM. IMO, this can help with better interpretation of the survey results by providing a broader context of the topics identified and thus help to more wisely select appropriate follow-up actions.\r\nLet‚Äôs check out how this works on sample comments taken from Glassdoor. You can download the data from Kaggle here. To preserve the anonymity of the company whose reviews I will be analyzing, I will not show all the filters used and will upload the pre-prepared data directly. This data consists of the cons reported by previous or current employees of one specific company at one specific location during the first half of 2021. Our input data looks as follows.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\nimport pandas as pd\r\n\r\ndf = pd.read_csv('company_cons.csv')\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(reticulate)\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  py$df,\r\n  rownames = FALSE, \r\n  options = list(pageLength = 5)\r\n)\r\n\r\n\r\n\r\nNow let‚Äôs extract individual pain points from employee feedback using LLM, specifically Mixtral-8x7B-Instruct-v0.1 from Mistral AI.\r\n\r\n\r\nShow code\r\nfrom huggingface_hub import login\r\nlogin(token=\"your_token\", add_to_git_credential=True)\r\nfrom transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\r\nimport torch\r\nimport accelerate\r\nimport bitsandbytes\r\nimport re\r\nimport ast\r\nimport numpy as np\r\n\r\n# specifying the llm model\r\nmodel_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n  model_id, \r\n  device_map=\"auto\", \r\n  load_in_4bit=True, \r\n  torch_dtype=torch.float32\r\n)\r\n\r\ngenerator = pipeline(\r\n    task='text-generation',\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=2048,\r\n    repetition_penalty=1.1,\r\n    device_map=\"auto\",\r\n    torch_dtype=torch.float32,\r\n    do_sample=True,\r\n    temperature=0.001\r\n)\r\n\r\nWe will loop over individual comments and extract pain points using the following prompt.\r\n\r\n\r\nShow code\r\n# looping over individual comments\r\nresponses = []\r\nfor row in range(0, df.shape[0]):\r\n    comment = df.loc[row, 'cons']\r\n    prompt = f'''[INST]\r\n    You are a first-class expert in analyzing feedback provided by employees.\r\n    Your task is to identify main pain points (negative feedback) in the comment.\r\n    Be short and precise: use short phrases or sentences only to record main pain points, i.e. what employee does not like or complaint about.\r\n    Use only those phrases or sentences that closely relate to the employee's feedback in the comment. Do not include points that indirectly or remotely relate to the employee's feedback.\r\n    Always return a json file with the following key-value pair: pain points - a string of short sentences or phrases separated by commas describing what employee does not like or complaint about. Don't use brackets or parentheses in the value part.\r\n    If there is no pain point mentioned in the comment or if you do not know which value to assign, always return an empty string with the key, i.e. {{\"pain points\": \"\"}}.\r\n    As an example, the following comment \"Poor decision-making. People who make decisions don't know much about the topic, and usually they don't consult experts. However, the workload is appropriate. I would like to see a more professional approach.\" will return the following json file: {{\"pain points\": \"poor decision-making, missing expertise in decision-making\"}}; the following comment \"My direct manager was completely incompetent.\" will return the following json file: {{\"pain points\": \"incompetent direct manager\"}}.\r\n    Return a json file as instructed. Always include the name of the key and its corresponding value, even if it is an empty string! Don't use brackets or parentheses in the value part.\r\n    Avoid duplications or similar phrases or sentences. Don't provide any alternative solutions. Don't make any comments or notes on the output, just provide me the json file in the format I asked for, i.e. {{\"pain points\": \"a string of short sentences or phrases separated by commas describing what employee does not like or complaint about\"}}!!!\r\n    Here is a comment to analyze: {comment}\r\n    [/INST]'''\r\n    response = generator(prompt)\r\n    extracted_content = response[0][\"generated_text\"].split(\"[/INST]\")[-1].strip()\r\n    extracted_dict = re.search(r\"\\{.*\\}\", extracted_content, re.DOTALL).group()\r\n    cleaned_dict = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', extracted_dict)\r\n    proper_dict = ast.literal_eval(cleaned_dict)\r\n    current_key = list(proper_dict.keys())[0]\r\n    proper_dict['pain points'] = proper_dict.pop(current_key)\r\n    responses.append(proper_dict)\r\n\r\n# enriching the original dataset with identified pain points\r\npain_points_df = pd.DataFrame(responses)\r\npain_points_df['pain points'].replace('', np.nan, inplace=True)\r\npain_points_df = pd.concat([pain_points_df, df], axis=1)\r\n\r\nBefore the next step, we need to explode the enriched dataset on a pain point basis to get them on separate lines while preserving information about the original comments from which they were extracted.\r\n\r\n\r\nShow code\r\n# exploading the df by pain points\r\npain_points_df['pain points'] = pain_points_df['pain points'].str.split(', ')\r\npain_points_df_exploaded = pain_points_df.explode('pain points')\r\n\r\nWe get the following resulting table.\r\n\r\n\r\nShow code\r\n\r\npain_points_df_exploaded = read.csv('pain_points_exploaded.csv')\r\n\r\nDT::datatable(\r\n  pain_points_df_exploaded,\r\n  rownames = FALSE, \r\n  options = list(pageLength = 5)\r\n)\r\n\r\n\r\n\r\nNow we can proceed further and use the pre-prepared list of topic labels to classify all the extracted pain points.\r\n\r\n\r\nShow code\r\n\r\n# list of employee experience topic labels\r\nemployee_experience_topics = pd.read_csv(\"employee_experience_topics.csv\")\r\nemployee_experience_topics['topic_description'] = employee_experience_topics['topic'] + \": \" + employee_experience_topics['description']\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\n\r\nDT::datatable(\r\n  py$employee_experience_topics %>% dplyr::select(topic, description),\r\n  rownames = FALSE, \r\n  options = list(pageLength = 5)\r\n)\r\n\r\n\r\n\r\nFor topic labeling, we will use the LLM again with the following prompt.\r\n\r\n\r\nShow code\r\n# making from topic labels one string that will be used in the prompt\r\nresponse_topic_bank = employee_experience_topics['topic_description'].to_list()\r\npain_points_topic_list_str = \"\\n\".join(response_topic_bank)\r\n\r\nresponses_topics = []\r\n\r\n# looping over individual pain points\r\nfor index, row in pain_points_df_exploaded.iterrows():\r\n    text = row['pain points']\r\n    if not text or pd.isna(text):\r\n        responses_topics.append(np.nan)\r\n    else:\r\n        prompt = f'''[INST]\r\n        You are a first-class expert in analyzing feedback provided by employees.\r\n        Your task is to select from the provided list and assign one specific topic label that best captures the general meaning of short snippet of text extracted from employee's feedback during exit interview while leaving the company.\r\n        Don't make up your own topic labels and use only the topic labels from the following list: {pain_points_topic_list_str}\r\n        If you consider more options from the provided list of labels, always choose only one of them!!! Don't provide me more alternatives, I want just one solution!\r\n        If you are not sure about the meaning of the text or you don't know what label from the provided list to assign to the text, use the label 'Other', nothing more.\r\n        Assign always only one label to the text. Don't provide any additional comment, note, or explanation of your reasoning before or after the suggested label. Don't provide me more alternatives, I want just one solution!\r\n        Don't mix two different labels into one label.\r\n        You are provided with the topic labels together with their description, but put into your output only the name of the topic, not its description!\r\n        Return a json file in the format {{\"topic\": \"suggested topic label\"}}.\r\n        As an example, the following text 'lack of sufficient staff in functional positions' will return the following json file: {{\"topic\": \"Staffing & Recruitment\"}}; text 'not selected for positions' will return the following json file: {{\"topic\": \"Career Development & Growth Opportunities\"}}; text 'lack of feedback' will return the following json file: {{\"topic\": \"Performance Management\"}}; text 'recognition based solely on sales results' will return the following json file: {{\"topic\": \"Recognition\"}}; text 'perceived threat from employee' will return the following json file: {{\"topic\": \"Psychological Safety\"}}; if you are not sure about the meaning of the text or you don't know what label from the provided list to assign to the text, you will return the following json file: {{\"topic\": \"Other\"}}.\r\n        Return a json file in the required format. Assign always only one label that matches the meaning of the text best!!! Provide me only with the topic label name, not its description!!!\r\n        Don't make any comment, note, description of your reasoning, or explanation of your reasoning. Don't provide me more alternatives. If there are more options, choose only one of them - the one you think is the best option!!! Avoid using any brackets or parentheses in your output! Return just one json file.\r\n        Remember, before giving me required output, check if you are giving one proper json file I can immediately process in Python! If not, wait and redo your work so I get what I asked for!!!\r\n        Now give me the best label available in the provided list of labels for the following text: {text}\r\n        [/INST]'''\r\n        response = generator(prompt)\r\n        extracted_content = response[0][\"generated_text\"].split(\"[/INST]\")[-1].strip()\r\n        extracted_dict = re.search(r\"\\{.*?\\}\", extracted_content, re.DOTALL).group()\r\n        proper_dict = ast.literal_eval(extracted_dict)\r\n        responses_topics.append(proper_dict)\r\n\r\n# enriching dataset with pain points with selected topic labels\r\nresponses_topics_df = pd.DataFrame([x if isinstance(x, dict) else {'topic': np.nan} for x in responses_topics])\r\npain_points_df_exploaded['topic_label'] = responses_topics_df['topic'].values\r\n\r\nNow we have for each pain point also a corresponding topic label.\r\n\r\n\r\nShow code\r\n\r\npain_points_topics_exploaded = read.csv('pain_points_topics_exploaded.csv')\r\n\r\nDT::datatable(\r\n  pain_points_topics_exploaded,\r\n  rownames = FALSE, \r\n  options = list(pageLength = 5)\r\n)\r\n\r\n\r\n\r\nIn the final step, we need to obtain descriptive statistics for the co-occurrence of all pairs of topics across all employee comments and in a format suitable for network analysis.\r\n\r\n\r\nShow code\r\nfrom itertools import combinations\r\nfrom collections import Counter\r\n\r\n# counter for storing co-occurrence counts\r\nco_occurrence_counter = Counter()\r\n\r\n# finding topic combinations within each employee comment\r\nfor _, group in pain_points_df_exploaded.groupby('id'):\r\n    topics = group['topic_label'].tolist()\r\n    for combo in combinations(sorted(topics), 2):\r\n        co_occurrence_counter[combo] += 1\r\n\r\n# converting the counter to a df\r\nco_occurrence_df = pd.DataFrame(\r\n    list(co_occurrence_counter.items()),\r\n    columns=['topic_pair', 'weight']\r\n)\r\n\r\n# splitting the tuple into separate columns\r\nco_occurrence_df[['from', 'to']] = pd.DataFrame(co_occurrence_df['topic_pair'].tolist(), index=co_occurrence_df.index)\r\n\r\n# dropping the original topic_pair column\r\nco_occurrence_df = co_occurrence_df.drop(columns='topic_pair')\r\n\r\n# removing edges between the same topic labels\r\nco_occurrence_df = co_occurrence_df[co_occurrence_df['from']!=co_occurrence_df['to']]\r\nco_occurrence_df.reset_index(drop=True, inplace=True)\r\n\r\n# changing order of cols\r\nco_occurrence_df = co_occurrence_df[['from', 'to', 'weight']]\r\nco_occurrence_df.sort_values(by='weight', ascending=False)\r\n\r\nAnd this is how the final table looks like.\r\n\r\n\r\nShow code\r\n\r\npain_points_cooccurrence = read.csv('pain_points_cooccurrence.csv')\r\n\r\nDT::datatable(\r\n  pain_points_cooccurrence,\r\n  rownames = FALSE, \r\n  options = list(pageLength = 5)\r\n)\r\n\r\n\r\n\r\nThe above table can then be used to create an undirected network object and visualize it using, for example, the d3Network package for D3 JavaScript network graphs. The resulting network graph shows the relationships between topics such that topics that are closer together and connected by stronger edges tend to appear more frequently in employee comments together. The node size indicates how often the topic occurs with other topics in employee comments (degree centrality), and the colors represent the communities (clusters) of topics as detected by the Louvain method for community detection. The graph is interactive, so you can zoom in/out and highlight specific parts of the graph you‚Äôre interested in.\r\n\r\n\r\nShow code\r\n\r\nlibrary(igraph)\r\nlibrary(networkD3)\r\n\r\n# creating network object\r\nnetwork <- graph_from_data_frame(pain_points_cooccurrence, directed=FALSE)\r\n\r\n# computing degree centrality and community detection using Louvain method\r\nV(network)$degree <- degree(network)\r\nclusters <- cluster_louvain(network) \r\nV(network)$community <- clusters$membership\r\n\r\n# preparing data for networkD3 dataviz\r\nnetworkD3_data <- igraph_to_networkD3(network)\r\nnetworkD3_data$nodes$group <- V(network)$community\r\nnetworkD3_data$nodes$degree <- V(network)$degree\r\nnetworkD3_data$nodes$reporting_name <- V(network)$name\r\n# specifying custom color scale\r\ncolors <- c(\"#1b39a6\", \"#b13aa0\", \"#72a239\", \"#895f22\", \"#d08311\", \"#ca4f1a\", \"#8764d9\")\r\n\r\n# networkD3 dataviz\r\nnetworkD3::forceNetwork(\r\n  Links = networkD3_data$links,\r\n  Nodes = networkD3_data$nodes,\r\n  Source = 'source',\r\n  Target = 'target',\r\n  NodeID = 'reporting_name',\r\n  Value = \"value\",\r\n  Group = \"group\",\r\n  Nodesize = 'degree',\r\n  arrows = FALSE,\r\n  legend = FALSE,\r\n  opacity = 1,\r\n  zoom = TRUE,\r\n  fontSize = 20,\r\n  opacityNoHover = 1,\r\n  linkDistance = 60,\r\n  charge = -900,\r\n  colourScale = JS(paste0(\"d3.scaleOrdinal().range([\\\"\", paste(colors, collapse = \"\\\", \\\"\"), \"\\\"])\"))\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-06-24-network-graph-employee-comments/./network_graph_animation.gif",
    "last_modified": "2024-07-02T13:27:44+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-06-04-cognitive-diversity/",
    "title": "Can there be too much cognitive diversity in teams?",
    "description": "The answer might be \"yes\", at least in the case of team collective intelligence, defined as a general ability of a team to work together across a wide array of tasks.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-06-04",
    "categories": [
      "diversity",
      "cognitive styles",
      "collective intelligence",
      "team performance",
      "team learning"
    ],
    "contents": "\r\nIn a very interesting study, Aggarwal et al.¬†(2019) investigated how the diversity of cognitive styles in teams (as measured by object-spatial imagery and OSIVQ verbal questionnaire) indirectly affects team learning through collective intelligence (as measured by a battery of various tasks, including the minimum-effort tacit coordination game to estimate team learning).\r\nThey found that cognitive style diversity has a curvilinear, inverted U-shaped relationship with collective intelligence, which is further positively related to the rate at which teams learn and is a mechanism guiding the indirect relationship between cognitive style diversity and team learning.\r\n\r\n\r\n\r\nThe relationship between cognitive style diversity and collective intelligence controlling for team size and cognitive style level.\r\nThe authors did not directly study the causes of this type of relationship, but the hypothesis they tested was based on an existing theorizing that contrasts cognitive diversity, which, on the one hand, brings a wealth of cognitive resources (knowledge, skills, etc.) useful for tackling the range of tasks the team faces, but on the other hand, also causes higher coordination costs due to different perspectives of team members.\r\nThese results thus suggest that good old ‚Äúmoderation‚Äù might be a sound guiding principle in this area, as in many others.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-06-04-cognitive-diversity/./chart.jpg",
    "last_modified": "2024-06-04T21:04:43+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-05-30-body-height-inventory/",
    "title": "Estimating body height using an inventory?",
    "description": "Why not?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-05-30",
    "categories": [
      "psychometrics",
      "measurement",
      "education"
    ],
    "contents": "\r\nOut of curiosity and a bit for fun, I just completed the 11-item height inventory by Hynek Cigler from the Department of Psychology, FSS MU, a teaching aid designed to demonstrate some of the basics of measurement in psychology and related fields.\r\nInstead of using a ruler, you indicate how much you agree with statements like ‚ÄúI often have to stand on my tiptoes to see better.‚Äù or ‚ÄúI have plenty of legroom on the bus.‚Äù\r\nIts decent psychometric qualities showed up in a fairly accurate estimate of my actual height (193 cm): it estimated my body height at 189 cm with a 95% confidence interval of [182-197] cm. The attached pic illustrates where the test put me. In terms of the height quotient (IQ), I was at 117 with a 95% CI of [100-129], so it‚Äôs not up to Mensa, but at least I won‚Äôt have to pay membership fees üòÅ\r\n\r\n\r\n\r\nIf you are doing training in psychometrics or need to educate your audience in this area, this tool can be very handy.\r\nThe inventory is in Czech, but the online translation in Chrome does a pretty good job, so you can try it out for yourself even if you‚Äôre not a Czech speaker (with all the limitations given by non-standard localization and not 100% adequate norms). You can find it here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-05-30-body-height-inventory/./height_histogram_results.jpg",
    "last_modified": "2024-05-30T10:37:29+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-05-21-probability-of-comments-in-a-survey/",
    "title": "What makes people more likely to comment on a question in an employee survey?",
    "description": "Is it satisfaction or dissatisfaction that drives comments? Or perhaps it‚Äôs the extremes on both ends of the satisfaction spectrum?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-05-21",
    "categories": [
      "employee survey",
      "employee experience",
      "machine learning",
      "ml interpretation",
      "python"
    ],
    "contents": "\r\nI‚Äôve been curious about this for a while, but until recently, I only had my personal pet theories to rely on. Luckily, one of my recent projects gave me the chance to explore this question with real-world data and satisfy my curiosity a bit.\r\nWhy bother? Well, you can get a better feel for the representativeness of the comments, from which a lot of useful insights into the employee experience can be gleaned.\r\nBtw, what‚Äôs your guess? And try to make a prediction before reading on and/or checking the charts ‚Äì with hindsight the results may seem too obvious üòâ\r\nTo test my ideas, I used a classification RF model to be able to capture non-linear relationships, and used item score and some common controls as predictors of whether an employee would leave a comment on a given item. Then, I applied Partial Dependence Plot ‚Äì a global ML interpretation tool ‚Äì to the fitted model to examine the relationship between item scores and the likelihood of leaving a comment.\r\n\r\n\r\n\r\nWhat were the results? Well, as usual, it depends. However, across the sample of items shown, we can observe a common pattern of a non-linear, S-reversed-shaped relationship. Dissatisfied employees tend to comment more, except for those who are extremely dissatisfied. As satisfaction increases, the probability of commenting decreases, only to slightly rise again as we approach a satisfaction level of 10. Generally, it can be said that less satisfied employees comment more on average. Given that we collect feedback from employees to improve things, it makes kind of sense, right? ü§ì\r\nDoes this match your expectations, or are you surprised? Would you expect different patterns for different items? Have you conducted a similar exercise with your own data? If so, what were the results? Perhaps you also know of some relevant research on this topic. Feel free to share.\r\nP.S. If you would like to replicate this analysis using your own data, you can use the following Python script as inspiration.\r\n\r\n\r\nShow code\r\n# required libraries\r\n# data manipulation\r\nimport pandas as pd\r\nimport numpy as np\r\nimport copy\r\n# dataviz\r\nfrom plotnine import *\r\n# ML\r\nfrom sklearn.model_selection import train_test_split, GridSearchCV\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n# ML explanation & interpretation\r\nfrom sklearn.inspection import partial_dependence\r\n\r\n# function for changing snake case names to titles (to beautify titles in generated charts)\r\ndef snake_to_title(snake_str):\r\n    # Split the string by underscores\r\n    words = snake_str.split('_')\r\n    # Capitalize each word\r\n    capitalized_words = [word.capitalize() for word in words]\r\n    # Join the words with spaces\r\n    title_case_string = ' '.join(capitalized_words)\r\n    return title_case_string\r\n\r\n\r\n# the analysis assumes wide-format data with individual-level records of employees' responses to employee survey questions on a scale 0-10 and their comments to these questions  \r\nmydata = pd.read_csv('your_data.csv')\r\n\r\n# list of survey items of interest\r\nitems = [\r\n    'autonomy',\r\n    'engagement',\r\n    'workload',\r\n    'recognition',\r\n    'reward',\r\n    'strategy',\r\n    'growth',\r\n    'management_support',\r\n    'peer_relationship',\r\n    'diversity_inclusion',\r\n    'health_wellbeing_balance'\r\n]\r\n\r\n# looping over individual items\r\nfor item in items:\r\n  \r\n    print(item)\r\n    \r\n    # dataset to be used for ML task\r\n    ml_data = copy.deepcopy(mydata)\r\n\r\n    # name of the field with comments to a specific question\r\n    item_comment = f'comment_{item}'\r\n\r\n    # creating a flag indicating presence/absence of a comment\r\n    ml_data[item_comment] = np.where(\r\n        ml_data[item_comment].isna(), False, True\r\n    )\r\n\r\n    # keeping only those employees who replied to the question on a scale 0-10\r\n    ml_data.dropna(subset=[item], inplace=True)\r\n\r\n    # defining the predictors and target variable\r\n    predictors = [item, 'age', 'gender', 'country', 'job_family_group', 'is_manager', 'management_level', 'org_unit', 'tenure']\r\n    target = item_comment\r\n\r\n    # stratified split of the data into training and testing sets\r\n    X = ml_data[predictors]\r\n    y = ml_data[target]\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1979, stratify=y)\r\n\r\n    # defining the column transformer for data pre-processing\r\n    preprocessor = ColumnTransformer(\r\n        transformers=[\r\n            ('num', StandardScaler(), ['age', 'tenure']),\r\n            ('cat', OneHotEncoder(drop='first'), [item, 'gender', 'country', 'job_family_group', 'is_manager', 'management_level', 'org_unit'])\r\n        ]\r\n    )\r\n\r\n    # Random Forest Classifier\r\n    # skipping hyper-parameter fine-tuning for the sake of brevity\r\n    rf = RandomForestClassifier(min_samples_leaf=5, min_samples_split=30, n_estimators=500,random_state=1979)\r\n    \r\n\r\n    # creating a pipeline\r\n    pipeline = Pipeline(steps=[\r\n        ('preprocessor', preprocessor),  \r\n        ('classifier', rf)\r\n    ])\r\n\r\n    # fitting the pipeline\r\n    pipeline.fit(X_train, y_train)\r\n    \r\n    # skipping assessment of the quality of the model for the sake of brevity\r\n\r\n    # PDP (Partial Dependence) plots\r\n    # size of the sample of individual conditional expectation (ICE) curves\r\n    n = 500\r\n    feature_names = X.columns\r\n    fIndex = np.where(feature_names == item)[0][0]\r\n    pdp_results = partial_dependence(pipeline, X_train, [fIndex], grid_resolution=50, kind=\"both\")\r\n\r\n    # extracting the data\r\n    values = pdp_results['values'][0]\r\n    average = pdp_results['average'][0]\r\n    individual = pdp_results['individual'][0]\r\n\r\n    # df for the average line\r\n    pdp_data_avg = pd.DataFrame({\r\n        'Score': values,\r\n        'Partial Dependence': average\r\n    })\r\n\r\n    # df for the ICE curves\r\n    pdp_data_ind = pd.DataFrame(individual, columns=values)\r\n    pdp_data_ind['ID'] = pdp_data_ind.index\r\n    pdp_data_ind = pdp_data_ind.melt(id_vars='ID', var_name='Score', value_name='Partial Dependence')\r\n\r\n    # sampling 500 unique IDs\r\n    sampled_ids = pdp_data_ind['ID'].unique()\r\n    if len(sampled_ids) > n:\r\n        np.random.seed(1979)\r\n        sampled_ids = np.random.choice(sampled_ids, n, replace=False)\r\n\r\n    pdp_data_ind = pdp_data_ind[pdp_data_ind['ID'].isin(sampled_ids)]\r\n    pdp_data_ind['Score'] = pdp_data_ind['Score'].astype(float)\r\n\r\n    # plotting the results\r\n    item_title = snake_to_title(item)\r\n    plot = (\r\n        ggplot() +\r\n        geom_line(aes(x='Score', y='Partial Dependence', group='ID'), size=0.1, alpha=0.2, data=pdp_data_ind) + \r\n        geom_line(aes(x='Score', y='Partial Dependence', group=1), size=1.5, data=pdp_data_avg) +  \r\n        scale_x_continuous(breaks=range(0,11)) +\r\n        labs(\r\n            title=f'PDP plot for score on \"{item_title}\" survey item',\r\n            x=f'Score on \"{item_title}\" survey item',\r\n            y='Probability of commenting'\r\n        ) +\r\n        theme_bw() + \r\n        theme(    \r\n            plot_title=element_text(size=18, margin={'t': 0, 'r': 0, 'b': 10, 'l': 0}), \r\n            axis_text=element_text(size=12), \r\n            axis_title=element_text(size=14), \r\n            axis_title_x=element_text(margin={'t': 10, 'r': 0, 'b': 0, 'l': 0}), \r\n            axis_title_y=element_text(margin={'t': 0, 'r': 10, 'b': 0, 'l': 0}), \r\n            strip_text_x=element_text(size=13),\r\n            panel_grid_major=element_blank(),  \r\n            panel_grid_minor=element_blank(),\r\n            figure_size=(11, 6)\r\n        ) \r\n    )\r\n\r\n    #print(plot)\r\n    \r\n    # saving the plot\r\n    plot.save(filename=f\"{item}_item_pdp.png\", width=11, height=6, dpi=500)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-05-21-probability-of-comments-in-a-survey/./comment_illustration.jpg",
    "last_modified": "2024-05-30T10:35:07+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-04-28-netlogo/",
    "title": "NetLogo: Don‚Äôt tell me, show me",
    "description": "An example of how to get a better understanding of various complex phenomena through simulation in NetLogo, a free programmable multi-agent modelling environment.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-04-28",
    "categories": [
      "simulation",
      "netlogo",
      "education"
    ],
    "contents": "\r\nRecently, my 10-year-old son came across the concept of the exploration vs.¬†exploitation dilemma in one of his books and wanted me to help him understand it.\r\nI got quite sweaty in explaining it before I remembered NetLogo, which contains a series of pre-programmed simulations of various multi-agent systems and emergent phenomena.\r\nOne of them shows how this particular dilemma is solved by a colony of ants foraging for food using a very simple but effective system of rules:\r\nRandom Exploration: Ants explore randomly, ensuring the discovery of new resources.\r\nPheromone Trails: When an ant finds a piece of food, it carries the food back to the nest, dropping a chemical as it moves. When other ants ‚Äúsniff‚Äù the chemical, they follow the chemical toward the food. As more ants carry food to the nest, they reinforce the chemical trail, focusing on exploitation.\r\nPheromone Decay: Trails weaken over time, prompting exploration when resources dwindle.\r\n\r\n\r\nYour browser does not support the video tag.\r\n\r\n\r\n\r\nAfter watching a few rounds of the simulation and a brief explanation, everything became much clearer to my son. If you are ever faced with similar types of questions, give NetLogo a chance. It‚Äôs free to use, contains a number of pre-programmed simulations, and if you don‚Äôt find what you‚Äôre looking for there, it‚Äôs not hard to learn how to program what you need in NetLogo.\r\nBtw, I can‚Äôt wait for my son to stumble upon the topic of how order can arise without some central controlling authority - there‚Äôs a very effective bird flocking simulation in NetLog for that üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-04-28-netlogo/./netlogo.jpg",
    "last_modified": "2024-04-29T10:36:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-04-21-feedback-effect-on-performance/",
    "title": "Feedback can be a gift or a curse...",
    "description": "...depending on where it directs your locus of attention.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-04-21",
    "categories": [
      "feedback",
      "performance",
      "meta-analysis"
    ],
    "contents": "\r\nFrom personal experience, I‚Äôve ‚Äúalways‚Äù known that feedback can have both positive and negative effects on my performance, depending on various factors (e.g.¬†clarity, tone, timing, etc.). However, I wasn‚Äôt aware of the data supporting this dual nature of feedback until I stumbled upon a meta-analysis by Kluger & DeNisi (1996).\r\nTheir study analyzed 607 effect sizes from 23,663 observations and found that while feedback interventions (FIs) generally improved performance (d=0.41), over one-third of the FIs actually decreased performance.\r\n\r\n\r\n\r\nTo explain this pattern, the authors proposed and tested, using moderator analysis, a theory suggesting that FIs shift the focus among three hierarchically organized levels of control, influencing how individuals process and react to feedback:\r\nMeta-tasks (including self-related processes): Feedback at this level often focuses on the individual‚Äôs feelings and identity rather than the task itself. It can be counterproductive by diverting attention away from the task and toward the self, potentially leading to defensive responses or disengagement.\r\nTask motivation processes: This level aims to affect the motivational factors that drive an individual to undertake and persist with a task. Effective feedback here can enhance motivation by setting goals, encouraging persistence, or highlighting the task‚Äôs importance, thereby boosting willingness to invest effort.\r\nTask learning processes: Feedback at this level provides specific guidance on improving task performance, including details on errors, corrective actions, or skill enhancement techniques.\r\nThe results showed that the effectiveness of FI indeed tends to decrease as the focus shifts upward in the hierarchy, moving closer to self-related aspects and away from the task at hand. So the next time you ask for feedback, you should now know better what to wish for üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-04-21-feedback-effect-on-performance/./effects_distribution.png",
    "last_modified": "2024-04-28T21:03:35+02:00",
    "input_file": {},
    "preview_width": 839,
    "preview_height": 689
  },
  {
    "path": "posts/2024-03-21-nice-leaders/",
    "title": "If you are a leader, don't be afraid to be a nice one",
    "description": "Do nice guys really finish last?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-03-21",
    "categories": [
      "personality",
      "agreeableness",
      "leadership emergence",
      "leadership effectiveness"
    ],
    "contents": "\r\nContrary to the old adage that ‚Äúnice guys finish last‚Äù, a meta-analysis by Blake et al.¬†(2022) that examined the relationship between leader agreeableness and leadership outcomes found that leader agreeableness was positively related to both leadership effectiveness and leadership emergence, though more strongly to the latter.\r\n\r\n\r\n\r\nIn the authors‚Äô own words, ‚ÄúIt seems that leaders no longer need to choose to be ‚Äúeffective‚Äù or ‚Äúnice,‚Äù but rather both can be achieved simultaneously.‚Äù\r\nInterestingly, this applies, though perhaps to varying degrees, to leaders regardless of whether they work in collectivist or individualist cultures, whether they are executive or non-executive leaders, or whether they are male or female.\r\nConsidering that a large part of a manager‚Äôs job is interacting with other people, be it individual employees, entire teams or larger groups, we probably shouldn‚Äôt be so surprised. Still, it‚Äôs good to see some support for this claim in the data.\r\nThe article also contains other interesting results of various moderation and post-hoc analyses, so I recommend you check out the original (open-access) article linked above.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-03-21-nice-leaders/./leader.jpg",
    "last_modified": "2024-03-21T11:21:37+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-03-06-agile-project-management/",
    "title": "Does agile pay off?",
    "description": "During my career, I have been part of several teams that have organized their work using some of the existing agile methodologies. For the most part, agile principles made good sense to me and subjectively seemed to work, so I had no reason to question their supposed benefits.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-03-06",
    "categories": [
      "organizational science",
      "project management",
      "agile",
      "meta-analysis"
    ],
    "contents": "\r\nI must admit with some shame that it was only when I came across the meta-analysis by Koch et al.¬†(2023) that I wondered whether there was any systematic evidence to support the benefits of an Agile Project Management (APM).\r\nThe authors of this pre-registered meta-analysis (with k = 41 independent studies, N = 73,825) focused specifically on the affective, behavioral and cognitive outcomes of APM.\r\nWhat did they find? Beneficial effects of APM across all three outcomes:\r\nFor the affective outcomes of job satisfaction, affective strain and organizational commitment, the effect sizes were, on average, small.\r\nFor the behavioral outcomes of performance and innovative behavior, the effect sizes were medium to large.\r\nFor the cognitive outcome of psychological empowerment, the results of the meta‚Äêanalysis suggest a medium effect.\r\n\r\n\r\n\r\nInteresting were also the results of the analysis of moderating effects of contextual factors, specifically team size (no significant moderating effect), and occupational groups (compared to studies conducted with software developers, the effect sizes were stronger in other occupations, such as manufacturing, health care and logistics).\r\nSo pretty good news for APM proponents, however, it‚Äôs good to keep in mind that this is ‚Äúonly‚Äù a preliminary meta-analysis, that there aren‚Äôt that many studies, especially regarding affective and cognitive outcomes, that the relationships found cannot be interpreted as causal as 90% of the studies only report cross-sectional correlations, and that there is also some evidence of publication bias that may overestimate the effects.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-03-06-agile-project-management/./agile_meeting.jpg",
    "last_modified": "2024-03-06T11:14:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-02-29-personality-frameworks-contest/",
    "title": "A showdown between the Big Five, Enneagram, MBTI, and astrology",
    "description": "Who you got your money on? üòâ",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-02-29",
    "categories": [
      "personality",
      "predictive validity",
      "big five",
      "enneagram",
      "mbti",
      "astrology"
    ],
    "contents": "\r\nJust came across a nice, however small (N=559) and, AFAIK, non-peer reviewed, study comparing accuracy of popular personality test frameworks at predicting 37 different life outcomes (e.g., frequency of exercise, life satisfaction, number of close friends, etc.).\r\nThe main findings? They shouldn‚Äôt be a big surprise to I/O psych folks, but IMO they still contain two or three interesting, not widely known details.\r\nThe Big Five personality test was more accurate than Jungian (MBTI-based) and Enneagram tests in predicting life outcomes. Fun fact, MBTI-style test was halfway between science and astrology, literally üßôÔ∏è\r\nThe accuracy of these tools is negatively related to their popularity as measured by the frequency of Google searches.\r\nRemoving Neuroticism from the Big Five reduced its predictive power significantly.\r\nMost personality traits form bell curves, suggesting that MBTI‚Äôs binary categorization may be less accurate.\r\nContinuous scores in the Jungian framework improved predictions over binary categories, but even with adjustments, the Big Five (minus Neuroticism) slightly outperformed the modified Jungian test.\r\nJungian traits correlated with specific Big Five traits, but combining both tests didn‚Äôt enhance predictive accuracy.\r\nDespite its simplicity, the Enneagram performed better than the binary Jungian Type but was still less effective than the Big Five.\r\nParticipants preferred their Jungian assessments, likely due to its more positive framing and missing Neuroticism trait.\r\n\r\n\r\n\r\nNote: Link to the related Scientific American article.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-29-personality-frameworks-contest/./phrenology.jfif",
    "last_modified": "2024-03-01T09:57:18+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-02-26-expected-remaining-time/",
    "title": "Does your team belong among ‚Äúlight bulbs‚Äù or ‚Äúwines‚Äù?",
    "description": "Sharing one learning from the awesome book Probably Overthinking It by Allen B. Downey.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-02-26",
    "categories": [
      "employee turnover",
      "statistical analysis",
      "python",
      "book tip"
    ],
    "contents": "\r\nFor those interested in data analytics, I highly recommend the book Probably Overthinking It by Allen B. Downey. It offers insightful details on the various, more or less known data analytical situations that one might encounter while using data to understand the world better or to make more informed decisions.\r\nEven if you are not a beginner in data analytics, there is a good chance that you will come across some new and valuable insights, as it happened to me.\r\nFor example, as a people analytics practitioner, I found it particularly enlightening to examine the survival function - commonly used in employee attrition modeling - from the perspective of average remaining time and related concepts of new better than used in expectation (NBUE) vs.¬†new worse than used in expectation (NWUE). From now on, for me, org units exhibiting NBUE and NWUE characteristics are ‚Äúlight bulbs‚Äù and ‚Äúwines‚Äù, respectively üòâ\r\nIf you‚Äôre curious about how to get from survival function to average remaining time, check out the short snippets of Python code below that do the trick. It‚Äôs not a big deal, but it can still save you some time üëá\r\nFirst, let‚Äôs upload the dummy data and create functions that allow us to estimate and display the survival curve and the corresponding expected remaining time curve.\r\n\r\n\r\nShow code\r\n\r\n# libraries used\r\nimport pandas as pd\r\nfrom plotnine import *\r\nfrom sksurv.nonparametric import kaplan_meier_estimator\r\n\r\n# uploading the data\r\ndata_bulb = pd.read_csv('./attrition_data_bulb.csv')\r\ndata_wine = pd.read_csv('./attrition_data_wine.csv')\r\n\r\n# function for estimating and plotting the survival function \r\ndef survival_function_estimation_plotting(data, plot_name = 'chart'):    \r\n    # estimating the survival function using the Kaplan-Meier estimator\r\n    time, survival_prob, conf_int = kaplan_meier_estimator(data[\"event\"], data[\"tenure_years\"], conf_level=0.95, conf_type=\"log-log\")\r\n    \r\n    # supp df for dataviz\r\n    supp_df = pd.DataFrame({\r\n        'time': time,\r\n        'survival_prob': survival_prob,\r\n        'lower_ci': conf_int[0],\r\n        'upper_ci': conf_int[1]\r\n    })\r\n    \r\n    # dataviz\r\n    plot = (\r\n        ggplot(supp_df, aes(x='time')) +\r\n        geom_step(aes(y='survival_prob'), color='#23004C', size=1) +\r\n        geom_ribbon(aes(ymin='lower_ci', ymax='upper_ci'), alpha=0.25) +\r\n        scale_y_continuous(limits=[0,1]) +\r\n        labs(\r\n            title=\"Probability of staying over time since joining the company\",\r\n            x='TIME (IN YEARS)', \r\n            y='ESTIMATED PROBABILITY OF STAYING'\r\n        ) +\r\n        theme_bw() +\r\n        theme(\r\n            plot_title=element_text(size=20, margin={'b': 12}, ha='left'),\r\n            axis_title_x=element_text(size=15, margin={'t': 15}),\r\n            axis_title_y=element_text(size=15, margin={'r': 15}),\r\n            axis_text_x=element_text(size=10),\r\n            axis_text_y=element_text(size=10),\r\n            strip_text_x=element_text(size=14, weight='bold'),\r\n            panel_grid_major=element_blank(),\r\n            panel_grid_minor=element_blank(),\r\n            figure_size=(12, 6.5)\r\n        ) \r\n    )\r\n    \r\n    # saving the plot\r\n    # ggsave(plot=plot, filename=f'survival_curve_{plot_name}.png', width=12, height=6, dpi=500)\r\n    \r\n    # printing the plot\r\n    print(plot)\r\n\r\n\r\n\r\n# function for computing the average remaining time in the company\r\ndef remaining_time(data):    \r\n    results = []\r\n    # iterating over each time point in the data's index\r\n    for t in data.index:\r\n        if data.loc[t, \"survival_prob\"] > 0:\r\n            # calculating the conditional survival probabilities from time t onwards\r\n            # by dividing the survival probabilities by the survival probability at time t\r\n            conditional_df = data.loc[t:, \"survival_prob\"] / data.loc[t, \"survival_prob\"]\r\n            \r\n            # removing the survival probability at time t from the calculations \r\n            # as it's not needed for the expected additional time calculation\r\n            conditional_df = conditional_df.iloc[1:]\r\n            \r\n            # calculating the expected additional time by taking the weighted average \r\n            # of the time points, using the conditional survival probabilities as weights\r\n            expected_additional_time = sum(conditional_df.values * (conditional_df.index - t)) / conditional_df.sum()\r\n            result = {\r\n                \"time\": t, \r\n                \"expected_additional_time\": expected_additional_time\r\n            }\r\n            \r\n            # adding the result to the results list\r\n            results.append(result)\r\n        else:\r\n            pass\r\n    \r\n    # converting the results list to a DataFrame\r\n    return pd.DataFrame(results)\r\n\r\n\r\n\r\n# function for estimating the uncertainty using the bootstrapping technique and for plotting the results\r\ndef remaining_time_bootstraping_plotting(data, n_bootstrap=100, plot_name='chart'):    \r\n    # calculating the expected remaining time using all the data\r\n    time, survival_prob = kaplan_meier_estimator(data[\"event\"], data[\"tenure_years\"])\r\n    supp_df_all = pd.DataFrame({'time': time, 'survival_prob': survival_prob})\r\n    supp_df_all = supp_df_all .set_index('time')\r\n    all_results_df = remaining_time(supp_df_all) \r\n\r\n    # estimating the uncertainty using the bootstrapping technique\r\n    all_curves = []\r\n    \r\n    for i in range(n_bootstrap):\r\n        # resampling the data with replacement\r\n        bootstrap_sample = data.sample(n=len(data), replace=True)\r\n        # estimating the survival function using the Kaplan-Meier estimator\r\n        time, survival_prob = kaplan_meier_estimator(bootstrap_sample[\"event\"], bootstrap_sample[\"tenure_years\"])\r\n        # supp df for further calculations\r\n        supp_df = pd.DataFrame({'time': time, 'survival_prob': survival_prob})\r\n        supp_df  = supp_df.set_index('time')\r\n        # calculating the expected remaining time\r\n        remaining_time_bootstrap = remaining_time(supp_df)\r\n        # adding a column to identify the bootstrap iteration\r\n        remaining_time_bootstrap['bootstrap_id'] = i\r\n        # adding the result to the all_curves list\r\n        all_curves.append(remaining_time_bootstrap)\r\n    \r\n    # concatenating all bootstrap results into a single df\r\n    bootstrap_results_df = pd.concat(all_curves)\r\n    \r\n    # dataviz\r\n    plot = (\r\n        ggplot() +\r\n        geom_step(bootstrap_results_df, aes(x='time', y='expected_additional_time', group='bootstrap_id'), color='grey', alpha=0.1) +\r\n        geom_step(all_results_df, aes(x='time', y='expected_additional_time'), color='#23004C', size=1) +\r\n        labs(\r\n            title=\"Average remaining time in the company\",\r\n            x='TIME SINCE JOINING THE COMPANY (IN YEARS)', \r\n            y='AVERAGE REMAINING TIME (IN YEARS)'\r\n        ) +\r\n        theme_bw() +\r\n        theme(\r\n            plot_title=element_text(size=20, margin={'b': 12}, ha='left'),\r\n            axis_title_x=element_text(size=15, margin={'t': 15}),\r\n            axis_title_y=element_text(size=15, margin={'r': 15}),\r\n            axis_text_x=element_text(size=10),\r\n            axis_text_y=element_text(size=10),\r\n            strip_text_x=element_text(size=14, weight='bold'),\r\n            panel_grid_major=element_blank(),\r\n            panel_grid_minor=element_blank(),\r\n            figure_size=(12, 6.5)\r\n        ) \r\n    )\r\n    \r\n    # saving the plot\r\n    # ggsave(plot=plot, filename=f'remaining_time_curve_{plot_name}.png', width=12, height=6, dpi=500)\r\n    \r\n    # printing the plot\r\n    print(plot)\r\n\r\nNow let‚Äôs estimate and visualize these two curves for a ‚Äúlight bulb‚Äù team (i.e., a team exhibiting the NBUE characteristic)‚Ä¶\r\n\r\n\r\nShow code\r\nsurvival_function_estimation_plotting(data=data_bulb)\r\n\r\nShow code\r\nremaining_time_bootstraping_plotting(data=data_bulb)\r\n\r\n\r\n‚Ä¶ and now for a ‚Äúwine‚Äù team (i.e., a team exhibiting, at least partially, the NWUE characteristic).\r\n\r\n\r\nShow code\r\nsurvival_function_estimation_plotting(data=data_wine)\r\n\r\nShow code\r\nremaining_time_bootstraping_plotting(data=data_wine)\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-26-expected-remaining-time/./bulb_wine.png",
    "last_modified": "2024-02-27T15:21:26+01:00",
    "input_file": {},
    "preview_width": 1001,
    "preview_height": 888
  },
  {
    "path": "posts/2024-02-15-video-interviews-and-biases/",
    "title": "Biases introduced by video backgrounds during video interviews",
    "description": "Every tool used for employee selection introduces its own potential biases into the process. This holds true also for video interviews, which have become increasingly popular among recruiters in recent years.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-02-15",
    "categories": [
      "employee selection",
      "video interview",
      "bias"
    ],
    "contents": "\r\nA recent set of experimental studies conducted by Roulin et al.¬†(2022) sought to determine whether and how the backgrounds in video interviews (VIs) could influence evaluators‚Äô judgments by revealing potentially stigmatizing features that are typically hidden in traditional selection contexts, but can become visible during VIs. The research specifically explored the impact of parental status, sexual orientation, and political affiliation on evaluators‚Äô perceptions.\r\n\r\n\r\n\r\nThe results?\r\nApplicants identified as parents were perceived as warmer and received higher ratings for interview performance, yet they were not judged more negatively in terms of competence or potential work performance.\r\nApplicants who supported the same political party as the evaluator were seen as warmer and received higher assessments of both interview performance and potential work performance.\r\nSexual orientation had no impact on any of the outcome variables.\r\nThese results suggest that standardizing video backgrounds during virtual interviews (VIs) should be considered a best practice, which is in line with the logic behind structured interviews that try to limit the variability in interview performance to the applicants‚Äô KSAOs relevant for job performance.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-15-video-interviews-and-biases/./video_backgrounds.png",
    "last_modified": "2024-02-15T19:14:57+01:00",
    "input_file": {},
    "preview_width": 1336,
    "preview_height": 771
  },
  {
    "path": "posts/2024-02-05-topic-analysis-with-genai/",
    "title": "Creating new candidate topic labels on the fly during topic analysis with GenAI",
    "description": "Description of a simple hack to simulate the work of a qualitative researcher classifying comments from respondents using GenAI.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-02-05",
    "categories": [
      "topic analysis",
      "feedback analysis",
      "genai",
      "gpt",
      "ai"
    ],
    "contents": "\r\nOne way to use GenAI for topic analysis of employee or customer feedback is to use zero-shot learning and test each comment against a pre-set list of topics that you think cover all relevant themes of interest.\r\nHowever, as you can imagine, real people can always surprise you and come up with themes that you hadn‚Äôt thought of beforehand.\r\nA simple hack to deal with this is to dynamically update the list of candidate topic labels within a loop where GenAI first tries to use the currently existing topic labels to capture the meaning of the comment being analyzed and create new topic label(s) if the existing ones don‚Äôt match the meaning of the comment. These new labels are then added to the list to be available for analysis of all subsequent comments.\r\nIn another higher-level loop, this analysis can be performed several times to ensure that we capture all the themes present in the corpus of available comments, and that we consider all the identified topic labels for all comments.\r\nThe whole process is thus not dissimilar to what a qualitative researcher does when going through the responses of their respondents and classifying those responses using categories based on previous responses or using new categories until new themes stop appearing.\r\nA bit of a challenge may be how to enforce that GenAI comes up with topic labels at a level of abstraction appropriate for a given use case. Based on my experiments, it seems that by providing some examples and counterexamples in the prompt and/or by providing ‚Äúseed‚Äù topic labels at the ‚Äúright‚Äù level of abstraction, you can nudge GenAI to create new labels at the desired level of abstraction.\r\nBelow is a short snippet of Python code implementing this approach to topic analysis that you can use for inspiration. Happy topic modeling and exploration üòâ\r\n\r\nfrom openai import OpenAI\r\nclient = OpenAI(api_key=myOpenAiApiKey)\r\n\r\n# function for detecting topics in comments\r\ndef identify_topics(text, topic_labels, item_text):\r\n    prompt = client.chat.completions.create(\r\n        model=\"gpt-4-1106-preview\",\r\n        temperature=0,\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": f\"\"\"\r\n            You are a first-class expert in analyzing feedback provided by employees in employee surveys.\r\n            Here is a list of available topic labels for feedback categorization: {','.join(topic_labels)}\"\"\"},\r\n            {\"role\": \"user\", \"content\": f\"\"\"              \r\n            First, summarize for yourself the feedback provided by an employee. When doing so, be sure to\r\n            consider the wording of the survey item on which the employee has commented to make sure you have\r\n            understood her feedback correctly. Here is the wording of the survey item: {item_text} \r\n            Then, based on this summarization, from the list of available topic labels, select those that\r\n            accurately capture all important aspects of the employee feedback. You are forbidden to use topic \r\n            labels that only indirectly or remotely relate to the meaning of employee feedback. Use the topic \r\n            label only if you are sure that it accurately captures one of the important aspects of employee\r\n            feedback. Don't make things up and report only what is really in the employee feedback.    \r\n            Important: If the topic labels available in the list don't capture some important aspects of employee\r\n            feedback, you must create and use a new topic label or multiple labels to capture these aspects of\r\n            employee feedback. New topic labels must be short, concise, not too general, and specific enough that\r\n            the management can easily understand what the feedback is about. For example, a more specific topic\r\n            label like \"HR Ticketing System Problems\" would be much better than a more general topic label like \r\n            \"HR System Inefficiencies\". \r\n            As a response provide only a string of relevant topic labels separated by commas, nothing more. \r\n            Here is the employee's feedback: {text}\r\n            \"\"\"}\r\n        ]\r\n    )\r\n    response = prompt.choices[0].message.content\r\n    return response\r\n\r\n# seed list of candidate topic labels to be enriched during the analysis\r\ntopic_labels = []\r\n# higher-level loop for repeating the analysis on all the comments\r\nfor run in range(0,3):\r\n    responses = []\r\n    # loop over individual comments\r\n    for row in range(0, df.shape[0]):\r\n        response = identify_topics(text=df.loc[row, 'feedback_text'], topic_labels=topic_labels, item_text=df.loc[row, 'question_text'])\r\n        responses.append(response)\r\n        # extracting identified topics\r\n        identified_topic_labels = response.split(\",\")\r\n        # removing leading or trailing white space\r\n        identified_topic_labels = [item.strip() for item in identified_topic_labels]\r\n        # enriching the list of candidate topic labels\r\n        topic_labels = topic_labels + identified_topic_labels\r\n        # deduplication of topic labels\r\n        topic_labels = list(set(topic_labels))\r\n\r\n# creating field with identified topics in the df\r\ndf['topic_labels'] = responses\r\n\r\n# adding fields with flags for identified topics for easier filtering of relevant comments\r\nfor topic in topic_labels:\r\n    df[topic] = df['topic_labels'].str.contains(topic)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-05-topic-analysis-with-genai/./feedback.jpg",
    "last_modified": "2024-02-05T19:09:14+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-21-psychological-safety-and-rct/",
    "title": "Fostering psychological safety in the workplace",
    "description": "Can a simple intervention such as an email campaign help with fostering psychological safety in the workplace?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-01-21",
    "categories": [
      "psychological safety",
      "rct",
      "field experiment",
      "evidence-based management",
      "causality"
    ],
    "contents": "\r\nAt Sanofi, we recently welcomed Prof.¬†Dr.¬†Florian Englmaier at our People Insights Seminar, where he shared with us his intriguing research on interventions that enhance psychological safety.\r\nAs he pointed out at the start of his talk, while we all understand the importance of psychological safety, the specific actions that effectively increase it are less clear. His research aimed to bridge this knowledge gap.\r\nConducted in a real-world setting, the study encompassed over 1,000 teams and more than 7,000 employees within a global healthcare company. It utilized a RCT research design, a method not commonly seen in People Analytics, but more effective for identifying causal relationships.\r\nThe research team compared two types of brief 6-week email campaigns, which encouraged managers to refocus and make regular their 1:1 meetings, against a control group:\r\nThe first group of managers was prompted to focus on addressing employees‚Äô individual needs and aspirations.\r\nThe second group was encouraged to help employees better execute tasks and eliminate obstacles impacting their performance.\r\nBelow you can see the used meeting guidance corresponding to the two treatment conditions (individual needs vs.¬†tasks focus).\r\n\r\n\r\n\r\n\r\n\r\n\r\nDespite their simplicity, these interventions led to a noticeable increase in psychological safety, particularly in the group focusing on employees‚Äô individual needs. Additionally, they improved the relationships to and perceptions of the managers.\r\nOther significant and instrumental findings included an increase in the frequency of 1:1 meetings, as evidenced by Microsoft Workplace Analytics and survey data, and the observation that junior managers benefited most from these interventions.\r\nFor more details, see the original paper.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-21-psychological-safety-and-rct/./free_communication.jpg",
    "last_modified": "2024-02-05T13:04:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-11-personality-assessments-and-faking/",
    "title": "Fighting with faking in personality assessments",
    "description": "A brief summary of the main results of a meta-analysis comparing psychometric characteristics of forced-choice and single-stimulus personality assessments in relation to faking.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-01-11",
    "categories": [
      "personality",
      "assessment",
      "psychometrics",
      "faking",
      "validity"
    ],
    "contents": "\r\nIf you are using personality assessments in a high-stakes setting where faking is a more frequent problem, such as in the context of employee selection, you should pay attention to the meta-analytic study by Speer et al.¬†(2023) that compared forced-choice (FC) and traditional single-stimulus (SS) personality assessments in terms of criterion-related validity and susceptibility to faking in terms of mean shifts and decreases in validity.\r\n\r\n\r\n\r\nA specific feature of this study was that it compared FC and SS measures after placing them on an equal playing field by relying only on studies that examined matched assessments of each format, and thus, avoiding the extraneous confound of using comparisons from different contexts.\r\nAnd what were the results?\r\nAverage scores increased from honest to faked samples for both FC (d = .41) and SS scores (d = .75), though the effect was more pronounced for SS measures and with larger effects for context-desirable traits (FC d = .61 vs.¬†SS d = .99).\r\nCriterion-related validity was similar between matched FC and SS measures overall (r‚Ä≤ = .19 vs.¬†r‚Ä≤ = .18), however, when in faking contexts, FC scores exhibited greater criterion-related validity than SS measures (r‚Ä≤ = .18 vs.¬†r‚Ä≤ = .04).\r\nSo, although FC measures are not completely immune to faking, they seem to show meaningful benefits over SS measures in contexts of faking.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-11-personality-assessments-and-faking/./faking_illustration.png",
    "last_modified": "2024-01-11T14:21:49+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2024-01-08-team-design-creativity-innovation/",
    "title": "How should teams be designed to be creative and innovative?",
    "description": "A brief overview of the results of a meta-analysis on the relationship between team design and team creativity and innovation.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-01-08",
    "categories": [
      "team design",
      "innovation",
      "creativity",
      "leadership",
      "meta-analysis"
    ],
    "contents": "\r\nIf you are responsible for managing a team that is expected to be creative and come up with innovative solutions, you may be interested in the findings of Byron et al.‚Äôs (2023) meta-analysis of team-design-related antecedents of team creativity and innovation. The meta-analysis aggregated results from 134 field studies (11,353 teams) and 35 student studies (2,485 teams) and revealed the following regularities, among others:\r\nStructuring teams to work interdependently (i.e.¬†with greater interdependence of tasks and goals) is supportive of team creativity and innovation.\r\nLeaders providing autonomy to team members have teams that are more creative and innovative and this relationship is stronger when leaders emphasize team autonomy than when they emphasize the autonomy of individual team members.\r\nRelated to the previous point, leaders who limit follower control also limit team creativity and innovation.\r\nTeam size, demographic diversity, and job-related diversity show only a weak relationship with team creativity and innovation.\r\nTeam task interdependence and supportive leadership are positively related to team creativity and innovation via processes of team collaboration (the extent to which team members work together to share information and knowledge) and team potency (the extent to which team members believe they can be effective).\r\nThere is evidence for a curvilinear relationship between team tenure and team creativity and innovation. Specifically, when teams are relatively new (~1 yr), being together longer leads to a slight decrease in team creativity/innovation; when teams are of moderate tenure (~2.5 yrs), being together longer has no effect on team creativity/innovation; and, when teams are quite mature (~9 yrs), being together longer leads to an increase in team creativity/innovation.\r\n\r\n\r\nShow code\r\n\r\nimport pandas as pd\r\nfrom plotnine import *\r\n\r\n# uploading table with meta-analysis results\r\ndata = pd.read_excel(\"./metaanalysis_results.xlsx\")\r\n# extracting higher and lower borders of 95% CI\r\ndata[['l95ci', 'h95ci']] = data['95CI'].str.split(',', expand=True).astype(float)\r\n\r\n# sorting the df for dataviz purposes\r\ndata['abs_P'] = data['P'].abs()\r\ndata_sorted = data.sort_values(by=['Category', 'abs_P'], ascending=[False, True])\r\ncategories_order = data_sorted['Variable'].unique()\r\ndata_sorted['Variable'] = pd.Categorical(data_sorted['Variable'], categories=categories_order, ordered=True)\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(reticulate)\r\nlibrary(ggplot2)\r\n\r\ncolors = c(\r\n  'Team composition'='#e15759',  \r\n  'Task structure'='#f28e2b',  \r\n  'Organizational support'='#4e79a7'\r\n)\r\n\r\nggplot(py$data_sorted, aes(x=Variable, y=P, fill = Category, color = Category)) +\r\n  geom_hline(yintercept=0, linetype=\"dashed\", color=\"grey\") +\r\n  geom_point(size=3) +\r\n  geom_errorbar(aes(ymin=l95ci, ymax=h95ci), width=0.3, size=1) +\r\n  scale_color_manual(values=colors) +\r\n  scale_fill_manual(values=colors) +\r\n  coord_flip() +\r\n  labs(\r\n    x = \"\",\r\n    y = \"Optimally-weighted and corrected mean correlation (œÅ) with 95% CI\",\r\n    title = \"Meta-analytic results for team design and team creativity/innovation\",\r\n    fill = \"\",\r\n    color = \"\",\r\n    caption = \"Data source: Byron et al. (2023)\"\r\n  ) +\r\n  theme_bw() +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 0),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"top\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nWhat might be the practical implications for managers?\r\nFocus on selecting team members who are diverse in terms of job-related factors such as educational background.\r\nEnsure task and goal interdependence - design projects to require collaboration, provide team-level feedback, and create team accountability systems.\r\nWhen leading innovative teams, adopt an approach aimed at supporting and encouraging - not controlling - the team as a whole as opposed to the individuals within the team.\r\nTry to keep high-quality people in the team as team tenure is positively related to team creativity and innovation.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-08-team-design-creativity-innovation/./team_creativity_illustration.jpg",
    "last_modified": "2024-02-28T12:06:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-02-strength-based-development-and-distributions/",
    "title": "Strength-based development and power-law vs. normal distribution of performance",
    "description": "Does a power-law distribution of performance, as opposed to a normal distribution, support the concept of strength-based development?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2024-01-02",
    "categories": [
      "performance management",
      "learning and development",
      "python"
    ],
    "contents": "\r\nI just came across the notion that power-law distribution of performance, as opposed to normal distribution, supports the concept of strength-based development. The reasoning was that developing specific strengths can lead to greater success than trying to raise all skills to at least average level.\r\nThis was quite surprising to me as my impression was that it was just the opposite. Given the multiplicative nature of power-law distribution, one should - in addition to developing one‚Äôs specific strengths - try to avoid having any of the contributing factors at zero or near-zero level, as anything multiplied by zero is still zero. Instead, a normal distribution with its additive nature would support the idea that one can compensate for one‚Äôs weaknesses by excessively developing one‚Äôs specific strengths.\r\nTo validate my impression, I ran a quick and dirty sim of where a person would rank with the same skill profile combining some excessive highs and lows with the middle grounds in a normal and power-law distribution of performance, respectively.\r\n\r\n\r\nShow code\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom scipy.stats import percentileofscore\r\n\r\n# creating a dataframe with 7 random variables following a normal distribution\r\nnp.random.seed(0)  \r\ndata = pd.DataFrame(np.random.normal(0, 1, (10000, 7)), columns=[f'var{i+1}' for i in range(7)])\r\n\r\n# transforming these variables to the percentile scale\r\nfor col in data.columns:\r\n    data[col] = data[col].rank(pct=True)\r\n\r\n# creating two new columns: 'Performance with normal distribution' and 'Performance with power-law distribution'\r\ndata['Performance with normal distribution'] = data.sum(axis=1)\r\ndata['Performance with power-law distribution'] = data.prod(axis=1)\r\n\r\n# creating a skill profile combining some excessive highs and lows with the middle grounds\r\nprofile = [1,   0.95,   0.6,    0.4,    0.5,    0.05,   0.025]\r\nprofile_normal = sum(profile)\r\nprofile_normal_pct = np.round(percentileofscore(data['Performance with normal distribution'], profile_normal), 1)\r\nprofile_powerlaw = np.prod(profile)\r\nprofile_powerlaw_pct = np.round(percentileofscore(data['Performance with power-law distribution'], profile_powerlaw), 1)\r\n\r\nAs you can see in the charts attached, the sim seems to support the notion that excessive strengths in the presence of excessive weaknesses help much more under the assumption of normally distributed performance.\r\n\r\n\r\nShow code\r\n\r\nfrom plotnine import *\r\n\r\n# plotting the normal performance distribution\r\nplot = (\r\n  ggplot(data) +\r\n  aes(x='Performance with normal distribution') +\r\n  geom_density(fill = 'grey', color = \"white\", alpha = 0.7) +\r\n  geom_vline(xintercept = profile_normal, linetype = \"dashed\") +\r\n  geom_text(\r\n    label=f\"A skill profile combining some excessive highs\\nand lows with the middle grounds\\n(corresponding to {profile_normal_pct} percentile)\", \r\n    x=profile_normal + 0.13, \r\n    y = 0.45, \r\n    ha = 'left',\r\n    size = 13\r\n    ) +\r\n  labs(\r\n    title=\"Performance with normal distribution\",\r\n    x = 'Performance',\r\n    y = 'Density'\r\n  ) +\r\n  theme_bw()+\r\n  theme(\r\n    plot_title = element_text(size=20, margin={'b': 12}, ha='left'), \r\n    axis_title_x = element_text(size=15, margin={'t': 15}), \r\n    axis_title_y = element_text(size=15, margin={'r': 15}), \r\n    axis_text_x = element_text(size=10),\r\n    axis_text_y = element_text(size=10),\r\n    panel_grid_major = element_blank(),\r\n    panel_grid_minor = element_blank(),\r\n    figure_size=(13, 6)\r\n  )\r\n)\r\n\r\nprint(plot)\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting the power-law performance distribution\r\nplot = (\r\n  ggplot(data) +\r\n  aes(x='Performance with power-law distribution') +\r\n  geom_density(fill = 'grey', color = \"white\", alpha = 0.7) +\r\n  geom_vline(xintercept = profile_powerlaw, linetype = \"dashed\") +\r\n  geom_text(\r\n    label=f\"A skill profile combining some excessive highs and lows with the middle grounds\\n(corresponding to {profile_powerlaw_pct} percentile)\", \r\n    x=profile_powerlaw + 0.05, \r\n    y = 50, \r\n    ha = 'left',\r\n    size = 13\r\n    ) +\r\n  labs(\r\n    title=\"Performance with power-law distribution\",\r\n    x = 'Performance',\r\n    y = 'Density'\r\n  ) +\r\n  theme_bw()+\r\n  theme(\r\n    plot_title = element_text(size=20, margin={'b': 12}, ha='left'), \r\n    axis_title_x = element_text(size=15, margin={'t': 15}), \r\n    axis_title_y = element_text(size=15, margin={'r': 15}), \r\n    axis_text_x = element_text(size=10),\r\n    axis_text_y = element_text(size=10),\r\n    panel_grid_major = element_blank(),\r\n    panel_grid_minor = element_blank(),\r\n    figure_size=(13, 6),\r\n  )\r\n)\r\n\r\nprint(plot)\r\n\r\n\r\nGiven these results, one should feel motivated to identify among the mix of factors affecting one‚Äôs performance those with multiplicative effect and try to bring them at least to the average level. The question is how one can easily identify which these are. Any tips?\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-02-strength-based-development-and-distributions/./bodybuilding.jpg",
    "last_modified": "2024-01-11T14:04:35+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-02-goal-setting/",
    "title": "Do high-low range goals aid in maintaining motivation over time?",
    "description": "Is there enough evidence to bet on this technique and give it a try?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-12-29",
    "categories": [
      "goal setting",
      "resolution",
      "motivation",
      "persistence"
    ],
    "contents": "\r\nBeing at the time of year when the ‚Äúfresh start‚Äù effect can help with achieving goals, like many, I‚Äôm contemplating what goals to set for the upcoming year and exploring strategies to enhance their attainability.\r\nRecently, I encountered a suggestion to establish high-low range goals rather than single-number targets. For example, aiming to run 8-12 km per week instead of a fixed 10 km. This approach is thought to offer a balanced mix of attainability and challenge, potentially leading to a greater feeling of accomplishment and, more importantly, a higher likelihood of goal re-engagement, i.e.¬†sustained motivation to continue with one‚Äôs goals over time.\r\nThis all makes pretty good sense, yet I was surprised to find scant research on this topic, particularly outside the context of weight loss (see, for example, the Scott & Nowlis‚Äô 2013 study). Maybe I was just looking in the wrong places, or using the wrong key terms. Does anyone have insights into relevant research from other fields, or have you experimented with this approach in your organization?\r\nHowever, given the low-stakes circumstances, the minimal investment required, the consistency of the technique with existing knowledge about human motivation and common sense, and the low likelihood of serious adverse effects, I think it is quite reasonable to bet on this particular technique and give it a try, even though there seems to be relatively little evidence available.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-02-goal-setting/./goals.jpg",
    "last_modified": "2024-01-02T12:47:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-12-13-gai-simulation-work-habits/",
    "title": "Does GenAI make me a better (more rational) thinker?",
    "description": "A short reflection on one of the impacts of using GenAI on my work habits.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-12-13",
    "categories": [
      "work habits",
      "generative ai",
      "thinking tools",
      "rationality"
    ],
    "contents": "\r\nWhen someone recently asked me how GenAI has changed my work habits, I realized that one of the profound changes is that I‚Äôm more often testing my ideas, assumptions, and hypotheses with various simulations, usually implemented in Python or R.\r\nI‚Äôve done this also in the past, but GenAI has made it much easier and thus lowered the passing threshold. In the past, there would have had to be something really important or compelling for me to be willing to invest my time and energy into creating a simulation. Now that GenAI can take care of a lot of the technical stuff for me (but still under my supervision), I find it much easier to reach for this very useful thinking tool.\r\nLast time, for example, I had a hard time imagining clearly enough what the impact of unequal ratios of different employee segments at higher levels of the org hierarchy would be on some metric of interest, assuming no bias in people‚Äôs judgment, so to compensate for my poor imagination, I used GenAI to help me build a simulation that would allow me to see the answer more clearly.\r\nTime will tell if this new habit will last long enough to have a chance to better calibrate my believes and lead me to better decisions ü§û\r\nWhat about you? What work habits has GenAI changed in your case, if any?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-13-gai-simulation-work-habits/./rodin-thinker2.png",
    "last_modified": "2024-01-02T12:28:03+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-12-12-reflective-and-intuitive-thinking-styles/",
    "title": "Reflective and intuitive thinking styles",
    "description": "A new construct on the block for employee selection and development?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-12-12",
    "categories": [
      "meta-analysis",
      "performance",
      "employee selection",
      "employee development",
      "psychometrics"
    ],
    "contents": "\r\nAlaybek et al.¬†(2021) quite recently conducted a meta-analysis examining the relations between individual differences in reflective (rational) and intuitive thinking styles and workplace task performance, and showed that reflective thinking style has a positive and non-zero relation with task performance (œÅ = 0.213).\r\nThis positive relation was stronger in environments characterized by higher task complexity, greater importance of creativity and innovation for work tasks, and higher time pressure associated with work tasks.\r\nIn the case of intuitive thinking style, there was a very small but positive relation with task performance (œÅ = 0.051), and this relation was stronger in environments characterized by higher task complexity.\r\nMore important, incremental validity analyses revealed that reflective thinking style explains unique variance in task performance, beyond conscientiousness and intelligence.\r\nIt seems that a reflective thinking style may bear some extra clues to workplace performance. Not super surprising, but still far away from ‚ÄúElementary, my dear Watson!‚Äù üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-12-reflective-and-intuitive-thinking-styles/./intuition_vs_reason.jpg",
    "last_modified": "2024-01-11T14:04:35+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-29-big-five-personality-and-earnings/",
    "title": "Link between the Big Five personality traits and earnings",
    "description": "Check how your personality supports your earnings.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-29",
    "categories": [
      "personality",
      "earnings",
      "meta-analysis"
    ],
    "contents": "\r\nI recently stumbled upon a meta-analysis by Alderotti, Rapallini, & Traverso (2023) that explores the interesting topic of the link between the Big Five personality traits and earnings.\r\nWith one exception (Agreeableness?) there were no big surprises, IMO: ‚ÄúThe association seems to be both positive and statistically significant for Openness, Conscientiousness, and Extraversion, while negative and statistically significant for Agreeableness and Neuroticism.‚Äù\r\nThe included meta-regression, which analysed the effect of differences between studies on the estimated effects, yielded some interesting but still quite expected findings and hypotheses:\r\nPart of the positive effect of Openness on earnings is mediated by cognitive ability, probably due to the fact that one aspect of Openness is correlated with intellect which directly and indirectly (through educational attainment) influences labor market success.\r\nConscientiousness can affect earnings not only directly, but also through its influence on workers‚Äô educational career.\r\nPart of Neuroticism‚Äôs negative effect on earnings can be mediated by its negative impact on individual‚Äôs cognitive performance.\r\nThe influence of Extraversion and Agreeableness may be related to the fact that people choose/are chosen for different jobs that pay differently (e.g., extroverts may seek jobs that require more social interaction and pay better on average, and more agreeable individuals may be more likely to work in helping professions that offer lower pay on average).\r\nI know of at least two aspects of my personality that are unnecessarily costing me some of my potential money ü§ë Looks like I‚Äôm going to have to find a personality coach ü§ì\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-29-big-five-personality-and-earnings/./money_leo.jpg",
    "last_modified": "2023-11-29T12:10:45+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-29-ebm-gpt-bot/",
    "title": "How to support the adoption of Evidence-Based Management with a specialized GPT bot",
    "description": "Let's create your own consultant to help you apply the principles of Evidence-Based Management.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-29",
    "categories": [
      "evidence-based management",
      "organizational science",
      "gpt",
      "ai"
    ],
    "contents": "\r\nOne of the reasons why managers and organizations generally don‚Äôt apply the principles of Evidence-Based Management as often as they could or should is the perceived difficulty and complexity of the entire process. Hiring professional consultants is one solution, but it may be too expensive for many.\r\nHowever, with the introduction of customizable GPTs, there is now an affordable way to build a kind of EBM consultant that can help managers go through all the necessary steps to increase the odds of solving the right problems with the right solutions.\r\nAs a first and modest step in this direction, I have created a GPT bot named Assistant to the Evidence-Based Manager. It provides EBM-related consultancy based on general instructions and reliable materials about EBM. It will definitely not replace an experienced EBM consultant or practitioner, but it may help overcome the resistance threshold for adopting EBM principles in organizations.\r\n\r\n\r\n\r\nGive it a try and let me know what you think about this approach. Or even better, try building your own GPT bot that will do better than mine and share it with us üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-29-ebm-gpt-bot/./gpt_bot.png",
    "last_modified": "2023-11-29T12:18:53+01:00",
    "input_file": {},
    "preview_width": 526,
    "preview_height": 292
  },
  {
    "path": "posts/2023-11-27-rebuilding-a-survey-with-the-help-of-nlp-tools/",
    "title": "Rebuilding an employee survey with the help of NLP tools",
    "description": "Using LLM and text embeddings to assist in implementing new constructs into the existing employee survey.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-27",
    "categories": [
      "employee survey",
      "nlp",
      "llm",
      "psychometrics"
    ],
    "contents": "\r\nSometimes you need to incorporate a new construct into an existing employee survey, but you also don‚Äôt want to burden respondents with too many items.\r\nOne way to avoid this is to reuse existing survey items to cover all or some of the facets of the new construct of interest. When you have many items, it can be quite challenging to figure out which items might be good candidates for such reuse. However, with the new NLP tools, this is now much easier.\r\nTo achieve this, in one of my projects, I combined a prompt requesting the LLM to semantically compare all existing items and facets of a new construct with a semantic similarity comparison of the two using text embeddings. Both approaches provided me with a pre-selected list of promising candidate items, which I then evaluated myself.\r\nThis way, I significantly reduced the time it takes to figure out which items I can reuse and which I‚Äôll have to write from scratch. But LLM can help with that, too - see, for example, this article from Hernandez & Nie (2022).\r\nI know this use case is not that common, but IMO still enough to make it worth knowing that this option exists. Happy prompting and cosine similarity computing üññ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-27-rebuilding-a-survey-with-the-help-of-nlp-tools/./slap_pic.jpg",
    "last_modified": "2023-11-27T09:45:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-22-gender-gap-in-hiring-decisions/",
    "title": "Evidence on the presence of gender bias in selection settings",
    "description": "Interesting results from a pre-registered meta-analysis of 44 years of field experiments on gender gaps in hiring decisions.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-22",
    "categories": [
      "gender discrimination",
      "field experiment",
      "meta-analysis",
      "open science",
      "forecasting"
    ],
    "contents": "\r\nSchaerer et al.¬†(2023) conducted a pre-registered meta-analysis of 44 years of field experiments on gender gaps in hiring decisions with quite interesting results:\r\nDiscrimination against women for male-typed and balanced jobs decreased across time.\r\nDiscrimination against men for female-typed jobs remained stable across time.\r\nAverage effect is small - the average odds of male applicants to receive a callback is 0.91 times the odds of equally qualified female applicants (with 95% CI from 0.86 to 0.97).\r\nHeterogeneity of true effects is very high, specifically 83% of total variance across studies can be attributed to heterogeneity rather than random chance, so the average effect is not so telling.\r\nIn addition to the meta-analysis, the study also included a forecasting challenge in which researchers and laypeople attempted to accurately estimate both time-trends and the current pervasiveness of gender biases in selection settings. Forecasters expected observed decline, but overestimated the degree of remaining bias.\r\n\r\n\r\n\r\nNote on the attached chart: In all figures, odds ratios above 1 indicate a greater preference for male applicants and odds ratios below 1 indicate greater preference for female applicants.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-22-gender-gap-in-hiring-decisions/./gendergap.jpg",
    "last_modified": "2024-02-05T13:04:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-18-job-demands-job-control-wellbeing/",
    "title": "Surprising finding on the impact of job demands and control on workers‚Äô well-being",
    "description": "I just came across an interesting and surprising result from a Bayesian meta-analysis on the effect of the interaction between job demands and job control on worker well-being.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-18",
    "categories": [
      "well-being",
      "job demands",
      "job control",
      "meta-analysis"
    ],
    "contents": "\r\nAccording to Huth & Chung-Yan (2023), contrary to the position of many theories in the field of occupational health and stress, job control does not reduce the negative impact of job demands on workers‚Äô well-being.\r\nAs you can see from the table below, the data provided strong evidence for the absence of the interaction between job demands and control.\r\n\r\n\r\n\r\nAt the same time, however, the authors themselves emphasize that ‚Äú[these] findings do not suggest that job demands and job control are not important work design features when considering the well-being of workers. Their direct effects on worker well-being are well-established in past research.‚Äù\r\n‚ÄúThe important conclusion of [the] study is that increased job control cannot offset the deleterious impact that high workloads have on workers. [This means that assuming] that employee well-being is a priority, workload should be restricted irrespective of the positive benefits of increasing employee control.‚Äù\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-18-job-demands-job-control-wellbeing/./pic.png",
    "last_modified": "2023-11-19T12:35:42+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-11-09-personas-based-on-ml-local-interpretation-algos/",
    "title": "Personas based on ML local interpretation algorithms",
    "description": "A demonstration of one method useful for sharing insights from fitted ML models.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-11-10",
    "categories": [
      "machine learning",
      "interpretability",
      "personas",
      "storytelling",
      "r",
      "python"
    ],
    "contents": "\r\nThere is one very useful albeit relatively underused method of sharing insights from fitted ML models, at least in my professional bubble.\r\nIt‚Äôs a method of identifying personas based on outputs from ML local interpretation algorithms, which provide information about the specific drivers of predictions for individual observations.\r\nIts implementation is pretty straightforward:\r\nFit a ML model.\r\nGenerate predictions for each observation using the fitted model.\r\nIdentify drivers of predictions for individual observation units using a ML local interpretation algorithm, e.g., LIME or SHAP.\r\nUse the data points from steps 2 and 3 to identify clusters with similar characteristics.\r\nName and describe personas corresponding to the identified clusters.\r\nLet‚Äôs see this in action using the Python code below. First, we need to create an artificial dataset on which we will demonstrate the method described above. We‚Äôll create a classification dataset - imagine, for example, that we‚Äôre trying to predict sales performance based on some collaboration metrics, but feel free to imagine any scenario you like - and prepare a training and testing set to train our ML.\r\n\r\n\r\nShow code\r\n\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\n\r\n\r\n# defining the number of samples and features for the dataset\r\nn_samples = 10000\r\nn_features = 10\r\n\r\n# creating the dataset\r\nX, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=6, n_redundant=4, n_clusters_per_class=3, flip_y=0.27, class_sep=1, random_state=1979)\r\n\r\n# creating a df from X and y\r\ndf = pd.DataFrame(X, columns=['feature_{}'.format(i) for i in range(n_features)])\r\ndf['criterion'] = y\r\n\r\n# splitting the dataset into training and test sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1979)\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\nlibrary(reticulate)\r\n\r\n# table dataviz\r\nDT::datatable(\r\n  py$df,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nNow we can fit and fine-tune our ML (XGBoost) model.\r\n\r\n\r\nShow code\r\nfrom xgboost import XGBClassifier\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\n# fitting a XGBoost model with hyperparameter tuning and 10-fold cross-validation\r\n# initializing the XGBoost classifier\r\nxgb_model = XGBClassifier(random_state=1979)\r\n\r\n# defining the parameter grid for hyperparameter tuning\r\nparam_grid = {\r\n  'n_estimators': [100, 200],\r\n  'max_depth': [3, 5, 7],\r\n  'learning_rate': [0.01, 0.1, 0.2]\r\n}\r\n\r\n# setting up the grid search with 10-fold cross-validation\r\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=10, scoring='f1')\r\n\r\n# fitting the model with hyperparameter tuning\r\ngrid_search.fit(X_train, y_train)\r\nGridSearchCV(cv=10,\r\n             estimator=XGBClassifier(base_score=None, booster=None,\r\n                                     callbacks=None, colsample_bylevel=None,\r\n                                     colsample_bynode=None,\r\n                                     colsample_bytree=None,\r\n                                     early_stopping_rounds=None,\r\n                                     enable_categorical=False, eval_metric=None,\r\n                                     gamma=None, gpu_id=None, grow_policy=None,\r\n                                     importance_type=None,\r\n                                     interaction_constraints=None,\r\n                                     learning_rate=None, max_bin=None,\r\n                                     max_cat_to_onehot=None,\r\n                                     max_delta_step=None, max_depth=None,\r\n                                     max_leaves=None, min_child_weight=None,\r\n                                     missing=nan, monotone_constraints=None,\r\n                                     n_estimators=100, n_jobs=None,\r\n                                     num_parallel_tree=None, predictor=None,\r\n                                     random_state=1979, reg_alpha=None,\r\n                                     reg_lambda=None, ...),\r\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\r\n                         'max_depth': [3, 5, 7], 'n_estimators': [100, 200]},\r\n             scoring='f1')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10,\r\n             estimator=XGBClassifier(base_score=None, booster=None,\r\n                                     callbacks=None, colsample_bylevel=None,\r\n                                     colsample_bynode=None,\r\n                                     colsample_bytree=None,\r\n                                     early_stopping_rounds=None,\r\n                                     enable_categorical=False, eval_metric=None,\r\n                                     gamma=None, gpu_id=None, grow_policy=None,\r\n                                     importance_type=None,\r\n                                     interaction_constraints=None,\r\n                                     learning_rate=None, max_bin=None,\r\n                                     max_cat_to_onehot=None,\r\n                                     max_delta_step=None, max_depth=None,\r\n                                     max_leaves=None, min_child_weight=None,\r\n                                     missing=nan, monotone_constraints=None,\r\n                                     n_estimators=100, n_jobs=None,\r\n                                     num_parallel_tree=None, predictor=None,\r\n                                     random_state=1979, reg_alpha=None,\r\n                                     reg_lambda=None, ...),\r\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\r\n                         'max_depth': [3, 5, 7], 'n_estimators': [100, 200]},\r\n             scoring='f1')estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\r\n              colsample_bylevel=None, colsample_bynode=None,\r\n              colsample_bytree=None, early_stopping_rounds=None,\r\n              enable_categorical=False, eval_metric=None, gamma=None,\r\n              gpu_id=None, grow_policy=None, importance_type=None,\r\n              interaction_constraints=None, learning_rate=None, max_bin=None,\r\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\r\n              max_leaves=None, min_child_weight=None, missing=nan,\r\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\r\n              num_parallel_tree=None, predictor=None, random_state=1979,\r\n              reg_alpha=None, reg_lambda=None, ...)XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\r\n              colsample_bylevel=None, colsample_bynode=None,\r\n              colsample_bytree=None, early_stopping_rounds=None,\r\n              enable_categorical=False, eval_metric=None, gamma=None,\r\n              gpu_id=None, grow_policy=None, importance_type=None,\r\n              interaction_constraints=None, learning_rate=None, max_bin=None,\r\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\r\n              max_leaves=None, min_child_weight=None, missing=nan,\r\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\r\n              num_parallel_tree=None, predictor=None, random_state=1979,\r\n              reg_alpha=None, reg_lambda=None, ...)\r\n\r\nShow code\r\n\r\n# getting the best estimator\r\nbest_xgb_model = grid_search.best_estimator_\r\n\r\nThe classification performance metrics below show that the fitted model performs well on the test data, so we can safely proceed further.\r\n\r\n\r\nShow code\r\nfrom sklearn.metrics import classification_report, roc_auc_score\r\nimport numpy as np\r\n\r\n# predictions on the test set\r\ny_pred = best_xgb_model.predict(X_test)\r\ny_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1] \r\n\r\n# classification report\r\nreport = classification_report(y_test, y_pred)\r\nprint(report)\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.81      0.78      0.80       977\r\n           1       0.80      0.83      0.81      1023\r\n\r\n    accuracy                           0.81      2000\r\n   macro avg       0.81      0.80      0.80      2000\r\nweighted avg       0.81      0.81      0.80      2000\r\n\r\nShow code\r\n# ROC AUC score\r\nroc_auc = roc_auc_score(y_test, y_pred_proba)\r\nprint('ROC AUC score: ', np.round(roc_auc, 2))\r\nROC AUC score:  0.85\r\n\r\nNow we will generate LIME explanations for each observation in the testing dataset and standardize them for later analysis and visualization (including the predicted probabilities of the positive class).\r\n\r\n\r\nShow code\r\n\r\nimport lime.lime_tabular\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\n# using LIME for local interpretation\r\n# initializing the LIME explainer\r\nexplainer = lime.lime_tabular.LimeTabularExplainer(\r\n    training_data=X_train,\r\n    feature_names=['Feature_{}'.format(i) for i in range(X_train.shape[1])],\r\n    class_names=['Low Performance', 'High Performance'],\r\n    mode='classification',\r\n    random_state=1234\r\n)\r\n\r\n\r\n# df for storing the LIME explanations for all observation\r\nexplanations_df = pd.DataFrame()\r\nfeature_names = df.columns[:-1].tolist()\r\n\r\n# generating LIME explanations for each observation in the test set\r\nfor i in range(X_test.shape[0]):\r\n    \r\n    # predicted probability for the positive class\r\n    predicted_class_proba = y_pred_proba[i]\r\n    \r\n    # generating the LIME explanation\r\n    exp = explainer.explain_instance(X_test[i], best_xgb_model.predict_proba, num_features=X_train.shape[1])\r\n    exp_list = exp.as_list()\r\n    \r\n    feature_values = {name: 0 for name in feature_names}\r\n    # looping through the employee's conditions and updating feature_values accordingly\r\n    for condition, value in exp_list:\r\n        for feature_name in feature_names:\r\n            if feature_name in condition.lower():\r\n                feature_values[feature_name] = value\r\n                break\r\n\r\n    # adding the predicted probability for the positive class\r\n    feature_values['predicted_class_proba'] = predicted_class_proba\r\n\r\n    supp_df = pd.DataFrame(feature_values, index=[0])  \r\n    explanations_df = pd.concat([explanations_df, supp_df], ignore_index=True) \r\n\r\n  \r\n# standardizing all features (including the probability for the positive class)\r\nscaler = StandardScaler()\r\nexplanations_scaled = scaler.fit_transform(explanations_df)\r\nexplanations_scaled_df = pd.DataFrame(explanations_scaled)\r\nexplanations_scaled_df.columns = explanations_df.columns\r\n\r\nUsing the UMAP 2D projection of the prediction explanations and predicted probabilities, we can see that that are several clusters of observations with similar predicted probabilities and their drivers.\r\n\r\n\r\nShow code\r\nimport umap\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"white\")\r\n\r\n# visualizing the personas using UMAP\r\n# initializing and fitting UMAP\r\nreducer = umap.UMAP(n_components=2, n_neighbors=50, min_dist=0.01, metric='euclidean', random_state=1979, n_jobs=1)\r\nembedding = reducer.fit_transform(explanations_scaled)  \r\n\r\n# plotting the explanations and predicted probability in 2D scatterplot \r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c='lightblue', s=50, alpha=0.5)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\nplt.show()\r\n\r\n\r\nA clustering algorithm, such as HDBSCAN, can help us identify the clusters.\r\n\r\n\r\nShow code\r\nimport hdbscan\r\nimport matplotlib.patches as mpatches\r\n\r\n# HDBSCAN clustering\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=10, cluster_selection_epsilon=0.3, prediction_data=True)\r\nclusterer.fit(embedding)\r\nHDBSCAN(cluster_selection_epsilon=0.3, min_cluster_size=25, min_samples=10,\r\n        prediction_data=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HDBSCANHDBSCAN(cluster_selection_epsilon=0.3, min_cluster_size=25, min_samples=10,\r\n        prediction_data=True)\r\n\r\nShow code\r\nclusters = clusterer.labels_\r\n\r\n# adding the cluster labels to the dataframe\r\nexplanations_df['cluster'] = clusters\r\n\r\n# plotting the clusters\r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\ncmap = plt.cm.get_cmap('tab20')\r\nnorm = plt.Normalize(clusters.min(), clusters.max())\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=50, cmap=cmap, norm=norm, alpha=0.5)\r\npatches = [mpatches.Patch(color=cmap(norm(i)), label=f'Cluster {i}') for i in np.unique(clusters)]\r\nplt.legend(handles=patches, fontsize=12)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities. The clusters were identified using HDBSCAN.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\nplt.show()\r\n\r\n\r\nThere seem to be about eight clusters and some outliers (cluster -1). Let‚Äôs look at how they differ in terms of predicted probabilities. According to the chart below, there appear to be four clusters with increased predicted probabilities (clusters 3, 5, 6, and 7), three with decreased predicted probabilities (clusters 0, 2, and 4), and one with more mixed predictions (cluster 1).\r\n\r\n\r\nShow code\r\nimport matplotlib.colorbar as colorbar\r\n\r\n# plotting the distribution of probability of positive classes\r\nplt.close()\r\nplt.figure(figsize=(12, 8))\r\nscatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=y_pred_proba, cmap='viridis', s=50, alpha = 0.5)\r\nplt.title('People with similar predictions and similar prediction drivers\\n', fontsize=24)\r\nplt.figtext(0.05, 0.05, \"UMAP projection of the LIME prediction explanations and predicted probabilities.\", wrap=True, horizontalalignment='left', fontsize=12)\r\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\r\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\r\ncbar = plt.colorbar(scatter)\r\ncbar.set_label('Predicted class probability', rotation=270, labelpad=15)\r\nplt.show()\r\n\r\n\r\nNow we can check for the selected clusters which features and in which direction most affect their respective predicted probabilities. For example, we can see from the table below that the clusters with lower predicted probabilities (clusters 0, 2 and 4) are driven either by the respective values in features 6 and 9 (cluster 0), or by the respective values in features 0, 3, 7 and 9 (cluster 2), or by the respective values in feature 0 (cluster 4).\r\n\r\n\r\nShow code\r\n\r\n# creating a summary for each cluster across all feature drivers of predicted probabilities of the positive class\r\ntab1 <- py$explanations_df %>% \r\n  dplyr::group_by(cluster) %>% \r\n  dplyr::summarise_all(~median(., na.rm = TRUE))\r\n\r\n# tab dataviz\r\nDT::datatable(\r\n  round(tab1,2),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 10, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  formatStyle(\r\n    names(tab1 %>% dplyr::select(-cluster, -predicted_class_proba)),\r\n    background = styleColorBar(range(tab1 %>% dplyr::select(-cluster, -predicted_class_proba)), 'lightblue'),\r\n    backgroundSize = '98% 90%',\r\n    backgroundRepeat = 'no-repeat',\r\n    backgroundPosition = 'center'\r\n  ) \r\n\r\n\r\n\r\nCombined with information on the median values of these specific features, we can get a good idea of the people who tend to under-perform and ‚Äúwhy‚Äù. For example, people in cluster 0 score too high in feature 6 and too low in feature 9; people in cluster 2 score too low in features 0, 3, 7 and 9; and people in cluster 4 score too low in features 0. Given these differences, it would be useful to consider different approaches to try to improve the sales performance of people based on information about which persona they belong to. We could also repeat a similar analysis for clusters with higher predicted probabilities to see which combination of features tends to be associated with higher performance.\r\n\r\n\r\nShow code\r\n\r\n# creating a df with X and y from testing part of the dataset\r\ntest_df = pd.DataFrame(X_test, columns=['feature_{}'.format(i) for i in range(n_features)])\r\ntest_df['predicted_class_proba'] = y_pred_proba\r\ntest_df['cluster'] = clusters\r\n\r\n\r\n\r\nShow code\r\n\r\n# creating a summary for each cluster across all raw data and predicted probability of the positive class\r\ntab2 <- py$test_df %>% \r\n  dplyr::group_by(cluster) %>% \r\n  dplyr::summarise_all(~median(., na.rm = TRUE))\r\n\r\n# tab dataviz\r\nDT::datatable(\r\n  round(tab2,2),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 10, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  formatStyle(\r\n    names(tab2 %>% dplyr::select(-cluster, -predicted_class_proba)),\r\n    background = styleColorBar(range(tab2 %>% dplyr::select(-cluster, -predicted_class_proba)), 'lightblue'),\r\n    backgroundSize = '98% 90%',\r\n    backgroundRepeat = 'no-repeat',\r\n    backgroundPosition = 'center'\r\n  ) \r\n\r\n\r\n\r\nMaybe you‚Äôll find the method described here useful in one of your ML projects. Happy data sleuthing üôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-09-personas-based-on-ml-local-interpretation-algos/./persona_lm_illustration.png",
    "last_modified": "2023-11-13T16:54:15+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-10-24-sentiment-analysis-validation/",
    "title": "Sentiment analysis of employee survey comments using zero-shot classification",
    "description": "An attempt to validate a zero-shot sentiment classification.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-10-24",
    "categories": [
      "zero-shot learning",
      "ai",
      "machine learning",
      "sentiment analysis",
      "employee survey",
      "python"
    ],
    "contents": "\r\nRecently, I experimented with zero-shot classification - a machine learning task where the model classifies data into categories it hasn‚Äôt encountered during training - to determine the sentiment of comments from an employee survey (positive, negative, mixed, neutral).\r\nI chose this approach as an alternative to the traditional lexicon and rule-based NLTK sentiment analyzer. Using the transformer architecture of the model (bart-large-mnli from Facebook), I aimed to capture context from entire comments when gauging their sentiment. I admit that I had some doubts because the model wasn‚Äôt specifically trained for sentiment analysis. Instead, it generalizes its understanding from the MNLI tasks to deduce sentiments.\r\nFortunately, each comment was tied to specific statements that also received ratings on a standard 0-10 scale. This allowed me to cross-reference the sentiment classification with the ratings people assigned to these statements.\r\nSo, how did the model do? As the attached chart shows, the average sentiment classification probability is pretty compellingly consistent with the ratings on the standard scale.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(readr)\r\n\r\nmydata <- readr::read_csv(\"./mydata.csv\")\r\n\r\nlabel_data <- mydata %>% \r\n  dplyr::group_by(sentiment_category) %>% \r\n  dplyr::slice(1)\r\n\r\nmydata %>% \r\n  ggplot2::ggplot(aes(x = Score, y = avg_sentiment_score, color = sentiment_category)) +\r\n  ggplot2::geom_line(linewidth = 1.5) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::scale_y_continuous(limits = c(0,NA), breaks = seq(0,1,0.1)) +\r\n  ggplot2::scale_x_continuous(limits = c(-1,10), breaks = seq(0,10,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Mixed\"=\"#7b00e7\", \"Negative\"=\"#db370e\", \"Neutral\"=\"grey\", \"Positive\"=\"#208600\")) +\r\n  ggplot2::geom_text(data=label_data, aes(label=sentiment_category), nudge_x=-0.5, hjust=0.75, vjust = 0, size = 5, fontface = \"bold\") +\r\n  ggplot2::labs(\r\n    x = \"SURVEY ITEM RATING (0-10)\",\r\n    y = \"AVERAGE PROBABILITY OF SENTIMENT CATEGORY\",\r\n    title = \"Validation of zero-shot sentiment classification\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"none\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nSeems like I can add another useful tool to my toolbox. If you are interested in trying it out, you can use a short code snippet below for it.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nfrom transformers import pipeline\r\n\r\n# sentiment analysis with zero-shot classification using the facebook/bart-large-mnli model\r\nsentiment_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\r\ncomments = df['comments'].to_list()\r\n# defining the candidate labels \r\ncandidate_labels = [\"positive\", \"negative\", \"neutral\", \"mixed\"]\r\n# setting the hypothesis template\r\nhypothesis_template = \"The sentiment of this employee feedback is {}.\"\r\n# estimating the sentiment labels\r\nprediction = sentiment_classifier(comments, candidate_labels, hypothesis_template=hypothesis_template)\r\nprediction = pd.DataFrame(prediction)\r\n# creating columns with predicted sentiment (label with the highest probability) and sentiment scores for individual labels  \r\ndf['sentiment_label'] = prediction['labels'].apply(lambda x: x[0])\r\ndf['sentiment_score_positive'] = prediction.apply(lambda x: x['scores'][x['labels'].index('positive')], axis=1)\r\ndf['sentiment_score_negative'] = prediction.apply(lambda x: x['scores'][x['labels'].index('negative')], axis=1)\r\ndf['sentiment_score_neutral'] = prediction.apply(lambda x: x['scores'][x['labels'].index('neutral')], axis=1)\r\ndf['sentiment_score_mixed'] = prediction.apply(lambda x: x['scores'][x['labels'].index('mixed')], axis=1)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-24-sentiment-analysis-validation/./cover_pic.png",
    "last_modified": "2023-10-24T12:56:59+02:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-10-23-nlp-llm-and-onboarding/",
    "title": "Using NLP & LLM to combat 'tip-of-the-tongue' moments during onboarding",
    "description": "How to make onboarding experience a little bit smoother with the help of NLP and LLM.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-10-10",
    "categories": [
      "onboarding",
      "nlp",
      "llm",
      "embeddings",
      "ai"
    ],
    "contents": "\r\nI believe that many of you have had a similar experience when you joined a new company: you have taken a ton of notes from various meetings and trainings, you know that the answer to your immediate question is in there somewhere, but it is too hard to find the right notes, so you capitulate and decide to try to find the answer in a different way, e.g.¬†by asking a more tenured colleague.\r\nTo make better use of my notes from my current onboarding, I created a quick and dirty app that uses embeddings to find the relevant files with my notes and LLM to summarise answers to my questions.\r\nSo, for example, if I need to find out who owns a certain business process, I can just type my question into the app and quickly get an answer, including tips on what files to look at to verify the answer or find other related information.\r\n\r\nIf you want to check out the code behind the app, you can find it in this GitHub repo.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-23-nlp-llm-and-onboarding/./lost_info.jpg",
    "last_modified": "2023-10-23T19:47:27+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-09-24-exploration-vs-exploitation-tradeoff/",
    "title": "Exploration vs. Exploitation trade-off in our calendars",
    "description": "As I was going through my calendar recently to check who I had already met during my onboarding at Sanofi, I realized that one way to look at the calendar is through the lens of the Exploration vs. Exploitation trade-off. What lessons can we take from this?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-25",
    "categories": [
      "calendar",
      "time management",
      "decision making",
      "exploration vs exploration trade-off"
    ],
    "contents": "\r\nAs a ‚Äúmaker‚Äù, I‚Äôm used to using my time in larger blocks of time to do focused work on various data products. Now, however, my time tends to be spread out between a series of 30-45 minute long meetings that help me navigate a complex organization like Sanofi, explore opportunities for collaboration, and make valuable contacts that can help me deliver on tasks in the future.\r\n\r\nIllustration of the Maker‚Äôs vs.¬†Manager‚Äôs schedule.\r\nI wondered if I could gain any potentially useful insights from this particular ‚Äúintuition pump‚Äù. I took the help of the book ‚ÄúAlgorithms to Live By‚Äù by Brian Christian and Tom Griffiths and found at least five such insights:\r\nAlthough one has a primary mode of operation (e.g.¬†exploitation for makers, exploration for managers), one should not completely ignore the other mode and should allocate a small, consistent portion of one‚Äôs time to ensure that one does not miss valuable insights or opportunities that lie outside one‚Äôs primary mode of operation (based on the Epsilon-Greedy strategy to solve the Multi-Armed Bandit problem).\r\nOver time, as one becomes more aware of the options available, the need for exploration may decrease, allowing for more targeted exploitation of known best opportunities (based on the Decaying Epsilon strategy to solve the Multi-Armed Bandit problem).\r\nAlthough our primary focus is on delivery, we should remain open and try new options that have the highest potential upside, despite the associated high uncertainty (based on the Upper Confidence Bound strategy to solution of the Multi-Armed Bandit problem).\r\nIf you plan to stay with the company for a while, you should be open to further exploration, as newly found valuable opportunities can be used later in the future, and vice versa (btw, this could be a useful signal of intentions to leave, as some research suggests that people actually tend to resolve this type of trade-offs this way).\r\nBe Bayesian, i.e., choose your paths based on your current beliefs, regularly check the balance between exploration and exploitation on your calendar, and update your beliefs and make new, more informed decisions as evidence of outcomes accumulates (based on the Thompson Sampling strategy to solution of the Multi-Armed Bandit problem).\r\nMaybe you‚Äôll find some of these insights helpful in finding a better balance between exploring new paths and following familiar ones, leading to more effective use of your time. If nothing else, take this as a recommendation to read the book mentioned above - there‚Äôs plenty in there for inspiration.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-24-exploration-vs-exploitation-tradeoff/./multiArmedBandit.png",
    "last_modified": "2023-09-29T12:59:19+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 544
  },
  {
    "path": "posts/2023-09-17-bayesian-simulation/",
    "title": "Harnessing Bayesian analysis for business process simulation",
    "description": "A demonstration of how the outputs of Bayesian analysis can be used to simulate business processes while preserving inherent uncertainties.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-17",
    "categories": [
      "bayesian statistics",
      "business process simulation",
      "python",
      "pymc"
    ],
    "contents": "\r\nOne of the advantages of doing statistical analysis in a Bayesian framework is that its generative part makes it a natural fit for business process simulation, which incorporates all the uncertainties inherent in the statistical models we use to capture the patterns of interest.\r\nOnce the parameters of the models have been estimated, their corresponding posterior distributions can be easily sampled and used in combination with a range of input values to simulate the expected outcomes, including the associated uncertainty that needs to be taken into account when making decisions.\r\nTo illustrate, imagine, for example, that as a CHRO you want to estimate the number of potential new employees brought in by employees who choose to participate in a new referral program. Fortunately, you have data from a small, three-month pilot of this program in which you offered participation to a small random sample of employees. Using a combination of simple Binomial and Poisson models, you can easily arrive at a reasonable estimate of the outcome of interest when you introduce the program to a larger portion of the company.\r\nLets‚Äô implement this simple illustrative example with PyMC, a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods.\r\nFirst, let‚Äôs upload the data from the pilot program. The first table includes 150 employees who were randomly selected and offered participation in the pilot program. The second table then shows 42 employees who chose to participate in the program and the number of potential new employees they brought in.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\n\r\n# table with all pilot nominees\r\nnominees=pd.read_excel(\"./dataBayesSim.xlsx\", sheet_name=\"nominees\")\r\n\r\n# table with all pilot participants\r\nparticipants=pd.read_excel(\"./dataBayesSim.xlsx\", sheet_name=\"participants\")\r\n\r\n# showing first few rows of the tables\r\nnominees.head(5)\r\n  employeeID  participation\r\n0         e1              0\r\n1         e2              0\r\n2         e3              0\r\n3         e4              0\r\n4         e5              0\r\n\r\nShow code\r\nparticipants.head(5)\r\n  employeeID  referrals\r\n0         e7          2\r\n1         e9          4\r\n2        e10          2\r\n3        e11          2\r\n4        e25          1\r\n\r\nIn order to estimate the expected number of new potential employees after the introduction of a new referral program to a larger part of the company, we want to model the probability that nominees actually participate in the program and the expected number of potential candidates that a participant brings in. To do this, we can fit Binomial and Poisson models to the data, respectively. As mentioned above, we will do this in a Bayesian framework that will make it easier to deal with uncertainty later in our simulation. A side note: for the sake of brevity, I omit the usual sanity checks that should be performed before drawing any conclusions from fitted models - e.g., checking for convergence of Markov chains or posterior predictive checks for how well the fitted models predict observed data.\r\nLet‚Äôs start with the first model. From the summary below, we see that the estimated probability of participating in the programme is between 0.21 and 0.35.\r\n\r\n\r\nShow code\r\nimport pymc as pm\r\n\r\nShow code\r\n# estimating the participation rate\r\nnominated = nominees.shape[0]\r\nparticipated = nominees['participation'].sum()\r\n\r\n# setting up the model\r\nwith pm.Model() as participationModel:\r\n  # assigning a flat Beta prior for p\r\n  p = pm.Beta(\"p\", alpha=1, beta=1)\r\n  \r\n  # defining likelihood\r\n  obs = pm.Binomial(\"obs\", p=p, n=nominated, observed=participated)\r\n  \r\n  # running mcmc\r\n  idata = pm.sample(3000, tune=500, chains=3, cores=1)\r\n  \r\n  # generating posterior predictive sample\r\n  participationModelPosterior = pm.sample_posterior_predictive(idata, extend_inferencedata=True)\r\n\r\n\r\n\r\nShow code\r\nimport arviz as az\r\n\r\n# trace plot showing the evolution of parameter vector over the iterations of Markov chain(s)\r\n#az.plot_trace(idata, kind=\"trace\", divergences=\"bottom\", show=True)\r\n\r\n# posterior predictive check\r\n#az.plot_ppc(participationModelPosterior, num_pp_samples=500, random_seed=7, alpha=0.3, textsize=14, kind='kde', show=True)\r\n\r\n# tabular and visual summary of the posterior probability distribution of the p parameter value\r\naz.summary(idata).round(2)\r\n   mean    sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat\r\np  0.28  0.04    0.22     0.35        0.0      0.0    4338.0    6229.0    1.0\r\n\r\nShow code\r\naz.plot_posterior(idata, hdi_prob=.95, show=True)\r\n\r\n\r\nThe second model then suggests that program participants brought in an average of 1.7 to 2.5 referrals.\r\n\r\n\r\nShow code\r\n# setting up the Poisson model\r\nwith pm.Model() as referralModel:\r\n  # weakly informative exponential prior for lambda parameter with mean 3\r\n  lambda_ = pm.Exponential('lambda', 1/3)\r\n  # alternative flat prior for lambda parameter\r\n  #lambda_ = pm.Uniform('lambda', lower=0, upper=25)\r\n  \r\n  # Poisson likelihood\r\n  y_obs = pm.Poisson('y_obs', mu=lambda_, observed=participants['referrals'])\r\n  \r\n  # running mcmc\r\n  trace = pm.sample(3000, tune=500, chains=3, cores=1)\r\n  \r\n  # generating posterior predictive sample\r\n  referralModelPosterior = pm.sample_posterior_predictive(trace, extend_inferencedata=True)\r\n\r\n\r\n\r\nShow code\r\n# trace plot showing the evolution of parameter vector over the iterations of Markov chain(s)\r\n#az.plot_trace(trace, kind=\"trace\", divergences=\"bottom\", show=True)\r\n\r\n# posterior predictive check\r\n#az.plot_ppc(referralModelPosterior, num_pp_samples=500, random_seed=7, alpha=0.3, textsize=14, kind='kde', show=True)\r\n\r\n# tabular and visual summary of the posterior probability distribution of the p parameter value\r\naz.summary(trace).round(2)\r\n        mean    sd  hdi_3%  hdi_97%  ...  mcse_sd  ess_bulk  ess_tail  r_hat\r\nlambda   2.1  0.22    1.69     2.51  ...      0.0    3676.0    6056.0    1.0\r\n\r\n[1 rows x 9 columns]\r\n\r\nShow code\r\naz.plot_posterior(trace, hdi_prob=.95, show=True)\r\n\r\n\r\nWe can now sample the posterior distributions of the parameters p and lambda, insert them into the dataframe, and for each row/combination calculate the expected number of referrals brought in by participating employees when the program is rolled out to the entire population of 1500 employees. As you can see below, using the IQR, our CHRO can expect recruiters to reach 790 to 990 potential candidates once the new company-wide referral program is in place.\r\n\r\n\r\nShow code\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"white\")\r\n\r\n# sampling from the posterior distribution the parameters p and lambda\r\nposterior = pd.DataFrame({\r\n    'p': idata['posterior']['p'].values.flatten(),\r\n    'lambda': trace['posterior']['lambda'].values.flatten()\r\n})\r\n\r\n# computing expected number of referrals with 1500 nominees\r\nposterior['expectedReferrals'] = 1500*posterior['p']*posterior['lambda']\r\n\r\n# computing summary statistics\r\nm = posterior['expectedReferrals'].mean().round(1)\r\nQ1 = np.percentile(posterior['expectedReferrals'], 25).round(1)\r\nQ2 = np.percentile(posterior['expectedReferrals'], 50).round(1)\r\nQ3 = np.percentile(posterior['expectedReferrals'], 75).round(1)\r\n\r\n# visualizing results\r\nsns.histplot(posterior['expectedReferrals'], bins=30, kde=True, color='#5b7db6').set(xlabel =\"Number of new referrals\", ylabel = \"Count\")\r\nplt.gcf().suptitle('Expected referrals for program rollout to all 1500 employees', fontsize=13)\r\nplt.gca().set_title(f'Mean={m}, Q1={Q1}, Median={Q2}, Q3={Q3}', fontsize=10)\r\nplt.show()\r\n\r\n\r\nAnd it doesn‚Äôt have to end there. For example, this estimate can be combined with other inputs, e.g.¬†the cost of a new referral program, the cost of an alternative solution, etc., to make a better informed decision, taking into account the existing uncertainty.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-17-bayesian-simulation/./plot.png",
    "last_modified": "2023-09-17T19:26:19+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-09-03-dag-and-double-ml/",
    "title": "A plausible model of data-generating process eats ML algorithms for breakfast",
    "description": "An illustration of one of the lessons I took away from studying the use of meta-learners for causal inference.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-09-03",
    "categories": [
      "causal inference",
      "double machine learning",
      "dag",
      "python"
    ],
    "contents": "\r\nI‚Äôm just in the middle of studying the use of meta-learners for causal inference, i.e.¬†how to repurpose conventional ML models to estimate treatment effect. One of the lessons I took away from this study so far is that no matter how fancy the ML algorithm we use, without a plausible model of the data-generating process, we cannot hope for an unbiased estimate of the treatment effect as without it, it is difficult to select relevant covariates and avoid those that introduce bias into our estimation (e.g.¬†colliders, mediators and their descendants).\r\nTo illustrate and reinforce this lesson for myself, I coded a small example of estimating the average treatment effect of training performance on productivity using synthetic data with a known data-generating process and the Double ML method.\r\nFirst, we create a DAG of the causal relationships behind our data. We see that, according to this DAG, employee productivity is affected by years of experience, job fit, available resources, and training performance, which is our variable of interest. Employee satisfaction is also part of the DAG, which is affected by available resources, employee productivity, and training performance.\r\n\r\n\r\nShow code\r\n\r\nimport networkx as nx\r\nimport matplotlib.pyplot as plt\r\n\r\n# initializing DAG\r\nG = nx.DiGraph()\r\n\r\n# adding nodes\r\nnodes = ['Years of experience', 'Resources', 'Job_fit', 'Training', 'Productivity', 'Satisfaction', 'Cognitive ability']\r\nG.add_nodes_from(nodes)\r\n\r\n# adding edges\r\nedges = [('Years of experience', 'Productivity'),\r\n         ('Cognitive ability', 'Productivity'),\r\n         ('Cognitive ability', 'Training'),\r\n         ('Resources', 'Productivity'),\r\n         ('Resources', 'Satisfaction'),\r\n         ('Job_fit', 'Productivity'),\r\n         ('Training', 'Productivity'),\r\n         ('Training', 'Satisfaction'),\r\n         ('Productivity', 'Satisfaction')]\r\n\r\nG.add_edges_from(edges)\r\n\r\n# drawing DAG\r\npos = nx.fruchterman_reingold_layout(G, seed=5, iterations = 500, k = 2)\r\nlabels = {node: node for node in G.nodes()}\r\nnx.draw(G, pos, with_labels=True, labels=labels, node_color='lightblue', font_weight='bold', node_size=1500, font_size=10)\r\nplt.title(\"DAG\")\r\nplt.show()\r\n\r\n\r\nNow let‚Äôs generate synthetic data corresponding to this DAG.\r\n\r\n\r\nShow code\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# generating synthetic data\r\nnp.random.seed(42)\r\nn = 1000\r\n\r\n# generating features years of experience, resources, job fit, and cognitive ability\r\nX = np.random.normal(0, 1, (n, 4))\r\n\r\n# setting the true causal effect of training performance \r\ntrue_causal_effect = 3.5\r\n\r\n# training is influence by cognitive ability\r\ntraining = 1.8 * X[:, 3] + np.random.normal(0, 1, n)\r\n\r\n# productivity is influenced by all features \r\nproductivity = 2 * X[:, 0] + 1 * X[:, 1] + 0.5 * X[:, 2] + 2.2 * X[:, 3] + true_causal_effect * training + np.random.normal(0, 1, n)\r\n\r\n# defining an employee satisfaction variable that is influenced by resources, productivity, and training\r\nsatisfaction = 0.3 * X[:, 1] +  0.2 * productivity + 1.3 * training + np.random.normal(0, 1, n)\r\n\r\n# creating a final dataFrame\r\ndf = pd.DataFrame(X, columns=['years_of_experience', 'resources', 'job_fit', 'cognitive_ability'])\r\ndf['training'] = training\r\ndf['productivity'] = productivity\r\ndf['satisfaction'] = satisfaction \r\n\r\nWithout DAG, i.e., without understanding the data-generating process behind our data, we might be tempted to ‚Äúthrow‚Äù all available variables into our estimator. We can give it a try and use Double ML for that - a popular framework designed to provide unbiased and consistent estimates of treatment effects in the presence of high-dimensional controls while reducing the risk of overfitting. The common Double ML procedure looks as follows:\r\nRandomly splitting the data into two parts: one for estimating the control function and the other for estimating the treatment effect.\r\nUsing the first part of the data to train a machine learning model to predict the outcome variable based on covariates (without the treatment variable). Similarly, training another model to predict the treatment variable based on covariates.\r\nUsing the second part of the data to form the residuals for outcome and treatment variable and running a simple linear regression of outcome residuals on treatment residuals.\r\nRepeating steps 1-3 but switching the roles of the two data splits and averaging the estimates to get the final average treatment estimate.\r\nLet‚Äôs apply this approach to our data and use XGBoost models to estimate the control function and see how successful we will be in our efforts to estimate the known causal effect of training on productivity.\r\n\r\n\r\nShow code\r\n\r\nimport numpy as np\r\nfrom xgboost import XGBRegressor\r\nfrom sklearn.linear_model import LinearRegression\r\n\r\n# creating a variable for data splits\r\nnp.random.seed(42)\r\ndf['part'] = np.random.choice([0, 1], size=len(df))\r\n\r\nestimates = []\r\n\r\nfor p in [0,1]:\r\n    \r\n    # auxiliary variables for switching the roles of the two data splits\r\n    firstPart = 1-p\r\n    secondPart = p-0\r\n    \r\n    # used covariates\r\n    covariates = ['years_of_experience', 'resources', 'job_fit', 'cognitive_ability', 'satisfaction']\r\n  \r\n    # preparing datasets\r\n    X_First = df.loc[df['part'] == firstPart, covariates].values\r\n    y_Outcome_First = df.loc[df['part'] == firstPart,'productivity'].values\r\n    y_Treatment_First = df.loc[df['part'] == firstPart,'training'].values\r\n    \r\n    X_Second = df.loc[df['part'] == secondPart, covariates].values\r\n    y_Outcome_Second = df.loc[df['part'] == secondPart,'productivity'].values\r\n    y_Treatment_Second = df.loc[df['part'] == secondPart,'training'].values\r\n    \r\n    # controlling for covariates for productivity using XGBoost\r\n    model_Outcome = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Outcome.fit(X_First, y_Outcome_First)\r\n    residual_Outcome = y_Outcome_Second - model_Outcome.predict(X_Second)\r\n    \r\n    # controlling for covariates for training using XGBoost\r\n    model_Treatment = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Treatment.fit(X_First, y_Treatment_First)\r\n    residual_Treatment = y_Treatment_Second - model_Treatment.predict(X_Second)\r\n    \r\n    # Part 3: Estimate the causal effect using the residuals with Linear Regression\r\n    model_causal = LinearRegression()\r\n    model_causal.fit(residual_Treatment.reshape(-1, 1), residual_Outcome)\r\n    double_ml_effect = model_causal.coef_[0]\r\n    \r\n    estimates.append(double_ml_effect)\r\n  \r\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\r\n\r\nShow code\r\nprint(\"The estimated Average Treatment Effect is:\", sum(estimates)/len(estimates))\r\nThe estimated Average Treatment Effect is: 2.8913607224732614\r\n\r\nWe see that the estimated causal effect of training is approximately 2.9, quite far from the known value of 3.5. How so? Well, if you look at the DAG above, you can see that by using all the variables as covariates, not only have we correctly blocked the backdoor by controlling for employee cognitive ability, but we have also introduced bias into our estimation by controlling for satisfaction, which is a collider - a variable that is affected by both the outcome (productivity) and the treatment (training). And controlling for colliders leads to spurious correlations between variables or, as in our case, deflates the size of the estimated effect. We can easily check this using DAGitty, which is a wonderful browser-based environment for creating and analysing causal diagrams.\r\n\r\nSo let‚Äôs repeat the estimation, but now without satisfaction variable as covariate. As you can see below, we are now much closer to the actual causal effect.\r\n\r\n\r\nShow code\r\n\r\n# creating a variable for data splits\r\nnp.random.seed(42)\r\ndf['part'] = np.random.choice([0, 1], size=len(df))\r\n\r\nestimates = []\r\n\r\nfor p in [0,1]:\r\n    \r\n    # auxiliary variables for switching the roles of the two data splits\r\n    firstPart = 1-p\r\n    secondPart = p-0\r\n    \r\n    # used covariates\r\n    covariates = ['years_of_experience', 'resources', 'job_fit', 'cognitive_ability']\r\n  \r\n    # preparing datasets\r\n    X_First = df.loc[df['part'] == firstPart, covariates].values\r\n    y_Outcome_First = df.loc[df['part'] == firstPart,'productivity'].values\r\n    y_Treatment_First = df.loc[df['part'] == firstPart,'training'].values\r\n    \r\n    X_Second = df.loc[df['part'] == secondPart, covariates].values\r\n    y_Outcome_Second = df.loc[df['part'] == secondPart,'productivity'].values\r\n    y_Treatment_Second = df.loc[df['part'] == secondPart,'training'].values\r\n    \r\n    # controlling for covariates for productivity using XGBoost\r\n    model_Outcome = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Outcome.fit(X_First, y_Outcome_First)\r\n    residual_Outcome = y_Outcome_Second - model_Outcome.predict(X_Second)\r\n    \r\n    # controlling for covariates for training using XGBoost\r\n    model_Treatment = XGBRegressor(eta = 0.1, n_estimators =25)\r\n    model_Treatment.fit(X_First, y_Treatment_First)\r\n    residual_Treatment = y_Treatment_Second - model_Treatment.predict(X_Second)\r\n    \r\n    # Part 3: Estimate the causal effect using the residuals with Linear Regression\r\n    model_causal = LinearRegression()\r\n    model_causal.fit(residual_Treatment.reshape(-1, 1), residual_Outcome)\r\n    double_ml_effect = model_causal.coef_[0]\r\n    \r\n    estimates.append(double_ml_effect)\r\n  \r\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\r\n\r\nShow code\r\nprint(\"The estimated Average Treatment Effect is:\", sum(estimates)/len(estimates))\r\nThe estimated Average Treatment Effect is: 3.477088075986001\r\n\r\nDespite the simplicity of the example presented, I think it nicely demonstrates that by using fancy ML algorithms like XGBoost, we are not relieved of the need to have a plausible model of the data-generating process when estimating causal effects from observational data, and we can‚Äôt just blindly rely on some magical powers of ML to squeeze what we need out of whatever input we give it.\r\nJust a side note: If you want to make your life a little bit easier when using Double ML, you can use the DoubleML library for Python and R. The code below illustrates this library in action on our synthetic data.\r\n\r\n\r\nShow code\r\n\r\nfrom doubleml import DoubleMLData, DoubleMLPLR\r\n\r\nnp.random.seed(42)\r\n\r\n# specifying data and roles of individual variables\r\ndml_data = DoubleMLData(df, y_col='productivity', d_cols='training', x_cols=['years_of_experience', 'resources', 'job_fit', 'cognitive_ability'])\r\n\r\n# specifying ML model(s) used for estimation of the nuisance parts\r\nml_xgb = XGBRegressor(eta = 0.1, n_estimators =25)\r\n\r\n# initializing and parametrizing the model object which will be used to perform the estimation\r\ndml_plr_xgb = DoubleMLPLR(\r\n  dml_data,\r\n  ml_l = ml_xgb,\r\n  ml_m = ml_xgb,\r\n  n_folds = 5,\r\n  n_rep = 10,\r\n  score = 'partialling out',\r\n  dml_procedure = 'dml2')\r\n\r\n# estimation and inference\r\ndml_plr_xgb.fit()\r\n<doubleml.double_ml_plr.DoubleMLPLR object at 0x000001A9267BACE0>\r\n\r\nShow code\r\ndml_plr_xgb.summary\r\n              coef   std err          t  P>|t|     2.5 %    97.5 %\r\ntraining  3.426416  0.049597  69.085291    0.0  3.329208  3.523624\r\n\r\nShow code\r\ndml_plr_xgb.confint()\r\n             2.5 %    97.5 %\r\ntraining  3.329208  3.523624\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-03-dag-and-double-ml/dag-and-double-ml_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-08-22-r-and-power-bi/",
    "title": "Embedding R (or Python) ML models in Power BI dashboards",
    "description": "In my new job, we currently rely a lot on Power BI when presenting people-related insights to our stakeholders. Since I know Power BI quite superficially and we also want to share insights from more complex analyses with our stakeholders, I spent part of the weekend studying how to incorporate ML models created in R or Python into Power BI dashboards. I put my learnings in this blog post. It's definitely not rocket science, but it may still shorten the learning path for some of you who are in a similar situation.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-08-22",
    "categories": [
      "power bi",
      "r",
      "python",
      "machine learning"
    ],
    "contents": "\r\nNote to start: Although the example presented in this post is demonstrated in R, it could be analogously implemented in Python as well.\r\nFor demonstration, I will use the well-known IBM artificial attrition dataset. First, we need to train the model. I suppose you know the drill and know all the steps to go through to get a useful and reliable model. The following R script implements the whole process of data preparation, (XGBoost) model tuning, training, and validation. After we have the prediction model ready, we have to save it, as we will use it later in the Power BI (PBI) dashboard.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\nlibrary(recipes)\r\nlibrary(themis)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors()) %>%\r\n  themis::step_smote(Attrition, over_ratio = 1)\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n  )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n    tune::collect_predictions(parameters = xgb_best) %>% \r\n    yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  parsnip::fit(data_train)\r\n\r\n\r\n# variable importance\r\nxgb_fit %>%\r\n  tune::extract_fit_parsnip() %>%\r\n  vip::vip(num_features = 10, geom = \"col\")\r\n\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n    yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# saving the final model\r\nsaveRDS(xgb_fit, \"./final_xgb_model.RDS\")\r\n\r\n# saving testing data as a new dataset that will be used in Power BI dashboard\r\ndata_test %>%\r\n  dplyr::select(-Attrition) %>%\r\n  writexl::write_xlsx(\"./newData.xlsx\")\r\n\r\n\r\nNow we can move on to PBI. We use the testing data as a new dataset that we will score by the trained model and show stakeholders how the predicted flight risk varies by department, job role, and gender.\r\nFirst, we should check in the PBI settings that PBI has access to our R (or Python) instance (File -> Options and setting -> GLOBAL/R scripting/Python scripting). If so, we can upload new data using the Get data dialog and go to the Transform tab in the Query editor. Here, we should first check that the data types match those in R when preparing the model, and then we can load and run our scoring algorithm using the Run R script dialog. The following script will do the job.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(workflows)\r\n\r\n# uploading trained model\r\nmodel <- readRDS(\"PathTo/YourModel.RDS\")\r\n\r\n# generate predictions\r\nprediction <- predict(model, dataset) %>% \r\n  dplyr::bind_cols(predict(model, dataset, type = \"prob\")) %>%\r\n  dplyr::rename(\r\n    predictedCat = .pred_class,\r\n    probLeave = .pred_1,\r\n    probStay = .pred_0\r\n  )\r\n\r\n# enriching original dataset\r\nprediction <- cbind(dataset, prediction)\r\n\r\n\r\nAfter running the script, our original data will be enriched with the generated predictions. We can apply this transformation, save it and exit the Query editor using the Close & Apply button. We will then have flight risk predictions that we can visualize along with the original data, allowing us to see how predicted flight risk varies by department, job role, and gender, among other things. Again, we could use R or Python for this purpose, as PBI does not offer some types of data visualization by default, especially those related to visualizing data variability. For example, to create the raincloud plots below, I used the ggplot2 and ggdist packages in R. Below is a script used within the R visual that implements one of the plots. To fine-tune them, I recommend using RStudio (if you have it on your computer), which is accessible directly from the R visual editor in PBI.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n# dataset <- data.frame(probLeave, Department)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggdist)\r\n\r\ndataset %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(Department, probLeave, mean, .na_rm = TRUE), y = probLeave)) + \r\n  ggdist::stat_halfeye(\r\n    adjust = .5, \r\n    width = .6, \r\n    .width = 0, \r\n    justification = -.3, \r\n    point_colour = NA,\r\n    fill = \"#bca36b\",\r\n    color = \"#bca36b\"\r\n  ) + \r\n  ggplot2::geom_boxplot(\r\n    width = .25, \r\n    outlier.shape = NA\r\n  ) +\r\n  ggplot2::stat_summary(fun.y=mean, geom=\"point\", shape=20, size=5, color=\"#bca36b\", fill=\"#bca36b\") +\r\n  geom_point(\r\n    size = 1.3,\r\n    alpha = .3,\r\n    position = position_jitter(seed = 1, width = .09, height = .008\r\n    )\r\n  ) + \r\n  ggplot2::coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"PROBABILITY OF LEAVING\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 strip.text.x = element_text(size = 11, face = \"plain\"),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=c(.95,.88),\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(0,0,0,0),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nWe may also want to allow dashboard users to enter custom values for specific personas of interest and let the scoring algorithm predict the corresponding risk of leaving the company. To make the demonstration of this feature somewhat easier, I will use a different prediction model that uses only some of the strongest predictors.\r\nTo enable this feature, we need to create several parameters that the user can set. For numeric predictors we can use the New parameter dialog box on the Modeling tab - we set the name of the parameters, their data type, their minimum, maximum, and default values, and confirm we want to show corresponding sliders in the dashboard. For categorical parameters, we need to create and then upload a table with all possible combinations of values of the categorical parameters used. When using R, we can use for example the expand.grid function to do this. The fields from this table are then used and displayed in the dashboard as single-selection filters. And all these parameters then serve as input to the R visual, where we load the model, change the names of some parameters to match those expected by the model, generate a prediction, and create the resulting visualization. All these steps are implemented by the following R script.\r\n\r\n\r\nShow code\r\n\r\n# 'dataset' holds the input data for this script\r\n# dataset <- data.frame(JobLevelParameter Value, YearsWithCurrManagerParameter Value, OverTime, Department, JobSatisfaction)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(workflows)\r\n\r\ndata <- dataset %>%\r\n    dplyr::rename(\r\n        JobLevel = `JobLevelParameter Value`, \r\n        YearsWithCurrManager = `YearsWithCurrManagerParameter Value`\r\n    )\r\n\r\n# uploading trained model\r\nmodel <- readRDS(\"PathTo/YourModel2.RDS\")\r\n\r\n# generate predictions\r\nprediction <- predict(model, data) %>% \r\n  dplyr::bind_cols(predict(model, data, type = \"prob\")) %>%\r\n  dplyr::rename(\r\n    predictedCat = .pred_class,\r\n    probLeave = .pred_1,\r\n    probStay = .pred_0\r\n  )\r\n\r\n# enriching original dataset\r\nprediction <- cbind(data, prediction)\r\n\r\nprediction %>%\r\n  ggplot2::ggplot(aes(x = 1, y = probLeave)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#444492\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::geom_text(aes(label = round(probLeave, 2)), nudge_y = 0.04, color = \"black\", size = 7, fontface = \"bold\") +\r\n  scale_y_continuous(limits = c(0,1.08), breaks = seq(0,1,0.1)) +\r\n  labs(\r\n    x = \"\",\r\n    y = \"PROBABILITY OF LEAVING\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 strip.text.x = element_text(size = 11, face = \"plain\"),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=c(.95,.88),\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(0,0,0,0),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nThis completed the work on our local computer. You can download the final dashboard here. If you use it, be sure to update the paths to the trained model in both the Query Editor and the R visuals. The next step is to set up the Power BI Service and install and configure the on-premises data gateway so that the R scripts will work in dashboards shared with others. More on this in a future blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-08-22-r-and-power-bi/./r_with_powerbi.png",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2023-07-20-vocational-interests/",
    "title": "Vocational interests don't seem so uninteresting after all",
    "description": "Quite surprising (at least to me) findings on the validity of vocational interests for predicting a range of important work outcomes.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-20",
    "categories": [
      "vocational interests",
      "meta-analysis",
      "predictive validity",
      "job performance"
    ],
    "contents": "\r\nTo be honest, until recently, I tended to underestimate the importance of vocational interests in job performance prediction and considered questions about them during a job interview as a formality. In my defense, this view has also been supported by the low estimates of their predictive validity reported by classics such as Schmidt & Hunter (1998).\r\nHowever, I adjusted my view after coming across the updated validity estimate in the Sackett et al.¬†meta-analysis (2022) and the results of the Nye et al.¬†meta-analysis (2017) on the validity of interests for predicting job performance.\r\nThe latter study reported the following interesting findings:\r\nCorrelation between interest scores and job performance (corrected for both indirect range restriction and unreliability in the criterion) is 0.16 (SE=0.03).\r\n\r\nInterest congruence/match between an individual‚Äôs interests and his or her work is a much stronger predictor of performance outcomes than interest scores alone, with baseline correlations of 0.32 and 0.16, respectively.\r\n\r\nInterests are significantly better predictors of organizational citizenship behavior than other criteria (job performance, task performance, OCB, persistence, CWB, and training performance) but are less valid for predicting CWB and task performance.\r\nIf you tend to think about vocational interests as I have until recently, perhaps these two studies will help you update your priors a little bit üòâ\r\nNote: The attached schemes are taken from another excellent resource on this topic by Nye et al.¬†(2012).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-20-vocational-interests/./scheme2.png",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {},
    "preview_width": 491,
    "preview_height": 283
  },
  {
    "path": "posts/2023-07-14-induced-centrality/",
    "title": "Induced centralities",
    "description": "A post about useful complement to common ONA centrality measures.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-14",
    "categories": [
      "organizational network analysis",
      "centrality measures",
      "r"
    ],
    "contents": "\r\nWhile reading the excellent publication Social Networks at Work from SIOP‚Äôs Organizational Frontiers series (btw, highly recommended to all PA professionals), I came across the interesting and useful concept of induced centrality.\r\nIt sort of reverses the logic of the common ONA centrality measures, which focus primarily on what one gets from the surrounding network of connections, and instead shows how individual nodes contribute to some global network characteristic of interest, i.e.¬†what one does for the network as a whole.\r\nIts calculation is quite simple and straightforward - you just need to first calculate the global characteristic of the network that you are interested in as a reference point, e.g.¬†its coherence, and then calculate how this measure changes when you remove individual nodes from the network. From this, you can deduce that the nodes that cause the most change in a specific direction contribute the most to a given measure.\r\nIn addition to its versatility and the interesting angle it offers, it can also be very useful in making visible otherwise hidden and invisible ‚Äúheroes‚Äù who contribute to the greater good under the radar of public recognition.\r\nWhat follows is a small demonstration of using induced centrality to estimate which people play the role of expressive leaders who shorten the lengths of paths in the network. It‚Äôs an implementation of the idea briefly described in the aforementioned publication Social Networks at Work:\r\n‚ÄúFor example, suppose one theorizes that there are certain individuals in groups (perhaps called expressive leaders) who provide a certain social glue such that they tend to shorten the lengths of paths in the network (see, for example, the Heidi Roizen case by McGinn and Tempest, 2010). This sounds like we should use closeness centrality, since it is concerned with path lengths. But there are two problems with this. First of all, closeness centrality only counts the shortest paths, and not the circuitous paths that things such as gossip often take. Second, closeness gets at how long it takes for information to reach a given node, who is then presumed to benefit from this information. But the concept we‚Äôve just outlined is about individuals who enable others to have short paths so that the whole group benefits. Closeness was not designed to measure this, and doesn‚Äôt. However an induced centrality measure can be created to measure exactly this: to what degree paths lengthen when you remove each node from the network.‚Äù\r\nFirst, let‚Äôs upload the data used for the demonstration and create the network object. I will use a dataset that captures information-sharing links between 15 members of my friendship network.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# uploading data\r\ndf <- readxl::read_excel(\"./friendshipNetwork.xlsx\")\r\n\r\n# creating network object\r\ng <- igraph::graph_from_data_frame(df, directed=TRUE) \r\n\r\n\r\nWe can now iterate over all directed pairs of nodes and compute the average length of paths between nodes we will use as a reference point. We won‚Äôt use all the paths but only the three shortest paths between each pair of nodes that would enable us to capture some of the circuitous paths mentioned in the problem description above.\r\n\r\n\r\nShow code\r\n\r\nall_nodes <- V(g)\r\ntotal_length <- 0\r\ntotal_paths <- 0\r\n# setting the number of 3 shortest paths between pair of nodes to capture also some of the circuitous paths \r\ntop_shortest_paths <- 3\r\n\r\n# loop for directed network\r\nfor (i in 1:length(all_nodes)) {\r\n  for (j in 1:length(all_nodes)) {\r\n    if (i != j) {\r\n      lengths <- unlist(igraph::all_simple_paths(g, all_nodes[i], all_nodes[j], mode = \"out\"))\r\n      lengths <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n      total_length <- total_length + sum(lengths, na.rm = TRUE)\r\n      total_paths <- total_paths + length(lengths)\r\n    }\r\n  }\r\n}\r\n\r\n# loop for undirected network \r\n# for (i in 1:(length(all_nodes) - 1)) {\r\n#   for (j in (i + 1):length(all_nodes)) {\r\n#     lengths <- all_simple_paths(g, all_nodes[i], all_nodes[j])\r\n#     lengths <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n#     total_length <- total_length + sum(lengths, na.rm = TRUE)\r\n#     total_paths <- total_paths + length(lengths)\r\n#   }\r\n# }\r\n\r\n# computing the average length of paths\r\naverage_length_ref <- total_length / total_paths\r\n\r\n\r\nNow let‚Äôs remove each node one at a time from the network and calculate the average lengths of paths between pairs consisting of the remaining nodes. We also need to deal somehow with situations when node removal leads to the disconnection of previously connected nodes (in such situations, the distance between nodes is by default assumed to be infinite or undefined, which would bias our estimation). I have decided to take the three shortest paths from the complete network and add 1 (this is somewhat equivalent to the additional effort required to find a new bonding connection). After this step, we can subtract the reference point from the obtained values and get the information about the absence of which nodes lengthens the paths between other nodes and thus act as a kind of social glue that facilitates the spread of information between nodes.\r\n\r\n\r\nShow code\r\n\r\n# vector for saving average lengths of paths for individual nodes\r\naverage_lengths <- numeric(length(all_nodes))\r\n\r\nfor (k in 1:length(all_nodes)) {\r\n\r\n  g_new <- g\r\n  g_new <- igraph::delete_vertices(g_new, all_nodes[k])\r\n  all_nodes_new <- V(g_new)\r\n  \r\n  total_length_new <- 0\r\n  total_paths_new <- 0\r\n  \r\n  # for directed network\r\n  for (i in 1:length(all_nodes_new)) {\r\n    for (j in 1:length(all_nodes_new)) {\r\n      if (i != j) {\r\n        # lengths of paths in the network with removed node \r\n        lengths_new <- unlist(igraph::all_simple_paths(g_new, all_nodes_new[i], all_nodes_new[j], mode=\"out\"))\r\n        # lengths of paths in the complete network\r\n        lengths <- unlist(igraph::all_simple_paths(g, all_nodes_new[i]$name, all_nodes_new[j]$name, mode=\"out\"))\r\n        # dealing with situations when node removal leads to disconnection of previously connected nodes by taking 3 shortest paths from the full network and adding 1\r\n        if(is.null(lengths_new) & !is.null(lengths)){\r\n          lengths_new <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n          lengths_new <- lengths_new + 1\r\n        } else{\r\n          lengths_new <- sort(lengths_new, decreasing = FALSE)[1:top_shortest_paths]\r\n        }\r\n        total_length_new <- total_length_new + sum(lengths_new, na.rm = TRUE)\r\n        total_paths_new <- total_paths_new + length(lengths_new)\r\n      }\r\n    }\r\n  }\r\n  \r\n  # for undirected network \r\n  # for (i in 1:(length(all_nodes_new) - 1)) {\r\n  #   for (j in (i + 1):length(all_nodes_new)) {\r\n  #     lengths_new <- length_of_all_paths(g_new, all_nodes_new[i], all_nodes_new[j])\r\n  #     lengths <- length_of_all_paths(g, all_nodes_new[i]$name, all_nodes_new[j]$name)\r\n  #     if(is.null(lengths_new) & !is.null(lengths)){\r\n  #      lengths_new <- sort(lengths, decreasing = FALSE)[1:top_shortest_paths]\r\n  #      lengths_new <- lengths_new + 1\r\n  #     } else{\r\n  #      lengths_new <- sort(lengths_new, decreasing = FALSE)[1:top_shortest_paths]\r\n  #     }\r\n  #     total_length_new <- total_length_new + sum(lengths_new, na.rm = TRUE)\r\n  #     total_paths_new <- total_paths_new + length(lengths_new)\r\n  #   }\r\n  # }\r\n  \r\n  average_lengths[k] <- total_length_new / total_paths_new\r\n}\r\n\r\n# computing the difference between average and reference point\r\naverage_length_diff <- average_lengths - average_length_ref\r\n\r\n# assigning computed differences to individual nodes\r\nV(g)$avg_length_diff <- average_length_diff\r\n\r\n\r\nThe graph below shows that nodes P2, P8, and P4 are the most critical in this respect.\r\n\r\n\r\nShow code\r\n\r\nggraph::ggraph(g, layout = \"kk\") + # other available layouts: 'star', 'circle', 'gem', 'dh', 'graphopt', 'grid', 'mds', 'randomly', 'fr', 'kk', 'drl', 'lgl'\r\n  ggraph::geom_edge_link(arrow = arrow(length = unit(2.5, 'mm')), end_cap = circle(2, 'mm')) +\r\n  ggraph::geom_node_point(aes(size = avg_length_diff), alpha = 1, color = ifelse(V(g)$avg_length_diff>0, \"#e15759\", \"black\")) +\r\n  ggplot2::scale_size_continuous(range = c(0.1,8)) +\r\n  ggraph::geom_node_label(aes(label = name), repel = TRUE) +\r\n  ggplot2::labs(\r\n    title = \"Expressive leaders who shorten path lengths in the network\",\r\n    subtitle = \"Demonstration of the concept of induced centrality\",\r\n    size = \"Increase in average path length after node removal\",\r\n    caption = \"\\nNodes with an increase greater than 0 are highlighted in red.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,9,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title = element_blank(),\r\n    axis.text = element_blank(),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    axis.line = element_blank(),\r\n    legend.position=\"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2\r\n3.4.0.\r\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere\r\n  instead.\r\nThis warning is displayed once every 8 hours.\r\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning\r\nwas generated.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-14-induced-centrality/./ona.png",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 1111
  },
  {
    "path": "posts/2023-07-11-searching-and-querying-aihr-blog-posts/",
    "title": "Searching & querying AIHR blog posts on People Analytics topics",
    "description": "I'm sharing a by-product of my learning about vector database search that may be useful to some of you who want to learn something new about People Analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-11",
    "categories": [
      "aihr",
      "people analytics",
      "vector database",
      "learning by doing"
    ],
    "contents": "\r\nIt‚Äôs an app that allows you to quickly search for People Analytics topics in a database of almost 260 AIHR blog posts and get key ideas and insights from the posts that best match your search topic, with the possibility to go to the original post via a provided link.\r\n\r\nThe cluster analysis revealed that the posts cover more than 30 topics, so there‚Äôs a lot to choose from. Feel free to give it a try and let me know how it works for you. Here‚Äôs a link to the app.\r\nLast but not least, a big ‚Äúthank you‚Äù goes to AIHR and all the contributing authors for putting together such an amazing resource on People Analytics üôè\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-11-searching-and-querying-aihr-blog-posts/./search.jpg",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-07-03-bayesian-shrinkage/",
    "title": "Using Bayesian shrinkage in reporting employee turnover",
    "description": "When you report turnover rates by team, do you take into account the size of individual teams, or do you take the turnover rate numbers as they are?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-07-03",
    "categories": [
      "employee turnover",
      "hr reporting",
      "hr metrics",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nIt‚Äôs a no-brainer that when comparing team performance on retention, the departure of one person in small teams with fewer members will have a more significant impact on turnover rates than it does in larger teams. However, this creates room for potential misinterpretation and inadequate actions.\r\nIt‚Äôs related to the well-known phenomenon of variation being more likely in smaller samples. Fans of Daniel Kahneman and Amos Tversky will probably recall the famous cognitive bias of insensitivity to sample size which occurs when people judge the probability of obtaining a statistic regardless of sample size.\r\nOne way to reduce this effect is Bayesian shrinkage. This approach, which serves as a kind of regularization, involves borrowing information from the overall company turnover rate to influence the turnover rate of smaller teams. It works by ‚Äúshrinking‚Äù the turnover rate of smaller teams towards the company average, and thus creating a balance between the observed rate and the company average.\r\nIt‚Äôs not dissimilar to what one intuitively does when deciding what movie to watch or what restaurant to go to, when movies and restaurants vary widely in the number of ratings available.\r\nYou can see this approach in action on the chart below. The smaller the team and the further its turnover rate is from the company-wide turnover rate (the vertical dashed line), the more the turnover rate estimate for that team is shifted towards the company-wide value (the distance between the red cross and the black dot) - see, for example, teams 4 and 6. For comparison, check teams 12 and 9 that don‚Äôt show much of a shrinkage effect due to the big size and small distance from the company-wide turnover rate, respectively.\r\n\r\n\r\nShow code\r\n\r\n# uploading the necessary libraries\r\nlibrary(tidyverse) # data manipulation and dataviz\r\nlibrary(brms) # bayesian stats\r\nlibrary(cmdstanr) # bayesian stats\r\nlibrary(ggdist) # dataviz\r\n\r\n# creating artificial data\r\n# setting a seed for reproducibility\r\nset.seed(123)\r\n# number of teams\r\nnTeams <- 12\r\n# generating team sizes ranging from 10 to 100\r\nteamSizes <- sample(10:100, nTeams, replace = TRUE)\r\n# generating 'true' turnover rates from a beta distribution\r\ntrueRates <- rbeta(nTeams, 2, 10)\r\n# for each team, simulating the number of employees who left\r\nnumberLeft <- rbinom(nTeams, teamSizes, trueRates)\r\n# generating team IDs\r\nteamId <- as.character(1:nTeams)\r\n# creating the data frame\r\nteamsData <- data.frame(teamId, teamSizes, numberLeft)\r\n\r\n# fitting multilevel Bayesian logistic regression with wide, uninformative priors\r\nmodel <- brms::brm(\r\n  numberLeft | trials(teamSizes) ~ 1 + (1 | teamId),\r\n  data = teamsData,\r\n  family = binomial(link = \"logit\"),\r\n  prior = prior(normal(0, 10), class = \"Intercept\"),\r\n  chains = 4,\r\n  iter = 5000,\r\n  control = list(adapt_delta = 0.95),\r\n  seed = 123,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n# model summary\r\n#summary(model)\r\n#fixef(model)\r\n#ranef(model)\r\n\r\n# extracting the posterior samples\r\n#parnames(model)\r\nposteriorDraws <- as_draws_df(model, variable = \"Intercept\", regex = TRUE) %>%\r\n  dplyr::select(-.draw, -.chain, -.iteration, -sd_teamId__Intercept) %>%\r\n  tidyr::pivot_longer(cols = -b_Intercept, names_to = \"teamId\", values_to = \"paramValues\") %>%\r\n  dplyr::mutate(\r\n    teamId = stringr::str_extract(teamId, \"\\\\d+\"),\r\n    team = stringr::str_glue(\"Team {teamId}\"),\r\n    logOdds = paramValues + b_Intercept,\r\n    estimatedTR = exp(logOdds) / (1 + exp(logOdds))\r\n    ) %>%\r\n  dplyr::left_join(teamsData %>% dplyr::select(teamId, teamSizes), by = \"teamId\") %>%\r\n  dplyr::mutate(\r\n    team = stringr::str_glue(\"{team} (n={teamSizes})\"),\r\n    team = forcats::fct_reorder(factor(team), as.numeric(teamId))\r\n    )\r\n\r\n# computing fixed, population-level effect estimate\r\nfixedEffect <- 1 / (1 + exp(-1*fixef(model)[1]))\r\n\r\n# computing observed turnover rate by team\r\nobservedRT <- teamsData %>% \r\n  dplyr::mutate(team = stringr::str_glue(\"Team {teamId}\")) %>%\r\n  dplyr::mutate(\r\n    observedRT = numberLeft/teamSizes,\r\n    team = stringr::str_glue(\"{team} (n={teamSizes})\"),\r\n    team = forcats::fct_reorder(factor(team), as.numeric(teamId))\r\n  )\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# dataviz\r\nggplot2::ggplot(data = posteriorDraws, aes(x=estimatedTR, group = team)) + \r\n  ggdist::stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\", fill = \"skyblue\", normalize = \"groups\") +\r\n  ggplot2::geom_point(data = observedRT, aes(x = observedRT, y = 0, group = team), color = \"red\", inherit.aes = F, size = 2.5, shape=3, stroke = 1.5) +\r\n  ggplot2::geom_vline(xintercept = fixedEffect, linetype = \"dashed\", color = \"#2C2F46\") +\r\n  ggplot2::facet_wrap(~team, ncol = 2) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,1,0.1)) +\r\n  ggplot2::labs(\r\n    y = \"NORMALIZED DENSITY\",\r\n    x = \"ESTIMATED TURNOVER RATE\",\r\n    title = \"Using Bayesian shrinkage in reporting employee turnover\",\r\n    caption = \"\\nThe black solid lines represent the 80% and 95% credibility intervals, respectively. The black dot represents the median of the Highest Confidence Interval.\\nThe vertical dashed line represents the fixed, population-level effect estimate. The red cross represents the observed turnover rate for a given team.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, hjust = 1),\r\n    axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 9),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nIf you are trying to deal with this effect in your reporting practice, can you share the approach you use and serves you well?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-03-bayesian-shrinkage/./shrinkage.png",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {},
    "preview_width": 421,
    "preview_height": 349
  },
  {
    "path": "posts/2023-06-28-job-comparator/",
    "title": "A bet on a new job",
    "description": "Sharing a by-product of my search for a new full-time job.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-29",
    "categories": [
      "job selection",
      "bayesian statistics",
      "r",
      "shiny"
    ],
    "contents": "\r\nBeing in the final phase of my new job search, I wanted to be able to aggregate the information and impressions I gathered during the hiring process, with all the uncertainties, to make the best decision possible.\r\nTo do this, I put together a ‚Äúback-of-the-envelope‚Äù calculation that combines, in a Bayesian way, the impressions one has of various aspects of the jobs one is applying for.\r\n\r\nIt works with several factors that research suggests are related to job satisfaction and that a person has the chance to estimate subjectively to some degree during the hiring process from job ads, interviews, sample tasks, company reviews from current or former employees on Glassdoor, etc. Specifically, it takes into account the following factors:\r\nSalary\r\nJob security\r\nWork-life balance\r\nCareer progression\r\nOrganizational culture\r\nJob content\r\nBenefits\r\nRelationships with supervisors\r\nRelationships with colleagues\r\nOne‚Äôs task is to simply determine, based on the available information, the range of how much he or she can expect to be satisfied with these factors in a given job. The app then aggregates the evidence and estimates the expected overall level of job satisfaction, including a level of uncertainty that can provide a guide as to where the person should try to obtain some additional information to reduce this uncertainty. One can also adjust the weights of each factor based on one‚Äôs personal preferences.\r\nIf you have at least two job offers to choose from, you may find the app as useful as I did. Here‚Äôs a link to the app.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-28-job-comparator/./decisionMaking.jpg",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-27-deloitte-hc-trends-themes/",
    "title": "Themes in Deloitte's Global Human Capital Trends between 2011 and 2023",
    "description": "Reading the latest release of Deloitte Global HC Trends made me wonder what common themes this regular series has been covering throughout its 12 years long history.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-27",
    "categories": [
      "global hc trends",
      "deloitte",
      "openai",
      "r",
      "python"
    ],
    "contents": "\r\nAside from satisfying a simple curiosity, it was also a good opportunity to try out a nerdy combination of various cool DS tools: openAI‚Äôs embeddings for determining trends similarity, UMAP for dimensionality reduction, DBSCAN for cluster analysis, openAI‚Äôs chat completion for cluster summarization and naming, Plotly for interactive dataviz, Shiny for dashboarding, and Python and R for orchestrating it all.\r\nThe result? The analysis revealed 13 distinct themes among the 118 specific trends:\r\nGlobal Talent Management Strategies (23)\r\nLeadership Development and Talent Management (16)\r\nHR Transformation and Innovation (15)\r\nHuman Capital and Workforce Strategies (12)\r\nWorkforce Data and Analytics (10)\r\nCognitive Technologies and Workforce (8)\r\nEmployee-Centric Learning and Development (7)\r\nPerformance Management and Compensation (7)\r\nImproving Employee Experience and Well-being (6)\r\nCloud Computing and HR Transformation (4)\r\nEmployee Engagement and Retention (4)\r\nDiversity in Business Strategy (3)\r\nWorkplace Flexibility Strategies (3)\r\n\r\nIt‚Äôs no wonder I‚Äôve had dejavu feelings about some trends over the years, but that‚Äôs why they are called trends, because they persist over time, right? üòâ\r\nIf you would like to check the analysis output interactively and in greater detail, you can use this simple dashboard.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-27-deloitte-hc-trends-themes/./dttHCTrends.png",
    "last_modified": "2023-09-16T13:24:38+02:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 1000
  },
  {
    "path": "posts/2023-06-22-team-level-predictors-of-innovation/",
    "title": "Team-level predictors of innovation at work",
    "description": "Team processes seem to beat team composition and structure when it comes to innovation at work.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-22",
    "categories": [
      "innovation",
      "team",
      "meta-analysis",
      "people analytics"
    ],
    "contents": "\r\nAt least this is suggested by an interesting meta-analysis of team-level predictors of innovation at work by H√ºlsheger, Anderson, & Salgado (2009).\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data with the results of the meta-analysis \r\ndata <- readxl::read_xlsx(\"./metaAnalysisResults.xlsx\")\r\n#dplyr::glimpse(data)\r\n\r\ndata %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(variable, rho), y = rho, group = area, color = area)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_errorbar(aes(ymin=l95, ymax=h95), width=.2, position=position_dodge(0.05), linewidth = 1) +\r\n  ggplot2::geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_color_manual(values = c(\"Team composition and structure\"=\"#4e79a7\", \"Team process\" = \"#f28e2b\")) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"AVERAGE CORRECTED CORRELATION\",\r\n    title = \"Team-Level Predictors of Innovation at Work\",\r\n    caption = \"\\nThe bars around the point estimates represent the 2.5% lower and 97.5% upper limits of the 95% confidence interval.\\nSource: H√ºlsheger, U. R., Anderson, N., & Salgado, J. F. (2009). Team-level predictors of innovation at work: A comprehensive meta-analysis spanning three decades\\nof research. Journal of Applied Psychology, 94(5), 1128‚Äì1145. https://doi.org/10.1037/a0015978\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,10,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 10, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"top\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.text = element_text(size = 12),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nIf that‚Äôs true, not sure whether it‚Äôs good news or bad news for companies‚Äô innovation initiatives. Is it easier to change processes or team composition? I expect there will be a lot of ‚Äúit depends‚Äù üòâ What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-22-team-level-predictors-of-innovation/./innovation.jpg",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-19-latent-class-analysis/",
    "title": "Latent Class Analysis of responses from employee surveys",
    "description": "How listening to a podcast about conspiracies and disinformation inspired me to try out a \"new\" statistical tool popular among sociologists.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-19",
    "categories": [
      "latent class analysis",
      "employee survey",
      "people analytics",
      "r"
    ],
    "contents": "\r\nI recently listened to a podcast about a very interesting study about conspiracies and disinformation in Czech society, and one of the authors of the study, Matous Pilnacek, a sociologist, spoke very enthusiastically and positively during the interview about the analytical possibilities offered by Latent Class Analysis (LCA).\r\nNot being a sociologist, among whom this tool is well-known, I was quite easily impressed and hooked üôÇ LCA allows probabilistic modeling of multivariate categorical data with the assumption that there are latent classes of people who are characterized by a common pattern of probabilities of responses to a set of questions on some categorical, e.g.¬†Likert scale.\r\nLCA is thus a natural fit for identifying subgroups of people with similar work views. Compared to other methods used in this context, such as Factor Analysis, k-means, or hierarchical clustering, it has several advantages:\r\nLCA can handle well categorical observed variables.\r\nLCA estimates probabilities of class membership, so it inherently incorporates uncertainty about which class each individual belongs to.\r\nLCA allows for meaningful analysis of missing answers or non-answers together with proper answers on a Likert scale.\r\nLCA provides a framework (via information criteria like BIC, AIC, or likelihood ratio tests) for comparing models with different numbers of classes.\r\nLCA‚Äôs output is intuitive and easy to interpret.\r\nWhat follows, is a a small demonstration of this tool on artificial employee survey data accompanying the book Predictive HR Analytics: Mastering the HR Metric by Edwards & Edwards (2019). It contains the survey responses of 832 employees on a 1 ‚Äòstrongly disagree‚Äô to 5 ‚Äòstrongly agree‚Äô response scale for a following set of statements.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\n\r\n# uploading legend to the data\r\nlegend <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Legend\") \r\n\r\n# user-friendly table with individual survey items\r\nDT::datatable(\r\n  legend %>% dplyr::mutate(Scale = as.factor(Scale)),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nBefore moving on to modeling, we first need to wrangle the data a bit - select the relevant variables, change their data type, and replace missing values with ‚ÄúPrefer Not to Say‚Äù reply category.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ndata <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Data\")\r\n\r\n# preparing the data for modeling\r\nmydata <- data %>%\r\n  dplyr::select(-sex:-ethnicity) %>%\r\n  dplyr::mutate_all(as.character) %>%\r\n  replace(is.na(.), \"Prefer Not to Say\") %>%\r\n  dplyr::mutate_all(factor, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"Prefer Not to Say\"))\r\n\r\n\r\nNow we can proceed with the modeling. For this we will use poLCA R package. One of the parameters to be set is the expected number of classes. To choose the right number, we need to fit several LCA models with different numbers of classes and, based on information criteria such as BIC or AIC, choose the model with the best balance between model complexity and good fit to the data. Here, I set the parameter to the best value I determined earlier.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(poLCA)\r\n\r\n# specifying and running the model\r\nset.seed(1234)\r\nlca_model <- poLCA::poLCA(\r\n  cbind(ManMot1, ManMot2, ManMot3, ManMot4, ocb1, ocb2, ocb3, ocb4, aut1, aut2, aut3, Justice1, Justice2, Justice3, JobSat1, JobSat2, Quit1, Quit2, Quit3, Man1, Man2, Man3, Eng1, Eng2, Eng3, Eng4, pos1, pos2, pos3)~1, \r\n  data = mydata, \r\n  nclass = 4, \r\n  nrep = 3,\r\n  verbose = FALSE,\r\n  graphs = FALSE\r\n)\r\n\r\n\r\nLet‚Äôs check some of the outputs of the analysis: 1) probability of responses to individual items by people from different classes,\r\n\r\n\r\nShow code\r\n\r\n# looking at the model output\r\nlca_model\r\n\r\nConditional item response (column) probabilities,\r\n by outcome variable, for each class (row) \r\n \r\n$ManMot1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1350 0.4245 0.3092 0.0875 0.0227 0.0211\r\nclass 2:  0.0732 0.1220 0.1463 0.0244 0.0000 0.6341\r\nclass 3:  0.0906 0.1342 0.2384 0.2976 0.2021 0.0371\r\nclass 4:  0.4765 0.3591 0.1274 0.0121 0.0132 0.0118\r\n\r\n$ManMot2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.3345 0.4531 0.1377 0.0534 0.0081 0.0132\r\nclass 2:  0.1220 0.1463 0.0976 0.0244 0.0000 0.6098\r\nclass 3:  0.2274 0.2580 0.2239 0.1508 0.1033 0.0366\r\nclass 4:  0.7507 0.1967 0.0325 0.0080 0.0121 0.0000\r\n\r\n$ManMot3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1180 0.3715 0.3610 0.1077 0.0209 0.0209\r\nclass 2:  0.0732 0.0732 0.2195 0.0000 0.0000 0.6341\r\nclass 3:  0.1005 0.1045 0.2517 0.3043 0.2019 0.0371\r\nclass 4:  0.4354 0.3590 0.1482 0.0252 0.0201 0.0121\r\n\r\n$ManMot4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1965 0.4915 0.1830 0.0853 0.0225 0.0212\r\nclass 2:  0.0732 0.1463 0.0976 0.0488 0.0000 0.6341\r\nclass 3:  0.1437 0.1539 0.2503 0.2418 0.1798 0.0305\r\nclass 4:  0.5329 0.3401 0.1070 0.0000 0.0121 0.0078\r\n\r\n$ocb1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.3092 0.4552 0.1880 0.0344 0.0026 0.0105\r\nclass 2:  0.1463 0.1707 0.0732 0.0000 0.0000 0.6098\r\nclass 3:  0.4200 0.3895 0.1463 0.0135 0.0122 0.0185\r\nclass 4:  0.6336 0.2680 0.0592 0.0272 0.0000 0.0120\r\n\r\n$ocb2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1645 0.4304 0.3103 0.0577 0.0131 0.0240\r\nclass 2:  0.0732 0.1707 0.1463 0.0000 0.0000 0.6098\r\nclass 3:  0.2486 0.4233 0.2233 0.0617 0.0246 0.0185\r\nclass 4:  0.3427 0.3929 0.1963 0.0403 0.0121 0.0156\r\n\r\n$ocb3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1373 0.3711 0.3566 0.0869 0.0293 0.0188\r\nclass 2:  0.0732 0.1707 0.0732 0.0488 0.0244 0.6098\r\nclass 3:  0.2291 0.3319 0.2798 0.0853 0.0493 0.0246\r\nclass 4:  0.3206 0.3222 0.2333 0.0647 0.0396 0.0196\r\n\r\n$ocb4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0971 0.2143 0.2582 0.2185 0.1988 0.0131\r\nclass 2:  0.0732 0.1220 0.0000 0.0976 0.0732 0.6341\r\nclass 3:  0.2431 0.1670 0.1081 0.1557 0.2955 0.0307\r\nclass 4:  0.3205 0.1708 0.1708 0.1759 0.1459 0.0162\r\n\r\n$aut1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5) Pr(6)\r\nclass 1:  0.1914 0.6010 0.1600 0.0476 0.0000     0\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000     1\r\nclass 3:  0.3128 0.3342 0.1095 0.1765 0.0671     0\r\nclass 4:  0.5015 0.4326 0.0659 0.0000 0.0000     0\r\n\r\n$aut2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5) Pr(6)\r\nclass 1:  0.0892 0.5412 0.2574 0.1121 0.0000     0\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000     1\r\nclass 3:  0.1521 0.3402 0.1754 0.2164 0.1159     0\r\nclass 4:  0.4131 0.4234 0.1476 0.0160 0.0000     0\r\n\r\n$aut3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.1199 0.5929 0.2172 0.0674 0.0025 0.0000\r\nclass 2:  0.0000 0.0000 0.0244 0.0000 0.0000 0.9756\r\nclass 3:  0.2286 0.3560 0.1749 0.1426 0.0918 0.0061\r\nclass 4:  0.4325 0.4548 0.0964 0.0082 0.0000 0.0081\r\n\r\n$Justice1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0782 0.7351 0.1756 0.0110 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0061 0.0190 0.2754 0.3353 0.3398 0.0245\r\nclass 4:  0.0526 0.3241 0.5002 0.0742 0.0247 0.0242\r\n\r\n$Justice2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0759 0.7355 0.1788 0.0099 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0061 0.0190 0.2458 0.3396 0.3528 0.0367\r\nclass 4:  0.0445 0.2954 0.5556 0.0463 0.0380 0.0202\r\n\r\n$Justice3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0332 0.6473 0.2877 0.0318 0.0000\r\nclass 2:  0.0000 0.0000 0.0000 0.0000 0.0000 1.0000\r\nclass 3:  0.0000 0.0305 0.2225 0.3209 0.3956 0.0305\r\nclass 4:  0.0404 0.2239 0.5407 0.1179 0.0569 0.0202\r\n\r\n$JobSat1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0749 0.7008 0.1795 0.0298 0.0053 0.0097\r\nclass 2:  0.0244 0.0000 0.0000 0.0000 0.0000 0.9756\r\nclass 3:  0.1355 0.3993 0.1967 0.1263 0.1220 0.0202\r\nclass 4:  0.4703 0.4749 0.0267 0.0242 0.0000 0.0041\r\n\r\n$JobSat2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0527 0.6334 0.2540 0.0546 0.0000 0.0053\r\nclass 2:  0.0244 0.0000 0.0000 0.0000 0.0000 0.9756\r\nclass 3:  0.1100 0.2836 0.2498 0.2345 0.1037 0.0183\r\nclass 4:  0.3717 0.5498 0.0388 0.0356 0.0000 0.0040\r\n\r\n$Quit1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0086 0.0503 0.3622 0.5410 0.0193 0.0185\r\nclass 2:  0.0488 0.0000 0.1463 0.1707 0.0244 0.6098\r\nclass 3:  0.1476 0.2228 0.1411 0.2832 0.1870 0.0182\r\nclass 4:  0.0103 0.0176 0.0660 0.3442 0.5619 0.0000\r\n\r\n$Quit2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0076 0.1285 0.2897 0.5154 0.0378 0.0210\r\nclass 2:  0.0732 0.0732 0.0732 0.1463 0.0488 0.5854\r\nclass 3:  0.1804 0.2770 0.1514 0.2373 0.1476 0.0063\r\nclass 4:  0.0103 0.0356 0.0533 0.3251 0.5758 0.0000\r\n\r\n$Quit3\r\n           Pr(1)  Pr(2) Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0065 0.0874 0.365 0.5037 0.0137 0.0237\r\nclass 2:  0.0244 0.0488 0.122 0.1463 0.0488 0.6098\r\nclass 3:  0.1862 0.2099 0.159 0.2557 0.1649 0.0243\r\nclass 4:  0.0081 0.0179 0.058 0.3631 0.5529 0.0000\r\n\r\n$Man1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0161 0.4123 0.4782 0.0434 0.0026 0.0474\r\nclass 2:  0.0244 0.2683 0.0732 0.0244 0.0000 0.6098\r\nclass 3:  0.0183 0.1371 0.4832 0.2147 0.1404 0.0062\r\nclass 4:  0.2745 0.5092 0.1949 0.0094 0.0000 0.0121\r\n\r\n$Man2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0187 0.3700 0.4954 0.0661 0.0051 0.0448\r\nclass 2:  0.0244 0.2683 0.0244 0.0732 0.0000 0.6098\r\nclass 3:  0.0244 0.1232 0.4177 0.2329 0.1956 0.0062\r\nclass 4:  0.2381 0.5147 0.1917 0.0354 0.0081 0.0121\r\n\r\n$Man3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0213 0.3718 0.5068 0.0536 0.0044 0.0422\r\nclass 2:  0.0244 0.2439 0.0732 0.0488 0.0000 0.6098\r\nclass 3:  0.0244 0.0998 0.4262 0.2522 0.1913 0.0062\r\nclass 4:  0.2341 0.5234 0.1969 0.0255 0.0040 0.0160\r\n\r\n$Eng1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0100 0.3362 0.4682 0.1654 0.0158 0.0044\r\nclass 2:  0.0000 0.1463 0.1463 0.0732 0.0488 0.5854\r\nclass 3:  0.1709 0.2392 0.1777 0.2413 0.1526 0.0183\r\nclass 4:  0.2432 0.4776 0.2106 0.0551 0.0040 0.0094\r\n\r\n$Eng2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0406 0.4844 0.4136 0.0581 0.0000 0.0032\r\nclass 2:  0.0732 0.1220 0.1463 0.0244 0.0488 0.5854\r\nclass 3:  0.2601 0.2988 0.1498 0.1815 0.1098 0.0000\r\nclass 4:  0.3312 0.5015 0.1391 0.0088 0.0000 0.0193\r\n\r\n$Eng3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0043 0.4245 0.5074 0.0572 0.0000 0.0065\r\nclass 2:  0.0244 0.0976 0.1463 0.0732 0.0732 0.5854\r\nclass 3:  0.1887 0.2142 0.2704 0.1802 0.1403 0.0061\r\nclass 4:  0.2442 0.5647 0.1576 0.0151 0.0000 0.0183\r\n\r\n$Eng4\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0026 0.3724 0.5065 0.1117 0.0026 0.0044\r\nclass 2:  0.0244 0.1463 0.1220 0.0732 0.0488 0.5854\r\nclass 3:  0.2015 0.1987 0.2597 0.2117 0.1283 0.0000\r\nclass 4:  0.2870 0.5095 0.1824 0.0117 0.0000 0.0095\r\n\r\n$pos1\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.1046 0.6481 0.2241 0.0073 0.0160\r\nclass 2:  0.0000 0.0488 0.2195 0.0488 0.0732 0.6098\r\nclass 3:  0.0305 0.0479 0.1663 0.4436 0.3117 0.0000\r\nclass 4:  0.0728 0.2523 0.4393 0.1949 0.0127 0.0280\r\n\r\n$pos2\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0023 0.1124 0.6717 0.1952 0.0024 0.0159\r\nclass 2:  0.0244 0.0488 0.2439 0.0488 0.0244 0.6098\r\nclass 3:  0.0549 0.0313 0.2471 0.3795 0.2812 0.0061\r\nclass 4:  0.0289 0.2999 0.4990 0.1482 0.0000 0.0240\r\n\r\n$pos3\r\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)\r\nclass 1:  0.0000 0.0608 0.6749 0.2278 0.0205 0.0159\r\nclass 2:  0.0244 0.0000 0.2927 0.0244 0.0488 0.6098\r\nclass 3:  0.0488 0.0180 0.2200 0.3711 0.3421 0.0000\r\nclass 4:  0.0202 0.2505 0.5242 0.1766 0.0086 0.0200\r\n\r\nEstimated class population shares \r\n 0.4564 0.0493 0.197 0.2973 \r\n \r\nPredicted class memberships (by modal posterior prob.) \r\n 0.4639 0.0493 0.1947 0.2921 \r\n \r\n========================================================= \r\nFit for 4 latent classes: \r\n========================================================= \r\nnumber of observations: 832 \r\nnumber of estimated parameters: 583 \r\nresidual degrees of freedom: 249 \r\nmaximum log-likelihood: -29354.91 \r\n \r\nAIC(4): 59875.81\r\nBIC(4): 62629.81\r\nG^2(4): 47676.67 (Likelihood ratio/deviance statistic) \r\nX^2(4): 1.120338e+25 (Chi-square goodness of fit) \r\n \r\n\r\nprobabilities of people belonging to individual classes,\r\n\r\n\r\nShow code\r\n\r\n# probabilities of belonging to individual classes (first 10 rows)\r\nhead(lca_model$posterior, n = 10)\r\n\r\n              [,1] [,2]         [,3]          [,4]\r\n [1,] 1.705863e-01    0 1.088439e-08  8.294137e-01\r\n [2,] 1.208337e-02    0 9.879166e-01  0.000000e+00\r\n [3,] 4.990889e-13    0 1.000000e+00  0.000000e+00\r\n [4,] 9.999903e-01    0 9.649316e-06  6.879833e-09\r\n [5,] 0.000000e+00    1 0.000000e+00  0.000000e+00\r\n [6,] 9.988548e-01    0 3.805196e-06  1.141380e-03\r\n [7,] 1.782575e-13    0 1.000000e+00  0.000000e+00\r\n [8,] 0.000000e+00    0 1.000000e+00  0.000000e+00\r\n [9,] 1.272725e-04    0 1.104259e-11  9.998727e-01\r\n[10,] 1.064019e-09    0 1.000000e+00 1.215934e-244\r\n\r\nand 3) predicted belongings of people to the classes.\r\n\r\n\r\nShow code\r\n\r\n# predicted belongings to the classes\r\nlca_model$predclass\r\n\r\n  [1] 4 3 3 1 2 1 3 3 4 3 1 1 1 4 4 1 1 1 4 1 1 1 3 4 2 3 4 1 4 3 1 4\r\n [33] 3 1 4 3 3 1 3 3 4 4 1 1 3 1 4 1 1 3 1 3 4 1 1 2 1 4 3 1 3 1 4 3\r\n [65] 4 1 4 3 4 1 1 1 4 3 2 4 1 1 4 1 3 3 1 4 1 4 1 3 4 4 4 4 1 1 1 4\r\n [97] 4 1 3 1 3 3 1 4 1 4 3 3 1 2 1 1 4 1 4 1 4 1 4 4 1 4 3 1 4 1 1 3\r\n[129] 1 1 4 4 4 4 4 1 4 3 1 1 3 1 1 1 1 1 4 3 1 3 4 1 4 3 1 1 1 1 3 3\r\n[161] 1 1 1 4 1 1 3 1 1 1 4 3 3 1 1 1 1 1 1 3 1 3 1 1 4 4 3 4 1 4 3 3\r\n[193] 3 3 4 1 4 2 3 4 4 3 1 4 1 4 4 4 4 3 3 1 4 1 4 3 1 3 4 1 1 4 4 1\r\n[225] 4 1 4 4 1 4 4 4 3 4 1 1 4 1 3 1 4 1 4 1 3 4 3 1 1 1 4 1 1 1 4 1\r\n[257] 4 1 1 1 1 3 1 1 4 1 1 1 4 1 1 1 1 1 3 4 4 1 1 4 4 3 4 3 1 3 1 3\r\n[289] 1 2 4 4 3 4 1 4 4 1 1 1 1 1 4 1 4 3 1 1 4 1 1 3 1 4 1 4 1 4 1 4\r\n[321] 4 4 1 4 1 4 3 4 4 1 1 4 4 2 4 1 1 1 1 4 1 4 3 1 4 4 4 1 1 4 3 2\r\n[353] 4 1 4 3 3 1 4 4 3 2 4 1 1 1 4 1 2 1 1 4 1 1 1 1 4 1 1 1 1 2 4 1\r\n[385] 1 1 2 1 1 3 4 1 1 3 1 4 1 1 1 2 1 1 4 1 3 2 1 1 3 1 4 4 1 1 1 1\r\n[417] 1 1 4 4 4 1 1 3 1 1 4 3 1 1 1 1 1 4 1 2 1 4 4 1 1 1 1 1 2 1 4 2\r\n[449] 1 4 1 3 1 1 3 4 3 1 1 3 4 4 4 4 4 3 1 4 1 1 3 3 3 1 1 3 1 4 3 1\r\n[481] 1 1 1 4 4 2 1 3 1 1 1 4 4 1 3 1 1 3 1 1 1 1 3 4 4 4 3 1 1 1 4 3\r\n[513] 4 1 1 4 1 1 1 4 2 1 4 1 4 1 3 1 3 2 4 2 3 4 1 1 4 4 4 1 3 1 4 1\r\n[545] 1 1 4 3 1 4 4 4 1 3 1 2 1 1 3 4 2 3 2 1 1 1 1 4 4 2 1 3 1 4 1 3\r\n[577] 4 3 3 4 1 4 1 2 4 3 1 1 1 1 1 1 3 4 4 1 4 1 1 4 1 1 1 4 1 4 1 1\r\n[609] 3 1 3 1 1 3 4 4 3 1 4 1 3 4 1 1 4 1 4 4 4 1 3 4 2 4 1 1 1 4 1 4\r\n[641] 1 1 1 1 1 3 1 3 1 1 3 1 1 4 1 4 3 1 4 1 2 2 1 3 4 3 1 4 2 3 4 1\r\n[673] 3 3 4 3 1 1 1 1 4 1 1 3 1 3 1 1 3 1 1 1 1 1 1 3 2 1 1 1 4 1 4 4\r\n[705] 4 1 1 1 1 1 1 3 4 3 4 1 3 1 1 1 2 3 3 3 3 3 4 3 1 1 1 3 3 1 3 2\r\n[737] 1 2 3 1 4 1 4 2 3 1 1 4 3 1 3 4 3 4 1 4 1 1 3 4 4 4 3 1 1 2 1 1\r\n[769] 1 1 4 3 3 4 4 1 3 4 2 4 4 3 3 1 3 1 2 4 1 1 3 1 4 3 4 1 1 1 4 1\r\n[801] 2 1 4 1 4 4 4 1 1 4 1 4 4 4 2 4 4 3 4 1 1 3 1 3 3 1 1 4 4 3 3 1\r\n\r\nShow code\r\n\r\n# checking the size of the classes\r\ntable(lca_model$predclass)\r\n\r\n\r\n  1   2   3   4 \r\n386  41 162 243 \r\n\r\nIf we wanted to visualize the results using our own charts, for example, with full-stacked bar charts showing average probability of responses per scale and class, we need to do some data wrangling of information extracted from the fitted model.\r\n\r\n\r\nShow code\r\n\r\n# extracting information from the model for dataviz\r\nscaleNames <- mydata %>% names()\r\nclasses <- 4\r\nresponsesDf <- data.frame()\r\n\r\nfor(s in scaleNames){\r\n  \r\n  df <- data.frame()\r\n  \r\n  counter <- 1\r\n  \r\n  for(v in 1:length(lca_model$probs[[s]])){\r\n    \r\n    p <- lca_model$probs[[s]][v]\r\n    cl <- paste0(\"Class \", counter)\r\n    \r\n    supp <- data.frame(item = s, class = cl, p = p)\r\n    \r\n    df <- rbind(df, supp)\r\n    \r\n    if(counter < classes){\r\n      \r\n      counter <- counter + 1\r\n      \r\n    } else{\r\n      \r\n      counter <- 1\r\n      \r\n    }\r\n    \r\n  }\r\n  \r\n  df <- df %>%\r\n    dplyr::mutate(choice = rep(c(\"Strongly Disagree\",\"Disagree\",\"Neither Agree nor Disagree\",\"Agree\",\"Strongly Agree\",\"Prefer Not to Say\"), each=classes))\r\n  \r\n  responsesDf <- rbind(responsesDf, df)\r\n  \r\n}\r\n\r\n# computing size of individual classes\r\nclassCnts <- table(lca_model$predclass) %>% \r\n  as.data.frame() %>%\r\n  dplyr::mutate(\r\n    Var1 = as.character(Var1),\r\n    Var1 =  paste0(\"Class \", Var1)\r\n  ) %>%\r\n  dplyr::rename(\r\n    class = Var1,\r\n    freq = Freq\r\n    )\r\n\r\n# final df for dataviz\r\ndatavizDf <- responsesDf %>%\r\n  dplyr::mutate(\r\n    scale = stringr::str_remove_all(item, \"\\\\d\"),\r\n    choice = factor(choice, levels = c(\"Strongly Disagree\",\"Disagree\",\"Neither Agree nor Disagree\",\"Agree\",\"Strongly Agree\",\"Prefer Not to Say\"), ordered = TRUE),\r\n    scale = case_when(\r\n      scale == \"aut\" ~ \"Autonomy\",\r\n      scale == \"Eng\" ~ \"Engagement\",\r\n      scale == \"JobSat\" ~ \"Job Satisfaction\",\r\n      scale == \"Justice\" ~ \"Proc.Justice\",\r\n      scale == \"Man\" ~ \"Management\",\r\n      scale == \"ManMot\" ~ \"Mng.Motivation\",\r\n      scale == \"ocb\" ~ \"OCB\",\r\n      scale == \"pos\" ~ \"Org.Support\",\r\n      scale == \"Quit\" ~ \"Quitting Intentions\"\r\n    )\r\n    ) %>%\r\n  dplyr::group_by(scale, class, choice) %>%\r\n  dplyr::summarise(p = mean(p)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::left_join(classCnts, by = \"class\") %>%\r\n  dplyr::mutate(class = stringr::str_glue(\"{class} (n = {freq})\"))\r\n\r\n\r\nIn the resulting graphs, we quickly see that there is one class of people who often prefer not to give their opinion in an employee survey (Class 2), one class of people with a strongly negative view of their employment experience (Class 4), one class of people with a neutral to slightly negative view of work (Class 1), and one class of people with a neutral to positive view of work (Class 3).\r\n\r\n\r\nShow code\r\n\r\n# dataviz\r\ndatavizDf %>%\r\n  ggplot2::ggplot(aes(x = scale, y = p, fill = choice)) +\r\n  ggplot2::scale_x_discrete(limits = rev) +\r\n  ggplot2::geom_bar(stat = \"identity\", position = position_fill(reverse = TRUE)) +\r\n  ggplot2::scale_fill_manual(values = c(\"Strongly Disagree\"=\"#c00000\",\"Disagree\"=\"#ed7d31\",\"Neither Agree nor Disagree\"=\"#ffc000\",\"Agree\"=\"#00b050\",\"Strongly Agree\"=\"#4472c4\",\"Prefer Not to Say\"=\"#d9d4d4\")) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::facet_wrap(~class,nrow = 2) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"AVERAGE PROBABILITY OF RESPONSE\",\r\n    fill = \"\",\r\n    title = \"Latent Class Analysis of responses from the employee survey\"\r\n    ) +\r\n  ggplot2::theme_bw() +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text = element_text(size = 12, face = \"bold\"),\r\n    strip.background = element_rect(fill = \"white\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    legend.box.margin=margin(0,0,0,0),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\",\r\n  ) +\r\n  ggplot2::guides(fill = guide_legend(nrow = 1))\r\n\r\n\r\n\r\nIt would certainly be possible to delve deeper into the results, but for a basic overview of the process of LCA and its outputs, this might be sufficient. If interested, a more detailed introduction to LCA can be found, for example, in this practitioner‚Äôs guide.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-19-latent-class-analysis/./scheme.png",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {},
    "preview_width": 930,
    "preview_height": 525
  },
  {
    "path": "posts/2023-06-05-mindfulness-and-objectivity/",
    "title": "Another positive effect of mindfulness meditation on the horizon?",
    "description": "Is it possibile to improve the objectivity of decision making through mindfulness meditation?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-05",
    "categories": [
      "decision-making",
      "mindfulness",
      "field experiment"
    ],
    "contents": "\r\nMaybe, at least this is suggested by a pre-registered field experiment by Ash et al.¬†(2023), where people practicing mindfulness meditation for 15 minutes a day for 2 weeks compared to an active control (listening to relaxing music) showed a reduced tendency to avoid painful information that may trigger worry or regret, with a likely mechanism for this effect being improved emotion regulation.\r\nIf this were the case, this would be great news for our decision making, as the ability to impartially evaluate all relevant information is one of the keys to good decision making.\r\nIt‚Äôs true that the observed effect was rather small and barely distinguishable from noise and we may be concerned about its reproducibility, but as a Trekkie and a big fan of Mr.¬†Spock, I like the idea that there might be a method that could make us a little more like this cool half-Vulcan and half-human üòâüññ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-05-mindfulness-and-objectivity/./spock.jpg",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-05-bayesian-networks-in-people-analytics/",
    "title": "Use of Bayesian networks in people analytics?",
    "description": "Bayesian networks seem to have some interesting properties that could make them useful for various people analytics use cases, but for some reason this is not the case.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-06-01",
    "categories": [
      "bayesian networks",
      "probabilistic graphical models",
      "people analytics"
    ],
    "contents": "\r\nIn a people analytics project I‚Äôm involved in, we were asked to come up with a prediction model that would perform reasonably well while being very easy to understand and interpret for non-technical users.\r\nIn considering various options, we also came across Bayesian networks (BNs), probabilistic graphical models consisting of nodes and directed edges that represent conditional (and under certain assumptions, causal) relationships between random variables. They seem to have several interesting properties that would suit our needs, namely:\r\nBN models are not ‚Äúblack boxes‚Äù, but all their parameters have a comprehensible interpretation, which may reduce users‚Äô algorithm aversion and increase their willingness to take algorithm outputs into account in their decision-making.\r\nExpert knowledge can easily be used in the development of BN models by building the network structure using expert knowledge of im/plausible (causal) relationships between the variables under study.\r\nThe model lends itself to a user-friendly visualization of its structure, helping to design new models that take into account both data-based constraints and the knowledge of experts in the field.\r\nIt is possible to use the data itself to estimate the possible network topology and thus gain preliminary insight into the structure of the problem domain defined by the available variables.\r\nAfter fitting the model, it can be used for reasoning, i.e., calculating probabilities of interest conditional on the available evidence. This reasoning can be done even with incomplete data, based only on the known values of any combination of the available variables.\r\nThe attached graph is for illustrative purposes only - the output from early experiments with BNs on artificial IBM employee attrition data.\r\nHowever, when searching for information about this method, we found that it is not particularly popular among people analytics and I/O psychology practitioners. Would anyone of the readers happen to have a good or bad experience using this tool and would also be willing to share it? üôè\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-05-bayesian-networks-in-people-analytics/./plot.png",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {},
    "preview_width": 1121,
    "preview_height": 746
  },
  {
    "path": "posts/2023-05-26-selection-procedures-validity-update/",
    "title": "Visualizing shifts in validity estimates for selection procedures",
    "description": "Let's take a slopegraph perspective to assess changes in estimates of the validity of selection procedures.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-26",
    "categories": [
      "i/o psychology",
      "validity",
      "employee selection",
      "meta-analysis",
      "data visualization"
    ],
    "contents": "\r\nI suppose that many, if not the majority, of I/O psychology and people analytics folks have already heard about a new meta-analytic estimation of validity for selection procedures that is based on a more realistic range-restriction correction performed by Sackett et al.¬†(2022).\r\nNumerous articles have explored the results of this meta-analysis and its presumed implications for the hiring process. However, what I found lacking was a clear visual representation of the changes in the validity estimates, including the original article.\r\nBecause I needed one for a training I was conducting on people analytics and EB-HRM, I created one. Given the nature of the change to be shown, I chose a slopegraph that nicely and intuitively illustrates a two-point change.\r\n\r\n\r\nShow code\r\n\r\n# uploading the necessary libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(ggrepel)\r\n\r\n# uploading data\r\ndata <- readxl::read_xlsx(\"./data.xlsx\")\r\n#glimpse(data)\r\n\r\n# transforming data\r\ndataLong <- data %>%\r\n  tidyr::drop_na() %>%\r\n  tidyr::pivot_longer(Hunter:Sackett, names_to = \"analysis\", values_to = \"validity\") %>%\r\n  dplyr::mutate(analysis = case_when(\r\n    analysis == \"Hunter\" ~ \"Schmidt & Hunter (1998)\",\r\n    analysis == \"Sackett\" ~ \"Sackett et al. (2022)\",\r\n    TRUE ~ \"unknown\"\r\n    ),\r\n    analysis = factor(analysis, levels = c(\"Schmidt & Hunter (1998)\", \"Sackett et al. (2022)\"))\r\n  )\r\n\r\n# creating custom color palette based on Tableau colors\r\nmy_palette <- c(\r\n  \"#4E79A7\", \"#F28E2C\", \"#E15759\", \"#76B7B2\", \"#59A14F\",\r\n  \"#EDC949\", \"#B07AA2\", \"#FF9DA7\", \"#9C755F\", \"#BAB0AB\",\r\n  \"#2F8AC4\"  # an additional distinct color\r\n)\r\n\r\n# creating the slopegraph\r\ndataLong %>%\r\n  ggplot2::ggplot(aes(x = analysis, y = validity, group = SelectionProcedure)) +\r\n  ggplot2::geom_line(aes(color = SelectionProcedure), linewidth = 1) +\r\n  ggplot2::geom_point(aes(color = SelectionProcedure), size = 3) +\r\n  ggrepel::geom_text_repel(data = dataLong %>% filter(analysis == \"Schmidt & Hunter (1998)\"), aes(label = SelectionProcedure, color = SelectionProcedure), size = 4.5, hjust = 1.2, vjust = 0.5, direction = \"y\", force = 1) +\r\n  ggrepel::geom_text_repel(data = dataLong %>% filter(analysis == \"Sackett et al. (2022)\"), aes(label = SelectionProcedure, color = SelectionProcedure), size = 4.5, hjust = -0.2, vjust = 0.5, direction = \"y\", force = 1) +\r\n  ggplot2::scale_color_manual(values = my_palette) +\r\n  ggplot2::labs(\r\n    title = \"Comparison of employee selection procedures validity estimates\",\r\n    y = \"VALIDITY ESTIMATES\", \r\n    x = \"\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 22, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_blank(),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"none\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nMaybe the visualization will come in handy for you as well when trying to ‚Äúrewire‚Äù your long-held beliefs and assumptions. It should make clearer in what direction and to what extent to do so üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-26-selection-procedures-validity-update/selection-procedures-validity-update_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1440
  },
  {
    "path": "posts/2023-05-16-psychometric-network-analysis/",
    "title": "Psychometric network analysis & employee survey data",
    "description": "A demonstration of how psychometric network analysis can be used to gain insights into employee survey data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-16",
    "categories": [
      "network analysis",
      "psychometrics",
      "employee survey",
      "employee engagement",
      "employee satisfaction",
      "r"
    ],
    "contents": "\r\nIf you‚Äôre looking for an alternative to factor analysis for processing employee survey data, consider using psychometric network analysis (hereafter PNA).\r\nIt provides insight into the interdependencies between different topics related to employee experience, and the resulting network diagrams make these insights easily accessible even to non-experts.\r\nIt helps to identify key topics (nodes) that have the most connections or influence within the network, which can be valuable for identifying areas of focus.\r\nAlthough PNA doesn‚Äôt work with latent factors, common community detection algorithms (e.g., Louvain‚Äôs method) can be used to identify clusters of more densely interconnected topics.\r\nIn interpreting the PNA outputs, one can rely on the apparatus of bootstrapping statistics to distinguish signal from noise.\r\nWhat follows is a small demonstration of the use of PNA on employee survey data. For this purpose, I used a sample dataset that accompanies the book Predictive HR Analytics: Mastering the HR Metric by Edwards & Edwards (2019). It contains the survey responses of 832 employees on a 1 ‚Äòstrongly disagree‚Äô to 5 ‚Äòstrongly agree‚Äô response scale for a following set of statements.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(readxl)\r\nlibrary(DT)\r\nlibrary(tidyverse)\r\n\r\n# uploading legend to the data\r\nlegend <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Legend\") \r\n\r\n# user-friendly table with individual survey items\r\nDT::datatable(\r\n  legend %>% dplyr::mutate(Scale = as.factor(Scale)),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nAnd here is a table with the survey responses we will analyse.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ndata <- readxl::read_excel(\"./surveyResults.xls\", sheet = \"Data\")\r\n\r\n# selecting relevant data\r\nmydata <- data %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\n# user-friendly table with the data used in the analysis\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\nWe will use the bootnet R package to estimate the regularized partial correlation network using the Spearman correlation matrix and LASSO regularization, and then select the final network using the Extended Bayesian Information Criterion (with Œ≥ parameter set to 0.5).\r\n\r\n\r\nShow code\r\n\r\n# estimating a regularized partial correlation network\r\nnetwork <- bootnet::estimateNetwork(\r\n  mydata,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE # when TRUE, enforces higher specificity, at the cost of sensitivity\r\n)\r\n\r\nprint(network)\r\n\r\n\r\n=== Estimated network ===\r\nNumber of nodes: 29 \r\nNumber of non-zero edges: 158 / 406 \r\nMean weight: 0.02974358 \r\nNetwork stored in network$graph \r\n \r\nDefault set used: EBICglasso \r\n \r\nUse plot(network) to plot estimated network \r\nUse bootnet(network) to bootstrap edge weights and centrality indices \r\n\r\nRelevant references:\r\n\r\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9 (3), 432-441.\r\n    Foygel, R., & Drton, M. (2010). Extended Bayesian information criteria for Gaussian graphical models. \r\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2014). glasso: Graphical lasso estimation of gaussian graphical models. Retrieved from https://CRAN.R-project.org/package=glasso\r\n    Epskamp, S., Cramer, A., Waldorp, L., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48 (1), 1-18.\r\n    Epskamp, S., Borsboom, D., & Fried, E. I. (2016). Estimating psychological networks and their accuracy: a tutorial paper. arXiv preprint, arXiv:1604.08462.\r\n\r\nThe above brief summary of the estimated network shows that it is a relatively dense network with 158 out of 406 possible connections (39%). Now let‚Äôs plot the network.\r\n\r\n\r\nShow code\r\n\r\n# plotting the estimated network \r\nplot(\r\n  network, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydata),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\"\r\n)\r\n\r\n\r\n\r\nWe can also plot the network in ggplot style, which can be useful when we need more control over the chart, e.g.¬†when we want to plot identified communities/clusters instead of theoretical scales. To do this, we just need to get the necessary information from the network object and do a few data manipulations.\r\n\r\n\r\nShow code\r\n\r\n# uploading \r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# extracting information about the connections form the network object\r\nngMatrix <- network$graph\r\n# inputting to upper part of the matrix zeroes (the edges between items are symmetrical, so we need just a half of the matrix)  \r\nngMatrix[upper.tri(ngMatrix)] <- 0\r\n\r\n# transforming matrix into dataframe\r\nngDf <- ngMatrix %>%\r\n  as.data.frame() %>%\r\n  tibble::rownames_to_column() %>%\r\n  dplyr::rename(itemID = rowname)\r\n\r\n# creating a dataframe with information about connections between items\r\nfromToList <- data.frame()\r\n\r\nfor(i in unique(ngDf$itemID)){\r\n  \r\n  suppDf <- ngDf %>%\r\n    dplyr::filter(itemID == i) %>%\r\n    tidyr::pivot_longer(-itemID, names_to = \"to\", values_to = \"weight\") %>%\r\n    dplyr::filter(weight != 0) %>%\r\n    dplyr::rename(from = itemID) %>%\r\n    dplyr::mutate(\r\n      sign = case_when(\r\n        weight < 0 ~ \"negative\",\r\n        weight > 0 ~ \"positive\",\r\n        TRUE ~ \"zero\"\r\n      ),\r\n      weight = abs(weight)\r\n    )\r\n  \r\n  fromToList <- dplyr::bind_rows(fromToList, suppDf)\r\n  \r\n}\r\n\r\n\r\n# creating a dataframe with information about individual items \r\nitems <- data.frame(\r\n  item = names(mydata),\r\n  scale = legend %>% pull(Scale)\r\n)\r\n\r\n# creating igraph object\r\nigraph_graph <- igraph::graph_from_data_frame(fromToList, directed=FALSE, vertices = items)\r\n\r\n# visualizing the network\r\nset.seed(123)\r\nggraph::ggraph(igraph_graph, layout = \"fr\", maxiter = 500) +  # fr, kk, drl, mds, maxiter = 500 is default\r\n  ggraph::geom_edge_link(aes(edge_width = weight, color = sign), alpha = 0.05) + \r\n  ggraph::geom_node_point(aes(color = scale), size = 5) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  ggraph::theme_graph(background = \"white\") +\r\n  ggraph::scale_edge_color_manual(values = c(\"negative\" = \"red\", \"positive\" = \"blue\")) +\r\n  ggplot2::scale_color_brewer(palette=\"Set1\")\r\n\r\n\r\n\r\nFrom the charts we can quickly gain a basic idea of the internal structure of the data. For example, we can notice that:\r\nitems from the same scales tend to cluster close to each other;\r\nthere are items from different scales that appear to measure motivational aspects of employee attitudes (engagement, organizational citizenship behavior, and quitting intentions), and satisfaction with the ‚Äúexternal forces‚Äù that affect employee experience (management, justice, and perceived organizational support);\r\nthe topic of autonomy is closely related to the topic of managerial motivation;\r\nmost of the negative partial correlations are between items measuring quitting intentions and other survey items.\r\nTo identify most influential topics (nodes) within the network, we can use centrality measures that quantify the relative importance of a node within a network based on different aspects of a node‚Äôs role in the network.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(qgraph)\r\n\r\n# computing centrality measures for individual survey items\r\nqgraph::centralityPlot(network, include = \"all\", orderBy = \"ExpectedInfluence\")\r\n\r\n\r\n\r\nBased on the expected influence centrality measure, which takes into account the presence of both negative and positive edges, the following items appear to be among the most influential ones:\r\nMan3: Management acts decisively.\r\nManMot3: My manager encourages me to do my best.\r\nJustice2: Procedures are applied fairly.\r\naut3: I am given enough leeway to get the job done.\r\nManMot1: My manager motivates me.\r\nBut to know how much we can rely on these measures, we must first test their stability using correlation stability analysis that repeatedly computes centrality indices of subsets of the data and correlates these with the centrality indices of the full data. This process generates a correlation stability coefficient (CS-Coefficient) for each centrality measure. The CS-Coefficient corresponds to the proportion of cases that can be dropped while retaining with 95% certainty a certain level of correlation (e.g., 0.7) between the original centrality measure and the centrality measure of the subset. The higher the CS-Coefficient, the more robust the centrality measure is to the reduction of cases. According to Epskamp & Fried (2018), the CS-coefficient should be above 0.5, and should be at least above 0.25. In our case, we can see the that we can make solid interpretations based on centrality measures of strength (CS(cor = 0.7) = 0.75) and expected influence (CS(cor = 0.7) = 0.75).\r\n\r\n\r\nShow code\r\n\r\n# estimating the stability of centrality measures\r\nbootnet_case_dropping <- bootnet::bootnet(\r\n  network, \r\n  nBoots = 2500,\r\n  type = \"case\",\r\n  nCores = 6,\r\n  statistics = c('strength', 'expectedInfluence', 'betweenness', 'closeness')\r\n)\r\n\r\n# plotting the results\r\nplot(bootnet_case_dropping, 'all')\r\n\r\n\r\nShow code\r\n\r\n# listing the results\r\nbootnet::corStability(bootnet_case_dropping)\r\n\r\n=== Correlation Stability Analysis === \r\n\r\nSampling levels tested:\r\n   nPerson Drop%   n\r\n1      208  75.0 232\r\n2      273  67.2 242\r\n3      337  59.5 254\r\n4      402  51.7 246\r\n5      467  43.9 207\r\n6      532  36.1 259\r\n7      596  28.4 279\r\n8      661  20.6 267\r\n9      726  12.7 251\r\n10     790   5.0 263\r\n\r\nMaximum drop proportions to retain correlation of 0.7 in at least 95% of the samples:\r\n\r\nbetweenness: 0.05 (CS-coefficient is lowest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0, caseMax = 0.127) \r\n\r\ncloseness: 0.05 (CS-coefficient is lowest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0, caseMax = 0.127) \r\n\r\nexpectedInfluence: 0.75 (CS-coefficient is highest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0.672, caseMax = 1) \r\n\r\nstrength: 0.75 (CS-coefficient is highest level tested)\r\n  - For more accuracy, run bootnet(..., caseMin = 0.672, caseMax = 1) \r\n\r\nAccuracy can also be increased by increasing both 'nBoots' and 'caseN'.\r\n\r\nIn a similar way, i.e.¬†using a bootstrapping, we can estimate the stability of the edge weights. This gives us information on how much the edge weights for individual connections vary with 95% confidence intervals. From the chart below, it‚Äôs apparent that some edge weights are more accurate than others, as they show a narrower band. Simultaneously, we observe that the majority of edges closer to zero appear to be non-significant, as they intersect with zero in the bootstrapped samples.\r\n\r\n\r\nShow code\r\n\r\n# estimating the stability of edge weights \r\nbootnet_nonpar <- bootnet::bootnet(\r\n  network, \r\n  nBoots = 1000,\r\n  nCores = 6\r\n  )\r\n\r\n# plotting the results\r\nplot(bootnet_nonpar, labels = FALSE, order = \"sample\")\r\n\r\n\r\n\r\nTo find internal structure in the estimated network, we need not rely solely on the visual inspection of the network diagram, but we can use some of the common community detection algorithms that can be used to identify clusters of more densely connected topics. In the example below, we have merely ‚Äù replicated‚Äù an existing clustering by scale, which is an indication that the authors have managed to construct a survey that measures distinct constructs that they originally intended to measure, but in practice you are likely to observe less distinct patterns more often, which can give you clues about how to improve the construction of the employee survey.\r\n\r\n\r\nShow code\r\n\r\n# identifying communities/clusters\r\nclu <- igraph::cluster_louvain(igraph_graph, weights = fromToList$weight) # cluster_optimal, cluster_louvain, cluster_leading_eigen, cluster_fast_greedy, cluster_walktrap,   cluster_edge_betweenness, cluster_spinglass\r\n\r\n# assigning communities/clusters to nodes\r\nmember <- membership(clu)\r\nV(igraph_graph)$cluster <- as.character(member)\r\n\r\n# visualizing the network with identified communities/clusters\r\nset.seed(123)\r\nggraph::ggraph(igraph_graph, layout = \"fr\", maxiter = 500) +  # fr, kk, drl, mds, maxiter = 500 is default\r\n  ggraph::geom_edge_link(aes(edge_width = weight, color = sign), alpha = 0.05) + \r\n  ggraph::geom_node_point(aes(color = cluster), size = 5) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  ggraph::theme_graph(background = \"white\") +\r\n  ggraph::scale_edge_color_manual(values = c(\"negative\" = \"red\", \"positive\" = \"blue\")) +\r\n  ggplot2::scale_color_brewer(palette=\"Set1\")\r\n\r\n\r\n\r\nAnother use case would be to test for differences in job attitudes‚Äô connectivity and centrality across different groups of employees. To do this, we can use the NetworkComparisonTest R package that implements permutation based hypothesis testing of differences between two networks. Let‚Äôs illustrate it with differences between networks estimated on data coming from females and males. We first filter data for both genders and than estimate and visualize their respective attitudinal networks.\r\n\r\n\r\nShow code\r\n\r\n# filtering data for females and males \r\nmydataFemale <- data %>%\r\n  dplyr::filter(sex == 2) %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\nmydataMale <- data %>%\r\n  dplyr::filter(sex == 1) %>%\r\n  dplyr::select(ManMot1:pos3)\r\n\r\n# estimating a regularized partial correlation network for females and males\r\nnetworkFemale <- bootnet::estimateNetwork(\r\n  mydataFemale,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE \r\n)\r\n\r\nnetworkMale <- bootnet::estimateNetwork(\r\n  mydataMale,\r\n  default = \"EBICglasso\",\r\n  corMethod = \"spearman\",\r\n  threshold = FALSE \r\n)\r\n\r\n# plotting the estimated networks\r\nplot(\r\n  networkFemale, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydataFemale),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\",\r\n  title = \"Females\"\r\n)\r\n\r\n\r\nShow code\r\n\r\nplot(\r\n  networkMale, \r\n  layout = \"spring\",\r\n  groups = legend %>% dplyr::pull(Scale),\r\n  nodeNames = names(mydataMale),\r\n  weighted = TRUE,\r\n  directed = FALSE,\r\n  label.cex = 0.7, \r\n  label.color = 'black', \r\n  label.prop = 0.9, \r\n  negDashed = TRUE, \r\n  legend.cex = 0.27, \r\n  legend.mode = 'style2',\r\n  font = 2,\r\n  theme = \"classic\",\r\n  title = \"Males\"\r\n)\r\n\r\n\r\n\r\nWe can check four basic types of differences between the networks:\r\nThe maximum difference in edge weights (M statistic) that tells us whether the structure of the network is identical across two compared groups, and thus whether the groups differ in the overall ‚Äúshape‚Äù or ‚Äúarchitecture‚Äù of their attitudes.\r\nThe difference in global strength (S statistic) that tells us whether the density of the network is identical across the groups, and thus whether they differ in their openness to change of their attitudes and behaviors (see, for example, Zwicker et al.¬†(2020)).\r\nThe difference in the centrality measures across the groups that tell us whether the groups differ in topics that are most important/influential in their attitudinal network.\r\nThe difference between groups in specific edges. According to van Borkulo et al.¬†(2017), when the M statistic is not statistically significant, it is recommended not to test group-level differences for specific edges as it increases the likelihood of Type 1 error.\r\nAs you can see below, (almost) all test results are statistically non-significant, so we don‚Äôt have sufficiently strong evidence for claiming that there are substantive differences between females and males in their respective attitudinal networks. Only in the case of the Eng1 item (I share the values of this organization.), females show a statistically significantly smaller strength centrality measure compared to males. Given that M statistic is statistically non-significant, we don‚Äôt test statistical significance of differences for specific edges.\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(NetworkComparisonTest)\r\n\r\n# testing the differences\r\nset.seed(123)\r\ntestGenderDiff <- NetworkComparisonTest::NCT(\r\n  networkFemale, \r\n  networkMale, \r\n  binary.data=FALSE, \r\n  paired=FALSE,\r\n  weighted=TRUE,\r\n  test.edges=FALSE, \r\n  edges=\"all\",\r\n  p.adjust.methods=\"holm\",\r\n  test.centrality=TRUE, \r\n  centrality=c(\"strength\",\"expectedInfluence\"),\r\n  nodes=\"all\",\r\n  progressbar=FALSE\r\n  )\r\n\r\n# printing test results\r\nprint(testGenderDiff)\r\n\r\n\r\n NETWORK INVARIANCE TEST \r\n Test statistic M:  0.1747811 \r\n p-value 0.63 \r\n\r\n GLOBAL STRENGTH INVARIANCE TEST \r\n Global strength per group:  12.8295 13.39245 \r\n Test statistic S:  0.5629521 \r\n p-value 0.18 \r\n\r\n EDGE INVARIANCE TEST \r\n\r\nNULL\r\n\r\n CENTRALITY INVARIANCE TEST \r\n \r\n         strength expectedInfluence\r\nManMot1         1              1.00\r\nManMot2         1              1.00\r\nManMot3         1              1.00\r\nManMot4         1              1.00\r\nocb1            1              1.00\r\nocb2            1              1.00\r\nocb3            1              1.00\r\nocb4            1              1.00\r\naut1            1              1.00\r\naut2            1              1.00\r\naut3            1              1.00\r\nJustice1        1              1.00\r\nJustice2        1              1.00\r\nJustice3        1              1.00\r\nJobSat1         1              1.00\r\nJobSat2         1              1.00\r\nQuit1           1              1.00\r\nQuit2           1              1.00\r\nQuit3           1              1.00\r\nMan1            1              1.00\r\nMan2            1              1.00\r\nMan3            1              1.00\r\nEng1            0              0.57\r\nEng2            1              1.00\r\nEng3            1              1.00\r\nEng4            1              1.00\r\npos1            1              1.00\r\npos2            1              1.00\r\npos3            1              1.00\r\n\r\nShow code\r\n\r\n# checking observed differences in centrality measures\r\n# testGenderDiff$diffcen.real \r\n\r\n# plotting results of the network structure invariance test\r\n# plot(testGenderDiff,what=\"network\")\r\n# plotting results of global strength invariance test\r\n# plot(testGenderDiff,what=\"strength\")\r\n# plotting results of the edge invariance test\r\n# plot(testGenderDiff,what=\"edge\")\r\n\r\n\r\nI hope you find this post useful and that it inspires you to try PNA on your own data. If you were looking for more authoritative sources on PNA, Sacha Epskamp‚Äôs site is a good place to start. An excellent introduction to the topic can also be found in Letouche & Wille (2022) and Dalege et al.¬†(2017), respectively.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-16-psychometric-network-analysis/psychometric-network-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2023-05-10-regression-to-the-mean/",
    "title": "Employee commitment over time & regression to the mean",
    "description": "A nice illustration of the regression to the mean phenomenon in the space of people analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-10",
    "categories": [
      "people analytics",
      "critical thinking",
      "regression to the mean",
      "r"
    ],
    "contents": "\r\nA vendor specializing in employee engagement measurements recently presented their findings that companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced the largest decreases and largest increases in commitment post-layoff, respectively. They post-hoc-hypothesized that highly committed employees feel especially hurt and betrayed when layoffs occur.\r\nHowever, when a similar pattern emerges, a warning light should always flash that regression to the mean (RTM) might actually be behind it. When a variable is imperfectly correlated with another variable, extreme values tend to gravitate towards the mean in subsequent measurements, which can make natural variations in repeated data appear like genuine change.\r\nThis doesn‚Äôt mean that RTM was the sole factor in the aforementioned case. However, to know better, it‚Äôs necessary to control for it.\r\nWhat follows is a simple simulation to illustrate how the reported finding could occur purely or partially due to RTM, and how one might control for it.\r\nFirst, let‚Äôs create correlated employee commitment observations per company from time 1 (T1) and time 2 (T2).\r\n\r\n\r\nShow code\r\n\r\n# uploading necessary libraries\r\nlibrary(tidyverse)\r\n\r\nset.seed(1) # seed for reproducibility\r\nn <- 2000 # number of observations\r\nT1 <- rnorm(n) # generating observations at time 1\r\nT2 <- 0.7*T1 + rnorm(n)*sqrt(1-0.7^2) # generating correlated observations at time 2\r\n\r\n# cor(T1, T2) # checking the correlation\r\n\r\ndf <- data.frame(T1=T1, T2=T2) # putting created variables into dataframe\r\n\r\n\r\nWe also need a benchmark for T1 so that we can calculate the difference between T1 values and T1 benchmark. In addition, we also need to calculate the difference between T2 and T1.\r\n\r\n\r\nShow code\r\n\r\n# computing benchmark for pre-layoff period (T1)\r\nbenchmark <- df %>%\r\n  dplyr::summarise(\r\n    benchmark = mean(T1)\r\n  ) %>%\r\n  dplyr::pull(benchmark)\r\n\r\n# computing differences between T2 and T1 and between T1 and T1 benchmark (average)\r\ndf <- df %>%\r\n  dplyr::mutate(\r\n    timeDiff = T2-T1,\r\n    benchmarkDiff = T1-benchmark\r\n  )\r\n\r\n\r\nThen all we have to do is randomly assign individual observations to the group of companies that made layoffs and those that did not.\r\n\r\n\r\nShow code\r\n\r\n# assigning each observation randomly one of two labels - Layoffs/NoLayoffs \r\ndf1 <- df %>%\r\n  dplyr::mutate(\r\n    Layoffs = sample(c(\"Layoffs\", \"NoLayoffs\"), size = n(), replace = TRUE, prob = c(0.15, 0.85))\r\n  )\r\n\r\n\r\nNow we can contrast the differences between T1 values and T1 benchmark on the one hand and the differences between T2 and T1 on the other. As we can see in the chart below, the pattern matches well with the originally reported finding\r\n- companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced the largest decreases and largest increases in commitment post-layoff, respectively, but now purely as a result of RTM.\r\n\r\n\r\nShow code\r\n\r\n# visualizing relationship between the \r\ndf1 %>%\r\n  dplyr::filter(Layoffs == \"Layoffs\") %>%\r\n  ggplot2::ggplot(aes(x = benchmarkDiff, y = timeDiff)) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = F) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-2,4,1)) +\r\n  labs(\r\n    x = \"DIFFERENCE BETWEEN T1 AND T1 BENCHMARK\",\r\n    y = \"DIFFERENCE BETWEEN T2 AND T1\",\r\n    title = \"Changes in employee commitment purely due to regression to the mean\",\r\n    caption = \"\\nA replication of the original finding that companies with the highest and lowest levels of commitment prior to a layoff compared to the benchmark, experienced\\nthe largest decreases and largest increases in commitment post-layoff, respectively, relying only on a regression to the mean phenomenon.\\nT1 and T2 refer to commitment measurements at time 1 (prior to a layoff) and time 2 (after a layoff), respectively.\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\nIn general, in order to make a valid estimate of the effect of layoffs on employee commitment when RTM is at play, we need to control for its effect. One way to do this is to include the difference between T1 values and T1 benchmark in the linear regression model as illustrated below.\r\n\r\n\r\nShow code\r\n\r\n# modeling T2 while controlling for the effect of regression to the mean\r\nmodel1 <- glm(T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), data = df1)\r\nsummary(model1)\r\n\r\n\r\nCall:\r\nglm(formula = T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), \r\n    data = df1)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-2.54938  -0.47760  -0.01441   0.50541   2.19243  \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       0.01538    0.04210   0.365    0.715    \r\nbenchmarkDiff     0.72305    0.01593  45.382   <2e-16 ***\r\nLayoffsNoLayoffs -0.01620    0.04577  -0.354    0.723    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 0.5458733)\r\n\r\n    Null deviance: 2214.4  on 1999  degrees of freedom\r\nResidual deviance: 1090.1  on 1997  degrees of freedom\r\nAIC: 4470\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nIt is clear from the estimated model that there is not much evidence in favor of the existence of a layoff effect, which should not be surprising since individual observations were purely randomly assigned to groups of companies with and without layoffs. However, when we adjusted the data to better reflect the hypothesized causal mechanism behind the observed pattern, the effect of layoffs was detected as statistically significant.\r\n\r\n\r\nShow code\r\n\r\n# creating a new dataset that better reflects the hypothesized causal mechanism behind the observed pattern\r\ndf2 <- df %>%\r\n  dplyr::rowwise() %>%\r\n  dplyr::mutate(\r\n    Layoffs = sample(c(\"Layoffs\", \"NoLayoffs\"), size = 1, replace = TRUE, prob = c(0.15, 0.85)),\r\n    T2 = ifelse(Layoffs == \"Layoffs\" & T1 >= 0.5, T2 - runif(min = 0.3, max = 1.5, n = 1), T2),\r\n    T2 = ifelse(Layoffs == \"Layoffs\" & T1 <= -0.5 , T2 - runif(min = -0.75, max = 0.2, n = 1), T2)\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(timeDiff = T2-T1) # recomputing timeDiff\r\n\r\n# modeling T2 while controlling for the effect of regression to the mean\r\nmodel2 <- glm(T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), data = df2)\r\nsummary(model2)\r\n\r\n\r\nCall:\r\nglm(formula = T2 ~ benchmarkDiff + Layoffs, family = gaussian(link = \"identity\"), \r\n    data = df2)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-2.99200  -0.49173  -0.00417   0.52662   2.52414  \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)      -0.19596    0.04404  -4.449  9.1e-06 ***\r\nbenchmarkDiff     0.66308    0.01662  39.908  < 2e-16 ***\r\nLayoffsNoLayoffs  0.19571    0.04786   4.089  4.5e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 0.5933289)\r\n\r\n    Null deviance: 2135.6  on 1999  degrees of freedom\r\nResidual deviance: 1184.9  on 1997  degrees of freedom\r\nAIC: 4636.7\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nFeel free to share your own experiences and encounters with the phenomena of regression to the mean in your people analytics practice.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-10-regression-to-the-mean/./rtm.gif",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-04-cv-job-match-career-site/",
    "title": "Improving a company career site with tools from OpenAI",
    "description": "How my own experience of exploring new job opportunities gave me the idea of how the company's career site could be easily improved using OpenAI's tools.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-04",
    "categories": [
      "recruitment",
      "candidate experience",
      "career site",
      "openai",
      "embeddings",
      "gpt",
      "python",
      "shiny"
    ],
    "contents": "\r\nMy recent exploration of new job opportunities has inspired me to consider what I would appreciate as a job candidate when visiting the career site of a company I‚Äôd like to work for. Specifically, I would appreciate the following flow:\r\nUploading my CV on the company‚Äôs career site.\r\nReceiving a list of jobs sorted by the degree of match to my CV.\r\nObtaining an overview of my major mis/matches for the selected job.\r\nHaving the option to ask specific questions about the job (e.g.¬†What could be the biggest challenge for me?)\r\nNot/Applying for the job after considering the provided information.\r\nIt‚Äôs clear that a process like this would make life easier not only for job candidates but also for companies, as more relevant candidates would likely apply for posted vacancies on average (as far as people are looking for similar jobs they have done in the past).\r\nI tested the feasibility of this idea by creating a functional POC career site, with OpenAI tools working behind the scenes, that supports this exact flow for 20 people-analytics-related job ads taken from One Model‚Äôs website with open roles in the people analytics space (by the way, kudos for that, One Model team üëè). You can try it for yourself with your own or sample CV on this webpage.\r\n\r\nLet me know in the comments what you think about the flow and/or how you would improve it to make it even more useful for job candidates and companies.\r\nP.S. It was also a good opportunity to try out Shiny for Python by Posit. Being a regular user of the R version of Shiny, I may be a little bit biased, but IMO it‚Äôs more user-friendly compared to Dash, so if you need to present the results of your Python code in an interactive web application, definitely give it a try.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-04-cv-job-match-career-site/./search.jpg",
    "last_modified": "2023-09-16T13:24:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/",
    "title": "GPT-4's performance in the knowledge test of evidence-based HRM practices",
    "description": "How did GPT-4 perform in the knowledge test of evidence-based HRM practices? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-02",
    "categories": [
      "gpt",
      "ai",
      "evidence-based management",
      "hr management",
      "people management",
      "hr practices"
    ],
    "contents": "\r\nSome time ago, I ‚Äúreplicated‚Äù Rynes, Colbert, and Brown‚Äôs 2002 study on HR practitioners‚Äô beliefs about effective HR practices on a convenience sample of more than 140 LinkedIn users. The results of this ‚Äúreplication‚Äù closely resembled the results of the original study. On average, respondents correctly answered 19.4 out of 35 items, achieving a 55% success rate, which was very close to the 57% average success rate in the original study (and also quite close to the 50% success rate that corresponds to random choice, given the TRUE/FALSE response format).\r\nI was curious to see how GPT-4 would perform in this test, as it had been evaluated on various standardized tests such as the SAT, GRE, Bar Exam, and AP. The prompts used had the following form: Read the following statement and indicate whether it is true or false. Keep in mind that the statement refers to general trends and patterns that apply on average but not necessarily to all cases. When evaluating the statement, ensure that you correctly interpret the words used in the statement and take into account existing scientific evidence. Give me the answer either true or false, without intermediate values, in a boolean way. Finally, briefly explain your reasoning behind your answer. The statement is as follows:‚Ä¶\r\nSo, what were the results? GPT-4 answered 29 out of 35 items right, i.e., it achieved an 83% success rate, which corresponds to the 99th and 97th percentiles in the original and ‚Äúreplicated‚Äù studies, respectively. GPT-4‚Äôs results were thus superior to majority of people who took the test.\r\n\r\nHowever, even when it gave a correct answer, it did not always rely on correct facts and/or valid reasoning, which could be a problem if management decided to act on the answers provided. See the table below to check the details of its responses.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(DT)\r\n\r\n# uploading data\r\nmydata <- readxl::read_xlsx(\"./gpt4Responses.xlsx\")\r\n\r\n# creating user-friendly table\r\nDT::datatable(\r\n  mydata %>% \r\n    dplyr::select(itemId, item, gpt4Response, correctAnswer, gpt4Reasoning, researchEvidence, possibleContingencies) %>%\r\n    dplyr::rename(\"Item ID\"=itemId, Item=item, \"GPT-4 response\"=gpt4Response, \"Correct answer\"=correctAnswer, \"GPT-4 reasoning\"=gpt4Reasoning, \"Research evidence\"=researchEvidence, \"Possible contingencies\"=possibleContingencies),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 3,\r\n    autoWidth = TRUE,\r\n    columnDefs = list(list(width = '500px', targets = c(\"Item\", \"GPT-4 reasoning\", \"Research evidence\", \"Possible contingencies\"))),\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  DT::formatStyle(1:7, 'vertical-align'='top')\r\n\r\n\r\n\r\nThe possible takeaway from this finding? Although GPT-4 can be a handy tool for exploring possible solutions to specific HR-related problems, on its own and in its current form it cannot replace the good old systematic search for and retrieval of evidence, critical evaluation of its reliability and relevance, and its weighing and synthesis as conducted and/or supervised by human experts.\r\nP.S. I didn‚Äôt test the reliability of GPT-4‚Äôs responses, nor did I set its temperature to 0, so it‚Äôs possible that you might obtain somewhat different results if you decide to replicate the test. In addition, please keep in mind that the comparison presented here is not entirely an apples-to-apples comparison, mainly due to the fact that new evidence may have emerged that does not match the correct answers in the original study conducted more than 20 years ago.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/./gpt4.jpg",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-28-employee-feedback-analysis-using-openai/",
    "title": "Employee feedback analysis using tools from OpenAI",
    "description": "How to use GPT and embeddings from OpenAI for identifying topics and related sentiments in employee feedback.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-28",
    "categories": [
      "openai",
      "gpt",
      "embeddings",
      "employee feedback",
      "employee survey",
      "topic analysis",
      "python",
      "r",
      "shiny app"
    ],
    "contents": "\r\nA while ago, I posted about the potential of using GPT for processing open-ended feedback from employees. I simply inputted a block of text into GPT and asked for a summary of the major topics found in the feedback. Although the output was quite accurate and the information compression achieved was very useful, this approach was somewhat limited in terms of scalability and granularity of the information provided.\r\nTo address these limitations, I experimented with another approach that includes the following steps:\r\nLooping over feedback from individual employees and sending them one by one to GPT.\r\nPrompting GPT to identify all present topics in each feedback, determining their respective sentiments (positive, negative, mixed, or neutral), and extracting the relevant parts of the feedback based on which the topic was identified.\r\nPrompting GPT to categorize identified topics using a provided list of topic categories (e.g., compensation and benefits, work-life balance, collaboration and teamwork, etc.), while taking into account contextual information in the relevant parts of the feedback. Alternatively, categorizing by matching embeddings of identified topics, contextual information, and topic categories.\r\nPlotting the topic categories by the number of their occurrences and type of associated sentiment.\r\nInteractive exploration of specific topics clustered by their semantic similarity based on their respective embeddings and visualized with the help of t-SNE dimensionality reduction technique.\r\nCreating a filterable table with identified topics and all original and extracted information that may be useful for further exploration of the feedback and for checking the precision of the topic identification.\r\nI had to experiment a bit with the prompts and include some data-munging inter-steps to get useful outputs, however, it now works relatively smoothly and provides pretty good results. To test the plausibility of this approach, I tried it on publicly available feedback from more than 300 current and former employees of an unnamed company published on Glassdoor and shared on Kaggle. You can check the results of the analysis yourself in this simple dashboard.\r\n\r\nIn my opinion, it works quite well and could represent a very time- and cost-effective way to gain useful insights from employee open-ended feedback at scale, with the caveat that one has to ensure the security of the processed data, for example, by using a local LLM. Let me know what you think about this approach. And if you are interested in the Python code behind the analysis so you can play with it on your own data, here‚Äôs the link to the GitHub page with the Python code.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-28-employee-feedback-analysis-using-openai/./Screenshot.png",
    "last_modified": "2024-01-02T12:41:31+01:00",
    "input_file": {},
    "preview_width": 1520,
    "preview_height": 895
  },
  {
    "path": "posts/2023-04-24-glassdoor/",
    "title": "When flawed statistical & causal reasoning leads to a valid conclusion anyway",
    "description": "Comparison of Glassdoor ratings from current and former employees.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-24",
    "categories": [
      "glassdoor",
      "employee experience",
      "employee satisfaction",
      "employee turnover"
    ],
    "contents": "\r\nOne simple lesson from the observation that former employees tend to rate their employers more harshly on Glassdoor compared to current employees: Strive to retain your employees, and you‚Äôll likely have a more satisfied workforce and better Glassdoor ratings üòÅ\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data (link to the original dataset: https://www.kaggle.com/datasets/davidgauthier/glassdoor-job-reviews/code)\r\n# data <- readr::read_csv(\"./glassdoor_reviews.csv\")\r\n# \r\n# # preparing data\r\n# mydata <- data %>%\r\n#   # selecting relevant vars\r\n#   dplyr::select(firm, current, overall_rating, work_life_balance, culture_values, career_opp, comp_benefits, senior_mgmt) %>%\r\n#   # keeping companies with at least 300 records\r\n#   dplyr::group_by(firm) %>%\r\n#   dplyr::mutate(n = n()) %>%\r\n#   dplyr::ungroup() %>%\r\n#   dplyr::filter(n >= 500) %>%\r\n#   dplyr::select(-n) %>%\r\n#   # renaming employee status and keeping only current and former employees\r\n#   dplyr::mutate(\r\n#     status = tolower(current),\r\n#     status = case_when(\r\n#       stringr::str_detect(status, \"\\\\bcurrent\\\\b\") ~ \"Current employee\",\r\n#       stringr::str_detect(status, \"\\\\bformer\\\\b\") ~ \"Former employee\",\r\n#       TRUE ~ \"Unknown\"\r\n#     )\r\n#     ) %>%\r\n#   dplyr::filter(status != \"Unknown\") %>%\r\n#   dplyr::select(-current) %>%\r\n#   # changing wide format to long one\r\n#   tidyr::pivot_longer(overall_rating:senior_mgmt, names_to = \"rating_dimension\", values_to = \"value\") %>%\r\n#   # removing missing values\r\n#   dplyr::filter(!is.na(value)) %>%\r\n#   # renaming rating dimensions\r\n#   dplyr::mutate(rating_dimension = case_when(\r\n#     rating_dimension == \"overall_rating\" ~ \"Overall rating\",\r\n#     rating_dimension == \"work_life_balance\" ~ \"Work-life balance\",\r\n#     rating_dimension == \"culture_values\" ~ \"Culture values\",\r\n#     rating_dimension == \"career_opp\" ~ \"Career opportunities\",\r\n#     rating_dimension == \"comp_benefits\" ~ \"Compensation & benefits\",\r\n#     rating_dimension == \"senior_mgmt\" ~ \"Senior management\",\r\n#     TRUE ~ \"Unknown\"\r\n#   )\r\n#   ) %>%\r\n#   # removing unknown rating dimensions\r\n#   dplyr::filter(rating_dimension != \"Unknown\")\r\n\r\n# to save space in my GitHub repo, I will upload already filtered dataset saved as .RDS file\r\nmydata <- readRDS(\"./glassdoor_reviews_filtered.RDS\")\r\n\r\n\r\n# dataviz \r\n# computing weighted average probability of a given rating for companies in the sample\r\nvizData <- mydata %>%\r\n  dplyr::group_by(firm, status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    n = n()\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::group_by(firm, status, rating_dimension) %>%\r\n  dplyr::mutate(nAll = sum(n)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    prop = n/nAll,\r\n    wprop = prop*nAll\r\n    ) %>%\r\n  dplyr::group_by(status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    wpropsum = sum(wprop),\r\n    w = sum(nAll)\r\n    ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(fprop = wpropsum/w)\r\n\r\n# chart\r\nvizData %>%\r\n  dplyr::mutate(rating_dimension = factor(rating_dimension, levels = c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\"))) %>%\r\n  ggplot2::ggplot(aes(x = value, y = fprop, fill = forcats::fct_rev(status))) +\r\n  ggplot2::geom_bar(stat = \"identity\", position=\"dodge\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_fill_manual(values = c(\"Current employee\" = \"#4E79A7\", \"Former employee\" = \"gray\")) +\r\n  ggplot2::facet_wrap(~rating_dimension, ncol = 3, scales = \"fixed\") +\r\n  ggplot2::labs(\r\n    title = \"<span style='font-size:22pt;font-weight:bold;'>**Comparison of Glassdoor ratings from** \r\n    <span style='color:#4E79A7;'>**current**<\/span> **and**\r\n    <span style='color:#999696;'>**former employees**<\/span>\r\n    <\/span>\",\r\n    caption = \"\\nBased on a sample of ratings from 792,390 individuals across 165 companies with more than 500 records each.\\nThe values represent the weighted average probability of a given rating for companies in the sample.\",\r\n    x = \"RATING\",\r\n    y = \"PROBABILITY OF RATING\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nOn a more serious note, it may be quite interesting and potentially useful to examine the order of estimated differences in specific areas between current and former employees as it may provide some insights on which areas to focus on when trying to retain employees within the company. We can use a multilevel ordered regression analysis on a random sample of 300 ratings per company for this purpose.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(ordinal)\r\nlibrary(broom.mixed)\r\nlibrary(parameters)\r\n\r\n# modeling responses using multilevel ordered regression analysis\r\nfits <- data.frame()\r\n# looping over individual dimensions\r\nfor(scale in c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\")){\r\n  #print(scale)\r\n  set.seed(1234)\r\n  model <- ordinal::clmm(\"value ~ status + (1 | firm)\", data = mydata %>% dplyr::filter(rating_dimension == scale) %>% dplyr::mutate(value = factor(value,ordered = TRUE)) %>% dplyr::group_by(firm) %>% dplyr::sample_n(300) %>% dplyr::ungroup())\r\n  #summary(model)\r\n  \r\n  # extracting information about fitted models\r\n  supp <- broom.mixed::tidy(model) %>%\r\n    dplyr::filter(term == \"statusFormer employee\") %>%\r\n    dplyr::bind_cols(parameters::ci(model) %>% filter(Parameter == \"statusFormer employee\") %>% select(CI_low, CI_high)) %>%\r\n    dplyr::select(-coef.type) %>%\r\n    dplyr::mutate(scale = scale) %>%\r\n    dplyr::select(scale, everything())\r\n  \r\n  fits <- dplyr::bind_rows(fits, supp)\r\n  \r\n}\r\n\r\nfits %>%\r\n  arrange(estimate)\r\n\r\n                    scale                  term   estimate  std.error\r\n1          Culture values statusFormer employee -0.6750302 0.01712937\r\n2          Overall rating statusFormer employee -0.6373467 0.01707703\r\n3       Senior management statusFormer employee -0.6314480 0.01693096\r\n4    Career opportunities statusFormer employee -0.5943039 0.01693091\r\n5       Work-life balance statusFormer employee -0.5263801 0.01689702\r\n6 Compensation & benefits statusFormer employee -0.3574820 0.01691018\r\n  statistic       p.value     CI_low    CI_high\r\n1 -39.40777  0.000000e+00 -0.7086032 -0.6414573\r\n2 -37.32187 7.251441e-305 -0.6708171 -0.6038763\r\n3 -37.29547 1.943268e-304 -0.6646320 -0.5982639\r\n4 -35.10171 6.347105e-270 -0.6274878 -0.5611199\r\n5 -31.15224 4.729748e-213 -0.5594977 -0.4932626\r\n6 -21.14005  3.407214e-99 -0.3906253 -0.3243386\r\n\r\nCaveat: As the title of this post implies, readers should be aware that numerous biases can distort the portrayal of employee experiences reflected in Glassdoor ratings. Some of the most significant biases include survivorship bias, social desirability, non-response bias, self-selection, and motivated reasoning.\r\nDr.¬†Paul De Young‚Äôs personal experience in this regard is quite telling: ‚ÄúThere is often a high preponderance of phony ratings among so-called current employees on Glassdoor. Beware of bogus ‚Äúpart time‚Äù current employees giving high ratings, especially if the company does not employ a lot of part-time employees.Also, I learned from an HR executive that if you want to get ratings up on Glassdoor, encourage ALL your employees to rate the company. Most often it is the mistreated employees who post because this is an outlet for their misfortune. By getting more employees to rate, chances are your ratings will increase. Watch for actively monitored employers on Glassdoor. You can usually tell a bogus rating because there is a high rating with very few comments in jobs that do not exist. The first thing I do when looking at a company is to filter out the part time employees and look at the impact on the overall scores. If they jump down, you have to wonder about the validity of the ratings. Read the comments, they are more telling. There are all kinds of ways to game the system. Glassdoor is helpful, but doesn‚Äôt always give you a valid picture without looking at the details, which is where the devil lives.‚Äù\r\nHowever, it doesn‚Äôt mean that there is no signal in Glassdoor ratings. For example, behavioral scientists at Culture Amp investigated the relationship between Glassdoor ratings and employee engagement data collected by Culture Amp. The findings suggested a strong correlation between employee engagement and Glassdoor ratings, particularly as the number of reviews increases (r = 0.69 for 100+ reviews). Companies with higher engagement scores tended to have better Glassdoor ratings, including higher CEO approval percentages and a greater likelihood of being recommended as a workplace. The study also identified the five factors with the largest relationship to Glassdoor scores, which included Learning and Development, Service and Quality Focus, Decision-making, Leadership, and Collaboration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-24-glassdoor/./glassdoor.png",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {},
    "preview_width": 768,
    "preview_height": 595
  },
  {
    "path": "posts/2023-04-18-multilevel-correlation/",
    "title": "In need of multilevel correlations?",
    "description": "A post about a great R package to reach for when you need to calculate correlations on nested data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-18",
    "categories": [
      "correlation",
      "multilevel analysis",
      "hierarchical analysis",
      "r"
    ],
    "contents": "\r\nI am currently in the middle of a project where I am working with nested data and need to report multilevel correlations.\r\nTo my surprise, for quite a long time I couldn‚Äôt find any libraries in the R or Python ecosystems that provided an easy-to-use implementation of this type of analysis. I thought I would have to code it from scratch using Stan or JAGS.\r\nFortunately, I discovered a fantastic correlation package (part of the easystats universe) that can compute various types of correlations, including multilevel correlations, partial correlations, Bayesian correlations, polychoric correlations, biweight correlations, distance correlations, and more.\r\nSince nested data is (almost) everywhere, consider trying this package as it can make your life as an analyst a little bit easier üòâ Check out the code below to see it in action.\r\n\r\n\r\nShow code\r\n\r\n# Uploading libraries and creating custom functions\r\nlibrary(tidyverse)\r\nlibrary(correlation)\r\nlibrary(ggsci)\r\nlibrary(MASS) \r\n\r\n# Creating simulated dataset with nested data\r\n\r\n# Setting some basic parameters of the dataset\r\nnum_teams <- 7\r\nteam_ids <- LETTERS[1:num_teams]\r\nmin_rows <- 35\r\n\r\n# Defining function to generate data for a team with specified correlation\r\ngenerate_team_data <- function(team_id, correlation, job_sat_mean, agility_maturity_mean) {\r\n  \r\n  # Creating covariance matrix\r\n  covariance <- correlation * (20 * 50)\r\n  means <- c(job_sat_mean, agility_maturity_mean)\r\n  cov_matrix <- matrix(c(100, covariance, covariance, 2500), nrow = 2)\r\n  \r\n  # Generating correlated data\r\n  data <- MASS::mvrnorm(n = min_rows, mu = means, Sigma = cov_matrix)\r\n  \r\n  # Scaling the data\r\n  data[, 1] <- scale(data[, 1],center = FALSE,scale = TRUE)\r\n  data[, 2] <- scale(data[, 2],center = FALSE,scale = TRUE)\r\n  \r\n  # Putting data into dataframe\r\n  df <- data.frame(\r\n    TeamID = team_id,\r\n    JobSatisfaction = data[, 1],\r\n    AgilityMaturity = data[, 2])\r\n  \r\n  return(df)\r\n  \r\n}\r\n\r\n# Generating random means for job satisfaction and agility maturity for each of the teams within some range\r\nset.seed(42)\r\njob_sat_means <- runif(num_teams, min = -5, max = 5)\r\nagility_maturity_means <- runif(num_teams, min = 40, max = 60)\r\n\r\n# Generating random correlations for each of the teams team within some range\r\nset.seed(421)\r\ncorrelations <- runif(num_teams, min = -0.3, max = 0.4)\r\n\r\n# Generating data for each team and store in a list\r\nset.seed(123)\r\nteam_data <- mapply(generate_team_data, team_id = team_ids, correlation = correlations, job_sat_mean = job_sat_means, agility_maturity_mean = agility_maturity_means, SIMPLIFY = FALSE)\r\n\r\n# Combining team data into a single data frame\r\nsimulated_data <- do.call(rbind, team_data)\r\n\r\n# Computing multilevel Bayesian Pearson  correlation analysis\r\nc <- correlation::correlation(\r\n  simulated_data,\r\n  method = \"pearson\", \r\n  multilevel = TRUE, \r\n  bayesian = TRUE\r\n)\r\n\r\n# Extracting results of the analysis to be included in the chart defined below\r\nPearson_r = c[1,3]\r\nCI95L = c[1,4]\r\nCI95H = c[1,5]\r\n\r\n# Plotting the chart\r\nggplot2::ggplot(simulated_data, aes(y = JobSatisfaction, x = AgilityMaturity, color = TeamID)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE, size = 1.5) + \r\n  ggsci::scale_color_tron() +\r\n  ggplot2::labs(\r\n    y = \"JOB SATISFACTION\",\r\n    x = \"PERCEIVED ORGANIZATIONAL AGILITY MATURITY\",\r\n    title = \"Is organizational agility related to job satisfaction?\",\r\n    subtitle = stringr::str_glue(\"Bayesian Pearson r = {round(Pearson_r,2)}; 95% CrI: [{round(CI95L,2)}, {round(CI95H,2)}]\")\r\n  ) +\r\n \r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-18-multilevel-correlation/./plot.png",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2023-04-17-time-management/",
    "title": "Consequences of time management in the workplace",
    "description": "Some interesting insights from a meta-analytic review of the consequences of time management behaviors in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-17",
    "categories": [
      "time management",
      "meta-analysis",
      "job satisfaction",
      "job performance",
      "stress",
      "burnout"
    ],
    "contents": "\r\nBedi & Sass (2022) conducted a meta-analytic review of the consequences of employee time management behaviors on several employee outcomes. What are the main insights?\r\nIt may not come as a big surprise, but it is encouraging that data support the association between time management and various beneficial employee outcomes, such as increased job satisfaction, job performance, and lower levels of stress and burnout. Unfortunately, the ‚Äúproven‚Äù association is not causal, as the majority of studies were cross-sectional. In fact, there are not many studies on the causal effects of time management. The exception to this is procrastination, for which there is evidence that time management can help - see, for example, the meta-analysis by Van Eerde & Klingsieck (2018) on this topic.\r\nThe relationship between time management and employee outcomes is not only direct but also partially mediated by work-family conflict. This finding underscores the importance of work-life balance and highlights the need for organizations to help employees better address this specific issue, as it may positively affect a variety of employee outcomes.\r\n\r\nPerceived control over time, achieved through the use of time management, shows incremental validity in predicting job satisfaction, job performance, and stress with respect to the personality trait of conscientiousness. This suggests that regardless of an individual‚Äôs innate level of prudence, they may benefit from adopting time management in their professional lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-17-time-management/./tm.jpg",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-13-interpretable-ml/",
    "title": "Interpretable machine learning with modelStudio",
    "description": "There's a new kid on the block in the R ecosystem that can help analysts understand the behavior of their ML models.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-13",
    "categories": [
      "interpretability",
      "explainability",
      "machine learning",
      "predictive models",
      "r"
    ],
    "contents": "\r\nThere‚Äôs a great new R package, modelStudio, that makes it much easier for analysts to create both global and local interpretations of predictive models using an interactive interface.\r\nOnce you‚Äôve trained the model, you just get the DALEX explainer object ready and start up modelStudio that will run the following analyses (among others) and show the corresponding plots:\r\nFeature Importance: A visual representation that ranks and displays the significance of each input variable in a predictive model, helping to identify the most influential features for model predictions.\r\nPartial Dependence: A visualization that shows the relationship between a feature and the predicted outcome while averaging out the effects of all other features, to understand the marginal impact of a specific feature on the model‚Äôs predictions.\r\nAccumulated Dependence: Similar to the previous method, but reducing the influence of the assumption of uncorrelated features, providing a more robust and reliable representation of the feature‚Äôs impact on the model‚Äôs predictions.\r\nBreak Down Plot: A graphical explanation tool that demonstrates the contribution of each feature to a specific instance‚Äôs prediction, allowing for individual-level interpretation of model outcomes.\r\nShapley Values: A cooperative game theory-based approach for fairly attributing each feature‚Äôs contribution to a specific prediction, providing interpretable and consistent explanations for machine learning models.\r\nCeteris Paribus: A method that helps with understanding the influence of individual features on specific predictions by isolating the effect of a single variable while holding all other variables constant.\r\nIf you use ML in HR or any other field where it‚Äôs crucial to explain why you make specific predictions, classifications, and the resulting recommendations or decisions, definitely give it a try.\r\nWhat follows is a short demonstration of the tool using the well-known artificial IBM attrition dataset. First, let‚Äôs import the attrition dataset from the modeldata library and change the coding of the criterion variable, which will later make it easier to set up the DALEX explainer, which requires numerical data type for criterion variable even in classification tasks.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n\r\nWe now split the data into training, test, and validation datasets so that we can tune the prediction model, fit the model, and test its performance on new, previously unseen data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\n\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\nNow let‚Äôs define the whole model training workflow, which includes the data adjustment pipeline and the specification of the model used. We will use XGBoost, presumably the best ML algorithm for tabular type of data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(recipes)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\n\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors())\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\nAlthough the XGBoost algorithm works quite well with the default hyper-parameters, we will use the validation dataset to tune some of its hyper-parameters to get the best performance out of it. As can be seen below, after tuning the best model performs quite well in terms of AUC, which has a value of around 0.8.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n    )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n# A tibble: 1 √ó 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.802\r\n\r\nShow code\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n\r\nNow we can set up the final model training workflow and fit the model to the entire training dataset and check its performance on out-of-sample data using k-fold cross-validation and testing dataset. As we see below, in both cases the model performance as measured by AUC is around the value of 0.8.\r\n\r\n\r\nShow code\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  fit(data_train)\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# A tibble: 1 √ó 6\r\n  .metric .estimator  mean     n std_err .config             \r\n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 roc_auc binary     0.799    10  0.0166 Preprocessor1_Model1\r\n\r\nShow code\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n  yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# A tibble: 1 √ó 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.871\r\n\r\nNow we‚Äôre ready to explore the inner workings of our trained model. After setting up the explainer from the DALEX package, we simply insert this object into the modelStudio function and run it. We then get an interactive interface in our browser that we can use to easily check what our model is doing to make its predictions. You can try it out for yourself using the interactive interface below.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(DALEX)\r\nlibrary(modelStudio)\r\n\r\n# setting up the DALEX explainer object\r\n# creating predict function\r\npred <- function(model, newdata)  {\r\n  results <- (predict(model, newdata, type = \"prob\")[[1]])\r\n  return(results)\r\n}\r\n\r\nexplainer <- DALEX::explain(\r\n  model = xgb_fit,\r\n  data = data_test %>% mutate(Attrition = as.integer(Attrition)),\r\n  y = data_test %>% mutate(Attrition = as.integer(Attrition)) %>% pull(Attrition),\r\n  predict_function = pred,\r\n  type = \"classification\",\r\n  verbose = FALSE \r\n)\r\n\r\n# running ModelStudio\r\nmodelStudio::modelStudio(\r\n  explainer,\r\n  max_features = 100, # Maximum number of features to be included in BD, SV, and FI plots\r\n  N = 300, # Number of observations used for the calculation of PD and AD\r\n  new_observation_n = 3, #Number of observations to be taken from the data\r\n  show_info = TRUE,\r\n  viewer = \"browser\",\r\n  facet_dim = c(2,2) # layout of the resultiing charts\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-13-interpretable-ml/./miracle.jpg",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-openai-personality-interpretation/",
    "title": "Ask your personality using GPT",
    "description": "Can Generative AI like GPT meaningfully interpret personality profiles?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-11",
    "categories": [
      "gpt",
      "ai",
      "personality",
      "self-awareness",
      "hogan personality inventory",
      "shiny app",
      "r"
    ],
    "contents": "\r\nOne of my friends recently asked me if I could provide him with an interpretation of a free personality test he took on the internet. As a joke, I asked him if he had already tried using GPT for this.\r\nThis sparked my interest in how GPT would actually handle this kind of task. So, I provided it with my Hogan Personality Inventory (HPI) profile (in percentile scores, as requested), and to my surprise, it performed quite well - even when asked about more complex questions like interactions between my scores on selected scales or my strengths and weaknesses for specific jobs and tasks.\r\nBased on this experience, I created a simple POC app where users can input their HPI profile and some contextual information, such as their current or desired job, and ask GPT predefined or their own questions about their personality.\r\nLink to the app: https://aanalytics.shinyapps.io/ask_your_personality/\r\n\r\nI‚Äôve only tested it on a few profiles, so if you know your HPI profile (or your Big Five traits that are behind HPI), I would be happy to hear how you perceive the face validity and potential usefulness of the generated interpretations. Perhaps I have just become a victim of the well-known Barnum effect üòâ\r\nI am well aware that there are clear risks associated with using a generic GPT for such a task. However, I believe that with proper fine-tuning, an explicit disclaimer, and access to a qualified professional for possible consultation, it could be a useful tool for helping people gain self-awareness more easily - in many situations a crucial prerequisite for high-quality decisions. What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-openai-personality-interpretation/./introPic2.jfif",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-engagement-interventions/",
    "title": "Effectiveness of interventions for encreasing employee engagement",
    "description": "What evidence do we have for the effectiveness of interventions for increasing employee engagement? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-27",
    "categories": [
      "employee engagement",
      "work engagement",
      "interventions",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is some evidence that engagement of employees has positive causal impact on the bottom line of organizations (see, for example, the meta-analysis by Harter et al.¬†(2010), however, be aware of the specific definition of engagement used there, which focuses more on the contextual factors and conditions enabling engagement and less on the psychological states of engagement). Consequently, we might be naturally interested in whether we can positively influence engagement of employees through the use of certain interventions.\r\nBased on a systematic review and meta-analysis of studies with controlled workplace interventions by Knight, Patterson, and Dawson (2017), it seems the answer might be yes. The authors found a small positive effect on work engagement and each of its three sub-components: vigor, dedication, and absorption, as measured by the Utrecht Work Engagement Scale (UWES) from Bakker and Schaufeli.\r\nWhen it comes to the types of intervention (personal resource building, job resource building, leadership training, and health promotion), a moderator analysis did not find evidence for their differing effectiveness. However, there was evidence for a medium to strong effect of intervention style in favor of group interventions (vs.¬†individual), with the possible explanation being that group interventions effectively influence certain work engagement antecedents, such as social support and influence in decision-making.\r\n\r\nRegarding the sustainability of effects, there was no significant effect of time in the case of overall work engagement. However, for the vigor sub-component, there were stronger effects immediately post-intervention than at follow-up, with the opposite being true for dedication and absorption sub-components.\r\nHave you implemented any interventions to boost engagement of employees in your organization? Did you measure their effectiveness? What were your experiences and results? Feel free to share your thoughts in the comments below.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-engagement-interventions/./training.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-managers-overconfidence/",
    "title": "Where do managers put on their rose-tinted glasses the most?",
    "description": "In which areas are managers and leaders prone to overconfidence, and how can this overconfidence potentially impact team functioning? Let's check some data to address this question.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-23",
    "categories": [
      "team management",
      "blind spots",
      "rocket model",
      "team assessment survey"
    ],
    "contents": "\r\nThe ‚Äúillusory superiority‚Äù or ‚Äúbetter-than-average effect‚Äù causes individuals to overestimate their abilities compared to others. Unsurprisingly, even managers and leaders fall for this trap. After all, they‚Äôre human too, right? üòâ For a review of studies on this topic, see, for example, the systematic review by Heavey et al.¬†(2022).\r\nTo pinpoint particular areas where managers and leaders are prone to overconfidence, data from the Team Assessment Survey (TAS), a team assessment tool rooted in the Rocket Model of team performance, can be valuable as it enables comparisons between how team leaders and team members perceive their team‚Äôs functioning. For example, based on data from a Slovak sample of 85 teams with 835 members, it seems that team leaders rate their team‚Äôs effectiveness way better than team members in the following five areas in descending order:\r\nResources: Does the team have the budget, equipment, authority, and political capital it needs to accomplish its goals?\r\nTalent: Is the team sized correctly? Are team members‚Äô roles clear? Does the team have the right skills to succeed? Are people developing new skills? Do rewards encourage or discourage teamwork?\r\nContext: Are team members in agreement about the team‚Äôs political and economic realities, customers, competitors, suppliers, and key assumptions and challenges?\r\nMission: What is the team‚Äôs purpose? What are its goals? How does the team define winning? What are its strategies and plans for accomplishing its goals?\r\nCourage: Do team members trust each other? Is there an optimal level of tension and collaboration on this team? Do team members challenge each other in a constructive manner?\r\n\r\nWhen managers get overconfident in these areas, it can lead to all sorts of issues like underinvestment in critical resources, inefficient resource allocation, ignoring skill gaps, insufficient role clarity, making bad decisions, disjointed strategic planning, confusion, misaligned priorities, uncoordinated efforts, blocking innovation, etc.\r\nWhat‚Äôs the fix? Among other things, managers need to be aware of their own biases and work on open communication, feedback, and collaboration with team members. Easier said than done, but it‚Äôs crucial to avoid hurting the performance of the team.\r\nObviously, the size and demographics of the sample used are limited and conclusions are therefore difficult to generalize, but I may try to check with Dr.¬†Gordon Curphy, the author of the Rocket Model of team performance and TAS, to see if this pattern is also replicated in a larger, international sample.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-managers-overconfidence/./pinkGlasses.jpg",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-app-piloting-and-dif/",
    "title": "Estimating the impact of a new business app by piloting & method of difference-in-differences",
    "description": "What is the benefit of using the difference-in-differences method in combination with piloting a new business app, and how can this help estimate the app's effectiveness on key outcomes like time spent with prospects or closed deals?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-20",
    "categories": [
      "pilot program",
      "difference in differences",
      "data-driven decision-making"
    ],
    "contents": "\r\nWhen considering the introduction of a new business app, one of the benefits of piloting it is that it provides a great opportunity to test its causal impact on business processes or outcomes of interest. For example, in our pilots, apps are typically implemented for 3 or 6 months only in some teams and not in others. Such a situation creates ideal conditions for applying the difference-in-differences (DiD) method, which is used to approximate an experimental research design with observational data only.\r\nTo use one specific example, one of the problems addressed by our Sales Analytics app is that sales reps spend more time collaborating internally instead of communicating with prospects. The premise may be that the better visibility into time spent that the app enables will help sales reps and their managers better plan activities during their regular weekly 1-on-1 meetings, all with (hopefully) a positive impact on time spent with prospects.\r\nBy piloting the app in just one team and finding another team with a similar, parallel trend in the selected criterion, we can try to estimate its effectiveness. As shown in the attached chart, the fitted DiD model in this particular case slightly supports the effectiveness of the app, at least in terms of the amount of time spent with prospects, but can easily be switched to another criterion that is closer to the company‚Äôs bottom line, e.g., the number of closed deals. Moreover, if we are aware of certain systematic differences between the teams, such as the experience level of the sales reps, we can include relevant control variables in the model to achieve a more accurate estimation of the app‚Äôs effectiveness.\r\n\r\nSo, next time you consider introducing a new business app, consider piloting it in combination with the DiD method to better understand its impact on your organization‚Äôs goals. Happy piloting! ‚úåÔ∏è\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-app-piloting-and-dif/./impact.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-meeting-yes-men/",
    "title": "Are there meeting ‚Äúyes-men‚Äù?",
    "description": "One of our clients was struggling with meeting overload and wanted to know if the people who attend too many meetings are the kind of \"yes-men\" who just can't say no to meeting invites. You know the type - always saying \"yes\" and never protecting their precious time. What did they find?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-17",
    "categories": [
      "meeting culture",
      "meeting overload"
    ],
    "contents": "\r\nTo test this hypothesis, they looked at the monthly number of meetings people attended and the relative frequency of their responses to meeting invitations. Here‚Äôs what they found:\r\nPeople seemed to be similarly explicit in signaling their intentions about the meetings they were invited to, regardless of how many meetings they had on their plate.\r\nPeople accepted fewer meeting invites the more meetings they attended.\r\nThose who went to a bunch of meetings were more likely to say they‚Äôre not sure if they can make it or not.\r\nPeople who were busy with meetings declined more meeting invites than those who had fewer meetings.\r\n\r\nThus, contrary to initial expectations, the data showed that people who attended more meetings, on average, tended to accept fewer invites, were more likely to be unsure about their availability, and actually declined more invites than those with fewer meetings.\r\nWhile the client couldn‚Äôt rule out that there might be some individuals fitting ‚Äúyes-men‚Äù description in their company (in fact, one can easily spot a few people in the corresponding chart who attended many meetings and at the same time underutilized the option of declining the meeting invites), these results suggested that there isn‚Äôt a systematic problem in this specific area. Time for our client to explore other avenues through which the problem with meeting overload could be addressed. More on that in some of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-meeting-yes-men/./yes_man_yes.gif",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/",
    "title": "Impact of employee satisfaction at work on a company's bottom line",
    "description": "While there is evidence supporting the connection between employee satisfaction and a company's bottom line, it's essential to determine whether higher satisfaction directly causes better performance. Is there some evidence for that? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-14",
    "categories": [
      "performance",
      "employee satisfaction",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is now little doubt that employee attitudes, such as employee satisfaction or employee engagement, are to some extent related to performance - for some evidence in support of this claim, see, for example, the meta-analysis of the relationships between employee satisfaction, employee engagement and business unit-level performance conducted by Harter, Schmidt, & Hayes (2002).\r\nBut does it mean that higher employee satisfaction causes higher performance? As any statistically savvy person knows, not necessarily.\r\nFortunately, there are research designs that can help us untangle this conundrum a bit. One of these designs is path analysis of longitudinal/time-series data. This approach was also taken by Harter et al.¬†(2010) in their meta-analysis ‚ÄúCausal Impact of Employee Work Perceptions on the Bottom Line of Organizations‚Äù with the following results:\r\n‚ÄúUsing a massive longitudinal database that included 2,178 business units in 10 large organizations, we found evidence supporting the causal impact of employee perceptions on [‚Ä¶] bottom-line measures [customer loyalty, employee retention, revenue, and profit]; reverse causality of bottom-line measures on employee perceptions existed but was weaker.‚Äù\r\nThe attached figure with the two alternative path models clearly shows that the causal path from employee perceptions to outcomes is stronger than the other way around, especially when it comes to theoretically more proximal outcomes such as employee retention and customer loyalty.\r\n\r\nDespite not being free of potential biases, these results add more weight to the argument that investing in improving employee job satisfaction also makes sense for improving a company‚Äôs bottom line, rather than ‚Äúonly‚Äù improving employee well-being. The question now is what interventions to improve employee satisfaction might work. Let‚Äôs review the available evidence on this issue in one of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/./successfulEmployees.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-excel-and-python/",
    "title": "Excel + Python = Word Document",
    "description": "Using combination of Excel and Python for semi-automatic Word document generation.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-10",
    "categories": [
      "excel",
      "python",
      "document",
      "automation"
    ],
    "contents": "\r\nIn the spirit of ‚ÄúExcel isn‚Äôt dead and is actually doing well‚Äù posts, I‚Äôm sharing one practical example of combining Excel and Python to do one specific job. It‚Äôs not as exciting and sexy as ChatGPT, but it may still come in handy for someone, as it did for one of my friends.\r\nA friend of mine who works in psychological counselling has to write a large number of reports which, among other things, contain many recommendations for various compensations and interventions depending on established diagnosis.\r\nTo make his job easier, he created a simple Excel spreadsheet with a list of diagnoses and corresponding recommendations. He needed to generate a simple Word document listing and describing the appropriate compensations and interventions after he had marked the appropriate diagnoses for the client in Excel.\r\nI originally wanted to do it all in Excel, but since I‚Äôm not the best friend with VBA, I couldn‚Äôt get rid of the various text formatting issues. So I reached for Python and one of its document-related libraries and linked it via macro to Excel, which acts only as a source of input data, based on which Python generates a simple report when a button is pressed in Excel. Once generated, the document is ready for further editing and tuning.\r\nIf you are interested in technical details, you can check my GitHub page.\r\nMay the Excel be with you üôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-excel-and-python/./comboPic.png",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2023-04-11-job-attitudes-and-employee-outcomes/",
    "title": "Employee outcomes & employees' job attitudes",
    "description": "What employee outcomes are predicted by what employees' job attitudes? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-06",
    "categories": [
      "job attitudes",
      "job performance",
      "employee turnover",
      "organizational citizenship behavior"
    ],
    "contents": "\r\nIf you‚Äôve ever measured employee job attitudes (i.e.¬†constructs such as organizational commitment, procedural justice, distributive justice, job involvement, job engagement, job satisfaction, etc.), you probably won‚Äôt be surprised to learn that job attitudes are usually quite strongly correlated.\r\nThis is also the conclusion of a meta-analytic review of job attitudes by Woznyj et al.¬†(2022), which showed that job attitudes are moderately to strongly correlated with each other, with most relations falling between œÅ = .50 and .69.\r\nDespite this, relative weights and incremental validity analyses revealed that some attitudes have greater validity in predicting key employee outcomes. As shown in the table attached,\r\nperformance is most strongly predicted by job satisfaction, job engagement, and distributive justice (an employee‚Äôs perceived fairness of outcomes),\r\nturnover intentions is most strongly predicted by job satisfaction, perceived organizational support (a general evaluation regarding the extent to which employees feel their organization values their contribution and cares about their well-being), and distributive justice, and\r\norganizational citizenship behaviors is most strongly predicted by job engagement, procedural justice (perceived fairness of the means, or procedures, used to determine outcomes), and job involvement (the degree to which a person identifies psychologically with his or her work, or the importance of work in his or her total self-image).\r\n\r\nIMO, knowing this can be quite useful in planning what candidate constructs to measure in your company in an effort to support specific employee outcomes.\r\nWhat job attitudes do you regularly measure in your company? And does it pay off in any way, i.e.¬†does it have any tangible impact?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-job-attitudes-and-employee-outcomes/./employees.jpg",
    "last_modified": "2023-09-16T13:24:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-05-micro-breaks/",
    "title": "Effectiveness of micro-breaks at work",
    "description": "Quite satisfying news from a meta-analysis on the efficacy of micro-breaks for increasing well-being and performance in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "micro-break",
      "workplace",
      "well-being",
      "performance",
      "meta-analysis"
    ],
    "contents": "\r\nIf you feel you need a short break, take one - even if it is no longer than 10 minutes, it can increase your perceived well-being and some types of performance.\r\nAs the authors of the study, Albulescu et al.¬†(2022), conclude: ‚ÄúOur results revealed that micro-breaks are efficient in preserving high levels of vigor and alleviating fatigue. It seems that the effects are univocal and generalizable for the well-being outcomes. These were relatively homogeneous, and none of the included moderators were significant. Hence, the data suggest that micro-breaks may be a panacea for fostering well-being during worktime.\r\nWhen it comes to performance, the data revealed some nuances. The break duration was a significant covariate of the effect of micro-breaks: the longer the break, the better the performance. Moreover, the type of task from which participants were taking the break also emerged as a significant moderator. Micro-breaks could significantly increase performance for clerical work or creative exercises and not for a cognitively demanding task.‚Äù\r\nThere is nothing like having a meta-analysis to back up your habits, or better yet, initially bad habits üòâ\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-05-micro-breaks/./break.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-01-distracted-time/",
    "title": "Not-so-hidden cost of working in an office",
    "description": "Just a few data-backed thoughts on why many of us may often feel more distracted when working in an office.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-01",
    "categories": [
      "remote work",
      "in-office work",
      "collaboration",
      "focus time",
      "distraction"
    ],
    "contents": "\r\nWe have only recently started to measure the allocation of work time to collaborative and non-collaborative activities at Time is Ltd..\r\n\r\nWhen checking the resulting numbers in the context of where people work from, it wasn‚Äôt that much of a surprise that people spend less time (on average ~37 minutes less per person per day) on collaborative activities when working remotely vs.¬†in the office. Such a pattern is probably a good thing in many cases as it means that people have more time for focused work when working remotely, and use their time for intensive collaboration when in the office.\r\nWhat was quite surprising to me, however, was the rather large difference in the amount of distracted time. When working in the office there was much more distracted time, i.e.¬†time when people were working on their tasks while being distracted by various collaborative activities. Therefore, they didn‚Äôt have enough time to get into the flow. On average, the difference was a staggering ~69 minutes per person per day.\r\nHowever, when I thought about it a bit more (and also after I realized how we actually calculate distraction timeÔ∏è), it started to make more sense to me. After all, collaborative activities don‚Äôt just ‚Äúrob‚Äù us of time per se, they also fragment our available time into smaller chunks, which carries a cost in the form of cognitive overhead associated with task switching.\r\nI suppose it‚Äôs a trade-off that can‚Äôt be completely solved in principle, but can only be mitigated with tools like batching or timeboxing. What is your own experience with this phenomenon and what tools do you use to deal with it?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-01-distracted-time/./distraction.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-chatgpt-and-employee-feedback/",
    "title": "Using ChatGPT to summarize and explore employee feedback?",
    "description": "What's the potential use of tools like ChatGPT in analyzing open-ended feedback from employee engagement and satisfaction surveys? Let's take a look at the result of my little experiment in this area.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "chatgpt",
      "ai",
      "hrm",
      "people analytics",
      "employee engagement",
      "surveys"
    ],
    "contents": "\r\nIMHO, ChatGPT and similar tools may be quite useful in the near future (and probably to some extent even today) to quickly summarize and explore the feedback provided by employees through open-ended questions in employee engagement and satisfaction surveys.\r\nI just experimented with a small sample of anonymized employee feedback (just dozens of lines of text from a question about what employees would change in the company) and asked ChatGPT to summarize the feedback, identify the main topics covered in the feedback, and provide me with details about the selected topics - see the attached image of the conversation for illustration.\r\n\r\nJust based on my very limited sample, I found that there was a pretty good balance between information compression and accuracy, while the interaction was very natural, similar to asking HRBP what the results of the last ESS were for my department/team. Certainly, there are still too many known and unknown risks associated with these tools to rely blindly on them alone, but I can imagine that in the foreseeable future, when many of these risks are successfully mitigated, this will be one of the ways managers will listen to the voice of their people.\r\nHas anyone experimented with ChatGPT on similar kinds of HR data?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-chatgpt-and-employee-feedback/./chatgpt_listening.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-large-and-recurring-meetings/",
    "title": "Where to look first when considering meeting reset?",
    "description": "Let's briefly discuss the potential benefits of focusing on optimizing large recurring meetings to save time in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-19",
    "categories": [
      "meeting overload",
      "meeting culture",
      "meeting reset"
    ],
    "contents": "\r\nWhen thinking about the meeting reset like they did in Asana, IMHO, it‚Äôs not a bad idea to focus on one specific category of meetings first.\r\nI am thinking of large recurring meetings that combine two big time wasters, and thus hide a greater opportunity for time savings:\r\nLarge meetings are quite often a waste of people‚Äôs time as they don‚Äôt allow everyone to meaningfully contribute. These meetings often serve only to disseminate information and can therefore be safely replaced by less intrusive asynchronous collaboration tools such as email, instant messaging or some kind of knowledge management tool.\r\nRecurring meetings have their place, but often tend to outlive their purpose over time and waste the time of everyone involved. One should therefore regularly ask oneself the following questions in this context and act accordingly: Are meetings being planned automatically, rather than out of necessity? Are attendees invited as a formality, or will they bring value? Is there still clear agenda for these meetings? Are we already sufficiently aligned or do we need some additional platform for doing so? Does the current frequency of our meetings meet our needs? etc.\r\nAs illustrated in the attached chart, large and recurring meetings can represent a relatively large chunk of time in an employee‚Äôs work month. Large meetings here account for approximately 37% of all meeting time and more than 80% of large meetings are recurring. This represents a relatively large room for optimizing the time spent in meetings.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-large-and-recurring-meetings/./fatigue.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-hrm-value-chain-and-sem/",
    "title": "HRM value chain and structural equation modeling - Moneyball case",
    "description": "What's the link between the HRM value chain and structural equation modeling? Let‚Äôs check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "hrm value chain",
      "people analytics",
      "path analysis",
      "structural equation modeling",
      "bayesian statistics",
      "r"
    ],
    "contents": "\r\nAs the economy cooled, more posts started appearing in my social\r\nbubble about how people analytics can prove its value, justify its\r\nexistence, and increase its resilience to layoffs.\r\nOne of the often-mentioned conceptual tools in this context is the\r\nHRM value chain which shows how people processes lead\r\nto achieving companies‚Äô desired business outcomes. There are more\r\nversions of this tool, e.g.¬†one from Jaap\r\nPaauwe and Ray Richardson or another one from Max\r\nBlumberg, but what they have in common is a kind of causal flow from\r\nHRM activities to HRM outcomes to business outcomes.\r\nExample of the HRM value chain\r\nmodel from Paauwe\r\n& Richardson (1997).\r\n\r\nExample of the HRM value chain\r\nmodel from Blumberg\r\n(2018).\r\nHowever, as potentially useful as this metaphor of the organization\r\nas a kind of ‚Äúmachine‚Äù with certain inputs, processes, and outputs is,\r\nit is still only a conceptual tool that may or may not correspond to the\r\nreality of a particular organization.\r\nIn this regard, it may help if we try to operationalize this\r\nmetaphor. In this effort, structural\r\nequation modeling can be very handy, as it allows us to\r\nformalize our ideas about the relationships between several different\r\nvariables and to assess the extent to which these ideas are consistent\r\nwith the available data. After all, no one wants to make decisions based\r\non false assumptions.\r\nTo illustrate this with a more tangible example, let‚Äôs use sabermetric data\r\nfrom the famous Moneyball\r\ncase and let‚Äôs try to formally model the Oakland A‚Äôs\r\n(OAK) as a ‚Äúmachine‚Äù that produces playoffs by trying to win more games\r\nor score more points than opposing teams, using inputs in the form of\r\nplayers‚Äô ability to play well at bat and in the field.\r\nBased on our expert knowledge of the game of baseball, our working\r\nhypotheses, and the results of some previous analyses, we can construct\r\na conceptual model of how a baseball team functions, as outlined\r\nbelow.\r\nConceptual model of how a\r\nbaseball team works.\r\nThe diagram shows clearly how this ‚Äúmachine‚Äù works: Its outputs are\r\nqualifications for the playoffs, which it achieves by trying to win more\r\ngames or score more points than the opposing teams; to do this, it uses\r\ninputs in the form of the players‚Äô ability to play well at bat and in\r\nthe field; the inputs affecting the ‚Äúmachine‚Äôs‚Äù operation are also the\r\nsimilar abilities of the opposing teams‚Äô players. This is, of course, a\r\nvery simplistic causal model of how the OAK team operates, but as the\r\nfamous statistical aphorism states, all models are\r\nwrong, but some are useful.\r\nHowever incomplete our models of how an organization works will\r\nalways be, it is essential to check that these models sufficiently\r\nreflect reality as conveyed by the available data. Only after such an\r\nassessment of the plausibility of the model it is reasonable to base\r\nfurther, e.g.¬†hiring or L&D decisions on it. And, as hinted at the\r\nbeginning of this post, we will use the statistical method of structural\r\nequation modeling to do this. So let‚Äôs apply this method to our model of\r\nthe OAK and test its plausibility.\r\nWe will use data from a publicly available database\r\nof MLB historical statistics. Specifically, we will use the\r\nfollowing variables:\r\nqualification for the playoffs in a given season\r\n(Playoffs),\r\nnumber of wins in a given season (W),\r\nnumber of points won in a given season (RS),\r\nnumber of points lost in a given season (RA),\r\naverage frequency with which a player reaches base per plate\r\nappearance in a given season (OBP - On-Base Percentage) and\r\nanalogous statistics for opponent teams (OOBP),\r\naverage number of bases players earn per at bat in a given season\r\n(SLG - Slugging Percentage) and analogous statistics for\r\nopponent teams (OSLG).\r\nIn terms of time, we will work with data from the years 1996-2001,\r\nwhich precede 2002, when the story of Moneyball mostly takes place.\r\n\r\n\r\nShow code\r\n\r\n# uploading set of libraries for data manipulation and visualization\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\nbaseball <- readr::read_csv(\"./baseball.csv\")\r\n\r\n# filtering data used for the analysis\r\nmyData <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995\r\n    ) %>%\r\n  dplyr::select(Team, OBP, SLG, OOBP, OSLG, RS, RA, W, Playoffs)\r\n\r\n# user-friendly table with data used for the analysis\r\nDT::datatable(\r\n  myData,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\nNow let‚Äôs define a formal model of the OAK and fit it to the data\r\nusing the brms R\r\npackage which allows us to make inferences about the model\r\nparameters within a Bayesian\r\ninferential framework. Specifically, we will build and fit a\r\nso-called path\r\nanalysis model, which is a special type of structural equation\r\nmodeling used to describe directed dependencies among a set of\r\nvariables. Given that we have data for a group of the same teams over\r\nseveral seasons, we must also incorporate the hierarchical nature of the\r\ndata into the model.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for fitting Bayesian generalized (non-)linear multivariate multilevel models using Stan for full Bayesian inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# specifying individual parts of the SEM (without modeling the correlation between response variables and using Student's t distribution instead of the Gaussian distribution to make the model more robust) \r\na <- brms::bf(W ~ 1 + RS + RA + (1 + RS + RA | Team), family = student())\r\nb <- brms::bf(RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team), family = student())\r\nc <- brms::bf(RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team), family = student())\r\nd <- brms::bf(Playoffs ~ 1 + W + (1 + W | Team), family = bernoulli())\r\n\r\n# fitting the model\r\nfit <- brms::brm(\r\n  a + b + c + d + set_rescor(FALSE), \r\n  data = myData,\r\n  iter = 3000,\r\n  chains = 3,\r\n  warmup = 500,\r\n  thin = 1,\r\n  seed = 123,\r\n  cores = 5,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n  )\r\n\r\n# checking MCMC convergence\r\n# plot(fit)\r\n# summary(fit)\r\n\r\n\r\n\r\nAfter verifying that the mechanics of the MCMC algorithm work well\r\n(not shown here for brevity reasons), we should also verify how well the\r\nmodel fits the data by using the posterior predictive check for each of\r\nthe observed response variables in our model.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for arbitrarily complex composition of ggplot plots\r\nlibrary(patchwork)\r\n\r\n# posterior predictive check for all predicted observed variables \r\nplayoffsPpc <- brms::pp_check(fit, resp = \"Playoffs\", ndraws = 100) + ggplot2::labs(title = \"Playoffs variable\")\r\nwPpc <- brms::pp_check(fit, resp = \"W\", ndraws = 100) + ggplot2::labs(title = \"W variable\")\r\nrsPpc <- brms::pp_check(fit, resp = \"RS\", ndraws = 100) + ggplot2::labs(title = \"RS variable\")\r\nraPpc <- brms::pp_check(fit, resp = \"RA\", ndraws = 100) + ggplot2::labs(title = \"RA variable\")\r\n\r\nppc <- (playoffsPpc + wPpc) / (rsPpc + raPpc)\r\n\r\nprint(ppc)\r\n\r\n\r\n\r\n\r\nAs you can see, the model fits the data pretty well. Now we can look\r\nat the coefficients in the individual parts of the model. All of them\r\nshow non-zero values and are in directions that are consistent with our\r\nexpectations embodied in our conceptual model, i.e., all are positive\r\nexcept for the coefficient of the RA variable (number of points\r\nlost) as a predictor of the number of games won (W). These\r\nresults thus give us greater confidence in following our conceptual\r\nmodel of team functioning when making decisions about allocating our\r\nlimited resources.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting functions for posterior analysis, MCMC diagnostics, prior and posterior predictive checks, and other visualizations to support the applied Bayesian workflow\r\nlibrary(bayesplot)\r\n\r\n# getting overview of all parameters \r\n# get_variables(fit)\r\n# relevant parameters: \"b_RS_OBP\", \"b_RS_SLG\", \"b_RA_OOBP\", \"b_RA_OSLG\", \"b_W_RS\", \"b_W_RA\", \"b_Playoffs_W\"\r\n\r\nb_RS_OBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_OBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OBP variable in the RS ~ OBP part\"\r\n    ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n    )\r\n\r\n\r\nb_RS_SLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_SLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"SLG variable in the RS ~ SLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OOBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OOBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OOBP variable in the RA ~ OOBP part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OSLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OSLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OSLG variable in the RA ~ OSLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RS <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RS\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RS variable in the W ~ RS part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RA <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RA\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RA variable in the W ~ RA part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_Playoffs_W <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_Playoffs_W\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"W variable in the Playoffs ~ W part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\ncoefCharts <- (b_RS_OBP + b_RS_SLG + b_RA_OOBP) / (b_RA_OSLG + b_W_RS + b_W_RA) / (b_Playoffs_W + patchwork::plot_spacer() + patchwork::plot_spacer()) +\r\n  plot_annotation(\r\n    title = 'Posterior interval estimations',\r\n    caption = \"The solid vertical lines represent the point (median) estimates and the shaded areas represent the 95% credible interval.\",\r\n    theme = theme(\r\n      plot.title = element_text(size = 18),\r\n      plot.caption = element_text(size = 11)\r\n      )\r\n    )\r\n\r\nprint(coefCharts)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model  \r\nsummary(fit)\r\n\r\n\r\n Family: MV(student, student, student, bernoulli) \r\n  Links: mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = logit \r\nFormula: W ~ 1 + RS + RA + (1 + RS + RA | Team) \r\n         RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team) \r\n         RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team) \r\n         Playoffs ~ 1 + W + (1 + W | Team) \r\n   Data: myData (Number of observations: 90) \r\n  Draws: 3 chains, each with iter = 2500; warmup = 0; thin = 1;\r\n         total post-warmup draws = 7500\r\n\r\nGroup-Level Effects: \r\n~Team (Number of levels: 30) \r\n                                   Estimate Est.Error l-95% CI\r\nsd(W_Intercept)                        1.63      1.95     0.04\r\nsd(W_RS)                               0.00      0.00     0.00\r\nsd(W_RA)                               0.00      0.00     0.00\r\nsd(RA_Intercept)                      12.21     10.88     0.57\r\nsd(RA_OOBP)                           32.60     30.04     1.38\r\nsd(RA_OSLG)                           26.88     23.83     1.07\r\nsd(RS_Intercept)                      11.86     10.01     0.45\r\nsd(RS_OBP)                            33.58     27.10     1.55\r\nsd(RS_SLG)                            28.59     22.99     1.18\r\nsd(Playoffs_Intercept)                 2.20      2.16     0.09\r\nsd(Playoffs_W)                         0.22      0.47     0.00\r\ncor(W_Intercept,W_RS)                 -0.24      0.50    -0.96\r\ncor(W_Intercept,W_RA)                 -0.23      0.52    -0.97\r\ncor(W_RS,W_RA)                        -0.22      0.51    -0.95\r\ncor(RA_Intercept,RA_OOBP)             -0.20      0.51    -0.94\r\ncor(RA_Intercept,RA_OSLG)             -0.21      0.51    -0.95\r\ncor(RA_OOBP,RA_OSLG)                  -0.19      0.51    -0.95\r\ncor(RS_Intercept,RS_OBP)              -0.16      0.52    -0.94\r\ncor(RS_Intercept,RS_SLG)              -0.18      0.52    -0.95\r\ncor(RS_OBP,RS_SLG)                    -0.20      0.51    -0.94\r\ncor(Playoffs_Intercept,Playoffs_W)    -0.18      0.58    -0.98\r\n                                   u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsd(W_Intercept)                        7.45 1.01     1084     1547\r\nsd(W_RS)                               0.01 1.01      934     1938\r\nsd(W_RA)                               0.01 1.01     1012     3299\r\nsd(RA_Intercept)                      40.57 1.00     2271     3697\r\nsd(RA_OOBP)                          111.19 1.00     1464     2342\r\nsd(RA_OSLG)                           85.24 1.00      894     2025\r\nsd(RS_Intercept)                      38.03 1.00     1569     1794\r\nsd(RS_OBP)                           101.92 1.00      507      468\r\nsd(RS_SLG)                            86.47 1.00      654     2468\r\nsd(Playoffs_Intercept)                 7.42 1.00     2058     2623\r\nsd(Playoffs_W)                         1.80 1.07       33       16\r\ncor(W_Intercept,W_RS)                  0.79 1.00     4156     4013\r\ncor(W_Intercept,W_RA)                  0.82 1.00     1522     1654\r\ncor(W_RS,W_RA)                         0.80 1.00     3192     4270\r\ncor(RA_Intercept,RA_OOBP)              0.81 1.00     3376     3918\r\ncor(RA_Intercept,RA_OSLG)              0.80 1.00     3097     4358\r\ncor(RA_OOBP,RA_OSLG)                   0.84 1.01     1295     1547\r\ncor(RS_Intercept,RS_OBP)               0.85 1.00     1162     2565\r\ncor(RS_Intercept,RS_SLG)               0.84 1.00      983     1246\r\ncor(RS_OBP,RS_SLG)                     0.82 1.01      503     1922\r\ncor(Playoffs_Intercept,Playoffs_W)     0.93 1.01      187      219\r\n\r\nPopulation-Level Effects: \r\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nW_Intercept           85.97      5.78    74.51    97.16 1.00     4765\r\nRA_Intercept        -852.33     64.99  -982.24  -724.71 1.00     2989\r\nRS_Intercept        -949.77     67.88 -1084.15  -820.70 1.00     4559\r\nPlayoffs_Intercept  -471.68    923.11 -3529.81   -41.17 1.07       30\r\nW_RS                   0.10      0.01     0.09     0.11 1.00     3938\r\nW_RA                  -0.10      0.01    -0.11    -0.09 1.00     4087\r\nRA_OOBP             2850.08    297.29  2262.09  3420.05 1.00     2622\r\nRA_OSLG             1600.06    187.41  1241.97  1976.70 1.00     4134\r\nRS_OBP              3475.86    272.82  2955.81  4015.43 1.00     4594\r\nRS_SLG              1330.99    160.56  1014.09  1637.60 1.00     3936\r\nPlayoffs_W             5.30     10.37     0.46    39.82 1.07       30\r\n                   Tail_ESS\r\nW_Intercept            4672\r\nRA_Intercept           3771\r\nRS_Intercept           5211\r\nPlayoffs_Intercept       16\r\nW_RS                   5662\r\nW_RA                   4003\r\nRA_OOBP                2861\r\nRA_OSLG                4506\r\nRS_OBP                 3139\r\nRS_SLG                 4062\r\nPlayoffs_W               16\r\n\r\nFamily Specific Parameters: \r\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma_W      3.30      0.31     2.70     3.91 1.00     1995     1334\r\nsigma_RA    22.29      2.41    17.83    27.36 1.00     1130     1360\r\nsigma_RS    18.97      2.15    14.93    23.35 1.00     2979     3893\r\nnu_W        23.65     14.23     5.60    59.37 1.00     2419     1912\r\nnu_RA       21.98     14.02     4.93    57.35 1.00     1839     2379\r\nnu_RS       20.98     13.86     4.49    56.41 1.00     1853     2595\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nHowever, the main advantage of such a formalized model is that we can\r\nnow make specific predictions or simulations that will help us make more\r\ngranular decisions about where and how much to invest our limited\r\nresources, e.g.¬†in terms of hiring and/or L&D activities. For\r\nexample, we know from the available data that teams need to win\r\napproximately 95 games to have a high chance of making the playoffs -\r\nsee the two graphs below.\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 1\r\nset.seed(1234)\r\nmyData %>%\r\n  # creating random numbers for dispersing points across the y axis of the graph\r\n  dplyr::mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot2::ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs))) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5)) +\r\n  ggplot2::scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"The team has not made it to the playoffs\", \"The team has made it to the playoffs\")) +\r\n  ggplot2::labs(\r\n    title = \"Teams that didn't/made the playoffs from 1996-2001\",\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 2\r\n\r\nmyData %>%\r\n  # dividing W variable into intervals \r\n  dplyr::mutate(\r\n    WCat = cut(W, breaks = 21, right = TRUE, include.lowest = TRUE, ordered_result = TRUE)\r\n  ) %>% \r\n  dplyr::group_by(WCat) %>%\r\n  dplyr::summarise(\r\n    PlayoffsProb = mean(Playoffs)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = WCat, y = PlayoffsProb)) +\r\n  ggplot2::geom_bar(stat = \"identity\", color = NA, fill = \"lightblue\") +\r\n  ggplot2::scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1), labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"PROBABILITY OF MAKING IT TO THE PLAYOFFS\",\r\n    title = \"Relation between the number of wins in the regular season and the probability\\nof advancing to the playoffs (1996-2001)\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nConsidering the last known OAK performance statistics from 2001, what\r\nnumber of games can OAK expect to win and what is the likelihood of\r\nmaking the playoffs next year? Let‚Äôs plug the 2001 OAK numbers into the\r\nmodel and check the prediction including all uncertainties.\r\n\r\n\r\nShow code\r\n\r\n# summary prediction of the number of matches won by OAK in 2002 using OAK's statistics from 2001 as input\r\n# set.seed(123)\r\n# predict(\r\n#   fit, \r\n#   resp = \"W\", \r\n#   newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n# )\r\n\r\n# generating predictions using OAK's statistics from 2001 as input\r\nset.seed(123)\r\nwp <- brms::posterior_predict(\r\n  fit, \r\n  resp = \"W\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n  )\r\n\r\n# actual number of matches won by OAK in 2002\r\nrealNWins <- baseball %>%\r\n  filter(Year == 2002, Team == \"OAK\") %>%\r\n  pull(W)\r\n\r\n# uploading library for visualizations of distributions and uncertainty\r\nlibrary(ggdist)\r\n\r\n# creating the graph\r\nwp %>%\r\n  as.data.frame() %>% \r\n  ggplot2::ggplot(aes(x = V1)) +\r\n  ggdist::stat_halfeye(\r\n    fill = \"lightblue\",\r\n    .width = c(0.80, 0.95),\r\n      ) +\r\n  ggplot2::geom_vline(xintercept = realNWins, linetype = \"dashed\") +\r\n  ggplot2::geom_text(x = realNWins+8.3, y = 1, label = \"Actual number of matches won by OAK in 2002\") +\r\n  ggplot2::scale_x_continuous(breaks = seq(80,120,5)) +\r\n  ggplot2::labs(\r\n    x = \"PREDICTED NUMBER OF MATCHES WON (W)\",\r\n    y = \"DENSITY\",\r\n    linetype = \"\",\r\n    title = \"Predicted number of matches won by OAK in 2002\",\r\n    caption = \"\\nThe black horizontal lines at the bottom of the graph represent the 80% and 95% probability intervals, respectively.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"none\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see that the 95% probability interval of the predicted number\r\nof games won by the OAK in 2002 is safely above the 95-point threshold.\r\nAnd when we compare the prediction to the 2002 reality, we see that they\r\nare pretty close. Also, the projected probability of OAK advancing to\r\nthe playoffs is very high, which is consistent with the fact that OAK\r\nsuccessfully qualified for the playoffs in 2002.\r\n\r\n\r\nShow code\r\n\r\n# prediction of OAK making the playoffs in 2002 using OAK's statistics from 2001 as input \r\nset.seed(123)\r\npredict(\r\n  fit, \r\n  resp = \"Playoffs\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG),\r\n  probs = c(0.025, 0.5, 0.975)\r\n)\r\n\r\n\r\n      Estimate Est.Error Q2.5 Q50 Q97.5\r\n[1,] 0.9993333  0.025813    1   1     1\r\n\r\nBased on this information, we may conclude that there is no urgent\r\nneed to invest heavily now in increasing the current quality of the\r\nteam‚Äôs play at bat and in the field (variables OBP and\r\nSLG), assuming that the opposing teams do not significantly\r\nincrease the quality of their play next year (variables OOBP\r\nand OSLG). Using the model, we can also obtain a specific range\r\nof the quality of the OAK‚Äôs play at bat and in the field that would\r\nstill allow it to reach at least 95 games won. Such information could be\r\nuseful, for example, in deciding which players can be safely traded\r\nwithout having a detrimental effect on the likelihood of making the\r\nplayoffs.\r\n\r\n\r\nShow code\r\n\r\n# simulation of the effect of SLG and OBP on the number of OAK games won in 2002\r\n\r\n# getting plausible range of values of OBP and SLG variables\r\nstats <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995, \r\n    Team == \"OAK\"\r\n    ) %>%\r\n  dplyr::summarise(\r\n    dplyr::across(c(OBP, SLG), list(mean = mean, sd = sd))\r\n  )\r\n\r\n# creating a sequence of plausible values for OBP and SLG variables (3 SDs around the mean value)   \r\nOBPSeq <- seq(from = stats$OBP_mean + (3*stats$OBP_sd), to = stats$OBP_mean - (3*stats$OBP_sd), length.out = 20)\r\nSLGSeq <- seq(from = stats$SLG_mean + (3*stats$SLG_sd), to = stats$SLG_mean - (3*stats$SLG_sd), length.out = 20)\r\n\r\n# creating df with all possible combinations of plausible values of OBP and SLG variables\r\nsimDf <- expand.grid(OBP = OBPSeq, SLG = SLGSeq)\r\n\r\n# getting variables necessary for prediction (statistics of OAK's opponent teams from 2001)\r\nstatsO <- baseball %>%\r\n  dplyr::filter(Year == 2001, Team == \"OAK\") \r\n\r\n# adding Team and W variables\r\nsimDf <- simDf %>%\r\n  dplyr::mutate(\r\n    Team = \"OAK\",\r\n    W = NaN\r\n  )\r\n\r\n# running the simulation\r\nfor(i in 1:nrow(simDf)){\r\n  \r\n  #print(i)\r\n  \r\n  # prediction of the RS response variable\r\n  RSPred <- predict(\r\n    fit, \r\n    resp = \"RS\", \r\n    newdata = simDf[i,]\r\n  )[1]\r\n  \r\n  # prediction of the W response variable\r\n  WPred <- predict(\r\n    fit, \r\n    resp = \"W\", \r\n    newdata = data.frame(Team = \"OAK\", RS = RSPred, RA = statsO$RA)\r\n  )[1]\r\n  \r\n  simDf[i,\"W\"] <- WPred  \r\n  \r\n}\r\n\r\n# creating the graph\r\nsimDf %>%\r\n  ggplot2::ggplot(aes(x = OBP, y = SLG, color = W)) +\r\n  ggplot2::geom_point(alpha = 18, size = 3) +\r\n  ggplot2::scale_y_continuous(breaks = seq(0.36, 0.52, 0.02)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0.32, 0.38, 0.01)) +\r\n  ggplot2::scale_colour_gradient2(\r\n    low = \"blue\",\r\n    mid = \"white\",\r\n    high = \"red\",\r\n    midpoint = 95,\r\n    space = \"Lab\",\r\n    na.value = \"grey50\",\r\n    guide = \"colourbar\",\r\n    aesthetics = \"colour\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Simulation of the effect of SLG and OBP on the number of OAK games won in 2002\",\r\n    caption = \"The white color corresponds to the 95-point threshold to qualify for the playoffs.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nAs many of you know, thanks to similar analyses, the OAK management\r\nbegan to select players for their team who, although they did not meet\r\nthe traditional criteria by which scouts judged the quality of baseball\r\nplayers, exhibited exactly those characteristics that, according to\r\nconducted analyses, predicted the number of points won and lost, and\r\nthus the likelihood of advancing to the playoffs, which was the\r\nmanagement‚Äôs main goal. Because competing teams underestimated the\r\nimportance of these player statistics and overestimated other, less\r\nimportant variables (e.g., batting average), OAK management was able to\r\npurchase players relatively inexpensively who enabled them to achieve\r\ntheir goals. As a result, the OAK won 20 more games per season than\r\nsimilarly ‚Äúpoor‚Äù teams and about the same number of games as 2 to 3\r\ntimes richer competition. The power of data in practice!\r\n\r\n\r\nShow code\r\n\r\n# salaries vs number of matches won \r\n# uploading data from the Lahman's Baseball Database, which is publicly available at https://www.seanlahman.com/baseball-archive/statistics/\r\n\r\nplayersSalaries <- readr::read_csv(\"./salaries.csv\")\r\nteamsWins <- readr::read_csv(\"./teams.csv\")\r\n\r\n# computing average sum of salaries paid by each team to its players in 1998-2001\r\nplayersSalariesAvg <- playersSalaries %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n    ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(playersSalariesAvg = sum(salary)/length(unique(yearID)))\r\n\r\n# calculating the average number of wins per season for each team from 1998-2001\r\nteamsWinsAvg <- teamsWins %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n  ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(teamsWinsAvg = sum(W)/length(unique(yearID)))\r\n\r\n\r\n\r\nplayersSalariesAvg %>%\r\n  dplyr::left_join(teamsWinsAvg , \"teamID\") %>%\r\n  dplyr::mutate(OAK = ifelse(teamID == \"OAK\", \"yes\", \"no\")) %>%\r\n  ggplot2::ggplot(aes(x= playersSalariesAvg, y = teamsWinsAvg, fill = OAK)) +\r\n  ggplot2::geom_point()+\r\n  ggplot2::labs(\r\n    title = \"Player salaries and number of wins in 1998-2001\",\r\n    x = \"AVERAGE SUM OF PLAYERS' SALARIES (USD)\",\r\n    y = \"AVERAGE NUMBER OF WINS PER SEASON\"\r\n    ) +\r\n  ggrepel::geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50') +\r\n  ggplot2::scale_fill_manual(\r\n    values = c(\"#ffffff\", \"#ffd400\"), \r\n    labels = c(\"no\",\"yes\")\r\n    ) +\r\n  ggplot2::scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  ggplot2::scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07), labels = scales::number_format(scale = 0.000001, suffix = \"M\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"node\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nFinally, let‚Äôs breathe some life into the dry numbers by watching a\r\nshort clip from the Moneyball film, which\r\nnicely summarizes some of the ideas presented in this blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-hrm-value-chain-and-sem/./baseball.jpg",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-org-chart-and-collaboration/",
    "title": "Org chart and collaboration",
    "description": "How to effectively combine information about the formal organizational structure of a company and the actual collaborative activities of its employees?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-26",
    "categories": [
      "organizational structure",
      "org chart",
      "collaboration",
      "data visualization"
    ],
    "contents": "\r\nHow to effectively combine information about the formal\r\norganizational structure of a company and the actual collaborative\r\nactivities of its employees?\r\nAt Time is Ltd. we are\r\nprimarily focused on collaboration data analytics, whether it‚Äôs\r\nmeetings, emails, chat, CRM, project management or version control\r\nsystems, but besides that we also have a product that helps companies\r\nmap their formal organisational structure. Btw, you can give it a try\r\nbecause many of its features are available for free on the Google Workspace\r\nMarketplace.\r\nWe are currently trying to connect these two ‚Äúworlds‚Äù because in\r\nsituations of organizational transformation it can be very useful to\r\nhave information about the relationship between the current and/or\r\nintended formal structure of the organization on the one hand and the\r\nactual patterns of collaboration on the other.\r\nOne option we‚Äôre considering is using a kind of heatmap, which you\r\nmay be familiar with from eye-tracking studies used in marketing, to see\r\nwhere people focus their attention when interacting with products and\r\nmaking purchasing decisions. Now imagine if we overlayed a similar\r\nheatmap showing the intensity of collaboration between a selected unit\r\nand the rest of the organization over an org chart - see the figure\r\nbelow for illustration.\r\n\r\nWould you consider such a visualization useful if you were engaged in\r\norganizational transformation? Would you suggest any other kind of\r\nvisualization or specific metric related to collaboration? Thank you in\r\nadvance for any input you may have - it will help us broaden the range\r\nof perspectives we are currently considering.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-org-chart-and-collaboration/./collaborationOrgChartOverlay.png",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {},
    "preview_width": 1661,
    "preview_height": 970
  },
  {
    "path": "posts/2023-02-08-span-of-control/",
    "title": "Span of control and collaboration data",
    "description": "How can collaboration data be used to determine the \"optimal\" scope of control?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-24",
    "categories": [
      "span of control",
      "collaboration",
      "meetings"
    ],
    "contents": "\r\nIn determining the ‚Äúoptimal‚Äù span of control (SOC) for the company\r\nand specific departments and teams, it is always advisable to consider\r\nthe context and strategy of the company, the way in which individual\r\ndepartments and teams should perform their work, and the level of\r\ncompetence of individual managers.\r\nFor example, McKinsey\r\nsuggested the following four specific aspects of managerial complexity\r\nthat should be considered in this endeavor:\r\nThe time a manager spends doing her or his own work vs.¬†managing\r\nothers.\r\nThe extent to which the work process is not standardized and\r\nformally structured.\r\nThe variety of work of the manager‚Äôs direct reports.\r\nThe amount of experience and training that team members need to do\r\ntheir jobs.\r\nThe more of the above, the smaller the SOC should be.\r\nIn addition to these factors, we can use as other useful inputs some\r\nmeasures of collaboration that can reasonably be expected to be related\r\nboth to SOC and to the obligations that managers have to their direct\r\nreports. A good example is the metric of the number of 1-on-1 meetings\r\nthat managers have with their direct reports. As the attached chart\r\nillustrates, when pitted against each other, we can look for points on\r\nthe SOC scale where managers start to fall short of the goal of a\r\ncertain minimum number of 1:1s they have with their people, e.g.¬†2\r\nmeetings per month.\r\n\r\nWhat factors do you typically consider when determining the optimal\r\nspan of control in your company? And do you regularly reassess the\r\nadequacy of the current SOC in the context of your current\r\nsituation?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-span-of-control/./scheme.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-09-slack-best-practices/",
    "title": "Attaching numbers to best practices for instant messaging",
    "description": "Slack and other instant messaging platforms can be both a blessing and a curse. Can we attach numbers to some of the recommendations on how to use them effectively? Let's take a look.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-20",
    "categories": [
      "slack",
      "instant messaging",
      "best practices"
    ],
    "contents": "\r\nAs many of you can probably confess, Slack and other instant\r\nmessaging (IM) platforms can be both a blessing and a curse. To support\r\nthe former and suppress the latter, authors of these tools and their\r\nusers themselves have come up with several recommendations on how to use\r\nthem effectively.\r\nSpecific numbers can be attached to some of these best practices to\r\nhelp teams and entire companies systematically shape their behavior on\r\nIM platforms in the desired direction. At Time Is Ltd., we currently measure\r\nthe following seven best practices:\r\nThread use: It helps create organized discussions\r\naround specific messages, and they let users discuss a topic in more\r\ndetail without adding clutter to a channel or direct message\r\nconversation.\r\nMention use: Mentioning specific people in messages\r\nin both public and private channels is one effective way to avoid\r\noverwhelming users with a large number of messages that are not relevant\r\nto them.\r\nShort messages use: Shorter messages often mean\r\nmore messages, more messages mean more notifications, and more\r\nnotifications mean more distractions, more frequent context switching,\r\nand decreased productivity.\r\nEmoji use: People should use emojis instead of\r\nshort messages as they are less distracting and more friendly to other\r\npeople‚Äôs attention.\r\nBatching: Responding to chat messages round the\r\nclock can be detrimental to employees‚Äô productivity as it can distract\r\nthem from focused work. Better strategy is checking messages every one\r\nor two hours instead of continuous handling of all incoming\r\nmessages.\r\nInactive channels: Non-archived channels that show\r\nno activity are just clutter that makes it difficult to navigate and\r\ncollaborate on the chat platform.\r\nTransparency: Direct and group messages have their\r\nplace in chat, especially when discussing sensitive issues or when\r\ntrying to avoid spamming other employees. However, when majority of chat\r\ncommunication occurs in direct and group messages, there is a higher\r\nrisk that information important for task alignment, problem-solving, or\r\ndecision will be hidden in them and out of view from relevant\r\npeople.\r\n\r\nWould you add some other best practices for IM that have worked well\r\nfor you and that would make sense to measure?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-09-slack-best-practices/./im.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-slack-batching/",
    "title": "Always messaging",
    "description": "Let's take a look at two concepts from computer science that can be used in the workplace to improve people's focus and productivity, and expose two methods for measuring their related behaviors when collaborating on instant messaging platforms.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-17",
    "categories": [
      "instant messaging",
      "collaboration",
      "focus time",
      "distractions",
      "operationalization",
      "measurement"
    ],
    "contents": "\r\nWhen dealing with a large number of tasks and frequent task\r\nswitching, two related concepts originating from computer science can be\r\nused: batch\r\nprocessing and interrupt\r\ncoalescing.\r\nIn computing, these terms refer to a situation where computers wait\r\nuntil a fixed interval and check everything, rather than contextually\r\nswitching and processing separate, uncoordinated interrupts from their\r\nvarious sub-components.\r\nWhen transposed into the world of human workers, this design\r\nprinciple can manifest in checking emails or instant messages every one\r\nor two hours instead of continuously handling all incoming emails and\r\nmessages. Such an arrangement prevents fragmentation of people‚Äôs time\r\nand provides them with more focus time they need for deep work and\r\nexperiencing flow.\r\nBut how to measure this behavior so that a number can be put on it to\r\nenable people to better shape their behavior in this regard? At Time is\r\nLtd.¬†we are currently experimenting with two different approaches:\r\nClustering of sent emails/messages using K-Means or PAM and calculation\r\nof time gaps between start/end points of identified clusters, including\r\nthe start and end of the working day. The larger the gaps, the stronger\r\nthe signal of batch behavior.\r\nPercentage of emails/messages sent during the 3 busiest working\r\nhours (defined by the number of emails/messages sent) during a given\r\nday. The higher the proportion, the stronger the signal of batch\r\nbehavior. This approach is inspired by the 2016\r\nstudy by Mark et al. ‚ÄúEmail Duration, Batching and\r\nSelf-interruption: Patterns of Email Use on Productivity and Stress‚Äù,\r\nwhere the authors used similar approach in the domain of email\r\ncommunication.\r\n\r\nThere are advantages and disadvantages to both methods (face\r\nvalidity, accuracy, sensitivity to edge cases, computational complexity,\r\netc.), but irrespective of these, which one would you prefer to see in\r\nyour collaboration report? To get a better idea of what outputs both of\r\nthe above approaches generate, you can take a look at the attached\r\ngraphs showing the prevalence of batch behavior during one of my work\r\nweeks on Slack according to these two approaches.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-slack-batching/./slack.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-overloaded-employees/",
    "title": "Warning system for overloaded employees",
    "description": "What tools and/or signals can we use to identify employees at increased risk of overload? Let's take a look at some of the options we have in this regard.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-12",
    "categories": [
      "overload",
      "layoffs",
      "retention",
      "engagement"
    ],
    "contents": "\r\nOne of the negative side-effects of layoffs and efforts to achieve\r\nthe same (or ideally more) with fewer employees can be an increased\r\nworkload for those who stay because they have to do the work of those\r\nwho have left, which may lead to an increased risk of overload,\r\ndisengagement, and voluntary quits.\r\nTo prevent this from happening, it is useful to combine active\r\nlistening (through engagement surveys, pulse surveys, stay interviews,\r\nsimple chat, etc. ) with signals that can be obtained from the traces\r\nleft by people in various digital workplace tools, such as project\r\nmanagement systems (ClickUp, Jira, Asana, etc.), version control systems\r\n(GitLab, GitHub, etc.), calendars, instant messaging, or emails.\r\nAt Time is Ltd., we are\r\ncurrently focusing on the following metrics that could be useful in this\r\nrespect:\r\nNumber of assigned tasks\r\nTask close rate\r\nNumber of assigned tasks that other people‚Äôs tasks depend on\r\nNumber of commits\r\nNumber of code reviews\r\nResponse time to received messages and e-mails\r\nAmount of focus time available\r\nAmount of distracted time\r\nAmount of time spent working after hours or on weekends\r\nBy checking the distribution of these metrics across individual team\r\nmembers and their changes over time, it is possible to identify\r\nemployees at higher risk of overload, as well as opportunities for a\r\nmore even distribution of the workload.\r\nWhat tools and/or signals do you use in your company to identify\r\nemployees at increased risk of overload?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-overloaded-employees/./overload.jpg",
    "last_modified": "2023-09-16T13:24:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/",
    "title": "Evaluation of the results of the evidence-based HRM knowledge test",
    "description": "Have we made any progress in knowledge of evidence-based HRM practices in the last 20 years? Apparently not. But let's look at the details.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-08",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I shared an app\r\nthat tests knowledge of evidence-based HRM practices using items from Rynes, Colbert, and\r\nBrown‚Äôs 2002 study on HR practitioners‚Äô beliefs about effective HR\r\npractices. The fact that more than 140 people completed the test allowed\r\nme to compare our current results with those of the participants in the\r\noriginal study (959 HR practitioners, mostly HR managers, with an\r\naverage of 13.8 years of HR experience).\r\nSo how did we do overall?\r\nOn average, we had 19.4 items out of 35 correct, i.e., we had a 55%\r\nsuccess rate, which is very close to the results of the original study\r\nwhere respondents had an average 57% success rate (and also pretty close\r\nto the 50% success rate corresponding to random choice). So these\r\nresults suggest that we have not progressed much as a group over the\r\nlast 20 years, however, see the disclaimer at the very end of the\r\npost.\r\n\r\nIn which HRM area did we have the largest & smallest\r\nknowledge gaps?\r\nThe biggest gap was in the staffing area (44% success rate) and the\r\nsmallest was in the training & employee development area (67%\r\nsuccess rate).\r\nIn which items did we do best?\r\nLeadership training is effective because good leaders are made, not\r\nborn (95% success rate).\r\nLecture-based training is not generally superior to other forms of\r\ntraining delivery (92% success rate).\r\nWhen pay must be reduced or frozen, a company can do something to\r\nreduce employee dissatisfaction and dysfunctional behaviors (90% success\r\nrate).\r\nIn which items did we do the worst?\r\nScoring positive on drug tests doesn‚Äôt mean one will be any less\r\nreliable or productive employee (11.6% success rate).\r\nSetting performance goals is, on average, more effective for\r\nimproving organizational performance than encouraging employees to\r\nparticipate in decision-making (12.3% success rate).\r\nMost errors in performance appraisals cannot be eliminated by\r\nproviding training that describes the kinds of errors managers tend to\r\nmake and suggesting ways to avoid them (15% success rate).\r\nIn which items were we most unsure?\r\nOlder adults don‚Äôt learn more from training than younger adults (38%\r\nuncertain).\r\nIntegrity tests that try to predict whether someone will steal, be\r\nabsent, or otherwise take advantage of an employer work well in practice\r\n(34% uncertain).\r\nCompanies with merit pay systems tend to have higher performance\r\nthan companies without them (17% uncertain).\r\nPlease keep in mind that the comparison presented here is not\r\nentirely an apples-to-apples comparison due to several limitations of\r\nthe data collection method (e.g., we don‚Äôt know the sociodemographics of\r\nthe new participants, new evidence may have emerged that does not match\r\nthe correct answers in the original study, some people may have taken\r\nthe test multiple times, etc.).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/./testEval.jpg",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-meetings-improvement/",
    "title": "How to improve effectiveness of meetings?",
    "description": "What suggestions do people have for improving the effectiveness of meetings? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-05",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nIn my last\r\npost I outlined the reasons why people think the meetings they\r\nattend are in/effective. Let‚Äôs now look at what they suggest can be done\r\nto improve the effectiveness of meetings. Who knows, maybe in these\r\nrecommendations you‚Äôll find an alternative to canceling most internal\r\nmeetings like they did at Shopify.\r\nThe list below is again based on Geimer\r\net al.‚Äôs 2015 paper ‚ÄúMeetings at work: Perceived effectiveness and\r\nrecommended improvements‚Äù.\r\nPEOPLE SIDE: üëâ Come prepared üëâ Arrive on time üëâ\r\nOpen to change üëâ Actively listen to what others are saying üëâ Display\r\nprofessionalism during the meeting\r\nMEETING STRUCTURE & ORGANIZATION SIDE: üëâ\r\nDistribute appropriate information via e-mail instead of in meeting üëâ\r\nAllow time to prepare for meetings üëâ Provide meaningful agenda üëâ\r\nClarify plan of action üëâ Use or rotate a facilitator/chair üëâ Invite\r\nappropriate attendees üëâ Pay attention to timing limit, start/end on\r\ntime üëâ Shorten meetings üëâ Hold meetings at appropriate intervals &\r\nmeet only when necessary üëâ Make the meeting environment more\r\ncomfortable\r\nMEETING ACTIVITIES SIDE: üëâ Make meetings more\r\ninteractive & seek input from all attendees üëâ Stay focused on the\r\ntopic üëâ Prioritize items üëâ Break into smaller groups (brainstorming,\r\netc.) üëâ Delegate responsibilities and set deadlines for assigned\r\ntasks\r\nMEETING OUTCOMES SIDE: üëâ Record and distribute\r\nmeeting minutes üëâ Follow up with proposed solutions\r\nIs there anything that you think is missing in the list, especially\r\nin the context of the fact that since 2015 the workplaces have been\r\noperating more in remote/hybrid mode?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-meetings-improvement/./meetingsDalle.png",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-02-08-ineffective-meetings/",
    "title": "Why are meetings in/effective?",
    "description": "If you are a regular organiser or attendee of meetings, you may be interested in what people think about the reasons why the meetings they attend are in/effective, as this can give you a better chance of contributing to making your meetings more effective and meaningful for you and others.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-03",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nThe list below is based on Geimer\r\net al.‚Äôs 2015 paper ‚ÄúMeetings at work: Perceived effectiveness and\r\nrecommended improvements‚Äù.\r\nPEOPLE SIDE: üö´ Late arrive üö´ Unprepared attendees\r\nüö´ One-way (top-down) communication üö´ Lack of open-mindedness &\r\nempathy üö´ Self-promotion, people talk just to appear to add value, and\r\nhidden agenda üö´ People interrupt/talk during meeting üö´ Interpersonal\r\nconflicts, incivility, and disrespect\r\nMEETING ORGANIZATION SIDE: ‚úÖ Agenda ‚úÖ Distribution\r\nof agenda in advance üö´ Lack of direction/goals ‚úÖ Chaired effectively\r\nüö´ Meetings held just to have them, just a routine with no real purpose\r\nüö´ Appropriate parties are not invited and inappropriate parties are\r\ninvited üö´ Too many attendees üö´ Time conflicts üö´ Meet at inappropriate\r\nintervals üö´ Meetings take too long üö´ Takes time to travel to\r\nmeeting\r\nMEETING ACTIVITIES SIDE: üö´ Insufficient\r\ninteraction, meeting activities are monotonous and boring üö´ No new\r\ninformation üö´ Discussion gets off target üö´ Core issues not discussed\r\nüö´ Lack of clarity about what the attendee is supposed to do\r\nMEETING OUTCOMES SIDE: üö´ Inaction post-meeting üö´\r\nDecisions have already been made, just a rubber stamp\r\nDo you identify with the above reasons? Is there anything that you\r\nthink is missing from the list, particularly as the workplaces have been\r\noperating more remotely/hybrid way since 2015?\r\nP.S. In the next post let‚Äôs check what suggestions people have for\r\nimproving the effectiveness of meetings.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-ineffective-meetings/./meetingsDalle.png",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2022-12-17-good-manager/",
    "title": "Signals of a good manager",
    "description": "Do you think it's possible to find signals in collaboration (meta)data that someone is a good manager? Let's give it some thought.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-17",
    "categories": [
      "google",
      "oxygen project",
      "collaboration data",
      "performance management"
    ],
    "contents": "\r\nMany of you have probably already heard about Google‚Äôs\r\nOxygen project and its findings on the factors differentiating the\r\nhighest and lowest-rated managers based on performance reviews, employee\r\nengagement surveys, interviews, and other sources of employee feedback.\r\nThe final list included the following eight characteristics:\r\nIs a good coach.\r\nEmpowers the team and does not micromanage.\r\nExpresses interest in and concern for team members‚Äô success and\r\npersonal well-being.\r\nIs productive and results-oriented.\r\nIs a good communicator - listens and shares information.\r\nHelps with career development.\r\nHas a clear vision and strategy for the team.\r\nHas key technical skills that help him or her advise the team.\r\nDo you think it would be possible to find any signals, however weak,\r\nof the presence of some of these managers‚Äô characteristics in the\r\ncollaboration (meta)data?\r\nOf the collaboration metrics we currently work with at Time is Ltd., I would bet on the\r\nfollowing:\r\nNumber of 1-on-1 meetings managers have with their direct reports\r\nand new hires [concern for employees‚Äô success and personal\r\nwell-being]\r\nNumber of team meetings managers have with their teams [information\r\nsharing, alignment on vision, strategy, and tactics)\r\nAfter-hours or weekend work and workday length of managers‚Äô direct\r\nreports [well-being & work-life balance]\r\nManagers‚Äô presence at meetings of their direct reports, excluding\r\n1-on-1s and team meetings [micromanagement]\r\nManagers being in CC/BCC of emails sent by their direct reports\r\n[micromanagement]\r\nTime it takes managers to respond to emails from their direct\r\nreports [interest in and concern for team members]\r\nDirect reports having skip-level meetings [career development &\r\nnew career opportunities]\r\nWould you agree? Or what other signals of a good manager would you\r\nlook for in collaboration (meta)data?\r\nP.S. In a later\r\nupdate, the list of top manager characteristics at Google also\r\nincludes the characteristic ‚ÄúCollaborates across Google‚Äù, which is\r\nrelatively straightforward to measure using collaboration data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-good-manager/./worlds-best-boss-funny-the-office-micromanagement.gif",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-evidence-based-hrm-knowledge-test/",
    "title": "Evidence-based HRM knowledge test",
    "description": "Interested in testing your knowledge of evidence-based HRM practices? If so, click and get started.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-15",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management",
      "shiny app"
    ],
    "contents": "\r\n\r\n\r\n\r\nIf you are interested in testing your knowledge of evidence-based HRM practices in the following five areas‚Ä¶\r\nManagement practices\r\nGeneral employment practices\r\nTraining & Employee development\r\nStaffing\r\nCompensation & Benefits\r\n‚Ä¶ then give a try on the test, which is based on the items used in Rynes, Colbert, and Brown‚Äôs 2002 study ‚ÄúHR practitioners‚Äô beliefs about effective HR practices: a comparison of research and practice‚Äù.\r\nI built a simple shiny app that administers you the test, scores your answers, and compares your results to the results of the participants in the original study (959 HR professionals, mostly HR managers, with an average of 13.8 years of experience in HR). You can also use it to check the accuracy of your answers at the item level to fill in specific gaps in your knowledge.\r\nHere is the link to the app.\r\nFeel free to share your results in the comments. Think of it as a form of public commitment to making some progress in evidence-based HRM in the coming year üòâ To walk the talk, I attached my own results. As you can see, it‚Äôs not bad, but there is still room for improvement, especially in the general employment practices area üòÉ\r\n\r\nP.S. Keep in mind that the test is based on evidence available up to 2002, so it is possible that some correct answers or their contingencies may have changed in that time. For all of them, consider, for example, the adjustment of the estimate of the magnitude of the predictive validity of personnel selection procedures in their most recent meta-analysis by Sackett et al.¬†(2022). If you come across any such discrepancy, it would be great if you share it with others in the comments.\r\nUpdate: With more than 140 people completing the test, I was able to compare our current results with those of the participants in the original study. You can see the results of the comparison in the post Evaluation of the results of the evidence-based HRM knowledge test. Spoiler: It‚Äôs not very good üòØ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-evidence-based-hrm-knowledge-test/./manOnTheRoad.jpg",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-collaboration-during-vacations/",
    "title": "Can you really unplug?",
    "description": "With the Christmas holidays approaching, the following question is more relevant than ever, with the exception of the summer vacations: Can we really disconnect from work during the vacations? And what can collaboration data tell us about this?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-10",
    "categories": [
      "vacation",
      "well-being",
      "work-life balance",
      "collaboration data"
    ],
    "contents": "\r\nAlthough most people know they need time off to stay mentally sharp, productive, and resilient over the long term, many confess they work when they‚Äôre on vacation.\r\nThe data we collect and analyze at Time is Ltd. confirms this sad truth. As the first chart illustrates, on more than 75% of vacation days people show some collaborative activity, here measured by attending non-recurring meetings, sending emails, and editing shared files.\r\n\r\nHowever, the same data also suggests what could be part of the remedy for this unsatisfactory state of affairs. The second chart shows that there is a small but not insignificant positive relationship between the time that managers and their direct reports spend collaborating during vacation. The chart also shows that managers tend to collaborate more intensely during vacation than their direct reports, which is probably not too much of a surprise.\r\n\r\nThe data is thus in line with the quite often mentioned suggestion that managers should be better role models for their direct reports on how to behave during vacation. As in other areas of people management, it is not enough to lay down the rules - the ‚Äúplaying captain‚Äù must play by those rules as well.\r\nHow are you doing in this respect? And do you have any tricks that help you unplug from work during vacations?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-collaboration-during-vacations/./outOfOffice.jpg",
    "last_modified": "2023-11-19T12:09:29+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-07-mission-scale-from-tas/",
    "title": "One does not simply do a business without getting lost",
    "description": "Breaking down one weekend association.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-07",
    "categories": [
      "team assessment survey",
      "mission",
      "purpose",
      "goals",
      "metrics",
      "review"
    ],
    "contents": "\r\n\r\n\r\n\r\nAs I was scrolling through one of my feeds over the weekend, I came across a funny meme that resonated with my recent experience on a family trip that expressed the deep truth that ‚ÄúOne does not simply do a road trip without getting lost.‚Äù üòÑ\r\n\r\nBesides that, and that‚Äôs why I‚Äôm writing about it here on my blog, it also reminded me of the results of a study I did together with Rastislav Duris and Slavka Silberg, on the characteristics of more than 80 teams from different industries and composed of more than 800 people using the Team Assessment Survey, Dr.¬†Gordon Curphy‚Äôs survey that measures some of the basic factors that determine team performance.\r\nSpecifically, I was reminded of the results on the Mission scale, which consists of the following four items:\r\nPurpose: Team members are clear about the team‚Äôs purpose.\r\nGoals: The team has a set of overall goals.\r\nMetrics: Metrics and benchmarks have been identified for each team goal.\r\nReviews: The team regularly reviews progress on team goals.\r\nAs you can see in the attached chart, the results of the ‚ÄúHeartbeat analysis‚Äù can be briefly summarized as ‚ÄúWe know where we‚Äôre going, at least some of us know the points to get there, but we‚Äôre not sure if we‚Äôre on the right track and if we should change our original plans.‚Äù\r\n\r\nThat sounds a lot like the description of our last family trip. üòÖ Do you think you‚Äôre better off in this regard in your team or company? And if so, what tips would you give others on how to improve in this respect?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-07-mission-scale-from-tas/./lost_in_desert.jpg",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-02-change-detection/",
    "title": "How to quickly navigate dashboard users to what they need to know?",
    "description": "Let's take a look at some tips and tricks to make dasboards more useful for their users.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-01",
    "categories": [
      "dashboard",
      "contextualization",
      "time series",
      "change detection",
      "python"
    ],
    "contents": "\r\nSince the beginning of our efforts at Time is Ltd. to develop a\r\ncomprehensive collaboration analytics platform, we have created several\r\nhundred collaboration metrics. As you can probably imagine, with such a\r\nhuge amount of metrics, it was quite difficult for users to get\r\nsomething useful out of the platform.\r\nTo make the metrics provided more digestible, we‚Äôve enhanced the\r\nplatform using several approaches, from simply selecting metrics with\r\nthe most straightforward call to action and creating apps for very\r\nspecific use cases to providing users with comparative, historical,\r\nintuitively scaled, and equivalent information. Brent Dykes described\r\nmany of these sense-making methods very neatly in his article Contextualized\r\nInsights: Six Ways To Put Your Numbers In Context.\r\nHowever, even after several rounds of these improvements, there was\r\nstill a need to help users quickly find areas that might be worth their\r\nattention and deeper exploration. One approach we took was based on the\r\n(validated) assumption that collaboration metrics are relatively stable\r\nover time and that the intentional and unintentional behavioral changes\r\nbehind these metrics evolve rather slowly. We therefore decided to\r\ndetect the most significant changes over the last 3 months.\r\nFor this purpose, we chose a method that is related to the Bollinger Bands\r\nmethod used in time series analysis or ‚ÄúHeartbeat Analysis‚Äù used in\r\nsurvey response analysis. Specifically, we look at the standard\r\ndeviation and average of the month-to-month changes for each metric,\r\nscale the changes to z-scores, and then identify the metrics with the\r\nhighest absolute value of average change over the last 3 months. To\r\nillustrate, see the attached chart where the metrics are sorted by\r\nmagnitude of their change in descending order from left to right and top\r\nto bottom.\r\n\r\nIf you want to try this method on your own data, you can use its\r\nPython implementation, which is on my\r\nGitHub page.\r\nWhat other methods do you find useful in identifying values or\r\nchanges that might be worth users‚Äô attention? Feel free to share them in\r\nthe comments for inspiration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-02-change-detection/./dashboard.jpg",
    "last_modified": "2023-09-16T13:24:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-26-meeting-matrix/",
    "title": "Eisenhower matrix for meetings",
    "description": "Meet the Eisenhower matrix for meetings ;)",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "meeting culture",
      "meeting overload",
      "focus time",
      "collaboration culture"
    ],
    "contents": "\r\nI assume many, if not all, of you are familiar with the\r\nEisenhower matrix, a simple tool for prioritizing tasks based on a\r\ncombination of their importance and urgency. It assumes 4 types of tasks\r\n- important & urgent, important & not urgent, not important\r\n& urgent, not important & not urgent - and for each of them\r\noffers a simple recommendation on what to do with them - do it, schedule\r\nit, delegate it, and delete it. By following these rules, people should\r\nbe able to more successfully combat the ‚Äúmere-urgency‚Äù effect, eliminate\r\ntime-wasters in their lives, and create more space to make progress\r\ntoward their goals.\r\n\r\nSomething similar can be created also for meetings, which are big\r\ntime and money guzzlers and deserve to be treated accordingly. Only\r\ninstead of the dimensions of importance and urgency, we will use the\r\nsize and length of meetings. The resulting matrix assumes 4 types of\r\nmeetings with corresponding recommendations on what to do with them:\r\nSmall & Short: These meetings are a bit tricky\r\nbecause they often involve useful meetings, e.g., short syncs of teams\r\nworking on some specific task or project or one-on-one meetings between\r\nmanagers and their direct reports, however, when there is a lot of them,\r\nthey may cause calendar fragmentation and drop in available focus time;\r\nto avoid this, one can think of batching such meetings into larger\r\nblocks of two or three meetings with appropriate small breaks in between\r\nto avoid meeting fatigue and late arrivals.\r\nSmall & Long: These meetings are great for\r\nbrainstorming, problem-solving, or decision-making, so make sure you use\r\nthem primarily for that purpose and don‚Äôt waste them on low-value-added\r\nactivities.\r\nLarge & Short: These meetings often serve only\r\nto disseminate information and can therefore be safely replaced by less\r\nintrusive asynchronous collaboration tools such as email, instant\r\nmessaging or some kind of knowledge management tool.\r\nLarge & Long: These meetings are quite often a\r\nwaste of people‚Äôs time by not allowing everyone to meaningfully\r\ncontribute and by making them both physically and mentally exhausted, so\r\ntry to move these meetings into the third quadrant if your goal is\r\nsimple information dissemination or into the second quadrant if the goal\r\nis to solve some problem or to make some important decision.\r\nThe meeting matrix is by no means a panacea for meeting overload, but\r\nby following the rules above, people should be able to better protect\r\ntheir time for focused work and make meetings more efficient,\r\nmeaningful, and valuable for themselves and the company. Just be aware\r\nthat unlike the Eisenhower matrix, in the case of the meeting matrix,\r\npeople need to rely more on others to follow these rules in order for\r\nits positive effect to materialize. So get ready to update your\r\n(hopefully already existing) team agreement on your collaboration\r\nculture.\r\nBtw, what is your guess as to how much time you spend in each cell of\r\nthe meeting matrix? For comparison I attach my monthly numbers from\r\nOctober.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-meeting-matrix/./meetingMatrix.png",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {},
    "preview_width": 850,
    "preview_height": 565
  },
  {
    "path": "posts/2022-11-26-resources-on-retention-and-downsizing/",
    "title": "Some resources on staff retention and downsizing",
    "description": "Probably due to the current situation in the talent market, where many companies are laying people off and at the same time are worried about losing their key employees, a few people have contacted me in recent weeks asking for some tips on evidence-based approaches to dealing with retention and downsizing.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "employee retention",
      "employee turnover",
      "layoffs",
      "downsizing",
      "people analytics",
      "evidence-based management"
    ],
    "contents": "\r\nI referred them to the following resources - maybe some of you will\r\nfind them useful as well:\r\nEmployment\r\nDownsizing and Its Alternatives guideline from the SHRM‚Äôs Science-to-Practice\r\nSeries.\r\nRetaining\r\nTalent guideline from the SHRM‚Äôs Science-to-Practice\r\nSeries.\r\nRubenstein et al.‚Äôs meta-analysis\r\nof voluntary turnover predictors; you can also use this\r\napp visually summarizing its results.\r\nSpeer et al.‚Äôs article Here\r\nto stay or go? Connecting turnover research to applied attrition\r\nmodeling.\r\nDemo\r\ndashboard for analysis of employee turnover.\r\nFeel free to share any other resources on this topic you think might\r\nbe useful to others.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-resources-on-retention-and-downsizing/./retentionDownsizing.png",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 720
  },
  {
    "path": "posts/2022-11-18-interventions-reducing-gender-pay-gap/",
    "title": "Evidence-based interventions that help reduce the gender pay gap",
    "description": "Pay inequality between men and women is not only an ethical and legal issue for companies, but also a marketing issue - it can have a negative impact on their \"employer brand\" and attractiveness as an employer. This means that if companies want to attract and retain talented employees, they must be able to ensure that they treat men and women equally in this respect. Let's look at what the existing evidence tells us about what might help us with this.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-18",
    "categories": [
      "gender pay gap",
      "evidence-based management"
    ],
    "contents": "\r\nIf you are responsible for DEI initiatives in your company, you might\r\nbe interested in a paper by The\r\nBehavioural Insights Team that lists several possible interventions\r\nto close the gender pay gap, divided into three categories based on the\r\nextent to which their effectiveness is supported by empirical\r\nevidence.\r\nü•á Actions with well-documented effectiveness:\r\nIncluding more women on shortlists in recruitment and promotion. *Ô∏è\r\nUse of tasks assessing job skill levels in the selection of new\r\nemployees.\r\nUse of structured interviews in recruitment and promotion.\r\nEncouraging salary negotiation through disclosure of existing salary\r\nranges.\r\nIntroducing transparent promotion and reward processes.\r\nAppointing a manager or establishing a corporate diversity task\r\nforce.\r\nü•à Potentially promising actions, but requiring further\r\nevidence of their effectiveness:\r\nIncreasing work flexibility for men and women.\r\nSupporting shared parental leave.\r\nRecruiting former employees who have had to interrupt their careers\r\nfor a prolonged period for various personal reasons.\r\nOffering mentoring and sponsorship.\r\nOffering networking programs.\r\nSetting internal targets.\r\nü•â Actions with mixed evidence of their\r\neffectiveness:\r\nTraining on the topic of unconscious bias.\r\nDiversity training.\r\nLeadership development training.\r\nDemographically diverse selection panels in external and internal\r\nrecruitment.\r\nFor those interested, here is the original document for a closer\r\nlook.\r\n\r\n\r\nThis browser does not support PDF files. Please download the PDF file to\r\nview it: Download PDF.\r\n\r\n\r\n\r\nA final note. As useful as it is to know which interventions have a\r\ndecent chance of reducing gender pay inequality, an integral part of\r\nthis fight is to regularly check the existence of this inequality across\r\nthe organisation and within different types of organisational processes.\r\nAs Alessandro\r\nLinari aptly noted in a discussion on Linkedin, ‚ÄúThe best way to\r\nkeep the gender pay gap under control is still to do a periodic pay\r\nequity analysis across the organisation and address any identified gap.\r\nI say unfortunately because there are a thousand different ways where\r\npay differences still manage to sneak into the workforce, through hiring\r\nand promotion practices, but also retention policies, bonus allocation,\r\nperformance management, and others.‚Äù If you want to learn more\r\nabout some of the technical details of such an analysis, check out one\r\nof my previous posts on this topic (unfortunately, it is now only\r\navailable in Czech, but you can use one of the online translators to get\r\naround this limitation üòâ).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-18-interventions-reducing-gender-pay-gap/./genderPayGap.jpg",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-15-people-related-metrics-distribution/",
    "title": "It's perfectly normal not to be normal",
    "description": "And it definitely applies to the shape of the distribution of many HR metrics. Let's look at this in a little more detail.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-15",
    "categories": [
      "hr metrics",
      "normal distribution",
      "collaboration",
      "performance",
      "r"
    ],
    "contents": "\r\nThe fact is that many HR practitioners overestimate the frequency\r\nwith which the phenomena they commonly encounter in their practice have\r\na normal, bell-shaped, symmetrical distribution.\r\nThis is very much the case, for example, in the area of communication\r\nand collaboration that we deal with at Time is Ltd., as illustrated in\r\nthe attached chart with some of our collaboration metrics, which show a\r\nwide range of distributions from log-normal and power law to\r\nexponential, gamma, Weibull and beta.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./collaborationMetrics.rds\")\r\n\r\n# External network size metric\r\nexternalNetworkSizeG <- mydata %>%\r\n    dplyr::filter(metric == \"externalNetworkSize\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(breaks = seq(0,120,20)) +\r\n    ggplot2::labs(\r\n      x = \"EXTERNAL NETWORK SIZE / PERSON / MONTH\",\r\n      y = \"FREQUENCY\",\r\n      title = \"External network size\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n  )\r\n  \r\n\r\n# Focus rate metric\r\nfocusRatePrctG <- mydata %>%\r\n    dplyr::filter(metric == \"focusRatePrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF FOCUS TIME / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Available focus time\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n\r\n# Multitasking in meetings metric\r\nmultitaskingInMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"multitaskingInMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"MULTITASKING RATE / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Meeting participations with multitasking\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Breaks between meetings metric\r\nbreaksBetweenMeetingsMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"breaksBetweenMeetingsMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,600,120)) +\r\n    ggplot2::labs(\r\n      x = \"LENGTH OF BREAK / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Lenght of breaks between meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Collaboration time metric\r\ncollaborationTimePersonHrsDayG <- mydata %>%\r\n    dplyr::filter(metric == \"collaborationTimePersonHrsDay\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" hrs\"), breaks = seq(0,16,2)) +\r\n    ggplot2::labs(\r\n      x = \"TIME SPENT COLLABORATING / PERSON / DAY\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Time spent collaborating\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Supervised meetings metric\r\nmicromngMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"micromngMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF SUPERVISED MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of supervised meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Recurring meetings metric\r\nrecurringMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"recurringMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"% OF RECURRING MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of recurring meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call ending metric\r\ncallEndingMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callEndingMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(-60,60,20)) +\r\n    ggplot2::labs(\r\n      x = \"PLANNED VS. ACTUAL END OF MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Planned vs. actual end of meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call delay metric\r\ncallDelayMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callDelayMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,40,5)) +\r\n    ggplot2::labs(\r\n      x = \"DELAY OF ONLINE MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Late arrivals to online meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\ng <- (externalNetworkSizeG + focusRatePrctG + multitaskingInMeetingsPrctG) / \r\n  (breaksBetweenMeetingsMinutesG + collaborationTimePersonHrsDayG + micromngMeetingsPrctG) / \r\n  (recurringMeetingsPrctG + callEndingMinutesG + callDelayMinutesG) +\r\n  patchwork::plot_annotation(\r\n    title = 'Distribution of selected collaboration metrics',\r\n    theme = theme(\r\n      plot.title = element_text(size = 26, margin=margin(20,0,12,0))\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBut it also applies to more traditional areas of HR concern such as\r\nindividual or team performance. For many job roles, especially\r\nknowledge-based ones, job performance has a power-law distribution,\r\ni.e., only a few percent of individuals or teams have disproportionately\r\nhigh performance and most have performance below the statistical average\r\n(many of you have probably heard of Pareto‚Äôs law, and the 80/20 rule in\r\nthis context). More detailed information on this particular topic can be\r\nfound, for example, in the following two articles - The\r\nBest and the Rest: Revisiting the Norm of Normality of Individual\r\nPerformance by O‚ÄôBoyle Jr.¬†& Aguinis (2012) and Team\r\nPerformance: Nature and Antecedents of Nonnormal Distributions by\r\nBradley & Aguinis (2022).\r\nWhy bother with that? Well, because, based on incorrect assumptions\r\nabout the distribution of specific phenomena, companies may make\r\ndecisions that ultimately harm them. For example, suppose a company\r\napplies the assumption of normal distribution in evaluating employees‚Äô\r\nperformance that actually has power law distribution. In that case, it\r\nwill result in underestimating the contribution of the best performers\r\nand overestimating the contribution of the worst performers, which may\r\nbe negatively reflected in various decisions regarding reward &\r\ncompensation, learning & development, promotions, succession\r\nplanning, etc.\r\nLessons learned? For frequent decisions or decisions with a\r\nsignificant expected impact, it is worth checking that our underlying\r\nassumptions match reality.\r\nBtw, this also applies to personal life. Here‚Äôs an example from mine:\r\nI thought I was in control of watching movies and TV shows on streaming\r\nplatforms, but I started getting signals from those around me that I was\r\nspending too much time there. So as a proper ‚ÄòQuantified Selfer‚Äô, I decided to\r\ntrack my daily screen time for a month. To my surprise, I found that I\r\nwas indeed spending a lot more time there than I thought and wished. I\r\nthen put in place simple solutions to prevent me from watching more than\r\nI should - I have started to pay extra fees into the family budget for\r\nwatching movies (loss aversion), I\r\nhave pre-selected time slots for watching movies (implementation\r\nintention), I‚Äôve removed streaming apps from my phone (lowering salience),\r\nI‚Äôve stopped watching movies alone (social control), and I‚Äôm also\r\nplaying with the idea of asking my wife to change the PINs (preventing\r\nimpulsive\r\nwatching).\r\nP.S. If you ever need to check the shape distribution of any of your\r\nmetrics, you should definitely try the amazing fitdistrplus\r\nR package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-15-people-related-metrics-distribution/./distributions.png",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {},
    "preview_width": 962,
    "preview_height": 670
  },
  {
    "path": "posts/2022-10-27-bayesian-belief-updating/",
    "title": "A visual introduction to Bayesian belief updating",
    "description": "Teacher: \"Bayesian belief updating involves combining existing or prior beliefs with an assessment of the strength of new evidence.\" Student: \"And could I please see this in action?\"",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-27",
    "categories": [
      "bayesian belief updating",
      "bayesian inference",
      "judgment",
      "forecasting",
      "critical thinking",
      "r"
    ],
    "contents": "\r\nWhen trying to reduce uncertainty, even very small pieces of\r\ninformation count if you are patient and have some tool to combine them\r\neffectively. I recently re-read Philip\r\nTetlock‚Äôs book Superforecasting\r\nand came across an excellent illustration of such a tool: Bayesian belief\r\nupdating.\r\n‚ÄúImagine you are sitting with your back to a billiards table. A\r\nfriend rolls a ball onto the table and it stops at a random spot. You\r\nwant to locate the ball without looking. How? Your friend rolls a second\r\nball, which stops at another random spot. You ask, ‚ÄúIs the second ball\r\nto the left or the right of the first?‚Äù Your friend says, ‚ÄúTo the left.‚Äù\r\nThat‚Äôs an almost trivial scrap of information. But it‚Äôs not nothing. It\r\ntells you that the first ball is not on the extreme left edge of the\r\ntable. And it makes it just a tad more likely that the first ball is on\r\nthe right side of the table. If your friend rolls another ball on the\r\ntable and the procedure is repeated, you get another scrap of\r\ninformation. If he says, ‚ÄúIt‚Äôs to the left,‚Äù the likelihood of the first\r\nball being on the right side of the table increases a little more. Keep\r\nrepeating the process and you slowly narrow the range of the possible\r\nlocations, zeroing in on the truth‚Äîalthough you will never eliminate\r\nuncertainty entirely.‚Äù\r\nAfter reading this paragraph, I thought it would be much more\r\npedagogically compelling (and fun üòâ) to see this update process live\r\nand in action. What a great opportunity to learn how to work with the gganimate\r\nR package.\r\nHere is the code I put together.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # data manipulation and visualization\r\nlibrary(gganimate) # animation of the charts\r\nlibrary(bayestestR) # Highest Density Interval computation\r\n\r\n# specifying the width of the billiards table  \r\nfield <- seq(0,178,1)\r\n\r\n# likelihoood function for the situation when the second ball stops to the right of the first one \r\nrightLikelihood <- (178-field)/178\r\n# likelihoood function for the situation when the second ball stops to the left of the first one \r\nleftLikelihood <- 1-rightLikelihood\r\n# flat prior for the very beginning of the updating process  \r\nfirstPrior <- rep(1/179, 179)\r\n\r\n# position where the first ball stopped\r\npoint <- 88\r\n\r\n# Bayesian belief updating\r\n# creating shell dataframe for final results\r\nposteriors <- data.frame()\r\n\r\n# setting random seed for ensuring reproducibility\r\nset.seed(1234)\r\n\r\n# specifying the number of trials\r\nfor(i in 1:500){\r\n  \r\n  # throwing the second ball\r\n  pointTrial <- runif(n = 1, min = 0, max = 178)\r\n  \r\n  # determining whether the second ball stopped to the left or to the right of the first one \r\n  side <- ifelse(pointTrial > point, \"right\", \"left\")\r\n  \r\n  # selecting appropriate prior\r\n  if(i==1){\r\n    \r\n    prior <- firstPrior\r\n    \r\n  } else{\r\n    \r\n    prior <- posteriors %>% \r\n      dplyr::filter(trial == i-1) %>% \r\n      dplyr::pull(posterior)\r\n    \r\n  }\r\n  \r\n  # combining prior and likelihood (evidence) and transforming the result into probabilities\r\n  if(side == \"right\"){\r\n    \r\n    likelihood <- rightLikelihood * prior \r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  } else{\r\n    \r\n    likelihood <- leftLikelihood * prior\r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  }\r\n  \r\n  # putting results into the dataframe\r\n  suppDf <- data.frame(\r\n    posterior = probability,\r\n    trial = i,\r\n    side = side,\r\n    pointTrial = pointTrial\r\n  )\r\n  \r\n  # computing Highest Density Interval\r\n  sampling <- sample(x = field, size = 10000, replace = TRUE, prob = probability)\r\n  \r\n  hdi <- bayestestR::hdi(sampling, ci = 0.95)\r\n  \r\n  suppDf <- suppDf %>%\r\n    dplyr::mutate(\r\n      lhdi = hdi$CI_low,\r\n      hhdi = hdi$CI_high\r\n    )\r\n  \r\n  # putting results into the shell dataframe \r\n  posteriors <- dplyr::bind_rows(posteriors, suppDf)\r\n\r\n}\r\n\r\n# adjusting data for visualization purposes\r\nposteriorsDf <- posteriors %>%\r\n  dplyr::group_by(trial) %>%\r\n  dplyr::mutate(place = dplyr::row_number()-1) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    hdi = ifelse(place >= lhdi & place <= hhdi, \"yes\", \"no\")\r\n  )\r\n\r\n# creating charts\r\nmyAnimation <- ggplot2::ggplot(data = posteriorsDf, aes(x = place, y = posterior)) +\r\n  ggplot2::geom_area(data = posteriorsDf %>% dplyr::filter(hdi == 'yes'), fill = 'light blue') +\r\n  ggplot2::geom_line(size = 1) +\r\n  ggplot2::geom_point(aes(x = pointTrial, y = 0), size = 4) +\r\n  ggplot2::geom_point(aes(x = point, y = 0), size = 4, color = \"red\") +\r\n  ggplot2::geom_text(aes(x = 8, y = 0.1, label = stringr::str_glue(\"Trial: {trial}\\n95% HDI: [{lhdi}, {hhdi}]\")), color = \"grey\") +\r\n  ggplot2::scale_y_continuous(limits = c(0,0.105)) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" cm\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Bayesian belief updating in the billiards table thought experiment\",\r\n    subtitle = \"Target place: 88 cm\",\r\n    x = 'PLACE ON THE BILLIARDS TABLE', \r\n    y = 'POSTERIOR PROBABILITY',\r\n    caption = \"\\n\\nThe static red point corresponds to a random and unknown location of the first ball. The moving black point then corresponds to the location where the second,\\nrepeatedly rolled ball randomly ended up. The area in blue corresponds to the 95% Highest Density Interval of the posterior distribution. All points inside this\\ninterval have a higher probability density than points outside this interval.\"\r\n    ) +\r\n  gganimate::transition_time(trial) +\r\n  gganimate::ease_aes('linear')\r\n\r\n# animating chart\r\ngganimate::animate(myAnimation, nframes = 125, fps = 5, height = 6, width = 11, units = \"in\", res = 125)\r\n\r\n# saving animated chart as a .gif file\r\ngganimate::anim_save(filename = \"./bayesianBelifUpdating.gif\", animation = last_animation())\r\n\r\n\r\n\r\nAnd here is the resulting animation of the Bayesian belief updating\r\nprocess across 500 trials.\r\n\r\nFor those who would like to incorporate Bayesian reasoning into their\r\nmanagerial decision-making under uncertainty, I can recommend this\r\narticle by Brian T.\r\nMcCann.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-27-bayesian-belief-updating/./bayesian-belief-updating.jpg",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-24-police-cadet-evaluation-dataset/",
    "title": "Police cadet evaluation dataset",
    "description": "A \"new\" real-world dataset useful for training in people analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-24",
    "categories": [
      "people analytics",
      "data",
      "recruitment",
      "hiring",
      "training"
    ],
    "contents": "\r\nWhile cleaning out my (very) old computer, I came across a hidden\r\ngem: a dataset with real-world data about police cadet evaluation. It\r\nwas part of a tutorial from Peltarion, an AI\r\nsoftware company providing specialized software (Synapse) for\r\ncreating, training, evaluating, and deploying artificial neural networks\r\nand other adaptive systems (recently acquired by King). AFAIK,\r\nthis dataset is not part of any publicly available database with\r\ntraining datasets, so it may add a bit to the portfolio of possibilities\r\nfor those involved and interested in people analytics.\r\nThe data were collected as part of an effort by the National\r\nPolice Services Agency and the Dutch\r\nMinistry of Justice and Security to objectively examine whether the\r\ndata collected at the time of graduation of police cadets can be used to\r\npredict the requirements for passing the standard five-year evaluation.\r\nThe main reason for the study was to find the key indicators for the\r\nthen 20% failure rate, which was considered unacceptable (data were\r\ncollected in the late 1990s), and to study the effects of lowering\r\nadmission standards (accepting cadets with past criminal records and\r\nlowering the minimum grade from 5.5 to 4.0).\r\nThe dataset has the following characteristics:\r\n2000 observations\r\n9 attributes:\r\nAge: the age at which the cadet started studying to\r\nbecome a police officer.\r\nAvG: average grade at the time of graduation (scale\r\n1-10).\r\nChdn: number of children at the time of\r\ngraduation.\r\nExEd: extra university-level or equivalent\r\neducation (years).\r\nCR: criminal record (0=No, 1=Yes).\r\nSex: sex of the cadet (0 = Male, 1 = Female).\r\nSecE: other experience in the security sector (0 =\r\nNo, 1 = Yes).\r\nAvgE: average yearly evaluation score (The average\r\nof five years. The evaluation is performed by a committee of 10 senior\r\npolice officers. Scale 1-5). This is a help attribute and not for use as\r\ninput.\r\nFinalE: final evaluation. Fail if average yearly\r\nevaluation score (Avg) < 2.0 otherwise pass. (1610 Pass / 390 Fail).\r\nThis is the target attribute.\r\n\r\nHere is a table you can use to check and download the data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and making user-friendly data table\r\nlibrary(tidyverse)\r\nlibrary(DT)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./policeCadetEvaluation.csv\")\r\n\r\n# adjusting the data type for some variables for tabulation and visualization purposes\r\ndata <- data %>%\r\n  mutate(\r\n    CR = as.factor(CR),\r\n    Sex = as.factor(Sex),\r\n    SecE = as.factor(SecE),\r\n    FinalE = as.factor(FinalE)\r\n  )\r\n\r\n# defining the table\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy', 'csv', 'excel'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nAnd here is a pairplot showing the distribution and relationships\r\nbetween variables in the dataset.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for the pairplot data visualization\r\nlibrary(GGally)\r\n\r\n# defining custom function for diagonal continuous variable chart  \r\nmy_dens <- function(data, mapping) {\r\n  ggplot(data = data, mapping = mapping) +\r\n    geom_density(alpha = 0.6, color = NA) \r\n}\r\n\r\n# pairplot\r\nGGally::ggpairs(\r\n  data = data,\r\n  title = \"Police cadet evaluation dataset\",\r\n  mapping=ggplot2::aes(fill = FinalE),\r\n  lower=list(\r\n    combo = wrap(\"facethist\", binwidth=1, alpha = 0.6),\r\n    continuous = wrap(\"points\", alpha = 0.3, size = 0.7),\r\n    discrete = wrap(\"facetbar\", alpha = 0.6)\r\n    ),\r\n  upper=list(\r\n    discrete = wrap(\"box\", alpha = 0.6),\r\n    combo = wrap(\"box\", alpha = 0.6)\r\n  ),\r\n  diag = list(\r\n    continuous = my_dens,\r\n    discrete = wrap(\"barDiag\", alpha = 0.6)\r\n    )\r\n  ) +\r\n  ggplot2::scale_fill_manual(values=c(\"Fail\" = \"#e53935\", \"Pass\" = \"#00897b\")) +\r\n  labs(caption = \"\\nThe color indicates the pass/fail result of the final evaluation, the target attribute.\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 12, hjust = 0),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you want to download the dataset, you can do so here via the table\r\nabove or via my\r\nGitHub page where you can also find more information about the\r\ndataset. Happy analysis üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-24-police-cadet-evaluation-dataset/./Politie_Nederland_nieuw_uniform.jpg",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-conditional-inference-tree/",
    "title": "Divide and... understand",
    "description": "Finding the breakpoint when people start to score significantly higher/lower on a given criterion - the use case for the Conditional Inference Tree algorithm.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-18",
    "categories": [
      "conditional inference tree",
      "decision tree",
      "machine learning",
      "statistics",
      "interpretability",
      "prediction",
      "r"
    ],
    "contents": "\r\nWhen correlating collaboration metrics with business criteria that\r\nour clients are interested in, such as the size of the internal network\r\nof salespeople vs.¬†their sales performance, we often encounter the\r\nquestion of where the breakpoint is when people start to score\r\nsignificantly higher/lower on a given criterion.\r\nTo answer this question, I find very handy the Conditional\r\nInference Tree algorithm - a non-parametric class of decision trees\r\nthat, unlike traditional decision trees, use a significance/permutation\r\ntest (corrected for multiple testing) to select covariates to split and\r\nrecurse the variable.\r\nWhen applied to just one numerical predictor, it will provide a set\r\nof partitions that allow you to split that predictor into bins in such a\r\nway that you end up with statistically significant differences between\r\nsome of the identified bins. With this information in hand, it is much\r\neasier for you to find the ‚Äúsweet spots‚Äù (there may be more than one)\r\nwhere the criterion starts to behave differently in relation to the\r\npredictor values. See charts below for illustration.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and visualuzation \r\nlibrary(tidyverse)\r\n\r\n# defining normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\n# creating artificial dataset with internal network size and sales performance variables\r\ninternalNetworkSize = seq(-6, 6, 0.1)\r\nsalesPerf = 1*(internalNetworkSize**3) + 2*(internalNetworkSize**2) + 1*internalNetworkSize + 3\r\nsalesPerf_noise = 70 * rnorm(mean = 0, sd = 1, n=length(salesPerf))\r\nsalesPerformance = salesPerf + salesPerf_noise\r\n\r\n# putting data into dataframe and making some transformations of the variables\r\ndata <- data.frame(\r\n  internalNetworkSize = internalNetworkSize,\r\n  salesPerformance = salesPerformance\r\n) %>%\r\n  dplyr::mutate(\r\n    internalNetworkSize = normalize(internalNetworkSize),\r\n    salesPerformance = normalize(salesPerformance),\r\n    internalNetworkSize = internalNetworkSize*189,\r\n    salesPerformance = salesPerformance*100\r\n  )\r\n\r\n# visualizing relationship between internal network size and sales performance\r\nggplot2::ggplot(data = data, aes(x = internalNetworkSize, y = salesPerformance)) +\r\n  ggplot2::geom_point(color = \"#4d009d\", size = 3, alpha = 0.8) +\r\n  ggplot2::labs(\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE\",\r\n    y = \"SALES PERFORMANCE\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,200, 20), limits = c(0,200)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for ctree algorithm and visualization of the results of statistical tests\r\nlibrary(partykit)\r\nlibrary(ggstatsplot)\r\n\r\n# defining formula\r\nfmla <- as.formula(\"salesPerformance ~ internalNetworkSize\")\r\n\r\n# binning internal network size variiablle using ctree algorithm\r\nctree <- partykit::ctree(\r\n  fmla,\r\n  data = data,\r\n  na.action = na.exclude,\r\n  control = partykit::ctree_control(minbucket = ceiling(round(0.05*nrow(data))))\r\n)\r\n\r\n# plotting resulting tree\r\n#plot(ctree)\r\n\r\n# number of identified bins\r\n#bins = partykit::width(ctree)\r\n\r\n# extracting bin borders\r\ncutvct = data.frame(matrix(ncol=0,nrow=0)) # Shell\r\nn = length(ctree) # Number of nodes\r\nfor (i in 1:n) {\r\n  cutvct = rbind(cutvct, ctree[i]$node$split$breaks)\r\n}\r\ncutvct = cutvct[order(cutvct[,1]),] # sorting / converting to an ordered vector (asc)\r\ncutvct = ifelse(cutvct<0,trunc(10000*cutvct)/10000,ceiling(10000*cutvct)/10000) # rounding to 4th decimal place to avoid borderline cases\r\n\r\n# adding minimum and maximum values\r\ncutvct <- append(cutvct, min(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct <- append(cutvct, max(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct = cutvct[order(cutvct)]\r\n\r\n# creating bin categories\r\nvalueCat <- cut(x = data %>% dplyr::pull(\"internalNetworkSize\"), breaks = cutvct, include.lowest = TRUE)\r\n\r\n# creating supplementary dataframe for visualization purposes \r\nsuppDf <- data %>%\r\n  dplyr::select(internalNetworkSize, salesPerformance) %>%\r\n  dplyr::mutate(category = valueCat) %>%\r\n  dplyr::filter(category != \"NA\")\r\n\r\n# visualizing relationship between internal network size and sales performance using ggbetweenstats from ggstatsplot package\r\nggstatsplot::ggbetweenstats(\r\n  data = suppDf %>% as.data.frame(),\r\n  x = category,\r\n  y = salesPerformance,\r\n  type = \"robust\"\r\n) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\"), breaks = seq(0,100,20)) +\r\n  ggplot2::labs(\r\n    y = \"SALES PERFORMANCE\",\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE (BINNED)\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n    ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you are dealing with similar use cases, give it a try. And if you\r\nuse any other tools/approaches for this, feel free to share them in\r\nreturn.\r\nP.S. Thanks to Filip\r\nTrojan, my former boss and colleague from the Deloitte Advanced\r\nAnalytics team, who introduced me to this tool. I still benefit from it\r\nto this day üôèüí™\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-18-conditional-inference-tree/./decision-tree-analysis.jpg",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-11-timeboxing/",
    "title": "Timeboxing. Does it really work?",
    "description": "Checking with real-world collaboration data whether timeboxing has a protective function in terms of time available for focused work.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-11",
    "categories": [
      "timeboxing",
      "timeblocking",
      "regression analysis",
      "control variables"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I addressed the question of why people don‚Äôt make more use of\r\ntimeboxing, a time\r\nmanagement tool relying on a well-researched self-regulatory technique\r\ncalled implementation\r\nintention - planning what you will do, when, and how.\r\nQuite surprisingly, I found in our collaborative data that there is\r\nnot a positive but a slightly negative relationship between the amount\r\nof time for focused work and the amount of blocked working time in the\r\ncalendar, which I interpreted to mean that people who have plenty of\r\ntime for focused work usually do not have a strong need to block time\r\nfor focused work in their calendars.\r\nHowever, based on these results, one colleague wondered whether this\r\nresult actually speaks against the usefulness of this tool. To answer\r\nher question properly, we should avoid comparing apples with pears and\r\ncontrol for the effect of the number of collaborative activities people\r\nparticipate in, as it can be assumed that those who spend more time\r\ncollaborating with others have less time for focused work and also use\r\nthe timeboxing technique more.\r\nUsing this approach and our clients‚Äô collaborative data, I looked at\r\nthe relationship between the proportion of work time blocked on the\r\ncalendar and the time available for focused work (i.e.¬†no meetings, no\r\nad-hoc calls, no email or instant messaging), and found that the\r\nmarginal effect of timeboxing is in line with the positive effect of the\r\ntimeboxing technique on the time available for focused work. The effect\r\nis not huge (each percentage point of working time blocked in the\r\ncalendar yields on average .19% of focus rate), however, timeboxing\r\nseems to be saved, phew üòâ\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-11-timeboxing/./time-blocking.png",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-09-17-multilevel-modeling/",
    "title": "Multilevel modeling in people analytics",
    "description": "Don't chase (statistical) ghosts and use multilevel models instead!",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-17",
    "categories": [
      "multilevel modeling",
      "hierarchical modeling",
      "mixed models",
      "nested data",
      "bayesian inference",
      "collaboration",
      "employee engagement",
      "r"
    ],
    "contents": "\r\nIn one of our projects,\r\nwhere we were trying to find out how collaborative behavior relates to\r\nemployee engagement, we repeatedly came across patterns that reminded\r\nthe client of internally well-known differences between the behavior of\r\nteams from different parts of the company. For example, we found that\r\nmore frequent participation in short and small meetings was related to\r\nlower employee engagement. This pattern matched well the client‚Äôs\r\nobservation that one particular part of the company had regular daily\r\nstand-up meetings and also lower engagement scores compared to the rest\r\nof the company due to some other aspects of their work. As further\r\nanalysis confirmed, this pattern was really just a statistical artifact\r\ncaused by the coincidence of these two facts.\r\nOne way analysts can protect themselves from this type of misleading\r\nconclusions is by using multilevel or hierarchical\r\nmodels that take into account the fact that the data\r\nhave a nested structure, i.e.¬†that some observations are not\r\nindependent of each other because they belong to the same higher-order\r\ngroup, e.g.¬†to an organizational unit (one of the basic assumptions of\r\nmost statistical models in use).\r\nThis is well illustrated in the graphs below. They show that the\r\nrelationship between the number of monthly 1:1s that employees have with\r\ntheir line manager and their subjectively perceived support from their\r\nline manager is slightly positive across most groups of teams (shown by\r\ncolored dots and lines), but when all teams are analyzed together, the\r\nrelationship is rather negative (shown by black dots and lines).\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(scales)\r\nlibrary(patchwork)\r\nlibrary(ggtext)\r\n\r\ndata <- readr::read_csv(\"./data.csv\")\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support while taking into account differences between organizational units \r\ng1 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(aes(col = Unit), size = 2.5, alpha = 0.5) + \r\n  ggplot2::geom_smooth(aes(col = Unit), method = 'lm', alpha=0.2, se = F) +\r\n  labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Unit A\" = \"#20066b\", \"Unit B\" = \"#e56b61\", \"Unit C\" = \"#b4ba0d\", \"Unit D\" = \"#32b2c7\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support without taking into account differences between organizational units \r\ng2 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(size = 2.5, alpha = 0.5) +\r\n  ggplot2::geom_smooth(method = 'lm', alpha=0.2, linetype = \"solid\", color = \"black\", se = F) +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- g2 + g1\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Relationship between the number of 1:1s and perceived managerial support across**\r\n  <br>\r\n  **and within organizational units**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,12,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# fitting Bayesian hierarchical linear regression model\r\nmodel <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones + (1 + oneonones | Unit)),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(model)\r\n# plot(model)\r\n# brms::pp_check(model, ndraws = 100)\r\n\r\n\r\n# fitting Bayesian non-hierarchical linear regression model\r\nmodelNonHierarchical <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(modelNonHierarchical)\r\n# plot(modelNonHierarchical)\r\n# brms::pp_check(modelNonHierarchical, ndraws = 100)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(emmeans)\r\nlibrary(tidybayes)\r\n\r\n# marginal effect of 1:1s in the Bayesian hierarchical linear regression model\r\navg_marginal_effect <- model %>% \r\n  emmeans::emmeans(~ oneonones,\r\n                   at = list(oneonones = seq(0, 6, by = 0.1)),\r\n                   epred = TRUE,\r\n                   re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf1 <- ggplot2::ggplot(avg_marginal_effect, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Hierarchical linear regression model\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n# marginal effect of 1:1s in the Bayesian non-hierarchical linear regression model\r\navg_marginal_effect_nonHierarchical <- modelNonHierarchical %>% \r\n  emmeans::emmeans(\r\n    ~ oneonones,\r\n    at = list(oneonones = seq(0, 6, by = 0.1)),\r\n    epred = TRUE,\r\n    re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf2 <- ggplot2::ggplot(avg_marginal_effect_nonHierarchical, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Non-hierarchical linear regression model\"\r\n  ) +\r\n    ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ngf <- gf2 + gf1\r\ngf <- gf + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Posterior average marginal effect of 1:1 meetings on perceived managerial support**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,0,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(gf)\r\n\r\n\r\n\r\n\r\nWithout the use of the hierarchical model (and/or a careful post-hoc\r\nvisual check of alternative explanations), we would reach a completely\r\nopposite (and incorrect) conclusion about the relationship between the\r\nnumber of 1:1s and perceived support from the line manager (a phenomenon\r\nknown as Simpson‚Äôs\r\nparadox). In this particular case, it is relatively easy to\r\nrecognize that something may be wrong, but the situation is not always\r\nso obvious and intuitive. In these other cases, it is advantageous to\r\nhave some tool at hand to compensate for our imperfect intuition and\r\nlimited knowledge and imagination. Hierarchical models are one such\r\ntool.\r\nIf you don‚Äôt already have it in your analytics toolbox, be sure to\r\ngive it a try. If you work with R, you can use the lme4\r\nor brms\r\npackages to implement it. In a Python\r\nenvironment, you can use the statsmodels or\r\nPyMC3 libraries to\r\ndo this. And if you‚Äôre more used to drag-and-drop tools, then JASP or jamovi (both open-source alternatives\r\nto SPSS)\r\nwill give you access to various mixed models through an easy-to-use\r\ngraphical interface.\r\nFor an accessible discussion of this topic in the context of people\r\nanalytics, including other useful tools for working with hierarchical\r\ndata, see also the excellent articles by Paul van der\r\nLaken, John\r\nLipinski, and Max\r\nBlumberg:\r\nSimpson‚Äôs\r\nParadox: Two HR examples with R code.\r\nHow\r\nto Avoid Aggregation Errors and Simpson‚Äôs Paradox in HR Analytics: Part\r\n1\r\nHow\r\nto Avoid Aggregation Errors and Simpson‚Äôs Paradox in HR Analytics: Part\r\n2\r\nWhy\r\nPeople Analytics should NOT be using regression to predict team\r\noutcomes\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-multilevel-modeling/./groupsofpeople.jpg",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-and-personality/",
    "title": "Collaboration and personality",
    "description": "Personality is not fate, at least when it comes to the level of engagement in corporate communication and collaboration.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-15",
    "categories": [
      "collaboration",
      "communication",
      "networking behavior",
      "personality",
      "big five"
    ],
    "contents": "\r\nOne of our clients once\r\nasked us to what extent employees‚Äô level of engagement in corporate\r\ncommunication and collaboration is driven by their personality and to\r\nwhat extent by their job role, the conditions in which they work, and\r\nother factors outside their personality.\r\nOur first answer was that the latter probably plays a more\r\nsignificant role than the former, but it was difficult to answer more\r\nspecifically because we did not yet have all the data we needed to\r\nquantify the tightness of this relationship. This motivated me to look\r\nat existing research on this topic to help us better set our apriori\r\nexpectations on this issue.\r\nWith the help of metaBus, an\r\namazing platform for curating, searching, and summarizing research\r\nfindings from the social and organizational sciences, I was able to get\r\nthe results of over 100 studies on the relationship between employees‚Äô\r\nBig\r\n5 personality traits and the amount of interaction and networking\r\nbehavior they engage in. Among the criteria for the amount of\r\ninteraction were variables like contact frequency, frequency of\r\nparticipation, communication frequency, meeting frequency, hours of\r\ninteraction, interaction duration, etc. For networking behavior there\r\nwere criteria as liaison, building networks, relationship building,\r\nnetwork activity, maintaining contacts, increasing internal visibility,\r\nnetwork ability, and informal network.\r\n\r\n\r\nShow code\r\n\r\n# The following concepts were used to search for relevant studies on the metaBus platform (their respective codes are given in brackets):\r\n# Big 5 (20443)  \r\n# Amount of interaction (20287) \r\n# Networking behavior (80017)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # for data manipulation and visualization \r\nlibrary(ggridges) # for data visualization\r\nlibrary(ggtext) # for enabling markdown in ggplots\r\nlibrary(patchwork) #  for combining ggplots\r\n\r\n# data preparation\r\n# uploading data\r\ninteraction <- readr::read_csv(\"./interactionAmount.csv\")\r\nnetworking <- readr::read_csv(\"./networkingBehavior.csv\")\r\n\r\n# preparing dataset for amount of interaction concept\r\ninteractionPrep <- interaction %>%\r\n  dplyr::filter(\r\n    # removing non-relevant personality characteristics \r\n    !Var1 %in% c(\"Empathic concern\"),\r\n    # limiting to studies conducted at the individual level\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Extraversion\") | stringr::str_detect(Var1, \"Extroversion\") | stringr::str_detect(Var1, \"extraversion\") ~ \"Extraversion\",\r\n      stringr::str_detect(Var1, \"Openness to experience\") | stringr::str_detect(Var1, \"openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"agreeableness\") ~ \"Agreeableness\",\r\n      stringr::str_detect(Var1, \"emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# preparing dataset for networking behavior concept\r\nnetworkingPrep <- networking %>%\r\n  dplyr::filter(\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"Emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# data visualization\r\n# creating chart for the amount of interaction concept\r\ninteractionChart <- interactionPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#e56b61\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.text.x = element_text(),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n)\r\n\r\n# creating chart for the networking behavior concept\r\nnetworkingChart <- networkingPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#32b2c7\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text.y = element_blank(),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts together\r\ng <- interactionChart + networkingChart \r\n\r\n# adding title and caption\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:20pt;font-weight:bold;'>**Do Big 5 traits predict** \r\n    <span style='color:#e56b61;'>**the amount of interaction**<\/span> **&**\r\n    <span style='color:#32b2c7;'>**networking behavior**<\/span> **of employees?**\r\n    <\/span>\",\r\n  \r\n  caption = \"The solid vertical lines represent quartile values.\\nBased on studies found on the metaBus platform using the concepts 'Big 5' (code: 20443), 'Amount of interaction' (code: 20287), and 'Networking behavior' (code: 80017).\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(10,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 9, hjust = 0)\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nAs can be seen from the graphs above, the relationship between\r\npersonality and the amount of interaction and networking behavior goes\r\nin the expected direction. Agreeable, open, and extraverted employees\r\nand to some extent also conscientious and emotionally stable employees\r\ntend to engage more in interactions with others and in networking.\r\nHowever, the relationships found are relatively weak. The middle 80% of\r\nobserved effects range from an absolute value of .02 to .24, so across\r\nthe studies shown, small effects prevail. And even in the case of the\r\nstrongest effect (r = .38), personality ‚Äúexplains‚Äù only 14% of\r\nthe variability in the networking behavior. There is therefore ample\r\nscope for the influence of a range of other factors.\r\nHow about you? Are you able to engage in interactions and networking\r\nin a way that supports your career, work performance, or other positive\r\noutcomes, perhaps despite your natural tendencies due to your\r\npersonality setup? Feel free to share your experience and thoughts in\r\nthe comments. Btw, you can find interesting information on this topic in\r\nthe excellent book 8\r\nSteps to High Performance by Marc Effron, specifically\r\nin chapters 4 and 6.\r\nCaveat: The graphs represent only a simple summary\r\nof the effects observed in the selected studies, not a proper\r\nmeta-analysis. If you‚Äôre interested in the specific analysis steps and\r\nchoices behind the graphs shown, you can check the code above or go to\r\nmy GitHub\r\npage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-and-personality/./personalityandcollaboration.jpeg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-overload-and-bottlenecks/",
    "title": "Hot spots of collaboration overload and collaboration bottlenecks and how to find them",
    "description": "One of the most useful insights that can be gleaned from collaboration data is where hot spots of potential collaboration overload and/or collaboration bottlenecks may exist in a company. Such insight can be especially valuable these days, when many companies are trying to fight the upcoming economic downturn by achieving more with less.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-08",
    "categories": [
      "collaboration overload",
      "collaboration bottlenecks",
      "organizational network analysis",
      "r"
    ],
    "contents": "\r\nOne simple way to identify such hot spots is to compare the\r\noutbound and inbound collaborative activities in which\r\nteams or individuals participate. The greater the difference between the\r\ntwo in favor of inbound collaboration activities, the stronger the\r\nsignal that collaboration overload and/or bottlenecks may be a problem\r\nfor that team or individual.\r\nTo illustrate, take a look at the attached charts that show the\r\npatterns of collaboration between several teams via Slack. The distances\r\nbetween teams, the thickness, and the direction of the arrows between\r\nthem tell us who is collaborating with whom and how much. The size of\r\nthe nodes then represents the amount of inbound and outbound\r\ncollaborative activities that the teams participate in, respectively.\r\nBased on the differences between them, the bar chart below shows us\r\nwhere the inbound collaboration activities outweigh the outbound ones\r\nthe most, and therefore where the risk of collaboration overload and/or\r\nbottlenecks is greatest.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nlibrary(patchwork)\r\n\r\n# uploading data with nodes/ties based on the current frequency of communication via Slack\r\nnodesR <- readr::read_csv(\"./nodes.csv\") \r\ntiesR <- readr::read_csv(\"./ties.csv\")\r\n\r\n# specifying cut-off value for showing only the x% of strongest edges\r\nprob = 1\r\n\r\n# changing coding of individual nodes in the network\r\nties <- tiesR %>%\r\n  dplyr::left_join(nodesR, by = c(\"from\" = \"id\")) %>%\r\n  dplyr::select(-from) %>% \r\n  dplyr::rename(from = name) %>%\r\n  dplyr::left_join(nodesR, by = c(\"to\" = \"id\")) %>%\r\n  dplyr::select(-to) %>%\r\n  dplyr::rename(to = name) %>%\r\n  dplyr::select(from, to, weight) %>%\r\n  dplyr::mutate(\r\n    from = stringr::str_to_title(from),\r\n    to = stringr::str_to_title(to),\r\n    from = stringr::str_replace(from, \"Development\", \"Dev\"),\r\n    to = stringr::str_replace(to, \"Development\", \"Dev\"),\r\n    from = stringr::str_replace(from, \"Dev Management\", \"Dev - Management\"),\r\n    to = stringr::str_replace(to, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n  \r\nnodes <- nodesR %>%\r\n  dplyr::select(-id) %>%\r\n  dplyr::mutate(\r\n    name = stringr::str_to_title(name),\r\n    name = stringr::str_replace(name, \"Development\", \"Dev\"),\r\n    name = stringr::str_replace(name, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n\r\n\r\n# making the network from the data frame \r\ng <- igraph::graph_from_data_frame(d = ties, vertices = nodes, directed = TRUE)\r\n\r\n# setting name of the network\r\ng$name <- \"Collaboration via Slack\"\r\n\r\n# assigning ids to nodes\r\nV(g)$id <- seq_len(vcount(g))\r\n\r\n# cutoff value for showing only the x% of strongest edges\r\ncutoff <- quantile(ties$weight, probs = prob)[[1]]\r\n\r\n# visualizing the inbound network\r\nset.seed(1234)\r\ninG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = inbound), alpha = 0.5, fill = \"#32b2c7\", color = \"#32b2c7\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n    ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"INBOUND COLLABORATION ACTIVITIES\")\r\n    )\r\n\r\n\r\n\r\n# visualizing the outbound network\r\nset.seed(1234)\r\noutG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = outbound), alpha = 0.5, fill = \"#46c8ae\", color = \"#46c8ae\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"OUTBOUND COLLABORATION ACTIVITIES\")\r\n  )\r\n\r\n\r\n# bar chart with info about difference between inbound and outbound collaboration activities\r\ninoutDiffG <- nodes %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(name, inoutDiff), y = inoutDiff)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = ifelse(nodes$inoutDiff > 0, \"#e56b61\", \"#20066b\"), alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(breaks = seq(-200,200,50)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"DIFFERENCE BETWEEN IN/OUTBOUND COLLABORATION ACTIVITIES\",\r\n    y = \"DIFFERENCE BETWEEN THE NUMBER OF IN/OUTBOUND INSTANT MESSAGES\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 30, margin=margin(0,0,12,0), hjust = 0.5),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 24, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 22, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- (inG + outG) / inoutDiffG\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nWe see that the ‚Äúhottest‚Äù hot spots are in teams Dev-Management and\r\nDev-Frontend. While this is not definitive proof that we have real\r\nproblems in these two specific teams, it should be a strong enough\r\nsignal to take notice and try to verify our suspicion with additional\r\ninformation, such as checking some relevant business metrics or simply\r\nasking a few people we know should be affected, if there is a problem.\r\nIf the initial suspicion is confirmed, appropriate action should be\r\ntaken, e.g.¬†consider the relevance of some requests, possibly redirect\r\nthem to other teams, automate some tasks, expand the team and recruit\r\nnew people, etc.\r\nFor more tips on how to leverage collaboration data in the current\r\nuncertain economic times, I recommend reading the articles Top\r\n7 Collaboration Metrics to Utilize in an Economic Crisis by Jan Rezab and Top\r\n6 Metrics to measure during an economic downturn by Shwetha Pai.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-overload-and-bottlenecks/./Organizational-network.jpg",
    "last_modified": "2023-09-16T13:24:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-back-to-back-meetings/",
    "title": "Are back-to-back meetings for good or bad?",
    "description": "A short post about the practice of back-to-back meetings and how to determine when it's for bad and when it's rather for good.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-01",
    "categories": [
      "meeting habits",
      "back-to-back meetings"
    ],
    "contents": "\r\n‚ÄúContext, Context, Context‚Äù could be the headline of this post.\r\nWhen we address the issue of good meeting habits with our clients, the length of breaks\r\nbetween successive meetings is one of the first metrics we focus on.\r\nAs many of you probably know from your firsthand experience,\r\nconsecutive meetings with no breaks, a.k.a. back-to-back\r\nmeetings, have many detrimental effects, from overload\r\nand exhaustion to not being adequately prepared for subsequent\r\nmeetings and arriving\r\nlate to them.\r\nAs useful as the above metric is, it does not tell the whole story\r\nand can lead to invalid conclusions and see a problem where there is\r\nnone. Data from one of our teams illustrates this well.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading datasets from the platform\r\ndata1 <- readr::read_delim(\"./timeisltd-chart1.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata2 <- readr::read_delim(\"./timeisltd-chart2.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata3 <- readr::read_delim(\"./timeisltd-chart3.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\n\r\n# Time between successive meetings\r\ng1 <- data1 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#20066b\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time between successive meetings\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Number of back-to-back meetings in a row\r\ng2 <- data2 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = as.numeric(cat)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#e56b61\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" mtgs\", accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Number of back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Time spent in back-to-back meetings in a row\r\ng3 <- data3 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = factor(cat, levels = c(\"31-60 Min\", \"61-90 Min\", \"91-120 Min\", \"121-150 Min\", \"151-180 Min\", \"181+ Min\"), ordered = TRUE)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#46c8ae\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time spent in back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# putting graphs together\r\ng <- g1/g2/g3\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBased purely on the time between successive meetings, we could\r\nconclude that a given team suffers from an unhealthy frequency of\r\nback-to-back meetings, as in more than a third of cases, there is no\r\nbreak between meetings. However, if we look at how long the series of\r\nback-to-back meetings tend to be (in 75% of cases it‚Äôs only 2 meetings\r\nin a row) and how much time people spend in them (in 42% of cases it‚Äôs\r\nbetween 31-60 minutes and in 30% of cases it‚Äôs between 61-90 minutes),\r\nthen the resulting picture is less pessimistic and more indicative of a\r\nrather healthy level of effort to protect time for focused\r\nwork by batching meetings into short blocks\r\nthat do not come at the cost of exhausting people and making meetings\r\nless effective.\r\nWhat is your approach to back-to-back meetings? Do you try to always\r\nhave at least a 5-minute buffer between two consecutive meetings? And\r\nhow successful are you at this? Are you aware of situations where it is\r\nappropriate to batch meetings into tight blocks without breaks? And do\r\nyou have a limit on how many meetings to put in a row? Feel free to\r\nshare your thoughts and experiences in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-back-to-back-meetings/./backtobackmeetings.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-20-people-analytics-popularity-after-covid/",
    "title": "The impact of the COVID pandemic on the popularity of people analytics",
    "description": "Many people analytics professionals think that after the COVID pandemic, organizations are more willing to listen to their insights and recommendations. Can we find any empirical support for their hunch? Let's check it out with data provided by Google Trends and segmented regression analysis of interrupted time series.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-29",
    "categories": [
      "people analytics",
      "hr analytics",
      "covid pandemic",
      "segmented regression analysis",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nPeople analytics popularity after pandemic\r\nData preparation\r\nModeling\r\nWhat do the model and data tell us?\r\n\r\nPeople analytics popularity after pandemic\r\nThere is a fairly common perception among the people analytics professionals with whom I am in contact that after the COVID pandemic, companies are much more willing to use the insights provided by people analytics teams and incorporate them into their business-related decision-making processes.\r\nI wondered if I could find any empirical support for this feeling in the surge of global web search interest in ‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù terms on Google after the pandemic outbreak, assuming the pandemic broke out in March 2020.\r\nLet‚Äôs start our quest with a simple visual inspection of a line chart showing the trend of worldwide web search interest in ‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù terms on Google from January 2007 to July 2022. (You can replicate this chart using the Google Trends website and the search terms, time range, location, and source for searches described above. If you are familiar with R, you can use the code below.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for getting data from Google Trends\r\nlibrary(gtrendsR)\r\n# uploading libraries for data manipulation\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\n# setting parameters for Google Trends\r\nsearchTerms   <- c(\"people analytics\", \"hr analytics\")\r\nlocation      <- \"\" # global\r\ntime          <- \"2007-01-01 2022-07-31\"\r\nsource         <- \"web\"\r\n\r\n# getting Google Trends data\r\ngtrendsResult <- gtrendsR::gtrends(\r\n    keyword = searchTerms, \r\n    geo     = location, \r\n    time    = time,\r\n    gprop   = source\r\n    )\r\n\r\n# cleaning data\r\ngtrendsResultDf <- gtrendsResult %>%\r\n    purrr::pluck(\"interest_over_time\") %>%\r\n    dplyr::select(date, hits, keyword) %>%\r\n    dplyr::mutate(date = lubridate::ymd(date))\r\n\r\n\r\n\r\nTowards the end of the time series, somewhere between September 2019 and March 2020, it seems that the trend stops increasing and starts to stagnate, except for the very last part of the graph, which shows a sharp increase in searches for both terms, but this may just be the result of the improved data collection system from January 2022 onwards (as indicated by the vertical line in the graph with a note). Thus, the data seems to suggest the opposite of what we would expect if a pandemic were to have a positive effect on interest in people analytics.\r\nHowever, after combining the results for the two search terms and plotting them on a graph together with the unadjusted regression trend lines, the resulting picture gives a slightly different impression. There appears to be little to no decline in interest in people analytics immediately after the pandemic outbreak, but a steeper slope of change after the pandemic.\r\n\r\n\r\nShow code\r\n\r\n# normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\ngtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100\r\n  ) %>%\r\n    # creating new pandemic variable \r\n  dplyr::mutate(\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ \"After the pandemic outbreak\",\r\n      TRUE ~ \"Before the pandemic outbreak\"\r\n    )\r\n  )%>%\r\n  ggplot2::ggplot(aes(x = date, y = interestInPeopleAnalytics, color = pandemic)) +\r\n  ggplot2::geom_point() +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggplot2::scale_x_date(breaks = \"1 year\", date_labels = \"%Y\") +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"Interest in people analytics\",\r\n    title = \"Interest in people analytics before and after the pandemic outbreak\",\r\n    caption = \"The solid lines represent unadjusted regression model trend lines before and after the pandemic outbreak.\"\r\n  ) +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nTo make the picture a little bit clearer, let us take the help of inferential statistics to answer the question we are interested in. The ideal analytical tool for our use case is a segmented regression analysis of interrupted time series that enables estimation of the changes in levels and trends of search interest before and after after a known ‚Äòintervention‚Äô or ‚Äòinterruption‚Äô (i.e., a change that could potentially affect the outcome variable). It does so by segmenting the time series data into different periods based on the known interruption points and modeling these segments separately. The model used has the following general structure:\r\n\\[Y_{t} = Œ≤_{0} + Œ≤_{1}*time_{t} + Œ≤_{2}*intervention_{t} + Œ≤_{3}*time after intervention_{t} + e_{t}\\]\r\nThe Œ≤0 coefficient estimates the baseline level of the outcome variable at time zero; Œ≤1 coefficient estimates the change in the mean of the outcome variable that occurs with each unit of time before the intervention (i.e.¬†the baseline trend); Œ≤2 coefficient estimates the level change in the mean of the outcome variable immediately after the intervention (i.e.¬†from the end of the preceding segment); and Œ≤3 estimates the change in the trend in the mean of the outcome variable per unit of time after the intervention, compared with the trend before the intervention (thus, the sum of Œ≤1 and Œ≤3 equals to the post-intervention slope). For a better understanding of the model, take a look at the illustrative chart taken from Turner et al.¬†(2021) below.\r\n\r\nSince we are dealing with correlated and truncated data, we should also include two additional terms in our model, an autocorrelation term and a truncation term, to handle these specific properties of our data.\r\nData preparation\r\nBut first, let‚Äôs prepare the data we will need for this type of analysis. Specifically, we will need the following five variables:\r\nsearch interest in people analytics ‚Äì numerical variable representing search interest in people analytics relative to the highest point on the chart for the given region and time; this variable is truncated within the interval between values of 0 and 100; a value of 100 is the peak popularity for the term; a value of 50 means that the term is half as popular; a score of 0 means that there was not enough data for this term; search interest for two monitored terms (‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù) was combined by simple summation and then normalized to a range of 0 to 100; this variable serves as a dependent (criterion) variable;\r\nelapsed time ‚Äì numerical variable representing the number of months that elapsed from the beginning of the time series; this variable enables estimation of the size and direction of the overall trend in the data before the intervention;\r\npandemic ‚Äì dichotomic variable indicating the presence/absence of pandemic; as already mentioned above, for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020; this variable enables estimation of the level change in the interest in people analytics immediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak ‚Äì numerical variable representing the number of months that elapsed from the beginning of pandemic; this variable enables estimation of the change in the trend in the interest in people analytics after the outbreak of pandemic;\r\nmonth ‚Äì categorical variable representing specific month within a year; this variable enables controlling for the effect of seasonality.\r\n\r\n\r\nShow code\r\n\r\n# munging data\r\nmydata <- gtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100,\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), ordered = FALSE)\r\n  ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(date) %>%\r\n    # creating new variables elapsed time, pandemic, and time elapsed after pandemic outbreak\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ 1,\r\n      TRUE ~ 0\r\n    ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic),\r\n    pandemic = as.factor(ifelse(pandemic == 1, \"After the pandemic outbreak\", \"Before the pandemic outbreak\"))\r\n  ) %>%\r\n  # final selection of variables\r\n  dplyr::select(\r\n    date, interestInPeopleAnalytics, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n  )\r\n\r\n\r\nHere is a table with the resulting data we will use for our analysis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making more user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nModeling\r\nWe can now fit the model to the data and test what it tells us about the impact of the pandemic on people‚Äôs search interest in people analytics. We will use the brms r package for this, which allows us to make inferences about the model parameters within a Bayesian inferential framework. For this reason, we must also specify some additional parameters (e.g.¬†chains, iter or warmup) of the Markov Chain Monte Carlo (MCMC) algorithm that generates posterior samples of our model‚Äôs parameters.\r\nThe Bayesian framework also allows us to specify priors for the estimated parameters and use them to incorporate our domain knowledge into the analysis. The specified priors are important for both parameter estimation and hypothesis testing because they define our initial information state before we consider our data. Here, we will use relatively broad, uninformative, and only slightly regularizing priors (that is, the inference results will be very close to the results of standard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors \r\npriors <- c(brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model\r\nmodel <- brms::brm(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  seed = 1234,\r\n  sample_prior = TRUE, \r\n  control = list(adapt_delta = 0.9),\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n\r\nBefore making any inferences, we should perform several validation checks to ensure that the mechanics of the MCMC algorithm worked well and that we can use the generated posterior samples to make inferences about the parameters of our model. There are many ways to do this, but here we will only use a visual check of the MCMC chains. We want the plots of these chains to look like a hairy caterpillar, indicating the convergence of the underlying Markov chain to stationarity and the convergence of the Monte Carlo estimates to population quantities, respectively. As can be seen in the graph below, we can observe the characteristics we are looking for in the MCMC chains described above. (For additional MCMC diagnostics procedures, see for example Bayesian Notes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains\r\nbayesplot::mcmc_trace(\r\n  model,\r\n  facet_args = list(nrow = 6)\r\n) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the model parameters\"\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nIt is also important to check how well the model fits the data. To do this, we can use the posterior predictive check, which uses a specified number of selected posterior values of the model parameters to show how well the fitted model predicts the observed data. In the graph below we see that the model fits the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model fit\r\n# specifying the number of samples\r\nndraws = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  ndraws = ndraws\r\n) +\r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive check (using {ndraws} samples)\")\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\nWhat do the model and data tell us?\r\nWe can now use the parameters of our model to obtain information about our main question. Specifically, we are interested in the value of the coefficient of the pandemic and the time after pandemic terms in our model. They represent how much and in what direction the search interest in people analytics changed after the pandemic outbreak.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the fitted model\r\nsummary(model)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: mydata (Number of observations: 187) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.67      0.07     0.53     0.82 1.00    36988    34009\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                           -19.75      7.34   -35.53\r\nelapsedTime                           0.47      0.04     0.42\r\npandemicBeforethepandemicoutbreak    10.96      4.99     1.62\r\nelapsedTimeAfterPandemic              0.65      0.29     0.10\r\nmonthFeb                              1.74      1.43    -1.08\r\nmonthMar                              2.77      1.87    -0.93\r\nmonthApr                              4.42      2.09     0.30\r\nmonthMay                              2.61      2.19    -1.69\r\nmonthJun                              0.60      2.25    -3.80\r\nmonthJul                              0.35      2.28    -4.16\r\nmonthAug                             -2.33      2.28    -6.83\r\nmonthSep                              1.67      2.22    -2.69\r\nmonthOct                             -0.56      2.10    -4.67\r\nmonthNov                             -1.01      1.88    -4.71\r\nmonthDec                             -6.14      1.44    -8.96\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            -6.98 1.00    30645    29199\r\nelapsedTime                           0.56 1.00    44619    28942\r\npandemicBeforethepandemicoutbreak    21.28 1.00    44289    44331\r\nelapsedTimeAfterPandemic              1.26 1.00    54010    46306\r\nmonthFeb                              4.55 1.00    37661    48486\r\nmonthMar                              6.43 1.00    26135    39337\r\nmonthApr                              8.51 1.00    22840    37029\r\nmonthMay                              6.91 1.00    21100    35358\r\nmonthJun                              5.03 1.00    20495    36016\r\nmonthJul                              4.81 1.00    20712    37600\r\nmonthAug                              2.16 1.00    21240    37079\r\nmonthSep                              6.07 1.00    22312    36687\r\nmonthOct                              3.56 1.00    24241    39183\r\nmonthNov                              2.68 1.00    27803    43248\r\nmonthDec                             -3.31 1.00    40223    50422\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     5.02      0.29     4.49     5.63 1.00    66571    52402\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nThe following graph shows the posterior distribution of the pandemic parameter. We can see that it is on the right-hand side of the zero value, which supports the claim that there is an effect of the pandemic on people‚Äôs interest in people analytics immediately after the pandemic outbreak; however, it is on the opposite side of the zero value than we would expect if the pandemic were to have a positive effect on people‚Äôs interest in people analytics. Thus, this suggests that immediately after the pandemic outbreak, interest in people analysis declined slightly (somewhere between ~1 and ~20 points, as indicated by the 95% credible interval).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(tidybayes)\r\n\r\nparamVizBeforethepandemicoutbreak <- model %>%\r\n  tidybayes::gather_draws(b_pandemicBeforethepandemicoutbreak) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizBeforethepandemicoutbreak$value)\r\n\r\nparamVizBeforethepandemicoutbreak <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_pandemicBeforethepandemicoutbreak parameter \r\nggplot2::ggplot(paramVizBeforethepandemicoutbreak, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\",\r\n    y = \"Density\",\r\n    x = \"pandemicBeforethepandemicoutbreak\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(posterior)\r\n\r\n# extracting posterior samples\r\nsamplesBeforethepandemicoutbreak <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being higher than 0\r\nsum(samplesBeforethepandemicoutbreak$b_pandemicBeforethepandemicoutbreak > 0) / nrow(samplesBeforethepandemicoutbreak)\r\n\r\n[1] 0.9902778\r\n\r\nNow let‚Äôs check the second key parameter of our model, the time after the pandemic term. In this case, the posterior distribution is again on the right-hand side of zero value, but now this result is consistent with the claim that there is a positive effect of the pandemic on people‚Äôs interest in people analytics, specifically in terms of the change in slope after the pandemic. Compared to the pre-pandemic trend, the post-pandemic trend is steeper by ~0 to ~1 point per month (as indicated by the 95% confidence interval). We should bear in mind, however, that this effect may in fact only be an artifact caused by the improved data collection system from January 2022, as mentioned at the very beginning of this post.\r\n\r\n\r\nShow code\r\n\r\nparamVizElapsedTimeAfterPandemic <- model %>%\r\n  tidybayes::gather_draws(b_elapsedTimeAfterPandemic) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizElapsedTimeAfterPandemic$value)\r\n\r\nparamVizElapsedTimeAfterPandemic <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_elapsedTimeAfterPandemic parameter \r\nggplot2::ggplot(paramVizElapsedTimeAfterPandemic, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\",\r\n    y = \"Density\",\r\n    x = \"elapsedTimeAfterPandemic\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamplesElapsedTimeAfterPandemic <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_elapsedTimeAfterPandemic coefficient being higher than 0\r\nsum(samplesElapsedTimeAfterPandemic$b_elapsedTimeAfterPandemic > 0) / nrow(samplesElapsedTimeAfterPandemic)\r\n\r\n[1] 0.9897361\r\n\r\nThe overall resulting picture thus partially supports the impression of many of my people analytics fellows about the growing importance of people analytics in HR and business leaders‚Äô decision making. However, given that the Google search interest in people analytics is a fairly distant proxy for its actual use in HR and business practice, we should take these results with a grain of salt and try to find other data sources that would support our results. For example, Frank Corrigan, head of analytics at Ponder, came up with the idea of analyzing changes in postings for people analytics job positions over time. A good inspiration for anyone willing to spend some time getting at and analyzing such data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-popularity-after-covid/./people-analytics.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/",
    "title": "People Analytics Challenge from Orgnostic: Plan for high growth",
    "description": "A brief summary of my participation in Orgnostic's People Analytics Challenge.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-19",
    "categories": [
      "recruitment channels",
      "workforce planning",
      "descriptive statistics",
      "r"
    ],
    "contents": "\r\nDuring the last few nights, I had the opportunity to participate in\r\nan interesting People Analytics Challenge from Orgnostic, a company providing people\r\nanalytics platform that links scattered HR and finance data, run surveys\r\non top, analyses the data, and get answers to the critical questions\r\nabout organizations and their employees.\r\nOut of three possible challenges, I chose one that was quite far from\r\nwhat I‚Äôm currently usually working on or what I‚Äôve worked on in the\r\npast. The chosen task was to analyze the effectiveness of\r\nrecruiting sources for a company that plans to double in size\r\nfrom its current 750+ employees.\r\nAlthough the dummy data provided was quite limited for obvious\r\nreasons and did not allow to answer all relevant questions (but you\r\ncould also use your own data which would not suffer from this\r\nshortcoming), after combining them and enriching them slightly based on\r\nrealistic assumptions, it was possible to arrive at quite interesting\r\ninsights and recommendations. See for yourself - the resulting\r\npresentation is attached to this post below.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it:\r\nDownload\r\nPDF.\r\n\r\n\r\n\r\nIf you‚Äôd like to look more into the guts of the analyses conducted,\r\nyou can find both the data provided by Orgnostic and the R script used\r\nto analyze it on my\r\nGitHub page.\r\nP.S. Oh, I almost forgot - there are exciting prizes in the form of\r\ntickets to HRtechX Copenhagen or HR Technology Conference in Las Vegas.\r\nSo please keep your fingers crossed for me ü§ûüòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/./recruitmentChannels.png",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {},
    "preview_width": 864,
    "preview_height": 515
  },
  {
    "path": "posts/2022-06-11-visual-inference-statistics/",
    "title": "Visual statistical inference",
    "description": "Visual statistical inference represents a valid alternative to standard statistical inference, and as a by-product it also helps with building intuition about the difference between signal and noise. Give it a try.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-06-13",
    "categories": [
      "statistical inference",
      "visual statistical inference",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nDo you have any experience with visual\r\nstatistical inference? Not only is it a valid alternative\r\nto standard statistical inference, but as a by-product it helps with\r\nbuilding intuition about the\r\ndifference between signal and noise.\r\nThe basis of the method is a so-called lineup\r\nprotocol that places a graph of the actual data between arrays\r\nof graphs of null data that are generated by a method consistent with\r\nthe null hypothesis. The lineup is shown to one or more observers who\r\nare asked to identify the graph that differs. If an observer can pick\r\nout the actual data as different from the others, this gives weight to\r\nthe statistical significance of the pattern in the graph.\r\nBecause people usually have a hard time recognizing randomness and\r\ntend to see patterns even where there are none, there is also a\r\nso-called Rorschach protocol that only displays graphs\r\ncreated with null datasets and which is used by observers to calibrate\r\ntheir eyes for variation due to sampling.\r\nYou can try it for yourself in the graphs below, which show the\r\nrelationship between two variables from my current area of work\r\n(collaboration\r\nanalytics), namely between time\r\navailable for focused work and the use of timeboxing\r\n(productivity enhancing technique of assigning a fixed unit of time to\r\nan activity within which a planned activity takes place).\r\nUse the first array of graphs (the Rorschach protocol) to calibrate\r\nyour eye for randomness and then try to identify the actual data in the\r\nsecond array of graphs (the lineup protocol). What‚Äôs your guess? Which\r\ngraph matches the actual data (1-20) and what relationship do you see in\r\nthe data? You can give your guess in the comments.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(nullabor)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./visualInferenceData.RDS\")\r\n\r\n# lineup protocol\r\nset.seed(1234)\r\nd <- lineup(null_permute(\"propBlockedTime\"), mydata)\r\n\r\nlplot <- ggplot(data=d, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"LINEUP PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n\r\n# Rorschach protocol\r\ndr <- rorschach(null_permute(\"propBlockedTime\"), mydata, n = 20, p = 0)\r\n\r\nrplot <- ggplot(data=dr, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"RORSCHACH PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n# combining plots\r\nfplot <- rplot / lplot\r\n\r\nprint(fplot)\r\n\r\n\r\n\r\n\r\nHere‚Äôs a check on your guess. The actual data are shown in chart #\r\n12. As you can see in the chart below, the relationship between time for\r\nfocused work and the use of timeboxing is slightly negative, which makes\r\npretty good sense, because people who have enough time for focused work\r\nusually don‚Äôt have such a strong need to block out time for focused work\r\nin their calendar.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot(aes(x = focusRate, y = propBlockedTime)) +\r\n  geom_point(size = 2, alpha = 0.5) +\r\n  geom_smooth(method = \"lm\", se = F) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format()) +\r\n  labs(\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\",\r\n    title = \"RELATIONSHIP BETWEEN FOCUSED TIME AND TIMEBOXING USAGE\",\r\n    caption = \"\\nThe blue line in the graph represents the linear regression line.\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can also test the relationship between time for focused work and\r\nthe use of timeboxing more formally by fitting a Bayesian\r\nbeta regression model to the data. As you can see in the summary\r\ntables and charts below, the null value is safely outside the 95%\r\ncredible interval of the mean of the focus rate parameter, and the\r\nmarginal effect of focus rate clearly shows its negative relationship\r\nwith the predicted proportion of working time blocked in the calendar.\r\nNote also that the relationship is non-linear, i.e.¬†the marginal effect\r\nof the focus rate is different depending on its level.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(bayesplot)\r\nlibrary(tidybayes) \r\nlibrary(ggdist)       \r\n\r\n# fitting Bayesian beta regression model\r\nmodel <- brms::brm(\r\n  bf(\r\n    propBlockedTime ~ focusRate,\r\n    phi ~ focusRate\r\n    ),\r\n  data=mydata,\r\n  family= Beta(),\r\n  seed = 1234,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  chains = 4,\r\n  cores = 6,\r\n  control = list(\r\n    adapt_delta = 0.9,\r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model\r\nsummary(model)\r\n\r\n\r\n Family: beta \r\n  Links: mu = logit; phi = log \r\nFormula: propBlockedTime ~ focusRate \r\n         phi ~ focusRate\r\n   Data: mydata (Number of observations: 306) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nPopulation-Level Effects: \r\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept        -0.85      0.25    -1.33    -0.35 1.00    61653\r\nphi_Intercept     2.09      0.39     1.30     2.84 1.00    59323\r\nfocusRate        -0.02      0.00    -0.03    -0.02 1.00    57209\r\nphi_focusRate     0.01      0.01    -0.01     0.02 1.00    56423\r\n              Tail_ESS\r\nIntercept        54084\r\nphi_Intercept    51459\r\nfocusRate        55167\r\nphi_focusRate    51573\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the estimated parameters\r\nposterior_beta <- model %>% \r\n  gather_draws(`b_.*`, regex = TRUE) %>% \r\n  mutate(component = ifelse(str_detect(.variable, \"phi_\"), \"Precision\", \"Mean\"),\r\n         intercept = str_detect(.variable, \"Intercept\")) %>%\r\n  filter(intercept == FALSE)\r\n\r\nggplot(posterior_beta, aes(x = .value, y = fct_rev(.variable), fill = component)) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  stat_halfeye(aes(slab_alpha = intercept), \r\n               .width = c(0.8, 0.95), point_interval = \"median_hdi\") +\r\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\r\n  scale_slab_alpha_discrete(range = c(1, 0.4)) +\r\n  guides(fill = \"none\", slab_alpha = \"none\") +\r\n  labs(\r\n    x = \"COEFFICIENT\", \r\n    y = \"\",\r\n    title = \"POSTERIOR DISTRIBUTION OF THE ESTIMATED PARAMETERS\",\r\n    caption = \"\\n80% and 95% credible intervals shown in black\") +\r\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing marginal effect of the focus rate\r\nmodel_pred <- model %>% \r\n  epred_draws(newdata = expand_grid(focusRate = seq(30, 100, by = 1)))\r\n\r\nggplot(model_pred , aes(x = focusRate, y = .epred)) +\r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Purples\") +\r\n  labs(x = \"FOCUS RATE\", \r\n       y = \"PREDICTED PROPORTION OF BLOCKED WORKING TIME\",\r\n       fill = \"Credible interval\",\r\n       title = \"MARGINAL EFFECT OF THE FOCUS RATE\"\r\n       ) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format(accuracy = 1)) +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nIf you would like to apply the visual statistical inference approach\r\nto your own data, you can easily do so using the nullabor R\r\npackage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-11-visual-inference-statistics/./lineup.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-06-standard-and-trend-predictors/",
    "title": "Standard vs. trend predictors",
    "description": "When modeling a phenomenon, one usually can't get by with just raw data but must use one's domain knowledge to select and transform the most relevant variables from raw data to be able to successfully grasp regularities in the domain of one's interest. Let's look at one simple example of such feature engineering from the domain of collaboration analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-02-06",
    "categories": [
      "predictive analytics",
      "feature engineering",
      "employee experience",
      "employee engagement",
      "employee satisfaction",
      "employee attrition",
      "collaboration data"
    ],
    "contents": "\r\nAs predictive analytics practitioners know, trend variables can be more useful in many situations for predicting certain phenomena than standard variables that simply refer to the state of the world at a particular time point or period.\r\nFor example, when trying to predict employee attrition, a downward trend in the use of a piece of company equipment, such as a printer/copier, over the 6 months prior to the resignation may be more predictive than the absolute number of pages printed/copied over the same period.\r\nThis is also true for our domain we focus on at Time is Ltd. where, among other things, we try to use collaboration data to infer some aspects of employee experience.\r\nTo illustrate, the attached chart shows the distribution of the typical daily amount of time people spend by collaboration for two groups of employees - one with above-average scores and the other with below-average scores on the employee satisfaction survey. As you can see, there is little difference between the two groups in terms of the average daily amount of time people spend by collaboration over the last six months (see the density plots), but there is a fairly clear difference in the trend of this metric over the same period, suggesting that less satisfied employees may be suffering from increasing collaboration overload (see the line charts with trend lines for individual employees and the estimated overall linear trend).\r\n\r\n\r\n\r\nDo you have a similar experience with or just a strong hunch about other metrics in your area of expertise? Let me know in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-06-standard-and-trend-predictors/./trendGraph.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-30-meeting-planning/",
    "title": "Fighting meeting overload",
    "description": "One of the most effective ways to fight meeting overload is to better plan meetings in terms of the time we spend in them. Let's look at how data can tell us how much room for improvement we have in this area.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-30",
    "categories": [
      "meeting planning",
      "meeting effectiveness"
    ],
    "contents": "\r\nOne of the recommended ways to save time in meetings is to plan them better in terms of the time we allocate for them. As in other activities, even here the well-known Parkinson‚Äôs rule applies that ‚Äúwork expands so as to fill the time available for its completion.‚Äù When this is combined with the automatic use of default meeting lengths, it leads to spending more time in meetings than is necessary.\r\nFor this reason, Steven Rogelberg suggests in his book The Surprising Science of Meetings that all meeting times should be reduced by 5-10 percent by default.\r\nTo assess whether you have room for improvement in this regard, it is useful to compare actual and planned meeting lengths. For illustration, the attached chart shows the distribution of the typical differences between actual and planned meeting lengths for each of our teams organizing online meetings over the course of a year. It clearly shows that a large proportion of teams are organizing meetings longer than necessary, by an average of 4 minutes. So in the case of our company Time is Ltd., there definitely seems to be room for implementing Steven Rogelberg‚Äôs suggestion.\r\n\r\n\r\nShow code\r\n\r\n# uploading package\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata <- readRDS(\"./tardiness.rds\")\r\n\r\n# preparing data for density plot\r\nmydata <- with(density(data %>% pull(tardiness)), data.frame(x, y)) %>%\r\n  mutate(col = ifelse(x >= 0, \"A\", \"B\"))\r\n\r\n# visualizing data\r\nmydata %>%\r\n  ggplot() +\r\n  geom_rug(data = filter(data, tardiness >=0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 1, position = \"identity\") +\r\n  geom_rug(data = filter(data, tardiness <0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 0.5, position = \"identity\") +\r\n  \r\n  geom_area(data = filter(mydata, col == 'A'), aes(x = x, y = y), fill = '#4d009d', alpha = 1) + \r\n  geom_area(data = filter(mydata, col == 'B'), aes(x = x, y = y),  fill = '#4d009d', alpha = 0.5) +\r\n  \r\n  geom_label(aes( x=-15.25, y=0.06, label=\" Shorter than planned \"), fill = \"#a67fce\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n  geom_label(aes( x=10.5, y=0.04, label=\" Longer than planned \"), fill = \"#4d009d\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n\r\n  labs(\r\n    x = \"TYPICAL DIFFERENCE BETWEEN ACTUAL AND PLANNED LENGTHS OF MEETINGS\",\r\n    y = \"DENSITY\",\r\n    title = \"Do our online meetings end on time?\",\r\n    subtitle = str_glue(\"On average, our teams organize online meetings {round(abs(mean(data$tardiness)),1)} minutes longer than necessary.\"),\r\n    caption = \"\\nPositive values indicate that online meetings tend to overrun; negative values indicate that online meetings are planned longer than they need to be.\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::label_number(suffix = \" min\"), breaks =  seq(-40, 20, 10)) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.2,.98),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.size = unit(0, \"cm\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt is also worth noting the reverse situation where meetings take longer than planned, as a late end to one meeting becomes a late start to the next meeting.\r\nHow do you feel about finishing meetings too early or too late? Are both similarly unpleasant for you? And isn‚Äôt actually having a shorter meeting than planned something positive?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-30-meeting-planning/./meetingPlanning.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-18-probability-words/",
    "title": "How do we perceive probability words?",
    "description": "Have you ever wondered exactly how much chance of success people give a project when they say they believe in it? If so, then you may find this post useful, as it attempts to answer that question at least in part with data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-18",
    "categories": [
      "probability",
      "perception"
    ],
    "contents": "\r\nNowadays - probably also due to the Covid pandemic and the associated predictions - we are more and more frequently encountering various probabilistic statements, but these are often expressed not in terms of precise numerical probabilities, but in terms of relatively vague probability words such as ‚Äúprobably‚Äù, ‚Äúmaybe‚Äù, ‚Äúunlikely‚Äù, etc.\r\nSince people may imagine different probabilities under these words, it would be useful to have something like a glossary to help us decipher these words and indicate what people usually mean when they use them.\r\nFortunately, there are some studies that examine what numerical probabilities people typically associate with probability words.\r\nFor this purpose, I used a collection of 123 responses to the Wade Fagen-Ulmschneider‚Äôs internet survey and created two similar graphs based on them. The first shows the distribution of the numerical probabilities that people associate with each word, and these are sorted in the graph by the median value of the corresponding probability in descending order. The second graph then differs only in that the words are sorted by the size of the interquartile range in descending order.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggridges)\r\nlibrary(ggpubr)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./survey-results.csv\")\r\n\r\n# getting ordered list of words based on the median value of corresponding probabilities\r\nwordsMedian <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(median = median(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, median)\r\n      )\r\n  \r\nlevelsMedian <- levels(wordsMedian$word)\r\n\r\n# getting ordered list of words based on the IQR of corresponding probabilities\r\nwordsVariability <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(sd = IQR(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, sd)\r\n  )\r\n\r\nlevelsVariability <- levels(wordsVariability$word)\r\n\r\n# graph 1\r\ng1 <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  mutate(word = factor(word, levels = levelsMedian, ordered = TRUE)) %>%\r\n  ggplot(aes(x = probability, y = word)) + \r\n  geom_density_ridges(\r\n    fill = \"#4d009d\",\r\n    alpha = 0.85,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 1, height = 0),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n    quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n  labs(\r\n    fill = \"Trend size\",\r\n    x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n    y = \"\",\r\n    title = \"How do people perceive probability words?\",\r\n    caption = \"\\nThe words are sorted by the median value of the corresponding probability in descending order.\\nThe white dashed lines represent the median values.\\nSource: A collection of 123 responses to an internet survey by Wade Fagen-Ulmschneider.\"\r\n  ) +\r\n  scale_fill_gradient2(\r\n    low = \"red\",\r\n    mid = \"white\",\r\n    high = \"blue\",\r\n    midpoint = 0,\r\n    space = \"Lab\"\r\n  ) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.text.x = element_text(),\r\n        legend.position = \"right\",\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  ) +\r\n  guides(\r\n    fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n  )\r\n\r\n\r\n# graph 2\r\ng2 <- data %>%\r\n    select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n    pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n    mutate(word = factor(word, levels = levelsVariability, ordered = TRUE)) %>%\r\n    ggplot(aes(x = probability, y = word)) + \r\n    geom_density_ridges(\r\n      fill = \"#4d009d\",\r\n      alpha = 0.85,\r\n      scale = 1,\r\n      jittered_points = TRUE,\r\n      position = position_points_jitter(width = 1, height = 0),\r\n      point_shape = '|', point_size = 1, point_alpha = 1, \r\n      quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n      quantile_fun=function(x,...)median(x)\r\n    ) +\r\n    scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n    labs(\r\n      fill = \"Trend size\",\r\n      x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n      y = \"\",\r\n      title = \"What probability words are the most noisy?\",\r\n      caption = \"\\nThe words are sorted by the size of the interquartile range in descending order.\\n\\n\"\r\n    ) +\r\n    scale_fill_gradient2(\r\n      low = \"red\",\r\n      mid = \"white\",\r\n      high = \"blue\",\r\n      midpoint = 0,\r\n      space = \"Lab\"\r\n    ) +\r\n    theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n          plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n          plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n          axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n          axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n          legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n          legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n          axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n          axis.text.x = element_text(),\r\n          legend.position = \"right\",\r\n          axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n          axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n          panel.background = element_blank(),\r\n          panel.grid.major.y = element_blank(),\r\n          panel.grid.major.x = element_blank(),\r\n          panel.grid.minor = element_blank(),\r\n          axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n          axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n          plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n          plot.title.position = \"plot\",\r\n          plot.caption.position =  \"plot\"\r\n    ) +\r\n    guides(\r\n      fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n    )\r\n\r\n# combining graphs\r\nggarrange(g1, g2, ncol = 2, nrow = 1)\r\n\r\n\r\n\r\n\r\nThe first graph can thus help us to use the right word, which in the mind of the other person is most likely to evoke the same probability we want to express. The second graph can then help us to identify the most noisy probability words, for which we will know to ask for a more precise definition because we will be aware that people may imagine very different probabilities under these words.\r\nHow about your perception of probability words? Is there anything in the graphs that surprised you? Would you expect differences between cultures? And what about other demographics? Btw, the original dataset also includes some demographic variables such as age, gender, and education level, so I‚Äôll probably come back to this question in a future post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-18-probability-words/./Probability-Word-Cards.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/",
    "title": "Hofstede's theory of cultural dimensions",
    "description": "Cultural diversity brings both positive effects and some challenges. To deal with the latter, it is useful to have some kind of map to help people better navigate the cultural specificities of people from different societies. Hofstede's theory of cultural dimensions is useful for such a purpose. Let's check how dis/similar countries are on these cultural dimensions with a simple app that could help us better understand, manage and appreciate cultural differences a little better.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-03",
    "categories": [
      "cultural diversity",
      "cultural awareness",
      "international management",
      "crosscultural communication",
      "shiny app"
    ],
    "contents": "\r\nOne of the advantages of switch to remote working is that companies can expand the pool of talent from which they choose their employees, without being too constrained by country or even continental boundaries. However, the resulting cultural diversity can bring not only positive effects (e.g.¬†a broader set of perspectives, a more diverse skill base, local market knowledge and insight, better creativity and innovation, etc.) but also some challenges (e.g.¬†risk of prejudice or negative cultural stereotypes, misinterpretation of communication, conflicting work styles, different understanding of professional etiquette, etc.).\r\nBetter knowledge and awareness of the cultural specificities of the societies from which people come is one way of dealing with these challenges. In this respect, Hofstede‚Äôs theory of cultural dimensions may be useful to us. Just as Big-5 theory facilitates our understanding of other people‚Äôs personalities, Hofstede‚Äôs theory facilitates our understanding of their cultural background by describing their social values and releated behaviors through the following six cultural dimensions:\r\n\r\nImage source: https://corporatefinanceinstitute.com/resources/knowledge/other/hofstedes-cultural-dimensions-theory/\r\nYou can easily check how countries are doing on these six dimensions on the Hofstede Insights website. To make it easier to compare cultural differences/similarities between countries, I built a simple app that projects the cultural profiles of countries into 2D space using dimensionality reduction technique called UMAP (Uniform manifold approximation and projection). By selecting a specific cultural dimension, you can see how it is distributed across countries and continents. In addition, you can select some specific countries in the comparator and compare them across all six cultural dimensions.\r\nCheck it out here ‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/Hofstede_Cultural_Dimensions/\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/./six-dimensions-hofstedes-cultural-dimensions-theory.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-19-makers-and-managers-schedule/",
    "title": "Makers' schedule and managers' schedule in collaboration data",
    "description": "Many of us have probably already heard of Paul Graham's two types of schedules - one that meets the needs of makers and one that meets the needs of managers. But can these two types of schedules be found in any real collaborative data? Let's find out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-19",
    "categories": [
      "schedule types",
      "makers and managers",
      "collaboration data"
    ],
    "contents": "\r\nI am sure that many of you have heard of the two types of schedules\r\nas described by Paul Graham in his famous article Maker‚Äôs Schedule,\r\nManager‚Äôs Schedule:\r\nThe manager‚Äôs schedule [is] embodied in the traditional\r\nappointment book, with each day cut into one hour intervals. [By]\r\ndefault you change what you‚Äôre doing every hour. But [makers] generally\r\nprefer to use time in units of half a day at least. You can‚Äôt write or\r\nprogram well in units of an hour. That‚Äôs barely enough time to get\r\nstarted. When you‚Äôre operating on the maker‚Äôs schedule, meetings are a\r\ndisaster. A single meeting can blow a whole afternoon, by breaking it\r\ninto two pieces each too small to do anything hard in. Plus you have to\r\nremember to go to the meeting.\r\nI recently realized that I have only seen illustrative pictures on\r\nthis topic so far, but not any real data. This inspired me to look at\r\nour own collaboration data at Time\r\nIs Ltd. and see if these two schedule categories can be found\r\nthere.\r\nWhen I contrasted the data on the average number of meetings per day\r\nand the average time between meetings, there were indeed categories of\r\npeople who either have relatively more meetings with relatively shorter\r\nbreaks (managers), or have relatively fewer meetings with\r\nrelatively longer breaks (makers).\r\n\r\nBut beyond that, there was a third type, which I called\r\nbatchers - they have relatively fewer meetings with relatively\r\nshorter breaks, which is a good strategy when you have to be both\r\nmanager and creator, which may be the case for more and more people as\r\nwe move to remote working.\r\nIn the charts below you can see how typical monthly calendars of\r\nthese three types of schedulers look like.\r\n\r\n\r\nWhat we cannot see in our own data, but could theoretically be there,\r\nis a fourth category I call overtimers, who have relatively\r\nmore meetings but manage to keep relatively longer breaks in between.\r\nHowever, this can only be achieved by making the meetings more spread\r\nout over time, i.e.¬†at the cost of working after hours.\r\nHow about you? Where would you fit in? And is there anyone among you\r\nwho would fit into the fourth, missing category?\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-19-makers-and-managers-schedule/./maker-schedule-vs-manager-schedule.jpg",
    "last_modified": "2023-09-16T13:24:32+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-16-linkedin-connections-analysis/",
    "title": "R Shiny app for LinkedIn connections analysis",
    "description": "An introduction of a simple R Shiny application for analysing LinkedIn connections.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-16",
    "categories": [
      "linkedin",
      "external networks",
      "social network analysis",
      "shiny app"
    ],
    "contents": "\r\nIf you like to use the end of the year as an opportunity for deeper self-reflection, you might enjoy this simple app I have put together over the past weekend.\r\nOnce you upload your LinkedIn connections data to the app (you can easily download the data by following the instructions in the app or in this video), it automatically generates basic descriptive statistics about your LinkedIn connections:\r\nCumulative number of connections over time\r\nNumber of established connections by years, months, and days of the week\r\nTop N companies by the number of established connections\r\nTop N positions by their frequency among your connections (based on whole position titles, bigrams and single words)\r\nProportion of connections by their gender (based on your connections‚Äô first name)\r\n\r\n\r\nYour browser does not support the video tag.\r\n\r\n\r\n\r\nUnfortunately, since there is no information about your connections‚Äô connections in the data, the app cannot perform more advanced SNA-type of analyses on it. Still, I think you may find some of the statistics useful, or at least interesting and entertaining.\r\nYou can take it as a kind of Christmas gift for my fellow LinkedIn users. Enjoy exploring your connections! And if you‚Äôd like to explore and better manage also your company‚Äôs internal collaboration networks, then check out what we do at Time is Ltd.\r\nP.S. The data you upload is not permanently stored anywhere. The app runs on the shinyapps.io server. If you don‚Äôt want to upload your own data, but would still like to see what the analysis output looks like, you can download and then upload ready-made sample data from the app.\r\nP.P.S. Big thanks to Sebastian Vorac for bringing me to this idea and for UX review. Any remaining errors are, of course, mine alone.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-16-linkedin-connections-analysis/./linkedinLogo.png",
    "last_modified": "2024-04-29T10:38:26+02:00",
    "input_file": {},
    "preview_width": 3753,
    "preview_height": 2352
  },
  {
    "path": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/",
    "title": "Overview of predictors of voluntary employee turnover",
    "description": "An introduction of a simple R Shiny application to facilitate extraction and digestion of information from meta-analysis of predictors of voluntary employee turnover.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-12",
    "categories": [
      "great resignation",
      "employee turnover",
      "turnover predictors",
      "meta-analysis",
      "shiny app"
    ],
    "contents": "\r\nAlthough the ‚ÄòGreat Resignation‚Äô in some parts of the world may be due in no small part to factors specific to the COVID-19 pandemic, it is still useful in this context to draw on the extensive research on employee turnover carried out in the run-up to the pandemic.\r\nA useful overview of such findings is provided, for example, by a 2017 meta-analysis by Rubenstein et al.¬†that summarizes the significance of 57 predictors of voluntary turnover from 9 different domains based on 316 studies from 1975 to 2016 involving more than 300,000 people.\r\nTo make it easier to assimilate these findings, I extracted them from the original article and visualized them in a simple shiny app that helps one to quickly explore and grasp the estimated magnitude, direction, and reliability of the effect of each factor, along with information on the degree of their actionability. The last feature is based purely on my own judgement, so please take it with a grain of salt, or adjust it in your mind using your own judgement. Try it out and let me know if you find it useful.\r\n‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/voluntary_turnover_predictors/\r\n\r\nAnd here is the original research paper on which the shiny app is based.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/./great_resignation.png",
    "last_modified": "2023-09-16T13:24:31+02:00",
    "input_file": {},
    "preview_width": 2700,
    "preview_height": 1800
  },
  {
    "path": "posts/2021-01-29-paygap/",
    "title": "Firemn√≠ audit rozd√≠lu mezi platy mu≈æ≈Ø a ≈æen",
    "description": "Platov√° nerovnost mezi mu≈æi a ≈æenami nen√≠ pro firmy jen z√°le≈æitost√≠ etickou a pr√°vn√≠, ale tak√© marketingovou - m≈Ø≈æe m√≠t toti≈æ negativn√≠ dopad na jejich \"employer brand\" a atraktivitu coby zamƒõstnavatele. To znamen√°, ≈æe pokud firmy chtƒõj√≠ p≈ôil√°kat a tak√© si udr≈æet talentovan√© zamƒõstnance, mus√≠ b√Ωt schopny zajistit, ≈æe se u nich s mu≈æi a ≈æenami bude v tomto ohledu zach√°zet stejnƒõ. Prvn√≠m krokem k tomu je zjistit, jak velk√Ω je rozd√≠l mezi platy mu≈æ≈Ø a ≈æen ve firmƒõ a do jak√© m√≠ry ho lze vysvƒõtlit jin√Ωmi faktory ne≈æ je samotn√© pohlav√≠ zamƒõstnance. V tomto ƒçl√°nku demonstruji, jak takovou anal√Ωzu prov√©st s pomoc√≠ analytick√©ho n√°stroje R a dat, kter√° m√° vƒõt≈°ina firem bƒõ≈ænƒõ k dispozici. Struƒçnƒõ se zmi≈àuji rovnƒõ≈æ o tom, jak√© mohou b√Ωt p≈ô√≠padn√© dal≈°√≠ kroky a doporuƒçen√≠ vypl√Ωvaj√≠c√≠ z v√Ωsledk≈Ø provedn√© anal√Ωzy.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-05-17",
    "categories": [
      "gender pay gap",
      "gender pay audit",
      "regression analysis",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nCo to je gender pay\r\ngap a jak ho mƒõ≈ôit?\r\nProƒç se\r\nzab√Ωvat platovou nerovnost√≠ ve Va≈°√≠ firmƒõ?\r\nAudit platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami\r\nPl√°n anal√Ωzy\r\nDostupn√°\r\ndata\r\nP≈ô√≠prava dat k anal√Ωze\r\nExploraƒçn√≠\r\nanal√Ωza\r\nStatistick√Ω model platov√©\r\nnerovnosti\r\nV√Ωsledky\r\nanal√Ωzy\r\nMo≈æn√© dal≈°√≠\r\nkroky\r\n\r\nCo to je gender pay\r\ngap a jak ho mƒõ≈ôit?\r\nGender pay gap (GPG), v p≈ôekladu genderov√° p≈ô√≠jmov√°\r\nnerovnost nebo p≈ô√≠jmov√° propast mezi mu≈æi a ≈æenami, oznaƒçuje\r\ntypick√Ω rozd√≠l mezi platov√Ωm ohodnocen√≠m pracuj√≠c√≠ch ≈æen a\r\nmu≈æ≈Ø. Obvykle je GPG vyjad≈ôov√°na procenty, pomƒõrem typick√©\r\nhrub√© hodinov√© (ƒçi roƒçn√≠) mzdy ≈æeny k typick√© mzdƒõ mu≈æe nebo pomƒõrem\r\nrozd√≠lu mezi typickou mzdou mu≈æ≈Ø a ≈æen v≈Øƒçi typick√© mzdƒõ mu≈æ≈Ø.\r\nBez ohledu na zp≈Øsob mƒõ≈ôen√≠ GPG, je dob≈ôe dolo≈æen√Ωm faktem, ≈æe ≈æeny\r\njsou obecnƒõ h≈Ø≈ôe placeny ne≈æ mu≈æi, jakkoli se tento rozd√≠l\r\npostupem ƒçasu zmen≈°uje. Rozd√≠ly v platech se p≈ôitom mohou v\r\njednotliv√Ωch zem√≠ch pomƒõrnƒõ dost li≈°it. N√°zornƒõ to ilustruje n√≠≈æe\r\nuveden√Ω graf, kter√Ω ukazuje v√Ωvoj (neadjustovan√©) GPG (definovan√© jako\r\npomƒõr rozd√≠lu medi√°nov√© mzdy zamƒõstnan√Ωch mu≈æ≈Ø a ≈æen a medi√°nov√© mzdy\r\nzamƒõstnan√Ωch mu≈æ≈Ø) v pr≈Øbƒõhu nƒõkolika minul√Ωch let v zem√≠ch OECD.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ngpgoecd <- readr::read_csv(\"./DP_LIVE_29012021212234147.csv\")\r\n\r\n# creating color palette\r\n# list of R color Brewer's palettes: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\r\nnbCols <- length(unique(gpgoecd$LOCATION))\r\nmyColors <- colorRampPalette(brewer.pal(8, \"Set1\"))(nbCols)\r\n\r\n# creating a graph\r\ng <- gpgoecd %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(LOCATION, Value), y = Value, fill = LOCATION,\r\n                      text = paste('Zemƒõ: ', LOCATION,\r\n                                 '<\/br><\/br>GPG: ', round(Value))))+\r\n  ggplot2::geom_col() +\r\n  ggplot2::facet_wrap(~ TIME, nrow = 4) +\r\n  ggplot2::labs(x = \"\",\r\n                y = \"GPG\",\r\n                title = \"Genderov√° p≈ô√≠jmov√° nerovnost v zem√≠ch OECD v letech 2016-2019\") +\r\n  ggthemes::theme_few() +\r\n  ggplot2::scale_fill_manual(values = myColors) +\r\n  ggplot2::theme(legend.position = \"\",\r\n                 legend.title = element_blank(),\r\n                 axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n\r\n\r\n# making the graph interactive\r\nplotly::ggplotly(\r\n  g, \r\n  width = 800,\r\n  height = 700,\r\n  tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nD≈Øvod≈Ø pro nevyv√°≈æenost p≈ô√≠jm≈Ø ≈æen a mu≈æ≈Ø pravdƒõpodobnƒõ existuje\r\nvƒõt≈°√≠ mno≈æstv√≠. Mezi nejƒçastƒõji uv√°dƒõn√© d≈Øvody pat≈ô√≠:\r\nDiskriminace na pracovi≈°ti. Stejn√© pr√°ce je\r\nodmƒõ≈àov√°na rozd√≠lnƒõ ƒçistƒõ na z√°kladƒõ pohlav√≠ pracovn√≠ka.\r\nGenderov√© stereotypy. P≈ôedsudky ohlednƒõ v√Ωkonnosti,\r\nschopnost√≠ a vlastnost√≠ ≈æen maj√≠ za n√°sledek oslaben√≠ jejich pozic a\r\nvytv√°≈ôen√≠ tzv. ‚Äûsklenƒõn√©ho stropu‚Äú, tj. neviditeln√© bari√©ry, na kterou\r\n≈æeny nar√°≈æ√≠ p≈ôi snaze o kari√©rn√≠ postup na l√©pe placen√© pozice.\r\nSegregace trhu. Odvƒõtv√≠, v nich≈æ je tradiƒçnƒõ\r\nzamƒõstn√°v√°no v√≠ce ≈æen ne≈æ mu≈æ≈Ø jako je zdravotnictv√≠, ≈°kolstv√≠ nebo\r\nve≈ôejn√° spr√°va, jsou spoleƒçnost√≠ vn√≠m√°na jako m√©nƒõ presti≈æn√≠, a tedy i\r\nh≈Ø≈ôe odmƒõ≈àov√°na.\r\nRodinn√Ω ≈æivot. ≈Ωeny vƒõt≈°inou nesou vƒõt≈°√≠ ƒç√°st\r\nz√°tƒõ≈æe spojen√© s rodinn√Ωm ≈æivotem (nap≈ô. p≈ôi odchodu na mate≈ôskou\r\ndovolenou, p≈ôi p√©ƒçi o nemocn√© dƒõti ƒçi jin√© ƒçleny dom√°cnosti), co≈æ jim\r\nv√Ωznamnƒõ stƒõ≈æuje jejich snahu o kari√©rn√≠ r≈Øst.\r\nV situaci, kdy p≈ôi reportov√°n√≠ GPG nerozli≈°ujeme mezi r≈Øzn√Ωmi d≈Øvody\r\npro platovou nerovnost, hovo≈ô√≠me o tzv. neadjustovan√©\r\nGPG. Pro pot≈ôeby firemn√≠ho auditu platov√© nerovnosti je v≈°ak\r\nd≈Øle≈æit√© zjistit rovnƒõ≈æ tzv. adjustovanou GPG, kter√° se\r\nsna≈æ√≠ vyj√°d≈ôit m√≠ru platov√© nerovnosti, kter√° je zp≈Øsobena ƒçistƒõ\r\npohlav√≠m zamƒõstnance. Zat√≠mco adjustovan√° GPG umo≈æ≈àuje firmƒõ\r\nidentifikovat mo≈ænou diskriminaci na pracovi≈°ti, neadjustovan√° GPG (p≈ôi\r\nneprok√°zan√© adjustovan√© GPG) m≈Ø≈æe poukazovat na existenci probl√©m≈Ø jako\r\njsou genderov√© stereotypy ƒçi nedostateƒçn√° podpora ≈æen p≈ôi snaze skloubit\r\nsv≈Øj osobn√≠ a profesn√≠ ≈æivot. Pro firmy je tak u≈æiteƒçn√© sledovat oba\r\nukazatele.\r\nProƒç se\r\nzab√Ωvat platovou nerovnost√≠ ve Va≈°√≠ firmƒõ?\r\nI kdybychom odhl√©dli od etick√Ωch ƒçi pr√°vn√≠ch aspekt≈Ø platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami, je ve velice pragmatick√©m z√°jmu ka≈æd√©\r\nfirmy, aby se tento druh nespravedlnosti v jej√≠m syst√©mu odmƒõ≈àov√°n√≠\r\nnevyskytoval. V dobƒõ soci√°ln√≠ch s√≠t√≠ a platforem na hodnocen√≠ firem\r\njejich souƒçasn√Ωmi i b√Ωval√Ωmi zamƒõstnanci (za v≈°echny zmi≈àme nap≈ô. Glassdoor nebo\r\nƒçesk√Ω Atmoskop) se toti≈æ\r\ninformace o nerovn√©m p≈ô√≠stupu m≈Ø≈æe velice snadno roz≈°√≠≈ôit mezi\r\npotenci√°ln√≠ i st√°vaj√≠c√≠ zamƒõstnance, kte≈ô√≠ ji mohou zohlednit p≈ôi sv√©m\r\nrozhodov√°n√≠, zda se v dan√© firmƒõ uch√°zet o pr√°ci, resp. zda v n√≠ i\r\nnad√°le z≈Østat.\r\nTuto skuteƒçnost dokl√°daj√≠ nap≈ô. v√Ωsledky pr≈Øzkumu\r\nproveden√©ho spoleƒçnost√≠ Glassdoor, podle kter√©ho cca 67 % (U.S.)\r\nzamƒõstnanc≈Ø by se neuch√°zelo o pr√°ci tam, kde by si myslelo, ≈æe mu≈æi a\r\n≈æeny maj√≠ nerovn√© platov√© podm√≠nky.\r\nAudit platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami\r\nStejnƒõ jako p≈ôi ≈ôe≈°en√≠ jak√©hokoli jin√©ho probl√©mu, i v tomto p≈ô√≠padƒõ\r\nplat√≠, ≈æe v prvn√≠ ≈ôadƒõ je p≈ôedev≈°√≠m pot≈ôeba ovƒõ≈ôit, ≈æe nƒõjak√Ω\r\nprobl√©m k ≈ôe≈°en√≠ v≈Øbec existuje. K tomu poslou≈æ√≠\r\nfiremn√≠ audit platov√© nerovnosti mezi mu≈æi a ≈æenami.\r\nTen prost≈ôednictv√≠m anal√Ωzy platov√Ωch, demografick√Ωch a organizaƒçn√≠ch\r\ndat ovƒõ≈ô√≠, zda m√°me nƒõjak√© doklady pro to, ≈æe v dan√© spoleƒçnosti\r\nexistuj√≠ platov√© rozd√≠ly mezi zamƒõstnanci spojen√© s jejich pohlav√≠m.\r\nTeprve na z√°kladƒõ v√Ωsledk≈Ø takov√© anal√Ωzy je mo≈æn√© se zaƒç√≠t poohl√≠≈æet po\r\nmo≈æn√Ωch opat≈ôen√≠ch v oblastech n√°boru, odmƒõ≈àov√°n√≠ a/nebo povy≈°ov√°n√≠,\r\nkter√° by mohla pomoct nespravedliv√© platov√© nerovnosti odstranit nebo\r\nalespo≈à zm√≠rnit.\r\nN√≠≈æe uveden√Ω p≈ô√≠klad takov√©ho auditu vych√°z√≠ z ƒçl√°nku How\r\nto Analyze Your Gender Pay Gap: An Employer‚Äôs Guide od Andrew\r\nChamberlaina, Ph.D., hlavn√≠ho ekonoma a vedouc√≠ho v√Ωzkumu ve\r\nspoleƒçnosti Glassdoor.\r\nPl√°n anal√Ωzy\r\nAnal√Ωzu platov√© nerovnosti mezi mu≈æi a ≈æenami provedeme v\r\nn√°sleduj√≠c√≠ch nƒõkolika kroc√≠ch:\r\nNaƒçteme si data, kter√° obsahuj√≠ informace o platech vzorku\r\nzamƒõstnanc≈Ø, jejich pohlav√≠, demografick√Ωch a organizaƒçn√≠ch\r\ncharakteristik√°ch, na kter√Ωch budeme testovat na≈°e hypot√©zy. Za t√≠mto\r\n√∫ƒçelem pou≈æijeme ilustraƒçn√≠ data\r\nposkytnut√° spoleƒçnost√≠ Glassdoor.\r\nV p≈ô√≠padƒõ pot≈ôeby si uprav√≠me data tak, aby l√©pe vyhovovala pot≈ôeb√°m\r\nna≈°√≠ anal√Ωzy.\r\nProvedeme exploraƒçn√≠ anal√Ωzu, kter√° n√°m poskytne z√°kladn√≠ p≈ôedstavu\r\no na≈°ich datech.\r\nSpoƒç√≠t√°me si neadjustovanou GPG.\r\nS pomoc√≠ hierarchick√© regresn√≠ anal√Ωzy vytvo≈ô√≠me statistick√Ω model\r\nGPG, kter√Ω n√°m umo≈æn√≠ l√©pe rozli≈°it ‚Äúvliv‚Äù r≈Øzn√Ωch faktor≈Ø, vƒçetnƒõ\r\njejich interakc√≠, na pozorovan√© rozd√≠ly v platech mu≈æ≈Ø a ≈æen.\r\nOvƒõ≈ô√≠me, zda samotn√© pohlav√≠ zamƒõstance - p≈ôi zohlednƒõn√≠ ‚Äúvlivu‚Äù\r\nostatn√≠ch faktor≈Ø, ke kter√Ωm m√°me k dispozici nƒõjak√° data - hraje\r\nnƒõjakou v√Ωznamnƒõj≈°√≠ roli ve v√Ω≈°i platu, kter√Ω zamƒõstnanec dost√°v√°.\r\nOvƒõ≈ô√≠me, zda pohlav√≠ zamƒõstnance neinteraguje s nƒõkter√Ωmi dal≈°√≠mi\r\nfaktory p≈ôi predikci v√Ω≈°e jejich mzdy.\r\nDostupn√° data\r\n\r\n\r\nShow code\r\n\r\ndata <- readr::read_csv(\"./GenderPay_Data.csv\")\r\n\r\n\r\n\r\nK dispozici m√°me n√°sleduj√≠c√≠ data ke vzorku 1000 zamƒõstnanc≈Ø:\r\nTyp pozice, na kter√© zamƒõstnanec pracuje (jobTitle)\r\nPohlav√≠ zamƒõstnance (gender)\r\nVƒõk zamƒõstnance (age)\r\nHodnocen√≠ pracovn√≠ho v√Ωkonu zamƒõstnance (perfEval)\r\n√örove≈à vzdƒõl√°n√≠ zamƒõstnance (edu)\r\nOddƒõlen√≠, ve kter√©m zamƒõstnanec pracuje (dpt)\r\nM√≠ra seniority zamƒõstnance (seniority)\r\nZ√°kladn√≠ mzda zamƒõstnance (basePay)\r\nBonusov√° slo≈æka platu zamƒõstnance (bonus)\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames = FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nP≈ô√≠prava dat k anal√Ωze\r\nZe zbƒõ≈æn√© kontroly povahy na≈°ich dat je patrn√©, ≈æe ne ka≈æd√° z\r\npromƒõnn√Ωch je v na≈°em datasetu reprezentov√°na pomoc√≠ adekv√°tn√≠ho\r\ndatov√©ho typu. P≈ôed samotnou anal√Ωzou si tedy budeme muset na≈°e data\r\nje≈°tƒõ trochu upravit.\r\n\r\n\r\nShow code\r\n\r\ndplyr::glimpse(data)\r\n\r\n\r\nRows: 1,000\r\nColumns: 9\r\n$ jobTitle  <chr> \"Graphic Designer\", \"Software Engineer\", \"Warehous~\r\n$ gender    <chr> \"Female\", \"Male\", \"Female\", \"Male\", \"Male\", \"Femal~\r\n$ age       <dbl> 18, 21, 19, 20, 26, 20, 20, 18, 33, 35, 24, 18, 19~\r\n$ perfEval  <dbl> 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,~\r\n$ edu       <chr> \"College\", \"College\", \"PhD\", \"Masters\", \"Masters\",~\r\n$ dept      <chr> \"Operations\", \"Management\", \"Administration\", \"Sal~\r\n$ seniority <dbl> 2, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 3, 3, 5, 4, 3, 5,~\r\n$ basePay   <dbl> 42363, 108476, 90208, 108080, 99464, 70890, 67585,~\r\n$ bonus     <dbl> 9938, 11128, 9268, 10154, 9319, 10126, 10541, 1024~\r\n\r\nKonkr√©tnƒõ budeme cht√≠t upravit v≈°echny textov√© promƒõnn√© (pracovn√≠\r\npozice, pohlav√≠, √∫rove≈à vzdƒõl√°n√≠ a pracovn√≠ oddƒõlen√≠) a dvƒõ numerick√©\r\npromƒõnn√© (hodnocen√≠ pracovn√≠ho v√Ωkonu a m√≠ru seniority) na faktorov√©\r\npromƒõnn√©. Ke t≈ôem z tƒõchto novƒõ vytvo≈ôen√Ωch faktorov√Ωch promƒõnn√Ωch\r\n(√∫rove≈à vzdƒõl√°n√≠, hodnocen√≠ pracovn√≠ho v√Ωkonu a m√≠ra senirotity) je\r\npotom pot≈ôeba p≈ôidat informaci o spr√°vn√©m po≈ôad√≠ jejich jednotliv√Ωch\r\nkategori√≠, proto≈æe reprezentuj√≠ ordin√°ln√≠ promƒõnn√©, u kter√Ωch lze\r\nsmysluplnƒõ hovo≈ôit o relativn√≠m po≈ôad√≠ kategori√≠ ve smyslu vy≈°≈°√≠/ni≈æ≈°√≠,\r\nresp. vƒõt≈°√≠/men≈°√≠. Takto upraven√° data ji≈æ odpov√≠daj√≠ typu informac√≠,\r\nkter√© reprezentuj√≠, a m≈Ø≈æeme je tedy zaƒç√≠t pou≈æ√≠vat pro anal√Ωzu na≈°eho\r\nprobl√©mu.\r\n\r\n\r\nShow code\r\n\r\nmydata <- data %>%\r\n  dplyr::mutate_if(is.character, as.factor) %>%\r\n  dplyr::mutate(edu = factor(edu, ordered = TRUE, levels = c(\"High School\", \"College\", \"Masters\", \"PhD\")),\r\n                perfEval = factor(as.character(perfEval), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")),\r\n                seniority = factor(as.character(seniority), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")))\r\n\r\n\r\n\r\nExploraƒçn√≠ anal√Ωza\r\nV n√≠≈æe uveden√Ωch tabulk√°ch jsou uvedeny z√°kladn√≠ popisn√© statistiky k\r\njednotliv√Ωm promƒõnn√Ωm. M≈Ø≈æeme z nich vyƒç√≠st nap≈ô. to, ≈æe na≈°ich 1000\r\nzamƒõstnanc≈Ø je relativnƒõ rovnomƒõnƒõ rozdƒõlen√Ωch do jednotliv√Ωch kategori√≠\r\nz hlediska pracovn√≠ pozice, pohlav√≠, hodnocen√≠ pracovn√≠ho v√Ωkonu, √∫rovnƒõ\r\nvzdƒõl√°n√≠, oddƒõlen√≠, ve kter√©m pracuj√≠, i m√≠ry jejich seniority. D√°le se\r\nz nich m≈Ø≈æeme dozvƒõdƒõt, ≈æe prost≈ôedn√≠ch 50 % zamƒõstnanc≈Ø je ve vƒõku mezi\r\n29 a 54 lety, jejich roƒçn√≠ z√°kladn√≠ mzda se pohybuje od 76 850 do 111\r\n558 USD a jejich bonusy za rok ƒçin√≠ 4 849 a≈æ 8 026 USD.\r\n\r\n\r\nShow code\r\n\r\nskimr::skim(mydata)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nmydata\r\nNumber of rows\r\n1000\r\nNumber of columns\r\n9\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n6\r\nnumeric\r\n3\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\njobTitle\r\n0\r\n1\r\nFALSE\r\n10\r\nMar: 118, Sof: 109, Dat: 107, Fin:\r\n107\r\ngender\r\n0\r\n1\r\nFALSE\r\n2\r\nMal: 532, Fem: 468\r\nperfEval\r\n0\r\n1\r\nTRUE\r\n5\r\n5: 209, 4: 207, 1: 198, 3: 194\r\nedu\r\n0\r\n1\r\nTRUE\r\n4\r\nHig: 265, Mas: 256, Col: 241, PhD:\r\n238\r\ndept\r\n0\r\n1\r\nFALSE\r\n5\r\nOpe: 210, Sal: 207, Man: 198, Adm:\r\n193\r\nseniority\r\n0\r\n1\r\nTRUE\r\n5\r\n3: 219, 2: 209, 1: 195, 5: 193\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n41.39\r\n14.29\r\n18\r\n29.00\r\n41.0\r\n54.25\r\n65\r\n‚ñá‚ñá‚ñÜ‚ñÜ‚ñá\r\nbasePay\r\n0\r\n1\r\n94472.65\r\n25337.49\r\n34208\r\n76850.25\r\n93327.5\r\n111558.00\r\n179726\r\n‚ñÇ‚ñá‚ñá‚ñÉ‚ñÅ\r\nbonus\r\n0\r\n1\r\n6467.16\r\n2004.38\r\n1703\r\n4849.50\r\n6507.0\r\n8026.00\r\n11293\r\n‚ñÇ‚ñá‚ñá‚ñÜ‚ñÇ\r\n\r\n Z hlediska n√°mi analyzovan√©ho probl√©mu jsou pro n√°s ale\r\nd≈Øle≈æitƒõj≈°√≠ vztahy mezi jednotliv√Ωmi promƒõnn√Ωmi, zejm√©na mezi pohlav√≠m a\r\nostatn√≠mi promƒõnn√Ωmi a jejich r≈Øzn√Ωmi kombinacemi. Rychl√Ω p≈ôehled o\r\nnƒõkter√Ωch tƒõchto vztaz√≠ch n√°m m≈Ø≈æe poskytnout n√≠≈æe uveden√Ω graf, kter√Ω\r\nzobrazuje souvislosti mezi jednotliv√Ωmi dvojicemi promƒõnn√Ωch a s pomoc√≠\r\nbarevn√©ho k√≥dov√°n√≠ nav√≠c nese informaci o tom, jak se tyto souvislosti\r\nli≈°√≠ mezi pohlav√≠mi. V grafu m≈Ø≈æeme nap≈ô. vidƒõt, ≈æe se v p≈ô√≠padƒõ\r\nnƒõkter√Ωch pracovn√≠ch pozic v√Ωznamnƒõ li≈°√≠ relativn√≠ zastoupen√≠ mu≈æ≈Ø a\r\n≈æen. V men≈°√≠ m√≠≈ôe se zd√° tento rozd√≠l platit i v p≈ô√≠padƒõ √∫rovnƒõ\r\nvzdƒõl√°n√≠. Urƒçit√Ω rozd√≠l mezi mu≈æi a ≈æenami se zd√° existovat rovnƒõ≈æ ve\r\nv√Ω≈°i jejich z√°kladn√≠ mzdy (narozd√≠l od bonusov√© slo≈æky, kter√° se zd√° b√Ωt\r\nu mu≈æ≈Ø a ≈æen obdobnƒõ vysok√°).\r\n\r\n\r\nShow code\r\n\r\nGGally::ggpairs(mydata, aes(color = gender, alpha = 0.4)) +\r\n  ggplot2::theme(\r\n      strip.text.x = element_text(\r\n        size = 22),\r\n      strip.text.y = element_text(\r\n        size = 22)\r\n      ) +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\")\r\n\r\n\r\n\r\n\r\nVizu√°ln√≠ dojem o rozd√≠ln√© v√Ω≈°i z√°kladn√≠ mzdy u mu≈æ≈Ø a ≈æen potvrzuje i\r\ndetailnƒõj≈°√≠ anal√Ωza tohoto rozd√≠lu. Ta ukazuje, ≈æe v na≈°em vzorku\r\nmedi√°nov√° mzda ≈æen ƒçin√≠ 89913.5 USD a medi√°nov√° mzda mu≈æ≈Ø 98223 USD. To\r\nodpov√≠d√° rozd√≠lu 8309.5 USD, resp. neadjustovan√© GPG (definovan√© jako\r\npomƒõr rozd√≠lu medi√°nov√© mzdy mu≈æ≈Ø a ≈æen a medi√°nov√© mzdy mu≈æ≈Ø) 8.5 %.\r\nM√≠ra platov√© nerovnosti se tak v n√°mi sledovan√© firmƒõ zd√° b√Ωt sp√≠≈°e\r\nni≈æ≈°√≠, srovnateln√° s celkovou hodnotou tohoto ukazatele v zem√≠ch jako je\r\nnap≈ô. ≈†v√©dsko nebo Nov√Ω Z√©land (viz graf z √∫vodu tohoto ƒçl√°nku).\r\nPokud bychom chtƒõli zohlednit m√≠ru na≈°√≠ nejistoty p≈ôi odhadu\r\nvelikosti rozd√≠lu mezi typick√Ωm platem mu≈æ≈Ø a ≈æen, kter√° je dan√° t√≠m, ≈æe\r\npracujeme pouze se vzorkem zamƒõstnanc≈Ø a nikoli s celou firmou, mƒõli\r\nbychom s√°hnout po inferenƒçn√≠ statistice. P≈ôi pou≈æit√≠ bayesovsk√©ho\r\nekvivalentu t-testu pro dva nez√°visl√© v√Ωbƒõry z√≠sk√°me takto informaci o\r\nposteriorn√≠ distribuci velikosti tohoto rozd√≠lu. Na grafu n√≠≈æe m≈Ø≈æeme\r\nvidƒõt, ≈æe 95% interval kredibility se nach√°z√≠ v rozmez√≠ od 5511 do 11615\r\nUSD, s medi√°novou hodnotou 8392 USD. Z grafu tak√© m≈Ø≈æeme vyƒç√≠st, ≈æe\r\ndostupn√° data mluv√≠ silnƒõ v neprospƒõch nulov√© hypot√©zy o neexistenci\r\nrozd√≠lu mezi pr≈Ømƒõrn√Ωm platem mu≈æ≈Ø a ≈æen - viz velmi n√≠zk√° hodnota\r\nlogaritmu Bayesova\r\nfaktoru ve prospƒõch nulov√© hypot√©zu BF01.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nggstatsplot::ggbetweenstats(\r\n  data = mydata,\r\n  x = gender,\r\n  y = basePay,\r\n  type = \"bayes\",\r\n  title = \"Rozd√≠l v z√°kladn√≠ mzdƒõ mezi mu≈æi a ≈æenami\",\r\n  palette = \"Dark2\"\r\n) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::labs(x = \"\")\r\n\r\n\r\n\r\n\r\nSamotn√Ω fakt rozd√≠ln√© v√Ω≈°e z√°kladn√≠ mzdy u mu≈æ≈Ø a ≈æen ale\r\nje≈°tƒõ nemus√≠ automaticky znamenat, ≈æe by se za n√≠m skr√Ωvala diskriminace\r\n≈æen. Pozorovan√Ω rozd√≠l m≈Ø≈æe b√Ωt toti≈æ nap≈ô. zp≈Øsoben√Ω t√≠m, ≈æe\r\n≈æeny zamƒõstnan√© v n√°mi sledovan√© firmƒõ maj√≠ typicky ni≈æ≈°√≠ vzdƒõl√°n√≠ ne≈æ\r\nve stejn√© firmƒõ zamƒõstnan√≠ mu≈æi. A vzhledem k tomu, ≈æe v√Ω≈°e vzdƒõl√°n√≠ (z\r\nhlediska ‚Äúmeritokratick√© spravedlnosti‚Äù zcela neproblematicky) pozitivnƒõ\r\nkoreluje s v√Ω≈°√≠ platu, projev√≠ se tato souvislost v ni≈æ≈°√≠ typick√© mzdƒõ\r\n≈æen (ponechme nyn√≠ stranou ot√°zku, v jak√© m√≠≈ôe maj√≠ ≈æeny obecnƒõ p≈ô√≠stup\r\nk vy≈°≈°√≠mu vzdƒõl√°n√≠ ve spoleƒçnosti, kde dan√° firma p≈Øsob√≠). Tuto hypot√©zu\r\nse zdaj√≠ podporovat i dva n√≠≈æe uveden√© grafy, kter√© vizualizuj√≠ vztah\r\nmezi √∫rovn√≠ vzdƒõl√°n√≠ zamƒõstnance a v√Ω≈°√≠ jeho z√°kladn√≠ mzdy, resp.\r\nsouvislost mezi pohlav√≠m zamƒõstnance a √∫rovn√≠ jeho vzdƒõl√°n√≠.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, y = basePay)) +\r\n  PupillometryR::geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8, fill = \"#a9b2d1\") +\r\n  ggplot2::geom_point(aes(y = basePay), position = position_jitter(width = .15), size = .5, alpha = 0.8, color = \"#a9b2d1\") +\r\n  ggplot2::geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5, fill = \"#a9b2d1\") +\r\n  ggplot2::expand_limits(x = 5.25) +\r\n  ggplot2::guides(fill = FALSE) +\r\n  ggplot2::guides(color = FALSE) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      ),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::theme(panel.border = element_blank()) +\r\n  ggplot2::labs(title = \"Vztah mezi √∫rovn√≠ vzdƒõl√°n√≠ a v√Ω≈°√≠ z√°kladn√≠ mzdy\",\r\n       x = \"\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, fill = gender)) +\r\n  ggplot2::geom_bar(position = \"fill\") +\r\n  ggplot2::scale_fill_hue() +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"M√≠ra zastoupen√≠ m≈Ø≈æ≈Ø a ≈æen v jednotliv√Ωch kategori√≠ch √∫rovnƒõ vzdƒõl√°n√≠\",\r\n                x = \"\",\r\n                y = \"\",\r\n                fill = \"\") +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\") +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nPodobn√Ωch kombinovan√Ωch souvislost√≠ m≈Ø≈æe v na≈°ich datech (a v\r\nrealitƒõ, kterou reprezentuj√≠) existovat vƒõt≈°√≠ mno≈æstv√≠. Pokud by ƒçten√°≈ô\r\nchtƒõl vztahy mezi r≈Øzn√Ωmi kombinacemi promƒõnn√Ωch prozkoumat s√°m a\r\ndetailnƒõji, m≈Ø≈æe za t√≠mto √∫ƒçelem vyu≈æ√≠t tuto\r\ninteraktivn√≠ aplikaci, kde jsou nahran√° na≈°e data a kde lze snadno\r\nr≈Øzn√Ωm zp≈Øsobem vizualizovat zadan√© kombinace promƒõnn√Ωch. Viz n√≠≈æe\r\nuveden√° uk√°zka vyu≈æit√≠ t√©to aplikace p≈ôi vizualizaci vztahu mezi v√Ω≈°√≠\r\nplatu, pohlav√≠m a pracovn√≠ pozic√≠, vƒçetnƒõ poƒçtu zamƒõstnanc≈Ø v\r\njednotliv√Ωch kombinovan√Ωch kategori√≠ch. Z tohoto konkr√©tn√≠ho grafu je\r\ndob≈ôe patrn√©, ≈æe ≈æeny jsou ve srovn√°n√≠ s mu≈æi disproporƒçnƒõ m√©nƒõ\r\nzastoupeny na dvou nadpr≈Ømƒõrnƒõ odmƒõ≈àovan√Ωch pozic√≠ch Manager a\r\nSoftware Engineer a naopak disproporƒçnƒõ v√≠ce jsou zastoupeny na\r\npodpr≈Ømƒõrnƒõ platovƒõ ohodnocen√© pozici Marketing Associate.\r\n\r\n D≈Øle≈æitou kategori√≠ vztah≈Ø mezi promƒõnn√Ωmi, kterou bychom mƒõli\r\nprozkoumat, pokud se chceme co nejbl√≠≈æe dostat k p≈ô√≠ƒçin√°m pozorovan√Ωch\r\nnerovnost√≠ v platech mu≈æ≈Ø a ≈æen a dob≈ôe zac√≠lit p≈ô√≠padn√© intervence,\r\njsou tzv. interakce. Ty popisuj√≠ situace, kdy vztah\r\nmezi dvƒõma promƒõnn√Ωmi z√°vis√≠ na hodnotƒõ nƒõjak√© t≈ôet√≠ promƒõnn√©. N√°s zde\r\nbude konkr√©tnƒõ zaj√≠mat interakce mezi na≈°√≠ hlavn√≠ nez√°vislou promƒõnnou\r\n(prediktorem), tj. pohlav√≠m zamƒõstnance, a dal≈°√≠mi nez√°visl√Ωmi\r\npromƒõnn√Ωmi (nap≈ô. vƒõkem, √∫rovn√≠ vzdƒõl√°n√≠, hodnocen√≠m pracovn√≠ho v√Ωkonu,\r\npracovn√≠ pozic√≠ nebo oddƒõlen√≠m) ve vztahu k na≈°√≠ z√°visl√© promƒõnn√©\r\n(krit√©riu), tedy z√°kladn√≠ mzdƒõ.\r\nP≈ô√≠kladem vizualizace tohoto druhu vztahu mezi promƒõnn√Ωmi je n√≠≈æe\r\nuveden√Ω graf, ze kter√©ho m≈Ø≈æeme vyƒç√≠st, ≈æe ≈æeny maj√≠ sice v pr≈Ømƒõru\r\nni≈æ≈°√≠ z√°kladn√≠ mzdu ne≈æ mu≈æi nap≈ô√≠ƒç cel√Ωm vƒõkov√Ωm spektrem (viz n√≠≈æe\r\npolo≈æen√° regresn√≠ p≈ô√≠mka pro skupinu ≈æen), ale fakt, ≈æe zobrazen√©\r\nregresn√≠ p≈ô√≠mky jsou rovnobƒõ≈æn√©, svƒõdƒç√≠ pro to, ≈æe v r√°mci obou skupin\r\nplat√≠ stejn√Ω typ vztahu mezi vƒõkem a v√Ω≈°√≠ platu, a tedy ≈æe mezi pohlav√≠m\r\na vƒõkem ve vztahu k v√Ω≈°i mzdy nedoch√°z√≠ k ≈æ√°dn√© interakci. Pokud by se\r\nexistence takov√© interakce potvrdila i p≈ôi zohlednƒõn√≠ dal≈°√≠ch\r\nrelevantn√≠ch faktor≈Ø, mƒõlo by to pro n√°s b√Ωt podnƒõtem k dal≈°√≠ exploraci\r\ntoho, co se pozorovan√Ωm rozd√≠lem skr√Ωv√°.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = age, y = basePay, fill = gender, colour = gender, group = gender)) +\r\n  ggplot2::geom_point(size = 1L, position = \"jitter\", alpha = 0.5) +\r\n  ggplot2::geom_smooth(span = 1L, method = \"lm\") +\r\n  ggplot2::scale_fill_brewer(palette = \"Dark2\") +\r\n  ggplot2::scale_color_brewer(palette = \"Dark2\") +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"Vztah mezi vƒõkem zamƒõstnanc≈Ø a v√Ω≈°√≠ jejich z√°kladn√≠ mzdy\",\r\n                fill = \"\",\r\n                color = \"\") +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nStatistick√Ω model platov√©\r\nnerovnosti\r\nAbychom dok√°zali izolovat vliv samotn√©ho pohlav√≠ zamƒõstnanc≈Ø na v√Ω≈°i\r\nplatu a zohlednit p≈ôitom z√°rove≈à vliv v≈°ech ostatn√≠ch relevantn√≠ch\r\nfaktor≈Ø, vƒçetnƒõ nƒõkter√Ωch jejich interakc√≠, mus√≠me s√°hnout po\r\nkomplexnƒõj≈°√≠m n√°stroji ne≈æ je popisn√° statistika. A t√≠mto n√°strojem je\r\nstatistick√© modelov√°n√≠.\r\nStatistick√© modelov√°n√≠, podobnƒõ jako jak√©koli jin√© modelov√°n√≠ ve\r\nvƒõdƒõ, ale i v bƒõ≈æn√©m ≈æivotƒõ, nen√≠ niƒç√≠m jin√Ωm ne≈æ snahou\r\nvytvo≈ôit men≈°√≠ a zjednodu≈°en√Ω model na≈°eho svƒõta, kter√Ω v≈°ak\r\njeho chov√°n√≠ odr√°≈æ√≠ dostateƒçnƒõ vƒõrnƒõ na to, abychom s jeho pomoc√≠ mohli\r\nƒçinit √∫sudky a p≈ôedpovƒõdi o skuteƒçn√©m svƒõtƒõ a zakl√°dat na nƒõm sv√°\r\nrozhodnut√≠ (k tomuto t√©matu viz srozumitelnƒõ napsan√Ω\r\npopularizuj√≠c√≠ ƒçl√°nek Modeluji,\r\ntedy jsem od Josefa ≈†lerky).\r\nStatistick√© modelov√°n√≠ se potom od jin√Ωch druh≈Ø modelov√°n√≠ li≈°√≠ v tom,\r\n≈æe se ve vƒõt≈°√≠ m√≠≈ôe op√≠r√° o n√°stroje matematick√© statistiky a teorie\r\npravdƒõpodobnosti.\r\nP≈ôekvapivƒõ mnoho jev≈Ø na≈°eho svƒõta se d√° √∫spƒõ≈°nƒõ modelovat a\r\np≈ôedpov√≠dat pomoc√≠ relativnƒõ jednoduch√Ωch statistick√Ωch model≈Ø\r\nzobecnƒõn√© line√°rn√≠ regrese (Generalized Linear\r\nModels, GLM). Ty p≈ôedpokl√°daj√≠, ≈æe z√°visl√° promƒõnn√°, transformovan√°\r\nprost≈ôednictv√≠m tzv. linkovac√≠ funkce (link\r\nfunction), je funkc√≠ line√°rn√≠ kombinace nez√°visl√Ωch promƒõnn√Ωch.\r\nNejzn√°mƒõj≈°√≠ z t√©to rodiny statistick√Ωch model≈Ø je klasick√Ω\r\nline√°rn√≠ model, kter√Ω p≈ôedpokl√°d√° norm√°ln√≠ rozdƒõlen√≠ z√°visl√©\r\npromƒõnn√©, resp. rezidu√≠ (chyb) okolo predikovan√©/ oƒçek√°van√© st≈ôedn√≠\r\nhodnoty z√°visl√© promƒõnn√© (viz ilustrativn√≠ obr√°zek n√≠≈æe).\r\n\r\nVzhledem k tomu, ≈æe n√°mi modelovan√° promƒõnn√° z√°kladn√≠ mzdy se zd√° m√≠t\r\nnorm√°ln√≠, nebo t√©mƒõ≈ô norm√°ln√≠ rozdƒõlen√≠ (viz nƒõkter√© grafy v ƒç√°sti\r\nvƒõnovan√© exploraƒçn√≠ anal√Ωze), m≈Ø≈æeme i my s√°hnout po tomto statistick√©m\r\nmodelu. Jako nez√°visl√© promƒõnn√© v na≈°em modelu pou≈æijeme v≈°echny n√°m\r\ndostupn√© prediktory, spolu s interakcemi mezi promƒõnnou pohlav√≠ na\r\nstranƒõ jedn√© a promƒõnn√Ωmi √∫rovnƒõ vzdƒõl√°n√≠, seniority, vƒõku a hodnocen√≠\r\npracovn√≠ho v√Ωkonu na stranƒõ druh√©. Proto≈æe zamƒõstnanci tvo≈ô√≠ p≈ôirozen√©\r\nshluky v r√°mci oddƒõlen√≠, nap≈ô√≠ƒç kter√Ωmi se li≈°√≠ v√Ω≈°e mzdy a tak√© by se\r\nmohla li≈°it povaha vztahu mezi pohlav√≠m zamƒõstnance a v√Ω≈°√≠ jeho mzdy,\r\npou≈æijeme hierarchickou/v√≠ce√∫rov≈àovou variantu modelu line√°rn√≠\r\nregrese, kter√° umo≈æ≈àuje, aby hodnoty vybran√Ωch parametr≈Ø modelu\r\nvariovaly v z√°vilosti na p≈ô√≠slu≈°nosti zamƒõstnanc≈Ø do konkr√©tn√≠ho\r\noddƒõlen√≠.\r\nK odhadu hodnot parametr≈Ø na≈°eho modelu pou≈æijeme inferenƒçn√≠\r\nr√°mec bayesovsk√© statistiky, kter√° ve srovn√°n√≠ s\r\nfrekventistickou statistikou nab√≠z√≠ bohat≈°√≠ a intuitivnƒõ sn√°ze\r\nuchopiteln√© v√Ωstupy. Pro apriorn√≠ distribuci parametr≈Ø modelu pou≈æijeme\r\ndefaultn√≠, ≈°irok√© a neinformativn√≠ hodnoty, tak≈æe v√Ωsledky anal√Ωzy budou\r\nnomin√°lnƒõ podobn√© tƒõm, kter√© bychom z√≠skali p≈ôi pou≈æit√≠ tradiƒçnƒõj≈°√≠\r\nfrekventistick√© inferenƒçn√≠ statistiky.\r\n\r\n\r\nShow code\r\n\r\n# defining and running the model\r\n\r\nmodel <- brms::brm(\r\n  basePay | trunc(lb = 0) \r\n  ~ 1 \r\n  + jobTitle \r\n  + gender \r\n  + age \r\n  + perfEval \r\n  + edu \r\n  + seniority \r\n  + gender:edu \r\n  + gender:seniority \r\n  + gender:age \r\n  + gender:perfEval \r\n  + (1 + gender | dept),  \r\n  data = mydata %>% dplyr::mutate_if(is.factor, as.character),\r\n  family = gaussian(link = \"identity\"),\r\n  iter = 3000,\r\n  chains = 3,\r\n  cores = 6,\r\n  warmup = 1000,\r\n  seed = 2809,\r\n  control = list(\r\n    adapt_delta = 0.99, \r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\nV√Ωsledky anal√Ωzy\r\nD≈ô√≠ve ne≈æ p≈ôistoup√≠me k interpretaci v√Ωsledk≈Ø anal√Ωzy je dobr√© si\r\novƒõ≈ôit, ≈æe n√°≈° statistick√Ω model dok√°≈æe dostateƒçnƒõ vƒõrnƒõ napodobit ƒçi\r\nsimulovat data reprezentuj√≠c√≠ firemn√≠ realitu, na jej√≠≈æ vlastnosti\r\nchceme s pomoc√≠ tohoto modelu usuzovat. Za t√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t\r\nn√°stroj posteriorn√≠ prediktivn√≠ kontroly (posterior predictive\r\ncheck), kter√Ω ovƒõ≈ôuje, jak moc dob≈ôe n√°mi zvolen√Ω a odhadnut√Ω model\r\npredikuje pozorovan√° data na z√°kladƒõ vzorku posteriorn√≠ch hodnot jeho\r\nparametr≈Ø. Z n√≠≈æe uveden√©ho grafu je dob≈ôe patrn√©, ≈æe n√°≈° model si z\r\ntohoto hlediska nevede v≈Øbec ≈°patnƒõ.\r\nPo t√©to kontrole (a tak√© po ovƒõ≈ôen√≠ dal≈°√≠ch technick√Ωch\r\nn√°le≈æitost√≠, jako je nap≈ô. konvergence MCMC\r\n≈ôetƒõzc≈Ø, kter√© umo≈æ≈àuj√≠ odhadnout posteriorne√≠ distribuci parametr≈Ø i\r\nkomplexnƒõj≈°√≠ch statistick√Ωch model≈Ø jako je ten n√°≈°) m≈Ø≈æeme zaƒç√≠t\r\nvyu≈æ√≠vat parametry na≈°eho modelu k usuzov√°n√≠ na pravdƒõpodobn√© vlastnosti\r\nn√°mi studovan√© firemn√≠ reality.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model's fit\r\n\r\n# specifying the number of samples\r\nnsamples = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posteriorn√≠ prediktivn√≠ kontrola modelu za pou≈æit√≠ vzorku o velikoti n = {nsamples}\")\r\n    )\r\n\r\n\r\n\r\n\r\nN√≠≈æe je uveden souhrn informac√≠ o na≈°em odhadnut√©m modelu. Prim√°rnƒõ\r\nn√°s zaj√≠m√° hodnota parametru pohlav√≠ (genderMale) v sekci\r\nvƒõnovan√© efekt≈Øm na √∫rovni cel√© populace (Population-Level\r\nEffects). 95% interval kredibility (Credible Interval),\r\nkter√Ω ud√°v√° kam v posteriorn√≠m rozdƒõlen√≠ spad√° hodnota nepozorovan√©ho\r\nparametru s 95% pravdƒõpodobnost√≠, se nach√°z√≠ v rozmez√≠ od -3750.04 USD\r\ndo 9081.92 USD, se st≈ôedn√≠ hodnotou 2717.57. Tzn., ≈æe podle na≈°eho\r\nmodelu m√° mu≈æ - p≈ôi zohlednƒõn√≠ ostatn√≠ch faktor≈Ø a jejich vybran√Ωch\r\ninterakc√≠ - typicky o cca 2700 USD vy≈°≈°√≠ z√°kladn√≠ mzdu ne≈æ jej√≠ ≈æensk√Ω\r\nprotƒõj≈°ek. Anal√Ωza na≈°ich dat tak do urƒçit√© m√≠ry podporuje hypot√©zu o\r\nexistenci platov√© diskriminace na z√°kladƒõ pohlav√≠ zamƒõstnance v n√°mi\r\nstudovan√© firmƒõ. S√≠la d≈Økazu ve prospƒõch t√©to hypot√©zy v≈°ak nen√≠ nijak\r\nv√Ωrazn√°, co≈æ vypl√Ωv√° z toho, ≈æe 95% interval kredibility zahrnuje vedle\r\nkladn√Ωch hodnot i nulovou hodnotu a z√°porn√© hodnoty parametru pohlav√≠\r\njako jeho plauzibiln√≠ hodnoty.\r\n\r\n\r\nShow code\r\n\r\nsummary(model)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: basePay | trunc(lb = 0) ~ 1 + jobTitle + gender + age + perfEval + edu + seniority + gender:edu + gender:seniority + gender:age + gender:perfEval + (1 + gender | dept) \r\n   Data: mydata %>% dplyr::mutate_if(is.factor, as.characte (Number of observations: 1000) \r\n  Draws: 3 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 6000\r\n\r\nGroup-Level Effects: \r\n~dept (Number of levels: 5) \r\n                          Estimate Est.Error l-95% CI u-95% CI Rhat\r\nsd(Intercept)              3659.02   2392.77  1181.83  9771.07 1.00\r\nsd(genderMale)             1808.55   1724.07    78.57  6119.45 1.00\r\ncor(Intercept,genderMale)     0.30      0.53    -0.83     0.98 1.00\r\n                          Bulk_ESS Tail_ESS\r\nsd(Intercept)                 2653     2634\r\nsd(genderMale)                3466     3552\r\ncor(Intercept,genderMale)     6333     4308\r\n\r\nPopulation-Level Effects: \r\n                            Estimate Est.Error  l-95% CI  u-95% CI\r\nIntercept                   29216.96   3175.03  23007.14  35319.81\r\njobTitleDriver              -3633.29   1488.22  -6452.10   -760.74\r\njobTitleFinancialAnalyst     3749.02   1419.10   1008.10   6517.61\r\njobTitleGraphicDesigner     -2832.43   1454.75  -5687.49     37.72\r\njobTitleIT                  -1869.36   1438.48  -4666.71    969.94\r\njobTitleManager             31411.39   1495.39  28444.26  34352.28\r\njobTitleMarketingAssociate -16475.88   1390.53 -19181.98 -13758.33\r\njobTitleSalesAssociate        316.91   1428.70  -2488.94   3110.02\r\njobTitleSoftwareEngineer    13286.51   1416.67  10487.13  16055.90\r\njobTitleWarehouseAssociate  -1040.96   1491.96  -3955.62   1848.02\r\ngenderMale                   2717.57   3238.37  -3750.04   9081.92\r\nage                           995.31     33.84    927.62   1061.68\r\nperfEval2                     246.70   1444.36  -2609.56   3094.73\r\nperfEval3                   -1515.48   1463.94  -4327.32   1361.02\r\nperfEval4                     183.45   1438.20  -2567.99   3065.99\r\nperfEval5                    1433.01   1470.70  -1405.12   4405.58\r\neduHighSchool                -417.81   1302.59  -2969.86   2140.64\r\neduMasters                   4149.49   1321.31   1548.67   6728.00\r\neduPhD                       7627.28   1342.16   5025.37  10270.97\r\nseniority2                   8000.31   1527.46   4989.21  10992.21\r\nseniority3                  17954.08   1486.71  15132.57  20868.40\r\nseniority4                  30596.32   1613.05  27406.56  33680.11\r\nseniority5                  39640.70   1530.88  36626.89  42651.21\r\ngenderMale:eduHighSchool    -1926.51   1868.39  -5590.15   1674.20\r\ngenderMale:eduMasters         780.52   1829.37  -2755.04   4346.52\r\ngenderMale:eduPhD           -3154.41   1842.55  -6757.49    436.31\r\ngenderMale:seniority2         898.79   2074.27  -3179.87   4874.13\r\ngenderMale:seniority3        -354.32   2018.99  -4315.71   3647.04\r\ngenderMale:seniority4       -2847.26   2095.05  -7044.18   1157.86\r\ngenderMale:seniority5       -3538.92   2098.61  -7564.69    565.50\r\ngenderMale:age                 16.69     44.85    -71.88    104.73\r\ngenderMale:perfEval2         -603.35   2084.98  -4768.78   3466.38\r\ngenderMale:perfEval3         1601.12   2062.41  -2375.63   5640.84\r\ngenderMale:perfEval4         -471.22   2020.88  -4447.73   3466.87\r\ngenderMale:perfEval5        -2795.97   2013.44  -6768.31   1080.21\r\n                           Rhat Bulk_ESS Tail_ESS\r\nIntercept                  1.00     2311     3592\r\njobTitleDriver             1.00     3819     4183\r\njobTitleFinancialAnalyst   1.00     3294     4284\r\njobTitleGraphicDesigner    1.00     3372     3844\r\njobTitleIT                 1.00     4040     4704\r\njobTitleManager            1.00     3617     4213\r\njobTitleMarketingAssociate 1.00     3495     4556\r\njobTitleSalesAssociate     1.00     3885     4347\r\njobTitleSoftwareEngineer   1.00     3327     4760\r\njobTitleWarehouseAssociate 1.00     3601     4249\r\ngenderMale                 1.00     3031     4168\r\nage                        1.00     5997     4475\r\nperfEval2                  1.00     4400     3950\r\nperfEval3                  1.00     4457     4393\r\nperfEval4                  1.00     4276     4384\r\nperfEval5                  1.00     4239     4453\r\neduHighSchool              1.00     4654     4274\r\neduMasters                 1.00     4692     4883\r\neduPhD                     1.00     4686     4832\r\nseniority2                 1.00     3890     4236\r\nseniority3                 1.00     3995     4891\r\nseniority4                 1.00     4234     4872\r\nseniority5                 1.00     4092     4550\r\ngenderMale:eduHighSchool   1.00     4494     4650\r\ngenderMale:eduMasters      1.00     4323     4943\r\ngenderMale:eduPhD          1.00     4369     4810\r\ngenderMale:seniority2      1.00     3919     4684\r\ngenderMale:seniority3      1.00     4535     5116\r\ngenderMale:seniority4      1.00     4379     4976\r\ngenderMale:seniority5      1.00     4594     4807\r\ngenderMale:age             1.00     5968     4280\r\ngenderMale:perfEval2       1.00     4211     4414\r\ngenderMale:perfEval3       1.00     4106     4449\r\ngenderMale:perfEval4       1.00     3960     4438\r\ngenderMale:perfEval5       1.00     3865     4221\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma 10095.58    236.20  9649.25 10569.40 1.00    10053     4303\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nPokud bychom chtƒõli p≈ôesnƒõji vyjad≈ôit m√≠ru, s n√≠≈æ na≈°e data v r√°mci\r\nna≈°eho modelu favorizuj√≠ hodnoty parametru pohlav√≠ vƒõt≈°√≠ ne≈æ nula (tj.\r\nhodnoty, kter√© jsou v souladu s hypot√©zou o existenci platov√©\r\ndiskriminace na z√°kladƒõ pohlav√≠ v neprospƒõch ≈æen), m≈Ø≈æeme se pod√≠vat na\r\nposteriorn√≠ distribuci tohoto parametru a jednodu≈°e na nƒõm spoƒç√≠tat, s\r\njakou pravdƒõpodobnost√≠ nab√Ωv√° kladn√Ωch hodnot.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the model's b_genderMale parameter \r\n\r\nparamViz <- model %>%\r\n  tidybayes::gather_draws(\r\n    b_genderMale\r\n    ) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramViz$value)\r\n\r\nparamViz <- tibble(x = dens$x, y = dens$y)\r\n\r\n\r\nggplot2::ggplot(\r\n  paramViz,\r\n  aes(x,y)\r\n    ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x > 0),\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x <= 0),\r\n    fill = \"grey\"\r\n  ) +\r\n  ggplot2::geom_line(\r\n  ) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-15000, 15000, 5000)) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(\r\n    title = \"Posteriorn√≠ distribuce parametru pohlav√≠ zamƒõstnance\",\r\n    y = \"Density\",\r\n    x = \"genderMale\"\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(model)\r\n\r\n# probability of b_genderMale coefficient being higher\r\nprop <- sum(samples$b_genderMale > 0) / nrow(samples)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Bayesian hypothesis test\r\nthe_test <- brms::hypothesis(model, \"genderMale > 0\")\r\n\r\n\r\n\r\nPo proveden√≠ tohoto v√Ωpoƒçtu n√°m vych√°z√≠ hodnota 80 %. To je v souladu\r\ns p≈ôedchoz√≠m tvrzen√≠m, ≈æe d≈Økaz ve prospƒõch testovan√© hypot√©zy nen√≠\r\np≈ô√≠li≈° siln√Ω. Dal≈°√≠ mo≈ænost√≠ by bylo pou≈æit√≠ tzv. Bayesova faktoru,\r\nkter√Ω vyjad≈ôuje m√≠ru s n√≠≈æ dostupn√° data favorizuj√≠ testovanou hypot√©zu\r\nve srovn√°n√≠ s modelem odpov√≠daj√≠c√≠m nulov√© hypot√©ze. Ten m√° pro na≈°i\r\nhypot√©zu hodnotu 4, co≈æ odpov√≠d√° v√Ωznamn√©mu, ale zdaleka nikoli siln√©mu\r\nƒçi rozhodn√©mu d≈Økazu ve prospƒõch na≈°√≠ hypot√©zy.\r\nVedle parametru pohlav√≠ m≈Ø≈æe b√Ωt pro n√°s potenci√°lnƒõ u≈æiteƒçn√© pod√≠vat\r\nse tak√© na vztah z√°kladn√≠ mzdy a ostatn√≠ch prediktor≈Ø pou≈æit√Ωch v na≈°em\r\nmodelu. Za t√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t vizualizaci margin√°ln√≠ch efekt≈Ø\r\njednotliv√Ωch prediktor≈Ø, kter√© vyjad≈ôuj√≠ vztah mezi prediktorem a\r\nkrit√©riem p≈ôi zohlednƒõn√≠ vlivu ostatn√≠ch prediktor≈Ø. Takto nap≈ô. m≈Ø≈æeme\r\nna jednom z graf≈Ø vidƒõt, ≈æe vztah mezi √∫rovn√≠ vzdƒõl√°n√≠ a v√Ω≈°√≠ z√°kladn√≠ho\r\nplatu se m√° tendenci u mu≈æ≈Ø a ≈æen li≈°it. Na jin√©m grafu si m≈Ø≈æeme zase\r\nv≈°imnout toho, ≈æe rozd√≠l mezi z√°kladn√≠ mzdou mu≈æ≈Ø a ≈æen m√° tendenci\r\nnar≈Østat s t√≠m, jak kles√° seniorita zamƒõstnanc≈Ø. Tyto a dal≈°√≠ podobn√©\r\nvhledy n√°m mohou pomoct p≈ôibl√≠≈æit se k d≈Øvod≈Øm za pozorovan√Ωmi\r\nnerovnostmi v platech mu≈æ≈Ø a ≈æen.\r\n\r\n\r\nShow code\r\n\r\n# plotting marginal effects of predictors used \r\n# Note: Conditional vs. Marginal Relationships: The regression coefficients in generalized linear mixed models represent conditional effects in the sense that they express comparisons holding the cluster-specific random effects (and covariates) constant. For this reason, conditional effects are sometimes referred to as cluster-specific effects. In contrast, marginal effects can be obtained by averaging the conditional expectation Œºij over the random effects distribution. Marginal effects express comparisons of entire sub-population strata defined by covariate values and are sometimes referred to as population-averaged effects.In linear mixed models (identity link), the regression coefficents can be interpreted as either conditional or marginal effects. However, conditional and marginal effects differ for most other link functions.\r\n\r\nmarginalEffplots <- plot(\r\n  brms::marginal_effects(\r\n    model, \r\n    effects = c(\"jobTitle\", \"age\", \"perfEval\", \"edu\", \"seniority\", \"gender:edu\", \"gender:seniority\", \"gender:age\", \"gender:perfEval\"),\r\n    probs = c(0.025, 0.975)),\r\n  ask = FALSE\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# putting all graphs with marginal effects together  \r\nggpubr::ggarrange(\r\n  plotlist = marginalEffplots, \r\n  nrow = 9,\r\n  ncol = 1\r\n)\r\n\r\n\r\n\r\n\r\nMo≈æn√© dal≈°√≠ kroky\r\nI v situaci, kdy anal√Ωza dat nepodpo≈ô√≠ na≈°e podez≈ôen√≠ na existenci\r\nplatov√© diskriminace na z√°kladƒõ pohlav√≠ zamƒõstnance, je st√°le mo≈æn√©, ≈æe\r\nza pozorovan√Ωm rozd√≠lem v platech mu≈æ≈Ø a ≈æen jsou jin√© faktory, kter√© s\r\npohlav√≠m zamƒõstance nƒõjak souvis√≠. Nap≈ô. skuteƒçnost, ≈æe jsou ≈æeny m√©nƒõ\r\nreprezentovan√© na l√©pe placen√Ωch seniornƒõj≈°√≠ch pozic√≠ch, by mohla\r\nsvƒõdƒçit o tom, ≈æe se ≈æeny na pracovi≈°ti mohou pot√Ωkat s genderov√Ωmi\r\nstereotypy a ≈æe p≈ôi snaze o kari√©rn√≠ postup na l√©pe placen√© pozice\r\nnar√°≈æej√≠ na tzv. ‚Äúsklenƒõn√Ω strop‚Äú. Pro uƒçinƒõn√≠ takov√©ho z√°vƒõru je v≈°ak\r\nzapot≈ôeb√≠ z√≠skat dal≈°√≠ data, a to sp√≠≈°e kvalitativn√≠ povahy, takov√°,\r\nkter√° sb√≠r√° a analyzuje nap≈ô. organizaƒçn√≠\r\nƒçi firemn√≠\r\nantropologie.\r\nV situaci, kdy m√°me dostateƒçnƒõ siln√© d≈Økazy pro to, ≈æe se za\r\npozorovanou platovou nerovnost√≠ mezi mu≈æi a ≈æenami skr√Ωvaj√≠ faktory\r\nsouvisej√≠c√≠ s pohlav√≠m zamƒõstnance, je mo≈æn√© zaƒç√≠t se poohl√≠≈æet po\r\nmo≈æn√Ωch ≈ôe≈°en√≠ch. Stejnƒõ jako p≈ôi identifikaci probl√©mu, i p≈ôi hled√°n√≠\r\nzp≈Øsobu jeho ≈ôe≈°en√≠ je dobr√© dr≈æet se z√°sad na\r\nd≈Økazech zalo≈æen√©ho managementu a volit pouze ≈ôe≈°en√≠ s dostateƒçnƒõ\r\nempiricky dolo≈æenou √∫ƒçinnost√≠, kter√° z√°rove≈à d√°vaj√≠ smysl ve specifick√©m\r\nkontextu dan√© firmy.\r\nU≈æiteƒçn√Ω p≈ôehled mo≈æn√Ωch akc√≠, kter√© zamƒõstnavatel√© mohou podniknout\r\ns c√≠lem sn√≠≈æit GPG ve sv√© organizaci, vytvo≈ôila zn√°m√° skupina odborn√≠k≈Ø\r\nna behavior√°ln√≠ vƒõdy v r√°mci tzv. The\r\nBehavioral Insights Team, kter√° sv√©ho ƒçasu vznikla pro to, aby\r\nbritsk√© vl√°dƒõ pom√°hala realizovat √∫ƒçinnou politiku zalo≈æenou na\r\nd≈Økazech. V dokumentu s n√°zvem Reducing the gender pay gap and\r\nimproving gender equality in organisations: Evidence-based actions for\r\nemployers tato skupina odborn√≠k≈Ø uv√°d√≠ nƒõkolik mo≈æn√Ωch intervenc√≠,\r\nkter√© ≈ôad√≠ do t≈ô√≠ kategori√≠ podle toho, jak dob≈ôe je jejich √∫ƒçinnost\r\npodlo≈æen√° empirick√Ωmi d≈Økazy.\r\nMezi akce s dob≈ôe dolo≈æenou √∫ƒçinnost√≠ ≈ôad√≠\r\nn√°sleduj√≠c√≠ intervence:\r\nZahrnut√≠ vƒõt≈°√≠ho poƒçtu ≈æen do u≈æ≈°√≠ch seznam≈Ø v r√°mci v√Ωbƒõru nov√Ωch\r\nzamƒõstnanc≈Ø a povy≈°ov√°n√≠.\r\nPou≈æ√≠v√°n√≠ √∫loh posuzuj√≠c√≠ch √∫rove≈à pracovn√≠ch dovednost√≠ v r√°mci\r\nv√Ωbƒõru nov√Ωch zamƒõstnanc≈Ø.\r\nPou≈æ√≠v√°n√≠ strukturovan√©ho interview v r√°mci v√Ωbƒõru nov√Ωch\r\nzamƒõstnanc≈Ø a povy≈°ov√°n√≠.\r\nPodpora vyjedn√°v√°n√≠ o v√Ω≈°i platu pomoc√≠ zvƒõ≈ôejnƒõn√≠ existuj√≠c√≠ho\r\nplatov√©ho rozmez√≠.\r\nZaveden√≠ transparentn√≠ch proces≈Ø povy≈°ov√°n√≠ a odmƒõ≈àov√°n√≠.\r\nJmenov√°n√≠ mana≈æera ƒçi z≈ô√≠zen√≠ pracovn√≠ skupiny pro firemn√≠\r\ndiverzitu.\r\nMezi potenci√°lnƒõ slibn√© akce, kter√© ale vy≈æaduj√≠ dal≈°√≠ d≈Økazy\r\no sv√© √∫ƒçinnosti, ≈ôad√≠ n√°sleduj√≠c√≠ postupy:\r\nZv√Ω≈°en√≠ pracovn√≠ flexibility pro mu≈æe a pro ≈æeny.\r\nPodporu sd√≠len√© rodiƒçovsk√© dovolen√©.\r\nN√°bor b√Ωval√Ωch zamƒõstnanc≈Ø, kte≈ô√≠ museli z r≈Øzn√Ωch osobn√≠ch d≈Øvod≈Ø\r\nna del≈°√≠ dobu p≈ôeru≈°it svou kari√©ru.\r\nNab√≠dku mentoringu and sponsorshipu.\r\nNab√≠dku networkingov√Ωch program≈Ø.\r\nNastaven√≠ intern√≠ch c√≠l≈Ø.\r\nA mezi akce se sm√≠≈°en√Ωmi doklady o jejich √∫ƒçinnosti\r\npotom ≈ôad√≠ n√°sleduj√≠c√≠ opat≈ôen√≠:\r\n≈†kolen√≠ vƒõnovan√© t√©matu nevƒõdom√Ωch p≈ôedsudk≈Ø.\r\n≈†kolen√≠ v oblasti diverzity.\r\n≈†kolen√≠ vƒõnovan√© rozvoji leadershipu.\r\nDemograficky r≈Øznorod√© v√Ωbƒõrov√© panely v r√°mci extern√≠ho i intern√≠ho\r\nn√°boru.\r\nZde je pro z√°jemce origin√°ln√≠ dokument k bli≈æ≈°√≠mu prostudov√°n√≠.\r\n\r\n\r\nTento prohl√≠≈æeƒç nepodporuje soubory PDF. Pro zobrazen√≠ si, pros√≠m, PDF\r\nsoubor st√°hnƒõte: St√°hnout PDF.\r\n\r\n\r\n\r\nSkript k anal√Ωze je k dispozici ke sta≈æen√≠ v podobƒõ Jupyter Notebooku\r\nna m√Ωch GitHub\r\nstr√°nk√°ch.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-29-paygap/./PayGap.png",
    "last_modified": "2023-09-16T13:24:31+02:00",
    "input_file": {},
    "preview_width": 1438,
    "preview_height": 897
  },
  {
    "path": "posts/2020-12-31-segmentedregression/",
    "title": "Modeling impact of the COVID-19 pandemic on people‚Äôs interest in work-life balance and well-being",
    "description": "Illustration of Bayesian segmented regression analysis of interrupted time series data with a testing hypothesis about the impact of the COVID-19 pandemic on increase in people's search interest in work-life balance and well-being.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "well-being",
      "work-life balance",
      "covid pandemic",
      "segmented regression",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nSearch interest in work-life balance and well-being\r\nBayesian segmented regression\r\nSome necessary sanity checks\r\nResults of the analysis\r\n\r\nThe turn of the year, which is full of all sorts of resolutions to change for the better in our private lives and in our organizations, is a good time to remind ourselves that analytic tools can be very helpful in our efforts to make these resolutions come true. One way they can help us is by verifying that we have really achieved our stated goals and that we are not just fooling ourselves into believing so. We need to keep in mind Richard Feynman‚Äôs famous principle of critical thinking‚Ä¶\r\n\r\n\r\nOne of the tools that can help us with that is segmented regression analysis of interrupted time series data (thanks to Masatake Hirono for pointing me to its existence). It allows us to model changes in various processes and outcomes that follow interventions, while controlling for other types of changes (e.g.¬†trends and seasonality) that may have occurred regardless of the interventions. It is thus very useful for data analysis conducted within studies with a quasi experimental study design that are often in the organizational context the best alternative to the ‚Äúgold standard‚Äù of randomized controlled trials (RCTs) that are not always realizable or politically acceptable.\r\nSearch interest in work-life balance and well-being\r\nFor illustration, let‚Äôs use this tool for testing hypothesis about people‚Äôs increased interest in topics related to work-life balance and well-being due to the COVID-19 pandemic and subsequent changes in the way people work. As a proxy measure of this interest we will use worldwide search interest data over the last 10 years from Google Trends using search terms work-life balance and well-being (see Fig. 1 and 2 below).\r\nFig. 1: Interest in ‚Äúwork-life balance‚Äù topic over the last 10 years measured as a search interest by Google Trends. The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\r\n\r\nFig. 2: Interest in ‚Äúwell-being‚Äù topic over the last 10 years measured as a search interest by Google Trends. The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\r\n\r\nBased solely on the visual inspection of the graphs, it is pretty difficult to tell whether there was some effect of the COVID-19 pandemic or not, especially in the case of work-life balance (for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020). For sure it‚Äôs not a job for ‚Äúinter-ocular trauma test‚Äù when the existence of the effect hits you directly between the eyes. We need to rely here on inferential statistics and its ability to help us with distinguishing signal from noise.\r\nBefore conducting the analysis itself, we need to wrangle the data from Google Trends a little bit using the recipe presented in the Wagner, Zhang, and Ross-Degnan‚Äôs paper. Specifically, we need the following five variables (or six, given that we have two dependent variables):\r\nsearch interest ‚Äì numerical variable representing search interest relative to the highest point on the chart for the given region and time; this variable is truncated within the interval between values of 0 and 100; a value of 100 is the peak popularity for the term; a value of 50 means that the term is half as popular; a score of 0 means that there was not enough data for this term; this variable serves as a dependent (criterion) variable;\r\nelapsed time ‚Äì numerical variable representing the number of months that elapsed from the beginning of the time series; this variable enables estimation of the size and direction of the overall trend in the data;\r\npandemic ‚Äì dichotomic variable indicating the presence/absence of pandemic; as already mentioned above, for the purpose of this analysis, the beginning of the pandemic is assumed to have started in March 2020; this variable enables estimation of the level change in the interest in work-life balance and well-being immediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak ‚Äì numerical variable representing the number of months that elapsed from the beginning of pandemic; this variable enables estimation of the change in the trend in the interest in work-life balance and well-being after the outbreak of pandemic;\r\nmonth ‚Äì categorical variable representing specific month within a year; this variable enables controlling for the effect of seasonality.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for data manipulation\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndfWorkLifeBalance <- readr::read_csv(\"./workLifeBalanceGoogleTrendData.csv\")\r\ndfWellBeing <- readr::read_csv(\"./wellBeingGoogleTrendData.csv\")\r\n\r\ndfAll <- dfWorkLifeBalance %>%\r\n  # joining both datasets\r\n  dplyr::left_join(\r\n    dfWellBeing, by = \"Month\"\r\n    ) %>%\r\n  # changing the format and name of Month variable\r\n  dplyr::mutate(\r\n    Month = stringr::str_glue(\"{Month}-01\"),\r\n    Month = lubridate::ymd(Month)\r\n    ) %>%\r\n  dplyr::rename(\r\n    date = Month\r\n    ) %>%\r\n  # creating new variable month\r\n  dplyr::mutate(\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, \r\n                   levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   ordered = FALSE)\r\n    ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(\r\n    date\r\n    ) %>%\r\n  # creating new variables\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= \"2020-03-01\" ~ 1,\r\n      TRUE ~ 0\r\n      ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic)\r\n  ) %>%\r\n  dplyr::mutate(\r\n    pandemic = as.factor(case_when(\r\n        pandemic == 1 ~ \"After the pandemic outbreak\",\r\n        TRUE ~ \"Before the pandemic outbreak\"\r\n        ))\r\n  ) %>%\r\n  # changing order of variables in df\r\n  dplyr::select(\r\n    date, workLifeBalance, wellBeing, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n    )\r\n\r\n\r\n\r\nHere is a table with the resulting data we will use for testing our hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  dfAll,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nTable 1: Final dataset used for testing hypothesis about impact of the COVID-19 pandemic on people‚Äôs interest in work-life balance and well-being.\r\n\r\nBayesian segmented regression\r\nWe will model our data using common segmented regression models that have following general structure:\r\n\\[Y_{t} = Œ≤_{0} + Œ≤_{1}*time_{t} + Œ≤_{2}*intervention_{t} + Œ≤_{3}*time after intervention_{t} + e_{t}\\]\r\nThe Œ≤0 coefficient estimates the baseline level of the outcome variable at time zero; Œ≤1 coefficient estimates the change in the mean of the outcome variable that occurs with each unit of time before the intervention (i.e.¬†the baseline trend); Œ≤2 coefficient estimates the level change in the mean of the outcome variable immediately after the intervention (i.e.¬†from the end of the preceding segment); and Œ≤3 estimates the change in the trend in the mean of the outcome variable per unit of time after the intervention, compared with the trend before the intervention (thus, the sum of Œ≤1 and Œ≤3 equals to the post-intervention slope). For a better understanding of the model, take a look at the illustrative chart below.\r\n\r\nSince we are dealing with correlated and truncated data, we should also include two additional terms in our model, an autocorrelation term and a truncation term, to handle these specific properties of our data.\r\nNow let‚Äôs fit the models to the data and check what they tell us about the effect of pandemic on people‚Äôs search interest in work-life balance and well-being. We will use brms r package that enables making inferences about statistical models‚Äô parameters within Bayesian inferential framework. Because of that, we also need to specify some additional parameters (e.g.¬†chains, iter or warmup) of the Markov Chain Monte Carlo (MCMC) algorithm that will generate posterior samples of our models‚Äô parameters.\r\nBayesian framework also enables us to specify priors for estimated parameter and through them include our domain knowledge in the analysis. The specified priors are important for both parameter estimation and hypothesis testing as they define our starting information state before we take into account our data. Here we will use rather wide, uninformative, and only mildly regularizing priors (it means that the results of the inference will be very close to the results of standard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\nbrms::get_prior(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors for predictors in both models \r\npriors <- c(set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model for work-life balance\r\nmodelWorkLifeBalance <- brms::brm(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 12345,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n# defining the statistical model for well-being\r\nmodelWellBeing <- brms::brm(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 678910,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n\r\n\r\nSome necessary sanity checks\r\nBefore making any inferences, we should make some sanity checks to be sure that the mechanics of the MCMC algorithm worked well and that we can use generated posterior samples for making inferences about our models‚Äô parameters. There are many ways for doing that, but here we will use only visual check of the MCMC chains. We want plots of these chains look like hairy caterpillar which would indicate convergence of the underlying Markov chain to stationarity and convergence of Monte Carlo estimators to population quantities, respectively. As can be seen in Graph 1 and 2 below, in case of both models we can observe wanted characteristics of the MCMC chains described above. (For additional MCMC diagnostics procedures, see for example Bayesian Notes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains for the modelWorkLifeBalance \r\nbayesplot::mcmc_trace(\r\n  modelWorkLifeBalance,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWorkLifeBalance's parameters\"\r\n    )\r\n\r\n\r\n\r\nGraph 1: Trace plots of Markov chains for individual parameters of the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting the MCMC chains for the modelWellBeing \r\nbayesplot::mcmc_trace(\r\n  modelWellBeing,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWellBeing's parameters\"\r\n    )\r\n\r\n\r\n\r\nGraph 2: Trace plots of Markov chains for individual parameters of the modelWellBeing.\r\n\r\nIt is also important to check how well the models fit the data. We can use for this purpose posterior predictive checks that use specified number of sampled posterior values of models‚Äô parameters and show how well the fitted models predict observed data. We can see in Graphs 3 and 4 that both models fit the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWorkLifeBalance fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWorkLifeBalance, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWorkLifeBalance (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\nGraph 3: Posterior predictive checks comparing simulated/replicated data under the fitted modelWorkLifeBalance with the observed data.\r\n\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWellBeing fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWellBeing, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWellBeing (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\nGraph 4: Posterior predictive checks comparing simulated/replicated data under the fitted modelWellBeing with the observed data.\r\n\r\nResults of the analysis\r\nNow, after having sufficient confidence that - using terminology from the Richard McElreath‚Äôs book Statistical Rethinking - our ‚Äúsmall worlds‚Äù can pretty accurately mimic the data coming from our real,‚Äúbig world‚Äù, we can use our models‚Äô parameters to learn something about our research questions. Our primary interest is in the coefficient value of the pandemicBeforethepandemicoutbreak and elapsedTimeAfterPandemic terms in our models. It expresses how much and in what direction people‚Äôs search interest in work-life balance and well-being changed immediately after the outbreak of pandemic, and how slope of the trend changed after the pandemic, respectively.\r\nIn Graph 5 and 6 we can see posterior distribution of the pandemicBeforethepandemicoutbreak parameter in our two models. In both cases the posterior distribution of the pandemic term is (predominantly or completely) on the left side of the zero value, which supports the claim about existence of the effect of pandemic on people‚Äôs increased search interest in work-life balance and well-being immediately after the outbreak of pandemic. As is apparent from the graphs, for well-being (Graph 6) this evidence is much stronger than for work-life balance (Graph 5), which corresponds to impression we might have when looking at the original Google Trends charts shown in Fig. 1 and 2.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for \r\nlibrary(tidybayes)\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\nGraph 5: Visualization of the posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\nGraph 6: Visualization of the posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing.\r\n\r\nTo generate more summary statistics about posterior distributions (and also some diagnostic information like Rhat or ESS), we can use summary() function.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWorkLifeBalance \r\nsummary(modelWorkLifeBalance)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.21      0.09     0.03     0.40 1.00     7232     5932\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            69.59     11.51    47.51\r\nelapsedTime                          -0.05      0.04    -0.13\r\npandemicBeforethepandemicoutbreak   -13.90     10.15   -34.52\r\nelapsedTimeAfterPandemic              0.40      1.59    -2.77\r\nmonthFeb                              3.50      4.47    -5.29\r\nmonthMar                              4.49      4.99    -5.30\r\nmonthApr                              6.45      5.10    -3.44\r\nmonthMay                              8.81      5.10    -1.18\r\nmonthJun                             -2.91      5.05   -12.82\r\nmonthJul                             -7.40      5.02   -17.18\r\nmonthAug                             -2.51      5.01   -12.52\r\nmonthSep                             -2.20      5.01   -11.87\r\nmonthOct                              5.35      5.01    -4.68\r\nmonthNov                             12.31      4.91     2.56\r\nmonthDec                             -0.64      4.51    -9.37\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            92.59 1.00     4930     4892\r\nelapsedTime                           0.03 1.00     8111     5233\r\npandemicBeforethepandemicoutbreak     5.76 1.00     5569     5338\r\nelapsedTimeAfterPandemic              3.63 1.00     5567     5543\r\nmonthFeb                             12.29 1.00     3805     4101\r\nmonthMar                             14.36 1.00     3316     4648\r\nmonthApr                             16.44 1.00     3225     4565\r\nmonthMay                             18.82 1.00     3148     4520\r\nmonthJun                              7.14 1.00     3008     4287\r\nmonthJul                              2.42 1.00     3148     4652\r\nmonthAug                              7.42 1.00     3206     4379\r\nmonthSep                              7.58 1.00     3234     4922\r\nmonthOct                             14.99 1.00     3173     4504\r\nmonthNov                             22.21 1.00     2945     4709\r\nmonthDec                              8.24 1.00     3662     5175\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma    11.54      0.77    10.17    13.16 1.00     6603     5274\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWellBeing \r\nsummary(modelWellBeing)\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.24      0.10     0.05     0.44 1.00     6461     5967\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            56.14      4.63    46.81\r\nelapsedTime                           0.11      0.02     0.08\r\npandemicBeforethepandemicoutbreak   -21.29      4.11   -29.26\r\nelapsedTimeAfterPandemic              0.50      0.64    -0.78\r\nmonthFeb                              8.61      1.80     5.09\r\nmonthMar                             10.63      2.02     6.70\r\nmonthApr                              9.57      2.02     5.66\r\nmonthMay                              3.52      2.07    -0.52\r\nmonthJun                             -4.35      2.05    -8.43\r\nmonthJul                             -8.05      2.06   -12.10\r\nmonthAug                             -5.76      2.05    -9.68\r\nmonthSep                              4.56      2.05     0.57\r\nmonthOct                              8.13      2.01     4.24\r\nmonthNov                              6.74      2.01     2.83\r\nmonthDec                             -4.95      1.80    -8.39\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            65.13 1.00     4316     5151\r\nelapsedTime                           0.14 1.00     8987     5597\r\npandemicBeforethepandemicoutbreak   -13.07 1.00     5825     5557\r\nelapsedTimeAfterPandemic              1.78 1.00     6106     5784\r\nmonthFeb                             12.19 1.00     3387     4626\r\nmonthMar                             14.59 1.00     2868     4601\r\nmonthApr                             13.59 1.00     2724     4233\r\nmonthMay                              7.60 1.00     2938     4027\r\nmonthJun                             -0.34 1.00     2815     4633\r\nmonthJul                             -4.01 1.00     2749     4036\r\nmonthAug                             -1.77 1.00     2863     4364\r\nmonthSep                              8.55 1.00     2969     3748\r\nmonthOct                             12.22 1.00     2907     3914\r\nmonthNov                             10.73 1.00     2962     4142\r\nmonthDec                             -1.35 1.00     3525     4630\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     4.63      0.31     4.07     5.28 1.00     7355     6173\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\nGiven that for work-life balance model the posterior distribution of pandemic term crosses the zero value, it would be useful to know how strong is the evidence in the favor of hypothesis that pandemic term is lower than zero. For that purpose we can extract posterior samples and use them for calculation of the proportion of values that are larger/smaller than zero. The resulting proportions show that the vast majority (around 92%) of posterior distribution lies below zero.\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(modelWorkLifeBalance, seed = 12345)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being lower than 0\r\nsum(samples$b_pandemicBeforethepandemicoutbreak < 0) / nrow(samples)\r\n\r\n[1] 0.9145\r\n\r\n\r\nNow let‚Äôs check the parameter elapsedTimeAfterPandemic. Its posterior distribution in both models ‚Äúsafely‚Äù includes zero value, which indicates that there is not huge support for positive change in trend after the outbreak of pandemic.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_elapsedTimeAfterPandemic\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_elapsedTimeAfterPandemic\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\nGraph 7: Visualization of the posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_elapsedTimeAfterPandemic\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_elapsedTimeAfterPandemic\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\nGraph 8: Visualization of the posterior distribution of the elapsedTimeAfterPandemic parameter in the modelWellBeing.\r\n\r\nIn conclusion, we can say that there is some evidence that the COVID-19 pandemic has prompted people to be more interested in topics related to work-life balance and well-being. I wish us all to be able to transform our increased interest in these topics into truly increased quality of our personal and professional lives. It would be a shame not to use that extra incentive many of us have now for making significant change in our lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-31-segmentedregression/./wellBeingData.jpeg",
    "last_modified": "2023-09-16T13:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/",
    "title": "HR analytika a odchodovost zamƒõstnanc≈Ø",
    "description": "Kter√© faktory p≈ôisp√≠vaj√≠ k odchodovosti zamƒõstnanc≈Ø a u kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø je zv√Ω≈°en√© riziko, ≈æe firmu bƒõhem nƒõkolika p≈ô√≠≈°t√≠ch mƒõs√≠c≈Ø opust√≠? Na tyto ot√°zky se ƒç√≠m d√°l t√≠m v√≠ce firem sna≈æ√≠ odpovƒõdƒõt pomoc√≠ anal√Ωzy dat o sv√Ωch vlastn√≠ch zamƒõstnanc√≠ch. V tomto ƒçl√°nku se prost≈ôednictv√≠m analytick√©ho n√°stroje R a vizualizaƒçn√≠ho n√°stroje Shiny pod√≠v√°me, jak m≈Ø≈æe b√Ωt tento druh HR analytick√©ho projektu pro firmy u≈æiteƒçn√Ω.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-14",
    "categories": [
      "employee turnover",
      "evidence-based hr",
      "shiny app"
    ],
    "contents": "\r\n\r\nContents\r\nCo je to HR analytika?\r\nOdchodovost zamƒõstnanc≈Ø a jej√≠ prediktory\r\nNa d≈Økazech zalo≈æen√° pro-retenƒçn√≠ opat≈ôen√≠\r\n\r\nCo je to HR analytika?\r\nHR analytika ve sv√© podstatƒõ vych√°z√≠ ze zn√°m√© z√°sady managementu, ≈æe co nelze mƒõ≈ôit, nelze ani (efektivnƒõ) ≈ô√≠dit a zlep≈°ovat, a aplikuje tuto z√°sadu na lidsk√© zdroje. V nƒõkolika posledn√≠ch letech potom k tomu nav√≠c p≈ôid√°v√° nadstavbu v podobƒõ pokroƒçilej≈°√≠ch analytick√Ωch postup≈Ø, kter√© maj√≠ vƒõt≈°√≠ potenci√°l p≈ôij√≠t s hlub≈°√≠mi vhledy a s doporuƒçen√≠mi s vƒõt≈°√≠m efektem. Ale a≈• u≈æ vyu≈æ√≠v√°te pouze z√°kladn√≠ reporting nebo nƒõjakou pokroƒçilej≈°√≠ analytiku, c√≠l je v≈ædy stejn√Ω ‚Äì sna≈æit se s pomoc√≠ dat a jejich anal√Ωzy ≈æ√°douc√≠m zp≈Øsobem ovlivnit jednotliv√© HR procesy, kter√© organizac√≠m pom√°haj√≠ dosahovat jejich strategick√Ωch c√≠l≈Ø. N√°zornƒõ to ilustruje n√≠≈æe uveden√© sch√©ma zachycuj√≠c√≠ mechanismus propojuj√≠c√≠ HR procesy s (nejen) finanƒçn√≠mi v√Ωsledky organizace (Paauwe & Richardson, 1997).\r\n\r\nHR analytika pom√°h√° optimalizovat nastaven√≠ tohoto mechanismu t√≠m, ≈æe umo≈æ≈àuje nal√©zat odpovƒõdi na nƒõkter√© kl√≠ƒçov√© ot√°zky, jako nap≈ô.:\r\nKter√Ωmi kan√°ly se k n√°m dost√°vaj√≠ ti nejlep≈°√≠ kandid√°ti?\r\nJak√© charakteristiky od sebe odli≈°uj√≠ √∫spƒõ≈°n√© a ne√∫spƒõ≈°n√© kandid√°ty?\r\nKter√© faktory p≈ôisp√≠vaj√≠ k √∫spƒõ≈°n√©mu onboardingu?\r\nKter√° ‚Äûk√°p√©√≠ƒçka‚Äú maj√≠ nejsilnƒõj≈°√≠ vazbu na finanƒçn√≠ v√Ωsledky firmy?\r\nJak√© tr√©ninky vedou s nejvy≈°≈°√≠ pravdƒõpodobnost√≠ ke zlep≈°en√≠ pracovn√≠ho v√Ωkonu?\r\nKter√© intervence maj√≠ nejvƒõt≈°√≠ dopad na zamƒõstnanci poci≈•ovan√Ω well-being nebo work-life balance?\r\nCo u zamƒõstnanc≈Ø zvy≈°uje, nebo naopak sni≈æuje m√≠ru jejich anga≈æovanosti?\r\nKde se v organizaci nach√°z√≠ izolovan√° sila a √∫zk√° hrdla znemo≈æ≈àuj√≠c√≠ efektivn√≠ komunikaci a spolupr√°ci mezi jednotliv√Ωmi zamƒõstnanci, t√Ωmy nebo i cel√Ωmi oddƒõlen√≠mi?\r\nKdo p≈ôedstavuje skryt√Ω talent, kter√Ω je pot≈ôeba podchytit a d√°le rozv√≠jet?\r\nKde lze oƒçek√°vat odpor v souvislosti s pl√°novan√Ωmi zmƒõnami ve firmƒõ a kdo naopak m≈Ø≈æe b√Ωt jejich ambasadorem a katalyz√°torem?\r\nKter√© faktory p≈ôisp√≠vaj√≠ k odchodovosti zamƒõstnanc≈Ø a u kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø je zv√Ω≈°en√© riziko, ≈æe firmu bƒõhem nƒõkolika p≈ô√≠≈°t√≠ch mƒõs√≠c≈Ø opust√≠?\r\nOdchodovost zamƒõstnanc≈Ø a jej√≠ prediktory\r\nPr√°vƒõ poslednƒõ jmenovan√Ω zp≈Øsob vyu≈æit√≠ HR analytiky ƒçasto p≈ôedstavuje jeden z prvn√≠ch druh≈Ø HR analytick√Ωch projekt≈Ø, kter√Ωmi se ve firm√°ch s HR analytikou zaƒç√≠n√°, a to z dob≈ôe pochopiteln√©ho d≈Øvodu. S ne≈æ√°douc√≠mi odchody zamƒõstnanc≈Ø jsou toti≈æ spojen√© vysok√© p≈ô√≠m√© i nep≈ô√≠m√© n√°klady, tak≈æe i pomƒõrnƒõ m√≠rn√© sn√≠≈æen√≠ odchodovosti zamƒõstnanc≈Ø m≈Ø≈æe p≈ôedstavovat znaƒçnou √∫sporu, kterou ocen√≠ management ka≈æd√© firmy. Nal√©havost tohoto probl√©mu nav√≠c je≈°tƒõ zvy≈°uje souƒçasn√° f√°ze ekonomick√©ho cyklu s rekordnƒõ n√≠zkou m√≠rou nezamƒõstnanosti, kter√° v kombinaci s r≈Øzn√Ωmi on-line platformami na zprost≈ôedkov√°n√≠ pr√°ce motivuje mnoho lid√≠ k hled√°n√≠ nov√©ho m√≠sta, kde, jak doufaj√≠, bude pr√°ce zaj√≠mavƒõj≈°√≠, smysluplnƒõj≈°√≠ a l√©pe placen√° a kde kolegov√© budou sympatiƒçtƒõj≈°√≠ a ≈°√©fov√© inspirativnƒõj≈°√≠. Viz tak√© graf n√≠≈æe, kter√Ω na datech z USA n√°zornƒõ dokl√°d√° tƒõsnost vztahu mezi m√≠rou nezamƒõstnanosti a m√≠rou dobrovoln√© odchodovosti zamƒõstnanc≈Ø (r = -0,95, p < 0,001 ).\r\n\r\n\r\n\r\nVzhledem k palƒçivosti tohoto probl√©mu, kter√Ω tr√°p√≠ nejednu firmu, nen√≠ ≈æ√°dn√Ωm velk√Ωm p≈ôekvapen√≠m, ≈æe se t√©matu odchodovosti zamƒõstnanc≈Ø vƒõnovalo a st√°le vƒõnuje velk√© mno≈æstv√≠ r≈Øzn√Ωch studi√≠. Takto nap≈ô. na konci roku 2017 vy≈°la rozs√°hl√° meta-anal√Ωza od autor≈Ø Rubensteina, Eberlyov√© a Leeho, kte≈ô√≠ syntetizovali v√Ωsledky v√≠ce ne≈æ 300 d√≠lƒç√≠ch v√Ωzkum≈Ø t√Ωkaj√≠c√≠ch se prediktor≈Ø odchodovosti. M≈Ø≈æeme se tak opr√°vnƒõnƒõ pt√°t, co nov√©ho n√°m m≈Ø≈æe p≈ôin√©st HR analytika zamƒõ≈ôen√° na odchodovost zamƒõstnanc≈Ø realizovan√° pouze v jedin√© organizaci. Nebylo v≈°e podstatn√© k tomuto t√©matu ji≈æ objeveno? (K t√©to ot√°zce viz nap≈ô. tento inspirativn√≠ a trochu provokativn√≠ ƒçl√°nek od Thomase Rasmussena.) Je pravda, ≈æe nen√≠ p≈ô√≠li≈° pravdƒõpodobn√©, ≈æe p≈ôi anal√Ωze va≈°ich vlastn√≠ch dat naraz√≠te na nƒõjak√Ω naprosto nov√Ω faktor souvisej√≠c√≠ s odchodovost√≠. Na druhou stranu je rovnƒõ≈æ pravda, ≈æe ka≈æd√° organizace je v nƒõƒçem jedineƒçn√°, tak≈æe nƒõkter√© z retenƒçn√≠ch faktor≈Ø pro danou organizaci budou pravdƒõpodobnƒõ v√≠ce a jin√© m√©nƒõ d≈Øle≈æit√©. Tato informace o relativn√≠ d≈Øle≈æitosti jednotliv√Ωch retenƒçn√≠ch faktor≈Ø je potom kl√≠ƒçov√° p≈ôi nastavov√°n√≠ retenƒçn√≠ho pl√°nu a HR analytika m≈Ø≈æe b√Ωt p≈ôi tomto velice n√°pomocn√°.\r\nNa d≈Økazech zalo≈æen√° pro-retenƒçn√≠ opat≈ôen√≠\r\nS pomoc√≠ tohoto dashboardu - vytvo≈ôen√©ho prost≈ôednictv√≠m analytick√©ho n√°stroje R a vizualizaƒçn√≠ho n√°stroje Shiny a za vyu≈æit√≠ uk√°zkov√Ωch dat od spoleƒçnosti IBM - si m≈Ø≈æete sami vyzkou≈°et, jak u≈æiteƒçn√© by pro V√°s mohly b√Ωt v√Ωstupy z takov√©ho HR analytick√©ho projektu zamƒõ≈ôen√©ho na odchodovost zamƒõstnanc≈Ø. Dashboard obsahuje informace, kter√© pom√°haj√≠ (nejen) managementu zodpovƒõdƒõt ≈ôadu kl√≠ƒçov√Ωch ot√°zek, kter√© stoj√≠ na poƒç√°tku ka≈æd√©ho √∫ƒçinn√©ho pl√°nu na retenci zamƒõstnanc≈Ø, jako nap≈ô.:\r\nKolik zamƒõstnanc≈Ø n√°s roƒçnƒõ opou≈°t√≠?\r\nKter√© skupiny zamƒõstnanc≈Ø odch√°zej√≠ nejƒçastƒõji?\r\nJak√Ω je extern√≠ benchmark? Jsme na tom podobƒõ jako konkurence v oboru?\r\nP≈ôedstavuje pro n√°s st√°vaj√≠c√≠ √∫rove≈à odchodovosti z√°va≈æn√Ω probl√©m, a vyplat√≠ se n√°m ho tedy ≈ôe≈°it?\r\nZ jak√Ωch d≈Øvod≈Ø lid√© obecnƒõ nejƒçastƒõji odch√°zej√≠ ze zamƒõstn√°n√≠?\r\nJak√© faktory p≈ôisp√≠vaj√≠ k odchodu specificky na≈°ich zamƒõstnanc≈Ø?\r\nJak√° pro-retenƒçn√≠ opat≈ôen√≠ jsou obecnƒõ k dispozici?\r\nJak√° pro-retenƒçn√≠ opat≈ôen√≠ bychom mƒõli zvolit vzhledem k pravdƒõpodobn√Ωm d≈Øvod≈Øm odchod≈Ø na≈°ich zamƒõstnanc≈Ø?\r\nNa jak√© skupiny zamƒõstnanc≈Ø se p≈ôedev≈°√≠m zamƒõ≈ôit z hlediska prevence jejich odchodovosti?\r\nU kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø existuje zv√Ω≈°en√© riziko, ≈æe odejdou, a na jak√© konkr√©tn√≠ retenƒçn√≠ faktory se u nich zamƒõ≈ôit v r√°mci pravideln√©ho stay interview?\r\nJak je z v√Ω≈°e uveden√©ho v√Ωƒçtu ot√°zek patrn√©, dashboard obsahuje informace, kter√© p≈ôi sv√©m rozhodov√°n√≠ mohou vyu≈æ√≠t nejen HR mana≈æe≈ôi, ale tak√© HR business partne≈ôi nebo p≈ô√≠mo team-leade≈ôi a liniov√≠ mana≈æe≈ôi jednotliv√Ωch t√Ωm≈Ø ƒçi oddƒõlen√≠. Kromƒõ toho dashboard obsahuje tak√© ≈ôadu technick√Ωch detail≈Ø o pou≈æit√©m predikƒçn√≠m modelu a samotn√° data, kter√© stoj√≠ v pozad√≠ v≈°ech prezentovan√Ωch vizualizac√≠ a anal√Ωz. S jejich pomoc√≠ tak HR/Business analytik m≈Ø≈æe nap≈ô. hledat optim√°ln√≠ zp≈Øsob, jak nastavit sk√≥rovac√≠ algoritmus, aby se maximalizoval pozitivn√≠ efekt pro-retenƒçn√≠ch opat≈ôen√≠, nebo m≈Ø≈æe v dostupn√Ωch datech s√°m hledat nƒõjak√© dal≈°√≠ u≈æiteƒçn√© informace. V√≠ce viz ji≈æ samotn√Ω dashboard, z nƒõho≈æ m≈Ø≈æete n√≠≈æe vidƒõt nƒõkolik screenshot≈Ø.\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje r≈Øzn√© ≈ôezy odchodovost√≠ zamƒõstnanc≈Ø, a d√°v√° tak dobr√Ω p≈ôehled o tom, kter√© skupiny zamƒõstnanc≈Ø jsou odchodovost√≠ nejv√≠ce ohro≈æen√©.\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje informace o pravdƒõpodobnosti odchodu jednotliv√Ωch zamƒõstnanc≈Ø spoleƒçnƒõ s dal≈°√≠mi informacemi, kter√© mohou poslou≈æit jako podklad pro individu√°ln√≠ intervence s c√≠lem p≈ôedej√≠t ne≈æ√°douc√≠m odchod≈Øm zamƒõstnanc≈Ø.\r\n\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje informace o v√Ωkonu/kvalitƒõ statistick√©ho modelu pou≈æit√©ho k identifikaci v√Ωznamn√Ωch prediktor≈Ø odchodovosti zamƒõstnanc≈Ø a k odhadu pravdƒõpodobnosti odchodu jednotliv√Ωch zamƒõstnanc≈Ø.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/./Types-of-Employee-Attrition.png",
    "last_modified": "2023-09-16T13:24:30+02:00",
    "input_file": {},
    "preview_width": 860,
    "preview_height": 396
  },
  {
    "path": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/",
    "title": "Moneyball v HR",
    "description": "P≈ôes popularitu t√©matu HR analytiky mezi HR profesion√°ly je st√°le relativnƒõ m√°lo spoleƒçnost√≠, kter√© HR analytiku re√°lnƒõ a systematicky vyu≈æ√≠vaj√≠. Jednou z mo≈æn√Ωch p≈ô√≠ƒçin je to, ≈æe tradiƒçn√≠ HR mnohdy postr√°d√° analytick√Ω mindset a nƒõkter√© z kompetenc√≠, kter√© jsou kl√≠ƒçov√© pro √∫spƒõ≈°nou realizaci HR analytick√Ωch projekt≈Ø. V takov√© situaci m≈Ø≈æe b√Ωt u≈æiteƒçn√© pod√≠vat se ve vƒõt≈°√≠m detailu na celkovou logiku i na konkr√©tn√≠ analytick√© kroky nƒõjak√©ho √∫spƒõ≈°n√©ho p≈ô√≠kladu vyu≈æit√≠ HR analytiky k optimalizaci nƒõkter√©ho z HR proces≈Ø s pozitivn√≠m dopadem na obchodn√≠ v√Ωsledky spoleƒçnosti. V tomto ƒçl√°nku se t√≠mto zp≈Øsobem pod√≠v√°me na zn√°m√Ω p≈ô√≠bƒõh oaklandsk√©ho baseballov√©ho t√Ωmu \"√Åƒçek\", jeho≈æ management pomƒõrnƒõ radik√°lnƒõ - a podle v≈°eho i √∫spƒõ≈°nƒõ - p≈ôehodnotil sv≈Øj dosavadn√≠ p≈ô√≠stup k v√Ωbƒõru nov√Ωch hr√°ƒç≈Ø na z√°kladƒõ v√Ωstup≈Ø statistick√© anal√Ωzy sabermetrick√Ωch dat o hern√≠m chov√°n√≠ hr√°ƒç≈Ø. Vyu≈æijeme p≈ôi tom volnƒõ dostupn√Ω statistick√Ω software R a ve≈ôejnƒõ dostupnou datab√°zi historick√Ωch √∫daj≈Ø o v√Ωsledc√≠ch v americk√© baseballov√© lize.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "employee selection",
      "correlation analysis",
      "multivariate regression analysis",
      "structural equation modeling",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\n1.\r\nkrok: Zaƒç√≠t od konce aneb strategick√Ω r√°mec HR analytiky\r\n2. krok: Definice\r\nprobl√©mu a kvantifikace c√≠le\r\n3. krok: Kladen√≠ ot√°zek a\r\nmƒõ≈ôen√≠\r\n4. krok: Kladen√≠\r\ndal≈°√≠ch ot√°zek a dal≈°√≠ mƒõ≈ôen√≠\r\n5.\r\nkrok: Propojen√≠ d√≠lƒç√≠ch vhled≈Ø aneb organizace jako stroj\r\n6. krok:\r\nIntervence\r\nOmezen√≠ HR analytiky\r\nZ√°vƒõr\r\n\r\nHR analytika u≈æ dnes nen√≠ ve svƒõtƒõ HR ≈æ√°dnou horkou novinkou. T√©mƒõ≈ô\r\nv≈°ichni z oboru u≈æ o HR analytice nƒõco sly≈°eli, nƒõco o n√≠ vƒõd√≠ a\r\np≈ô√≠padnƒõ se j√≠ u≈æ tak√© pokou≈°√≠ ve sv√Ωch organizac√≠ch v nƒõjak√© podobƒõ\r\nzav√°dƒõt. Z√°rove≈à vƒõt≈°inou uzn√°vaj√≠ jej√≠ d≈Øle≈æitost p≈ôi transformaci HR z\r\npodp≈Ørn√© a administrativn√≠ funkce na funkci, kter√° dok√°≈æe organizac√≠m\r\nbezprost≈ôednƒõ pom√°hat dosahovat jejich strategick√Ωch c√≠l≈Ø. Navzdory\r\ntomuto v≈°eobecn√©mu povƒõdom√≠ o HR analytice a navzdory ≈ôadƒõ √∫spƒõ≈°nƒõ\r\nrealizovan√Ωch HR analytick√Ωch projekt≈Ø (viz nap≈ô. s√©rie ƒçl√°nk≈Ø od Davida Greena - ƒçl√°nek\r\n1, ƒçl√°nek\r\n2, ƒçl√°nek\r\n3, ƒçl√°nek\r\n4) p≈ôekvapivƒõ m√°lo organizac√≠ HR analytiku re√°lnƒõ a systematicky\r\nvyu≈æ√≠v√°. Tento stav reflektuj√≠ i v√Ωsledky v√Ωzkumu 2018\r\nHuman Capital Trends od spoleƒçnosti Deloitte, ze kter√Ωch vypl√Ωv√°, ≈æe\r\norganizace si vƒõt≈°inou uvƒõdomuj√≠ strategickou d≈Øle≈æitost v√Ωzvy, kterou\r\np≈ôedstavuje datifikace HR, z√°rove≈à se ale nec√≠t√≠ b√Ωt na ƒçelen√≠ t√©to\r\nv√Ωzvƒõ p≈ô√≠li≈° dob≈ôe p≈ôipraveny. U≈æ nƒõjakou dobu plat√≠, ≈æe kdy≈æ u≈æ se v\r\norganizaci s HR daty nƒõjak pracuje, tak je to vƒõt≈°inou pouze na √∫rovni\r\nnƒõjak√©ho z√°kladn√≠ho reportingu vybran√Ωch HR metrik a KPIs typu n√°klady\r\nna n√°bor, d√©lka obdob√≠ neobsazenosti voln√© pracovn√≠ pozice, m√≠ra\r\nne/dobrovoln√© odchodovosti zamƒõstnanc≈Ø, poƒçet zamƒõstnanc≈Ø na jednoho HR\r\nbusiness partnera apod. Slabinou tohoto p≈ô√≠stupu je, ≈æe takto sledovan√©\r\nmetriky jsou ƒçasto relevantn√≠ pouze pro monitorov√°n√≠ a ≈ô√≠zen√≠\r\nefektivnosti HR coby n√°kladov√©ho st≈ôediska, ale ji≈æ m√©nƒõ pro dosahov√°n√≠\r\nstrategick√Ωch c√≠l≈Ø organizace. Sp√≠≈°e v√Ωjimeƒçnƒõ se potom v tomto kontextu\r\nvyu≈æ√≠vaj√≠ nƒõjak√© pokroƒçilej≈°√≠ analytiky, kter√© obecnƒõ maj√≠ vƒõt≈°√≠\r\npotenci√°l p≈ôich√°zet s doporuƒçen√≠mi s p≈ô√≠m√Ωm dopadem na schopnost\r\norganizac√≠ dosahovat sv√Ωch strategick√Ωch c√≠l≈Ø.\r\nV√Ωsledky v√Ωzkumu\r\nproveden√©ho spoleƒçnostmi MIT Sloan Management Review a SAS\r\nnaznaƒçuj√≠, ≈æe tento nevyu≈æit√Ω potenci√°l HR analytiky m√° dvƒõ hlavn√≠\r\np≈ô√≠ƒçiny. Prvn√≠ z nich je to, ≈æe tradiƒçn√≠ HR mnohdy postr√°d√° analytick√Ω\r\nmindset a nƒõkter√© z kompetenc√≠, kter√© jsou kl√≠ƒçov√© pro √∫spƒõ≈°nou\r\nrealizaci HR analytick√Ωch projekt≈Ø (p≈ôehled tƒõchto kompetenc√≠ a d≈Øsledk≈Ø\r\njejich absence ƒçi nedostateƒçn√© √∫rovnƒõ viz nap≈ô. tento\r\nƒçl√°nek od Mortena Kamp\r\nAndersena). Ve stejn√©m duchu Josh Bersin ve sv√© zpr√°vƒõ\r\nHR\r\nTechnology Disruptions for 2018 konstatuje, ≈æe zvl√°dnut√≠ z√°kladn√≠ch\r\nanalytick√Ωch dovednost√≠ pat≈ô√≠ mezi nejd≈Øle≈æitƒõj≈°√≠ prediktory efektivn√≠\r\nimplementace HR analytiky v organizac√≠ch: ‚ÄúEquip all HR staff with\r\nbasic data literacy skills. All HR practitioners should know basic\r\nstatistical concepts, where to find data, how to slice and dice it, how\r\nto read a dashboard, and how to bring data and analytics to bear on\r\nbusiness issues. Our research reveals that such basic skills are among\r\nthe most important predictors of high-performing people\r\nanalytics.‚Äù\r\nDruhou hlavn√≠ p≈ô√≠ƒçinou je potom to, ≈æe HR analytick√© projekty\r\nneb√Ωvaj√≠ ukotveny v r√°mci nƒõjak√© ≈°ir≈°√≠ strategie, jak data systematicky\r\nvyu≈æ√≠vat p≈ôi ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø, nav√≠c zp≈Øsobem, kter√Ω by byl\r\nsladƒõn√Ω se strategick√Ωmi c√≠li spoleƒçnosti. Zde plat√≠ prax√≠ osvƒõdƒçen√°\r\npravda projektov√©ho managementu, ≈æe p≈ôi implementaci projekt≈Ø je pot≈ôeba\r\nv≈ædy zaƒç√≠nat od konce. V kontextu HR analytick√Ωch projekt≈Ø to tedy\r\nznamen√° zaƒç√≠nat nikoli od dat, ale od toho, k ƒçemu maj√≠ b√Ωt HR\r\nanalytick√© v√Ωstupy pou≈æity. A oƒçek√°v√°n√≠ managementu je, ≈æe HR analytika\r\nbude v posledku hlavnƒõ pom√°hat zlep≈°ovat obchodn√≠ v√Ωsledky spoleƒçnosti.\r\nN√°zornƒõ to ilustruje n√≠≈æe uveden√© sch√©ma (p≈ôevzat√© z ƒçl√°nku\r\nMaxe Blumberga),\r\nkter√© zachycuje p≈ôedpokl√°dan√Ω kauz√°ln√≠ ≈ôetƒõzec spojuj√≠c√≠ HR procesy s\r\nobchodn√≠mi v√Ωsledky. √ökolem HR analytiky je potom s pomoc√≠ dat a\r\nanalytick√Ωch n√°stroj≈Ø tyto dvƒõ oblasti propojit a zjistit, jak\r\noptimalizac√≠ prvn√≠ho zajistit zlep≈°en√≠ toho druh√©ho.\r\n\r\n≈òadƒõ organizac√≠ by v tomto ohledu mohl b√Ωt inspirac√≠ zn√°m√Ω p≈ô√≠bƒõh oaklandsk√©ho\r\nbaseballov√©ho t√Ωmu ‚Äû√Åƒçek‚Äú, kter√Ω se stal p≈ôedlohou pro knihu Moneyball a z\r\nn√≠ vych√°zej√≠c√≠ stejnojmenn√Ω film.\r\nPr√°vƒõ tento p≈ô√≠bƒõh jako jeden z prvn√≠ch uk√°zal a mezi ≈°irokou ve≈ôejnost√≠\r\nzpopularizoval mo≈ænosti vyu≈æit√≠ statistick√© anal√Ωzy ve svƒõtƒõ sportu a\r\npota≈æmo tak√© v r√°mci ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø. D√≠ky radik√°ln√≠ zmƒõnƒõ\r\ndosavadn√≠ho p≈ô√≠stupu k v√Ωbƒõru nov√Ωch hr√°ƒç≈Ø, kter√Ω se zaƒçal v√≠ce op√≠rat o\r\nv√Ωstupy statistick√© anal√Ωzy sabermetrick√Ωch\r\ndat o hern√≠m chov√°n√≠ hr√°ƒç≈Ø, dok√°zal management oaklandsk√©ho\r\nbaseballov√©ho t√Ωmu ‚Äû√Åƒçek‚Äú p≈ôij√≠mat rozhodnut√≠, kter√° z jednoho z\r\nnejchud≈°√≠ch t√Ωm≈Ø americk√© baseballov√© ligy uƒçinila jeden z\r\nnej√∫spƒõ≈°nƒõj≈°√≠ch t√Ωm≈Ø soutƒõ≈æe (mƒõ≈ôeno poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti\r\nsoutƒõ≈æe a poƒçtem postup≈Ø do play-off). Abychom mohli tento p≈ô√≠bƒõh plnƒõ\r\nvytƒõ≈æit coby inspiraci, jak analyzovat sv√° vlastn√≠ zamƒõstnaneck√° data,\r\nbude u≈æiteƒçn√©, kdy≈æ se na jednotliv√© analytick√© kroky, kter√© st√°ly v\r\npozad√≠ √∫spƒõchu oklandsk√Ωch ‚Äú√Åƒçek‚Äù, pod√≠v√°me trochu podrobnƒõji. A uƒçin√≠me\r\ntak za vyu≈æit√≠ volnƒõ dostupn√©ho statistick√©ho softwaru R a ve≈ôejnƒõ\r\ndostupn√© datab√°ze\r\nhistorick√Ωch √∫daj≈Ø o v√Ωsledc√≠ch v americk√© baseballov√© lize.\r\n1.\r\nkrok: Zaƒç√≠t od konce aneb strategick√Ω r√°mec HR analytiky\r\nJak bylo uvedeno v√Ω≈°e, ƒçasto podce≈àovan√Ωm krokem p≈ôi zav√°dƒõn√≠ HR\r\nanalytiky do firem a organizac√≠ je zasazen√≠ HR analytiky do nƒõjak√©ho\r\n≈°ir≈°√≠ho strategick√©ho r√°mce, ze kter√©ho by jasnƒõ vypl√Ωvalo, ƒçemu m√°\r\nvlastnƒõ HR analytika slou≈æit. HR analytika je pouze n√°stroj, konkr√©tnƒõ\r\nn√°stroj na zodpov√≠d√°n√≠ ot√°zek, resp. na testov√°n√≠ r≈Øzn√Ωch hypot√©z. To,\r\nzda bude tento n√°stroj u≈æiteƒçn√Ω, z√°vis√≠ na tom, zda si dok√°≈æeme kl√°st ty\r\nspr√°vn√© ot√°zky. To je p≈ôitom z velk√© ƒç√°sti d√°no t√≠m, zda si jsme vƒõdomi,\r\njak√© jsou strategick√© c√≠le na≈°√≠ organizace. Jen ve svƒõtle tƒõchto c√≠l≈Ø\r\nd√°v√° smysl kl√°st si nƒõjak√© ot√°zky, sb√≠rat a analyzovat nƒõjak√° data za\r\n√∫ƒçelem nalezen√≠ odpovƒõd√≠ na polo≈æen√© ot√°zky a posl√©ze ƒçinit nƒõjak√°\r\nkonkr√©tn√≠ rozhodnut√≠ na z√°kladƒõ nalezen√Ωch odpovƒõd√≠. V p≈ô√≠padƒõ\r\noaklandsk√Ωch ‚Äû√Åƒçek‚Äú byl c√≠l jasn√Ω ‚Äì kvalifikovat se do play-off.\r\n2. krok: Definice\r\nprobl√©mu a kvantifikace c√≠le\r\nPaul\r\nDePodesta, kter√©ho gener√°ln√≠ mana≈æer oaklandsk√Ωch ‚Äû√Åƒçek‚Äú Billy Beane p≈ôijal\r\ndo t√Ωmu jako statistick√©ho analytika, redukoval tento c√≠l na celkem\r\njednoduch√Ω matematick√Ω probl√©m: Kolik z√°pas≈Ø mus√≠ t√Ωm vyhr√°t, aby se\r\nkvalifikoval do play-off? K zodpovƒõzen√≠ t√©to ot√°zky DePodesta pot≈ôeboval\r\nhistorick√° data o poƒçtu v√≠tƒõzstv√≠ jednotliv√Ωch t√Ωm≈Ø v minul√Ωch sez√≥n√°ch\r\na o tom, zda se jim poda≈ôilo postoupit do play-off, ƒçi nikoli.\r\n\r\n\r\nShow code\r\n\r\n# Naƒçtƒõme si knihovnu, kter√° n√°m umo≈æn√≠ si naƒç√≠st a p≈ôedp≈ôipravit data k anal√Ωze a tak√© je i vizualizovat. \r\nlibrary(tidyverse)\r\n\r\n# Naƒçteme si na≈°e data.\r\nbaseball <- read_csv(\"baseball.csv\")\r\n\r\n# Pro z√≠sk√°n√≠ lep≈°√≠ p≈ôedstavy o nich se pod√≠vejme na jejich prvn√≠ch deset ≈ô√°dk≈Ø.\r\nhead(baseball, 10)\r\n\r\n\r\n# A tibble: 10 x 15\r\n   Team  League  Year    RS    RA     W   OBP   SLG    BA Playoffs\r\n   <chr> <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>\r\n 1 ARI   NL      2012   734   688    81 0.328 0.418 0.259        0\r\n 2 ATL   NL      2012   700   600    94 0.32  0.389 0.247        1\r\n 3 BAL   AL      2012   712   705    93 0.311 0.417 0.247        1\r\n 4 BOS   AL      2012   734   806    69 0.315 0.415 0.26         0\r\n 5 CHC   NL      2012   613   759    61 0.302 0.378 0.24         0\r\n 6 CHW   AL      2012   748   676    85 0.318 0.422 0.255        0\r\n 7 CIN   NL      2012   669   588    97 0.315 0.411 0.251        1\r\n 8 CLE   AL      2012   667   845    68 0.324 0.381 0.251        0\r\n 9 COL   NL      2012   758   890    64 0.33  0.436 0.274        0\r\n10 DET   AL      2012   726   670    88 0.335 0.422 0.268        1\r\n# ... with 5 more variables: RankSeason <dbl>, RankPlayoffs <dbl>,\r\n#   G <dbl>, OOBP <dbl>, OSLG <dbl>\r\n\r\nShow code\r\n\r\n# Pro n√°sleduj√≠c√≠ anal√Ωzy si potom vytvo≈ôme podmno≈æinu dat, kter√° mƒõli k dispozici v Oaklandu v roce 2002, kdy se dƒõj Moneyballu p≈ôev√°≈ænƒõ odehr√°v√°.\r\nmoneyball <- baseball %>%\r\n  filter(Year < 2002)\r\n\r\n\r\n\r\nPod√≠v√°me-li se na data mezi lety 1996‚Äì2001, tj. na data z relativnƒõ\r\nned√°vn√© minulosti (vzta≈æeno k roku 2002, kdy se dƒõj Moneyballu p≈ôev√°≈ænƒõ\r\nodehr√°v√°), z grafick√©ho vyj√°d≈ôen√≠ vztahu mezi poƒçtem v√≠tƒõzstv√≠ v\r\nz√°kladn√≠ ƒç√°st√≠ soutƒõ≈æe a postupem do play-off je dob≈ôe patrn√©, ≈æe ƒç√≠m\r\nv√≠ce z√°pas≈Ø t√Ωm vyhraje v z√°kladn√≠ soutƒõ≈æi, t√≠m vƒõt≈°√≠ je ≈°ance, ≈æe se\r\ntak√© dostane do play-off.\r\n\r\n\r\nShow code\r\n\r\n# Vytvo≈ôme si graf zachycuj√≠c√≠ vztah mezi poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a postupem do play-off\r\nmoneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select (W, Playoffs) %>%\r\n  mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs)))+\r\n  geom_point(size = 2)+\r\n  scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5))+\r\n  scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"T√Ωm nepostoupil do play-off\",\"T√Ωm postoupil do play-off\"))+\r\n  ggtitle(\"Postupy t√Ωm≈Ø do play-off mezi lety 1996-2001\")+\r\n  ylab(\"\")+\r\n  xlab(\"Poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe\")+\r\n  theme(legend.position = \"bottom\",\r\n        axis.ticks.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.text.x = element_text(size=11),\r\n        axis.title.x = element_text(size=11),\r\n        legend.text = element_text(size=11),\r\n        legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nS daty, kter√° m√°me k dispozici, m√°me tu v√Ωhodu, ≈æe m≈Ø≈æeme vztah mezi\r\npoƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a ≈°anc√≠ na postup do play-off\r\np≈ôesnƒõ kvantifikovat. Provedeme-li podrobnƒõj≈°√≠ anal√Ωzu na≈°ich dat, uk√°≈æe\r\nse, ≈æe velkou (p≈ôibli≈ænƒõ 95%) ≈°anci na postup do play-off m√° t√Ωm tehdy,\r\nkdy≈æ v z√°kladn√≠ ƒç√°sti vyhraje minim√°lnƒõ 95 z√°pas≈Ø. Tƒõchto 95 v√≠tƒõzstv√≠\r\np≈ôedstavuje dob≈ôe definovan√Ω a kvantifikovan√Ω c√≠l, kter√©ho by se\r\noaklandsk√° ‚Äû√Åƒçka‚Äú mƒõla sna≈æit dos√°hnout.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si opƒõt data mezi lety 1996-2001. \r\nmoneyball2 <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995)\r\n\r\n# Vytvo≈ôme si seznam nƒõkolika r≈Øzn√Ωch hodnot poƒçtu v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe.\r\npocet_vitezstvi <- seq(60,115,5)\r\nucast_v_playoff <- vector(mode=\"numeric\", length=length(pocet_vitezstvi))\r\nplayoff_data <- data.frame(pocet_vitezstvi = pocet_vitezstvi, ucast_v_playoff = ucast_v_playoff)\r\n\r\n# Vypoƒçtƒõme si, jak√° je pravdƒõpodobnost postupu do play-off p≈ôi r≈Øzn√©m poƒçtu v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe.\r\nfor(i in 1:nrow(playoff_data)){\r\n playoff_data$ucast_v_playoff[i] <- length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i] & moneyball2$Playoffs == 1])/length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i]]) \r\n}\r\n\r\n# A nyn√≠ si vztah mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a pravdƒõpodobnost√≠ √∫ƒçasti v play-off vizualizujme.\r\nggplot(playoff_data, aes(x = pocet_vitezstvi, y = ucast_v_playoff))+\r\n  geom_point(size = 2)+\r\n  geom_line()+\r\n  ggtitle(\"Souvislost mezi poƒçtem v√Ωher v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a\\npravdƒõpodobnost√≠ postupu t√Ωmu do play-off (1996-2001)\")+\r\n  ylab(\"Pravdƒõpodobnost postupu t√Ωmu do play-off\")+\r\n  xlab(\"Poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe\")+\r\n  scale_x_continuous(limits=c(60,115), breaks = seq(60,115,5))+\r\n  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1))+\r\n  theme(axis.text = element_text(size=11),\r\n        axis.title = element_text(size=11))\r\n\r\n\r\n\r\n\r\n3. krok: Kladen√≠ ot√°zek a\r\nmƒõ≈ôen√≠\r\nS takto definovan√Ωm a kvantifikovan√Ωm c√≠lem si potom m≈Ø≈æeme kl√°st\r\ndal≈°√≠ch ot√°zky, na kter√© kdy≈æ si dok√°≈æeme odpovƒõdƒõt, zv√Ω≈°√≠me t√≠m na≈°e\r\n≈°ance na to, ≈æe tohoto c√≠le dos√°hneme. V p≈ô√≠padƒõ oaklandsk√Ωch ‚Äû√Åƒçek‚Äú se\r\nm≈Ø≈æeme pt√°t, d√≠ky ƒçemu t√Ωm dosahuje v z√°pasech v√≠tƒõzstv√≠? Celkem zjevn√°\r\nodpovƒõƒè zn√≠, ≈æe d√≠ky tomu, ≈æe dok√°≈æe z√≠skat v√≠ce bod≈Ø ne≈æ jeho soupe≈ôi.\r\nOt√°zkou ale je, p≈ôesnƒõ o kolik bod≈Ø nav√≠c mus√≠ t√Ωm z√≠skat, aby v\r\nz√°kladn√≠ ƒç√°sti soutƒõ≈æe dos√°hl na minim√°lnƒõ 95 v√≠tƒõzstv√≠. K zodpovƒõzen√≠\r\nt√©to ot√°zky opƒõt pot≈ôebujeme historick√° data (√∫daje o vyhran√Ωch a\r\nprohran√Ωch bodech) a relativnƒõ jednoduch√Ω statistick√Ω model zvan√Ω line√°rn√≠\r\nregrese, pomoc√≠ kter√©ho m≈Ø≈æeme popsat vztah mezi poƒçtem vyhran√Ωch\r\nz√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi\r\nbody. Z n√≠≈æe uveden√©ho grafu je z≈ôejm√©, ≈æe mezi tƒõmito dvƒõma promƒõnn√Ωmi\r\nje velice tƒõsn√Ω vztah a ≈æe spolu velice silnƒõ koreluj√≠.\r\n\r\n\r\nShow code\r\n\r\n# Vypoƒçtƒõme si rozd√≠l mezi vyhran√Ωmi a prohran√Ωmi body\r\nmoneyball <- moneyball %>%\r\n  mutate(RD = RS - RA)\r\n\r\n# Graficky si zn√°zornƒõme vztah mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body\r\nlibrary(ggpubr)\r\nggplot(moneyball, aes(x = RD , y = W))+\r\n  geom_point(alpha = 0.5, size = 2)+\r\n  geom_smooth(method = \"lm\", se = FALSE)+\r\n  ggtitle(\"Vztah mezi poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a\\nrozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body\")+\r\n  xlab(\"Rozd√≠l mezi poƒçtem vyhran√Ωch a prohran√Ωch bod≈Ø\")+\r\n  ylab(\"Poƒçet v√≠tƒõzstv√≠\")+\r\n  theme(axis.title = element_text(size = 11),\r\n        axis.text = element_text(size = 11))+\r\n  scale_x_continuous(limits = c(-350,350), breaks = seq(-350,350,50))+\r\n  scale_y_continuous(limits = c(40, 120), breaks = seq(40,120,10))+\r\n  stat_cor(method = \"pearson\", label.x = 175, label.y = 45)\r\n\r\n\r\n\r\n\r\nP≈ôi pou≈æit√≠ modelu line√°rn√≠ regrese m≈Ø≈æeme vztah mezi tƒõmito dvƒõma\r\npromƒõnn√Ωmi popsat trochu podrobnƒõji.\r\n\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body \r\nreg_model1 <- glm(W ~ RD, data = moneyball, family = \"gaussian\")\r\nsummary(reg_model1)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = W ~ RD, family = \"gaussian\", data = moneyball)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-14.2662   -2.6509    0.1234    2.9364   11.6570  \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 80.881375   0.131157  616.67   <2e-16 ***\r\nRD           0.105766   0.001297   81.55   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 15.51641)\r\n\r\n    Null deviance: 117164  on 901  degrees of freedom\r\nResidual deviance:  13965  on 900  degrees of freedom\r\nAIC: 5037\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nV√Ωsledn√° regresn√≠ rovnice n√°m ≈ô√≠k√°, ≈æe oƒçek√°van√Ω poƒçet v√≠tƒõzstv√≠\r\n= 80.88 + 0.106 x Rozd√≠lov√Ω sk√≥r. Tzn., ≈æe p≈ôi vyrovnan√©m pomƒõru\r\nvyhran√Ωch a prohran√Ωch bod≈Ø m≈Ø≈æeme oƒçek√°vat, ≈æe t√Ωm vyhraje p≈ôibli≈ænƒõ 80\r\nz√°pas≈Ø za sez√≥nu, a ≈æe kdy≈æ se rozd√≠lov√© sk√≥re nav√Ω≈°√≠ o deset bod≈Ø,\r\nm≈Ø≈æeme oƒçek√°vat, ≈æe t√Ωm vyhraje v pr≈Ømƒõru o jeden z√°pas za sez√≥nu nav√≠c.\r\nKl√≠ƒçov√© je ale pro n√°s to, ≈æe s pomoc√≠ t√©to rovnice a s trochou algebry\r\nsi m≈Ø≈æeme jednodu≈°e vypoƒç√≠tat, ≈æe k dosa≈æen√≠ minim√°lnƒõ 95 v√≠tƒõzstv√≠ za\r\nsez√≥nu pot≈ôebuje t√Ωm vyhr√°t p≈ôibli≈ænƒõ o 133 bod≈Ø v√≠ce, ne≈æ kolik jich se\r\nsoupe≈ôi prohraje ((95 - 80.88) / 0.106).\r\n4. krok: Kladen√≠\r\ndal≈°√≠ch ot√°zek a dal≈°√≠ mƒõ≈ôen√≠\r\nT√≠mto zji≈°tƒõn√≠m se n√°≈° c√≠l opƒõt trochu v√≠ce specifikuje a vyvol√°v√°\r\ndal≈°√≠ ot√°zky. Ot√°zka, kter√° se t√©mƒõ≈ô sama nab√≠z√≠, se t√Ωk√° charakteristik\r\nhr√°ƒç≈Ø, kter√© nejl√©pe p≈ôedpov√≠daj√≠ poƒçet vyhran√Ωch a prohran√Ωch bod≈Ø, a\r\nt√≠m tedy tak√© pravdƒõpodobnost postupu t√Ωmu do play-off. DePodesta na\r\nz√°kladƒõ sv√Ωch anal√Ωz zjistil, ≈æe poƒçet vyhran√Ωch bod≈Ø nejtƒõsnƒõji souvis√≠\r\ns procentem p≈ô√≠pad≈Ø, kdy se hr√°ƒç dostane na metu (tzv. On-Base\r\nPercentage - OBP), a to, jak daleko se hr√°ƒç dostane p≈ôi sv√©m odpalu\r\n(tzv. Slugging Percentage - SLG). Analogick√© statistiky pro\r\nt√Ωmy soupe≈ô≈Ø (OOBP a OSLG) potom stejnƒõ dob≈ôe p≈ôedpov√≠daj√≠ poƒçet\r\nprohran√Ωch bod≈Ø. Kdy≈æ vztah mezi tƒõmito promƒõnn√Ωmi pop√≠≈°eme opƒõt pomoc√≠\r\nmodelu line√°rn√≠ regrese, m≈Ø≈æeme se s jeho pomoc√≠ pokusit p≈ôedpovƒõdƒõt,\r\njak si t√Ωm povede p≈ô√≠≈°t√≠ sez√≥nu. Takov√° p≈ôedpovƒõƒè by p≈ôitom mohla b√Ωt\r\npotenci√°lnƒõ velice u≈æiteƒçn√°, proto≈æe na jej√≠m z√°kladƒõ bychom p≈ô√≠padnƒõ\r\nmohli upravit nƒõkter√° sv√° rozhodnut√≠ o koupi nebo prodeji vybran√Ωch\r\nhr√°ƒç≈Ø. Pojƒème tuto p≈ôedpovƒõƒè vytvo≈ôit pro t√Ωm oaklandsk√Ωch ‚Äû√Åƒçek‚Äú pro\r\nsez√≥nu 2002 na z√°kladƒõ dat z let 1962-2001. Z p≈ôedchoz√≠ anal√Ωzy ji≈æ\r\nv√≠me, ≈æe‚Ä¶\r\nPoƒçet v√≠tƒõzstv√≠ = 80.88 + 0.106 x (Poƒçet vyhran√Ωch bod≈Ø - Poƒçet\r\nprohran√Ωch bod≈Ø).\r\nNyn√≠ pot≈ôebujeme urƒçit, jak√Ω bude pravdƒõpodobn√Ω poƒçet vyhran√Ωch a\r\nprohran√Ωch bod≈Ø. Pom≈Ø≈æeme si opƒõt regresn√≠ anal√Ωzou.\r\n\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem vyhran√Ωch bod≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a dvƒõma vybran√Ωmi hr√°ƒçsk√Ωmi/t√Ωmov√Ωmi statistikami \r\nregModel2 = lm(RS ~ OBP + SLG, data=moneyball)\r\nsummary(regModel2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RS ~ OBP + SLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-70.838 -17.174  -1.108  16.770  90.036 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -804.63      18.92  -42.53   <2e-16 ***\r\nOBP          2737.77      90.68   30.19   <2e-16 ***\r\nSLG          1584.91      42.16   37.60   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 24.79 on 899 degrees of freedom\r\nMultiple R-squared:  0.9296,    Adjusted R-squared:  0.9294 \r\nF-statistic:  5934 on 2 and 899 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem prohran√Ωch bod≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a dvƒõma vybran√Ωmi hr√°ƒçsk√Ωmi/t√Ωmov√Ωmi statistikami \r\nregModel3 = lm(RA ~ OOBP + OSLG, data=moneyball)\r\nsummary(regModel3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RA ~ OOBP + OSLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-82.397 -15.178  -0.129  17.679  60.955 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -837.38      60.26 -13.897  < 2e-16 ***\r\nOOBP         2913.60     291.97   9.979 4.46e-16 ***\r\nOSLG         1514.29     175.43   8.632 2.55e-13 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.67 on 87 degrees of freedom\r\n  (812 observations deleted due to missingness)\r\nMultiple R-squared:  0.9073,    Adjusted R-squared:  0.9052 \r\nF-statistic: 425.8 on 2 and 87 DF,  p-value: < 2.2e-16\r\n\r\nS pomoc√≠ regresn√≠ anal√Ωzy jsme zjistili, ≈æe‚Ä¶\r\nPoƒçet vyhran√Ωch bod≈Ø = -804.63 + 2737.77 x OBP + 1584.91 x\r\nSLGPoƒçet prohran√Ωch bod≈Ø = -837.38 + 2913.60 x OOBP + 1514.29 x\r\nOSLG.\r\nSe znalost√≠ hr√°ƒçsk√Ωch/t√Ωmov√Ωch statistik oaklandsk√Ωch ‚Äû√Åƒçek‚Äú za rok\r\n2001 se nyn√≠ m≈Ø≈æeme pokusit p≈ôedpovƒõdƒõt nejd≈ô√≠ve poƒçet vyhran√Ωch a\r\nprohran√Ωch bod≈Ø a potom tak√© p≈ôedpokl√°dan√Ω poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠\r\nƒç√°sti soutƒõ≈æe. P≈ôi formulov√°n√≠ t√©to p≈ôedpovƒõdi vych√°z√≠me z p≈ôedpokladu,\r\n≈æe se slo≈æen√≠ t√Ωmu v pr≈Øbƒõhu sez√≥ny 2002 nebude (nap≈ô. z d≈Øvodu zranƒõn√≠\r\nhr√°ƒç≈Ø) p≈ô√≠li≈° li≈°it od jeho slo≈æen√≠ v roce 2001.\r\n\r\n\r\nShow code\r\n\r\n# Hr√°ƒçsk√©/t√Ωmov√© statistiky oaklandsk√Ωch ‚Äû√Åƒçek‚Äú za rok 2001\r\nOBP_OAK <- moneyball$OBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nSLG_OAK <- moneyball$SLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOOBP_OAK <- moneyball$OOBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOSLG_OAK <- moneyball$OSLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\n\r\n# Pravdƒõpodobn√© hodnoty vybran√Ωch statistik oaklandsk√Ωch \"√Åƒçek\" pro rok 2002 vypoƒç√≠tan√© s pomoc√≠ odhadnut√Ωch regresn√≠ch model≈Ø\r\npocet_vyhranych_bodu_pred <- round(-804.63 + 2737.77*OBP_OAK + 1584.91*SLG_OAK)\r\npocet_prohranych_bodu_pred <- round(-837.38 + 2913.60*OOBP_OAK + 1514.29*OSLG_OAK)\r\npocet_vitezstvi_pred <- round(80.88 + 0.106 * (pocet_vyhranych_bodu_pred - pocet_prohranych_bodu_pred), 0)\r\n\r\n# Skuteƒçn√© hodnoty vybran√Ωch statistik oaklandsk√Ωch \"√Åƒçek\" pro rok 2002\r\npocet_vyhranych_bodu_real <- baseball$RS[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_prohranych_bodu_real <- baseball$RA[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_vitezstvi_real <- baseball$W[which(baseball$Team==\"OAK\" & baseball$Year == 2002)]\r\n\r\n# Tabulka porovn√°vaj√≠c√≠ statistick√© p≈ôedpovƒõdi se skuteƒçnost√≠ \r\npred <- c(pocet_vyhranych_bodu_pred, pocet_prohranych_bodu_pred, pocet_vitezstvi_pred)\r\nreal <- c(pocet_vyhranych_bodu_real, pocet_prohranych_bodu_real, pocet_vitezstvi_real)\r\ntable <- data.frame(\"P≈ôedpovƒõd\" = pred, \"Skuteƒçnost\" = real)\r\nrow.names(table) <- c(\"Vyhran√© body\", \"Prohran√© body\", \"Poƒçet v√≠tƒõzstv√≠\")\r\ntable\r\n\r\n\r\n                P≈ôedpovƒõd Skuteƒçnost\r\nVyhran√© body          836        800\r\nProhran√© body         635        654\r\nPoƒçet v√≠tƒõzstv√≠       102        103\r\n\r\nPorovn√°n√≠ na≈°ich p≈ôedpovƒõd√≠ s re√°ln√Ωmi v√Ωsledky za sez√≥nu 2002\r\nukazuje, ≈æe se n√°m poda≈ôilo velice p≈ôesnƒõ p≈ôedpovƒõdƒõt v√Ωsledky v\r\nnadch√°zej√≠c√≠ ligov√© sez√≥nƒõ, a v√Ωznamnƒõ tak sn√≠≈æit m√≠ru na≈°√≠ nejistoty\r\np≈ôi jej√≠m pl√°nov√°n√≠.\r\n5.\r\nkrok: Propojen√≠ d√≠lƒç√≠ch vhled≈Ø aneb organizace jako stroj\r\nMatt Dancho ve\r\nsv√© metodice k datovƒõ-analytick√Ωm projekt≈Øm doporuƒçuje, abychom se p≈ôi\r\nsnaze o pochopen√≠ obchodn√≠ho probl√©mu organizace na danou organizaci\r\nd√≠vali jako na druh stroje, kter√Ω m√° urƒçit√© vstupy, procesy a v√Ωstupy.\r\nTuto metaforu stroje m≈Ø≈æeme nyn√≠ vyu≈æ√≠t k tomu, abychom v≈°echny v√Ω≈°e\r\nuveden√© d√≠lƒç√≠ vhledy spojili do jednotn√©ho r√°mce. V nƒõm budou m√≠t\r\noaklandsk√° ‚Äú√Åƒçka‚Äù podobu jednoduch√©ho stroje na v√Ωrobu postup≈Ø do\r\nplay-off - viz obr√°zek n√≠≈æe.\r\n\r\nZe sch√©matu je dob≈ôe patrn√©, jak tento stroj funguje: Jeho v√Ωstupy\r\njsou postupy do play-off, kter√Ωch dosahuje tak, ≈æe se sna≈æ√≠ vyhr√°t v√≠ce\r\nz√°pas≈Ø, resp. z√≠skat v√≠ce bod≈Ø ne≈æ soupe≈ô√≠c√≠ t√Ωmy; k tomu vyu≈æ√≠v√° vstupy\r\nv podobƒõ schopnosti hr√°ƒç≈Ø hr√°t dob≈ôe na p√°lce a v poli; vstupem\r\novliv≈àuj√≠c√≠m chod stroje jsou rovnƒõ≈æ obdobn√© schopnosti hr√°ƒç≈Ø\r\nsoupe≈ô√≠c√≠ch t√Ωm≈Ø. Jedn√° se samoz≈ôejmƒõ o velmi zjednodu≈°en√Ω kauz√°ln√≠\r\nmodel fungov√°n√≠ t√Ωmu oakladnsk√Ωch ‚Äú√Åƒçek‚Äù, ale jak konstatuje slavn√Ω\r\nstatistick√Ω aforismus, modely jsou\r\nv≈ædy nep≈ôesn√©, ale nƒõkter√© z nich jsou u≈æiteƒçn√©.\r\nJakkoli na≈°e modely fungov√°n√≠ organizace budou v≈ædy ne√∫pln√©, je\r\nd≈Øle≈æit√© ovƒõ≈ôit, zda tyto modely i p≈ôes svou omezenost v dostateƒçn√© m√≠≈ôe\r\nodr√°≈æej√≠ realitu tak, jak n√°m ji zprost≈ôedkov√°vaj√≠ dostupn√° data. Za\r\nt√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t statistickou metodu struktur√°ln√≠ho\r\nmodelov√°n√≠, kter√° umo≈æ≈àuje formalizovat na≈°e p≈ôedstavy o vz√°jemn√Ωch\r\nvztaz√≠ch mezi nƒõkolika r≈Øzn√Ωmi promƒõnn√Ωmi a zhodnotit m√≠ru souladu\r\ntƒõchto na≈°ich p≈ôedstav s dostupn√Ωmi daty. Teprve po takov√©m zhodnocen√≠\r\nvƒõrohodnosti modelu je rozumn√© na nƒõm zakl√°dat sv√° dal≈°√≠ rozhodnut√≠.\r\nPojƒème tedy tuto metodu pou≈æ√≠t rovnƒõ≈æ na n√°≈° novƒõ vytvo≈ôen√Ω model\r\nfungov√°n√≠ t√Ωmu oaklandsk√Ωch ‚Äú√Åƒçek‚Äù a ovƒõ≈ôit m√≠ru jeho vƒõrohodnosti.\r\n\r\n\r\nShow code\r\n\r\n# Data, kter√° budeme pot≈ôebovat pro ovƒõ≈ôen√≠ vƒõrohodnosti na≈°eho modelu fungov√°n√≠ oaklandsk√Ωch \"√Åƒçek\" \r\nsem_data <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select(RS, RA, RD, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n\r\n# Definice modelu, kter√° je v souladu s v√Ω≈°e uveden√Ωm sch√©matem\r\nlibrary(lavaan)\r\noak_model <- '\r\n     Playoffs ~ W\r\n     W ~ RS + RA\r\n     RA ~ OOBP + OSLG\r\n     RS ~ OBP + SLG \r\n'\r\n# Odhad parametr≈Ø modelu\r\nfit_oak_model <- sem(oak_model, data = sem_data, missing = \"pairwise\", estimator = \"WLSMV\", ordered = \"Playoffs\")\r\nsummary(fit_oak_model, standardized = T, fit.measures = T, rsq = T)\r\n\r\n\r\nlavaan 0.6-9 ended normally after 148 iterations\r\n\r\n  Estimator                                       DWLS\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        14\r\n                                                      \r\n                                                  Used       Total\r\n  Number of observations                            90         176\r\n  Number of missing patterns                         1            \r\n                                                                  \r\nModel Test User Model:\r\n                                              Standard      Robust\r\n  Test Statistic                                 8.354      10.470\r\n  Degrees of freedom                                15          15\r\n  P-value (Chi-square)                           0.909       0.789\r\n  Scaling correction factor                                  1.159\r\n  Shift parameter                                            3.260\r\n       simple second-order correction                             \r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                               167.943     150.692\r\n  Degrees of freedom                                 6           6\r\n  P-value                                        0.000       0.000\r\n  Scaling correction factor                                  1.119\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    1.000       1.000\r\n  Tucker-Lewis Index (TLI)                       1.016       1.013\r\n                                                                  \r\n  Robust Comparative Fit Index (CFI)                            NA\r\n  Robust Tucker-Lewis Index (TLI)                               NA\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.000       0.000\r\n  90 Percent confidence interval - lower         0.000       0.000\r\n  90 Percent confidence interval - upper         0.040       0.067\r\n  P-value RMSEA <= 0.05                          0.964       0.903\r\n                                                                  \r\n  Robust RMSEA                                                  NA\r\n  90 Percent confidence interval - lower                     0.000\r\n  90 Percent confidence interval - upper                        NA\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  SRMR                                           0.108       0.108\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                           Robust.sem\r\n  Information                                 Expected\r\n  Information saturated (h1) model        Unstructured\r\n\r\nRegressions:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n  Playoffs ~                                                     \r\n    W                  0.234    0.025    9.181    0.000     0.234\r\n  W ~                                                            \r\n    RS                 0.093    0.006   15.189    0.000     0.093\r\n    RA                -0.094    0.006  -16.031    0.000    -0.094\r\n  RA ~                                                           \r\n    OOBP            3158.695  360.178    8.770    0.000  3158.695\r\n    OSLG            1520.258  213.163    7.132    0.000  1520.258\r\n  RS ~                                                           \r\n    OBP             3621.290  258.284   14.021    0.000  3621.290\r\n    SLG             1418.260  144.885    9.789    0.000  1418.260\r\n  Std.all\r\n         \r\n    0.993\r\n         \r\n    0.682\r\n   -0.726\r\n         \r\n    0.564\r\n    0.452\r\n         \r\n    0.606\r\n    0.425\r\n\r\nIntercepts:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.000                                0.000\r\n   .W                 96.691   11.568    8.358    0.000    96.691\r\n   .RA              -808.808  116.566   -6.939    0.000  -808.808\r\n   .RS             -1041.496   73.943  -14.085    0.000 -1041.496\r\n  Std.all\r\n    0.000\r\n    8.629\r\n   -9.367\r\n  -12.661\r\n\r\nThresholds:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs|t1       22.653    6.998    3.237    0.001    22.653\r\n  Std.all\r\n    8.578\r\n\r\nVariances:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.101                                0.101\r\n   .W                  7.779    2.306    3.374    0.001     7.779\r\n   .RA               536.111   97.833    5.480    0.000   536.111\r\n   .RS               449.043   80.457    5.581    0.000   449.043\r\n  Std.all\r\n    0.014\r\n    0.062\r\n    0.072\r\n    0.066\r\n\r\nScales y*:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs           1.000                                1.000\r\n  Std.all\r\n    1.000\r\n\r\nR-Square:\r\n                   Estimate \r\n    Playoffs           0.986\r\n    W                  0.938\r\n    RA                 0.928\r\n    RS                 0.934\r\n\r\n\r\n\r\nShow code\r\n\r\n# Grafick√© zn√°zornƒõn√≠ modelu fungov√°n√≠ oaklandsk√Ωch \"√Åƒçek\" \r\nlibrary(semPlot)\r\nsemPaths(fit_oak_model, \r\n         whatLabels=\"std\", \r\n         intercepts=FALSE, \r\n         style=\"lisrel\",\r\n         nCharNodes=0,\r\n         nCharEdges=0,\r\n         curveAdjacent = TRUE,\r\n         title=TRUE,\r\n         layout=\"tree2\",\r\n         curvePivot=TRUE,\r\n         rotation =3)\r\n\r\n\r\n\r\n\r\nV√Ωstupy proveden√© tzv. pƒõ≈°inkov√©\r\nanal√Ωzy, kter√° je speci√°ln√≠m typem struktur√°ln√≠ho modelov√°n√≠,\r\nnaznaƒçuj√≠, ≈æe n√°mi navr≈æen√Ω model je v souladu s daty, kter√° m√°me k\r\ndispozici (viz ‚Äúp≈ô√≠zniv√©‚Äù hodnoty index≈Ø shody, resp. neshody jako je\r\nTLI a CFI, resp. RMSEA, a tak√© vysok√© hodnoty standardizovan√Ωch\r\nregresn√≠ch koeficient≈Ø). D√°vaj√≠ n√°m tak dobr√Ω d≈Øvod vƒõ≈ôit, ≈æe na≈°e dal≈°√≠\r\nkroky a rozhodnut√≠, kter√° zalo≈æ√≠me na tomto modelu, budou m√≠t ≈æ√°douc√≠\r\nefekt na po≈æadovan√© v√Ωstupy, tj. na postup oaklandsk√Ωch ‚Äú√Åƒçek‚Äù do\r\nplay-off.\r\n6. krok: Intervence\r\nNa z√°kladƒõ v√Ω≈°e uveden√Ωch zji≈°tƒõn√≠ zaƒçal management oaklandsk√Ωch\r\n‚Äû√Åƒçek‚Äú do sv√©ho t√Ωmu vyb√≠rat hr√°ƒçe, kte≈ô√≠ sice nevyhovovali tradiƒçn√≠m\r\nkrit√©ri√≠m, podle kter√Ωch hr√°ƒç≈°t√≠ skauti posuzovali kvalitu baseballov√Ωch\r\nhr√°ƒç≈Ø, ale za to vykazovali p≈ôesnƒõ ty charakteristiky, kter√© podle\r\nDePodestov√Ωch anal√Ωz p≈ôedpov√≠daly poƒçet vyhran√Ωch a prohran√Ωch bod≈Ø, a\r\npota≈æmo tedy tak√© pravdƒõpodobnost √∫ƒçasti v play-off, kter√° byla hlavn√≠m\r\nc√≠lem managementu. D√≠ky tomu, ≈æe konkurenƒçn√≠ t√Ωmy d≈Øle≈æitost tƒõchto\r\nhr√°ƒçsk√Ωch statistik podce≈àovaly a naopak p≈ôece≈àovaly jin√©, m√©nƒõ d≈Øle≈æit√©\r\npromƒõnn√© (nap≈ô. m√≠ru √∫spƒõ≈°nosti odpal≈Ø, tzv. Batting Average),\r\nmohl management oaklandsk√Ωch ‚Äû√Åƒçek‚Äú relativnƒõ levnƒõ skupovat hr√°ƒçe,\r\nkte≈ô√≠ jim umo≈æ≈àovali dosahovat stanoven√©ho c√≠le. V√Ωsledkem bylo to, ≈æe\r\noaklandsk√° ‚Äû√Åƒçka‚Äú vyhr√°vala zhruba o 20 z√°pas≈Ø za sez√≥nu v√≠ce ne≈æ stejnƒõ\r\n‚Äûchud√©‚Äú t√Ωmy a p≈ôibli≈ænƒõ stejnƒõ tolik z√°pas≈Ø jako 2kr√°t a≈æ 3kr√°t bohat≈°√≠\r\nkonkurence - viz graf n√≠≈æe.\r\n\r\n\r\nShow code\r\n\r\n# Naƒçtƒõme si pot≈ôebn√° data Lahmanovy baseballov√© datab√°ze, kter√° je ve≈ôejnƒõ p≈ô√≠stupn√° na adrese http://seanlahman.com/baseball-archive/statistics/\r\nmzdyHracu <- read_csv(\"salaries.csv\")\r\nvyhryTymu <- read_csv(\"teams.csv\")\r\n\r\n# Vypoƒçtƒõme si pr≈Ømƒõrnou sumu mezd vypl√°cen√Ωch jednotliv√Ωmi t√Ωmy sv√Ωm hr√°ƒç≈Øm v letech 1998-2001 \r\nprumerna_suma_MezdHracu <- mzdyHracu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypoƒçtƒõme si pro jednotliv√© t√Ωmy pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu v letech 1998-2001\r\nprumerny_pocet_vyher <- vyhryTymu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyj√°d≈ôeme si graficky vztah mezi poƒçtem v√Ωher a mno≈æstv√≠m penƒõz, kter√© t√Ωmy vynakl√°daj√≠ na mzdy sv√Ωch hr√°ƒç≈Ø \r\nlibrary(ggrepel)\r\nprumerna_suma_MezdHracu %>%\r\n  left_join(prumerny_pocet_vyher, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hr√°ƒç≈Ø a poƒçet v√≠tƒõzstv√≠ v letech 1998-2001\")+\r\n  xlab(\"Pr≈Ømƒõrn√° suma mezd hr√°ƒç≈Ø (USD)\")+\r\n  ylab(\"Pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\"))+\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07))\r\n\r\n\r\n\r\n\r\nOmezen√≠ HR analytiky\r\nP≈ôes ve≈°kerou p≈ôidanou hodnotu, kterou HR analytika pro organizaci\r\nm≈Ø≈æe m√≠t, je vhodn√© si v≈Øƒçi n√≠ zachovat zdravou m√≠ru skepse a b√Ωt si\r\nvƒõdom jej√≠ch omezen√≠. N√≠≈æe uv√°d√≠m p≈ôehled nƒõkolika z nich.\r\nKvalita a u≈æiteƒçnost v√Ωstup≈Ø HR analytiky je z√°visl√° na kvalitƒõ\r\ndat, kter√° do n√≠ vstupuj√≠. Jako kdekoli jinde i zde plat√≠ ok≈ô√≠dlen√©\r\nrƒçen√≠ ‚Äûrubbish in, rubbish out‚Äú. Schopnost z√≠skat pot≈ôebn√° data\r\nvƒças, v dostateƒçn√© kvalitƒõ a v dostateƒçn√©m mno≈æstv√≠ p≈ôitom p≈ôedstavuje\r\njedno z neju≈æ≈°√≠ch hrdel cel√©ho procesu zav√°dƒõn√≠ HR analytiky v\r\norganizac√≠ch.\r\nHR analytika pracuje s historick√Ωmi daty a vych√°z√≠ z p≈ôedpokladu,\r\n≈æe minulost je dobr√Ωm prediktorem budoucnosti. Ale jak n√°s na to\r\nopakovanƒõ upozor≈àuj√≠ odborn√≠ci jako Nassim\r\nTaleb nebo Philip\r\nTetlock, tento vztah mezi minulost√≠ a budoucnost√≠ plat√≠ pouze do\r\nurƒçit√© m√≠ry a pouze v relativnƒõ kr√°tk√©m ƒçasov√©m horizontu. Na ka≈æd√©m\r\nrohu na n√°s ƒç√≠h√° nƒõjak√° potenci√°ln√≠ ƒçern√°\r\nlabu≈•, kter√° m≈Ø≈æe postavit na hlavu v≈°echno, co jsme se na z√°kladƒõ\r\nna≈°ich minul√Ωch zku≈°enost√≠ nauƒçili br√°t jako samoz≈ôejmou\r\njistotu.\r\nNe ka≈æd√© prost≈ôed√≠ je stejnƒõ p≈ôedv√≠dateln√© jako svƒõt sportu.\r\nPomƒõr sign√°lu\r\na ≈°umu se m≈Ø≈æe nap≈ô√≠ƒç r≈Øzn√Ωmi oblastmi v√Ωznamnƒõ li≈°it a ƒç√≠m v√≠ce\r\np≈ôevl√°d√° n√°hodn√Ω ≈°um nad sign√°lem, t√≠m m√©nƒõ jsou v√Ωstupy z HR analytiky\r\nu≈æiteƒçn√©. P≈ô√≠kladem zde m≈Ø≈æe b√Ωt relativnƒõ ne√∫spƒõ≈°n√° snaha p≈ôedpov√≠dat\r\nto, jak si baseballov√© t√Ωmu povedou v play-off. Na rozd√≠l od z√°kladn√≠\r\nƒç√°sti soutƒõ≈æe, kde se hraje dostatek z√°pas≈Ø na to, aby se vyru≈°il vliv\r\nn√°hodn√©ho ≈°tƒõst√≠ a sm≈Øly, v pƒõtiz√°pasov√Ωch kolech play-off hraje n√°hoda\r\ntak v√Ωznamnou roli, ≈æe souvislost mezi celkov√Ωm poƒçtem v√≠tƒõzstv√≠ v\r\nz√°kladn√≠ ƒç√°sti a po≈ôad√≠m t√Ωmu v play-off je t√©mƒõ≈ô nulov√°.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si data mezi lety 1994-2011, kdy v play-off hraje 8 t√Ωm≈Ø.\r\nmoneyball3 <- moneyball %>%\r\n  filter(Year < 2012 & Year > 1993)\r\n  \r\n# V√Ωpoƒçtƒõme si Kendallovu po≈ôadovou korelaci mezi mezi celkov√Ωmm poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a po≈ôad√≠m t√Ωmu v play-off mezi lety 1994-2011. \r\nsuppressWarnings(cor.test(~ W + RankPlayoffs, data = moneyball3, method = \"kendall\"))\r\n\r\n\r\n\r\n    Kendall's rank correlation tau\r\n\r\ndata:  W and RankPlayoffs\r\nz = -0.48318, p-value = 0.629\r\nalternative hypothesis: true tau is not equal to 0\r\nsample estimates:\r\n        tau \r\n-0.05541167 \r\n\r\nƒå√≠sla maj√≠ tu zvl√°≈°tn√≠ moc, ≈æe dok√°≈æou v ƒçlovƒõku velice snadno\r\nvzbudit dojem, ≈æe toho v√≠me mnohem v√≠ce ne≈æ je tomu ve skuteƒçnosti. Je\r\nv≈°ak dobr√© si b√Ωt vƒõdom toho, ≈æe ka≈æd√° statistick√° p≈ôedpovƒõƒè je v≈ædy\r\nzat√≠≈æena nƒõjakou m√≠rou chyby, tu vƒõt≈°√≠, tu men≈°√≠. Velkou v√Ωhodou\r\nstatistick√Ωch model≈Ø je to, ≈æe tato chyba je u nich explicitnƒõ\r\nvyƒç√≠slena, tak≈æe s n√≠ lze dop≈ôedu poƒç√≠tat a zohlednit ji p≈ôi n√°sledn√©m\r\nrozhodov√°n√≠. Tato ‚Äûup≈ô√≠mnost‚Äú ohlednƒõ sv√© vlastn√≠ omylnosti paradoxnƒõ\r\nmnohdy stav√≠ statistick√© modely do hor≈°√≠ho svƒõtla ne≈æ jinak m√©nƒõ p≈ôesn√©\r\nintuitivn√≠ √∫sudky expert≈Ø, pro kter√© podobn√© √∫daje o m√≠≈ôe jejich\r\nomylnosti vƒõt≈°inou nejsou v≈Øbec k dispozici.\r\nVelikost v√Ωhody, kterou n√°m zaveden√≠ HR analytiky d√°v√°, m≈Ø≈æe b√Ωt\r\nz√°visl√° na tom, zda podobn√© postupy vyu≈æ√≠v√° tak√© na≈°e konkurence. Opƒõt\r\nto lze celkem dob≈ôe dolo≈æit na oaklandsk√Ωch ‚Äû√Åƒçk√°ch‚Äú. Jejich v√Ωsledky se\r\nmezi lety 2002 a≈æ 2012, tj. v dobƒõ po zve≈ôejnƒõn√≠ Moneyballu, kdy ji≈æ\r\nv≈°echny t√Ωmy mƒõly p≈ô√≠le≈æitost sezn√°mit se s principy prediktivn√≠\r\nanalytiky a zav√©st ji do sv√© praxe, zaƒçaly v√≠ce p≈ôibli≈æovat v√Ωsledk≈Øm\r\npodobnƒõ ‚Äûchud√Ωch‚Äú soupe≈ô≈Ø a naopak jejich bohat≈°√≠ soupe≈ôi jim sv√Ωm\r\nv√Ωkonem zase trochu odskoƒçili - viz graf n√≠≈æe. Z toho mimo jin√© vypl√Ωv√°,\r\n≈æe s t√≠m, jak se st√°le v√≠ce spoleƒçnost√≠ bude p≈ôi ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø\r\nspol√©hat na v√Ωstupy z HR analytiky, p≈ôestane b√Ωt HR analytika nƒõjakou\r\nz√°sadn√≠ konkurenƒçn√≠ v√Ωhodou a stane se z n√≠ nƒõco, co organizaci ‚Äúpouze‚Äù\r\numo≈æn√≠ dr≈æet krok s konkurenc√≠.\r\n\r\n\r\nShow code\r\n\r\n# Vypoƒçtƒõme si pr≈Ømƒõrnou sumu mezd vypl√°cen√Ωch jednotliv√Ωmi t√Ωmy sv√Ωm hr√°ƒç≈Øm v letech 2002-2012 \r\nprumerna_suma_MezdHracu2 <- mzdyHracu %>%\r\n  filter(yearID > 2001 & yearID <= 2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypoƒçtƒõme si pro jednotliv√© t√Ωmy pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu v letech 2002-2012\r\nprumerny_pocet_vyher2 <- vyhryTymu %>%\r\n  filter(yearID > 2001 & yearID <=2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyj√°d≈ôeme si graficky vztah mezi poƒçtem v√Ωher a mno≈æstv√≠m penƒõz, kter√© t√Ωmy vynakl√°daj√≠ na mzdy sv√Ωch hr√°ƒç≈Ø \r\nprumerna_suma_MezdHracu2 %>%\r\n  left_join(prumerny_pocet_vyher2, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hr√°ƒç≈Ø a poƒçet v√≠tƒõzstv√≠ v letech 2002-2012\")+\r\n  xlab(\"Pr≈Ømƒõrn√° suma mezd hr√°ƒç≈Ø (USD)\")+\r\n  ylab(\"Pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\")) +\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(3e+07,2e+08), breaks = seq(3e+07,2e+08,2e+07))\r\n\r\n\r\n\r\n\r\nZ√°vƒõr\r\nNa p≈ô√≠kladu oaklandsk√©ho baseballov√©ho mu≈æstva jsme takto mohli\r\nsledovat obvykl√Ω postup aplikace HR analytiky na urƒçit√Ω druh probl√©mu,\r\nkter√Ω se sna≈æ√≠ v dan√© organizaci vy≈ôe≈°it. Vzhledem ke specifick√©mu\r\np≈ôedmƒõtu podnik√°n√≠ oaklandsk√Ωch ‚Äû√Åƒçek‚Äú bylo t√≠mto c√≠lem dos√°hnout\r\npostupu do play-off a to v situaci, kdy management nemƒõl dostatek\r\nfinanƒçn√≠ch prost≈ôedk≈Ø na zaplacen√≠ hr√°ƒç≈Ø pova≈æovan√Ωch dle tradiƒçn√≠ch\r\nmƒõ≈ô√≠tek za kvalitn√≠ a perspektivn√≠. Od tohoto c√≠le se potom odv√≠jela\r\n≈ôada krok≈Ø, kter√© bl√≠≈æe specifikovaly jeho povahu a identifikovaly\r\nfaktory (mimo jin√© i ty person√°ln√≠), kter√© s jeho dosa≈æen√≠m souvis√≠. Na\r\nz√°kladƒõ t√©to znalosti potom bylo mo≈æn√© formulovat urƒçit√© p≈ôedpovƒõdi a\r\nuƒçinit jist√° rozhodnut√≠, kter√° zv√Ω≈°ila pravdƒõpodobnost toho, ≈æe se\r\npoda≈ô√≠ vytƒçen√©ho c√≠le dos√°hnout. P≈ôesto≈æe tento p≈ô√≠bƒõh o vyu≈æit√≠ HR\r\nanalytiky se odehr√°l ve svƒõtƒõ sportu, jeho logika je platn√° i v kontextu\r\ntradiƒçnƒõj≈°√≠ho typu organizac√≠. Ostatnƒõ ve v≈°ech typech\r\norganizac√≠ jde nakonec p≈ôedev≈°√≠m o to m√≠t na spr√°vn√©m m√≠stƒõ a ve spr√°vn√Ω\r\nƒças ty spr√°vn√© lidi - jedinƒõ tak tyto organizace mohou\r\nsystematicky dosahovat sv√Ωch strategick√Ωch c√≠l≈Ø.\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/./Pitcher-at-the-mound.jpg",
    "last_modified": "2023-09-16T13:24:30+02:00",
    "input_file": {}
  }
]
