[
  {
    "path": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/",
    "title": "GPT-4's performance in the knowledge test of evidence-based HRM practices",
    "description": "How did GPT-4 perform in the knowledge test of evidence-based HRM practices? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-05-02",
    "categories": [
      "gpt",
      "ai",
      "evidence-based management",
      "hr management",
      "people management",
      "hr practices"
    ],
    "contents": "\r\nSome time ago, I ‚Äúreplicated‚Äù Rynes, Colbert, and Brown‚Äôs 2002 study on HR practitioners‚Äô beliefs about effective HR practices on a convenience sample of more than 140 LinkedIn users. The results of this ‚Äúreplication‚Äù closely resembled the results of the original study. On average, respondents correctly answered 19.4 out of 35 items, achieving a 55% success rate, which was very close to the 57% average success rate in the original study (and also quite close to the 50% success rate that corresponds to random choice, given the TRUE/FALSE response format).\r\nI was curious to see how GPT-4 would perform in this test, as it had been evaluated on various standardized tests such as the SAT, GRE, Bar Exam, and AP. The prompts used had the following form: Read the following statement and indicate whether it is true or false. Keep in mind that the statement refers to general trends and patterns that apply on average but not necessarily to all cases. When evaluating the statement, ensure that you correctly interpret the words used in the statement and take into account existing scientific evidence. Give me the answer either true or false, without intermediate values, in a boolean way. Finally, briefly explain your reasoning behind your answer. The statement is as follows:‚Ä¶\r\nSo, what were the results? GPT-4 answered 29 out of 35 items right, i.e., it achieved a 83% success rate, which corresponds to the 99th and 97th percentiles in the original and ‚Äúreplicated‚Äù studies, respectively. GPT-4‚Äôs results are thus superior to majority of people who took the test.\r\n\r\nHowever, even when it gave a correct answer, it did not always rely on correct facts and/or valid reasoning, which could be a problem if management decided to act on the answers provided. See the table below to check the details of its responses.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(DT)\r\n\r\n# uploading data\r\nmydata <- readxl::read_xlsx(\"./gpt4Responses.xlsx\")\r\n\r\n# creating user-friendly table\r\nDT::datatable(\r\n  mydata %>% \r\n    dplyr::select(itemId, item, gpt4Response, correctAnswer, gpt4Reasoning, researchEvidence, possibleContingencies) %>%\r\n    dplyr::rename(\"Item ID\"=itemId, Item=item, \"GPT-4 response\"=gpt4Response, \"Correct answer\"=correctAnswer, \"GPT-4 reasoning\"=gpt4Reasoning, \"Research evidence\"=researchEvidence, \"Possible contingencies\"=possibleContingencies),\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 3,\r\n    autoWidth = TRUE,\r\n    columnDefs = list(list(width = '500px', targets = c(\"Item\", \"GPT-4 reasoning\", \"Research evidence\", \"Possible contingencies\"))),\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n) %>%\r\n  DT::formatStyle(1:7, 'vertical-align'='top')\r\n\r\n\r\n\r\nThe takeaway from this finding? Although GPT-4 can be a handy tool for exploring possible solutions to specific HR-related problems, on its own and in its current form it cannot replace the good old systematic search for and retrieval of evidence, critical evaluation of its reliability and relevance, and its weighing and synthesis as conducted and/or supervised by human experts.\r\nP.S. I didn‚Äôt test the reliability of GPT-4‚Äôs responses, nor did I set its temperature to 0, so it‚Äôs possible that you might obtain somewhat different results if you decide to replicate the test. In addition, keep in mind that the comparison presented here is not entirely an apples-to-apples comparison, mainly due to the fact that new evidence may have emerged that does not match the correct answers in the original study conducted more than 20 years ago.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-01-gpt4-and-e-b-hrm-practices/./gpt4.jpg",
    "last_modified": "2023-05-01T22:05:43+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-28-employee-feedback-analysis-using-openai/",
    "title": "Employee feedback analysis using tools from OpenAI",
    "description": "How to use GPT and embeddings from OpenAI for identifying topics and related sentiments in employee feedback.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-28",
    "categories": [
      "openai",
      "gpt",
      "embeddings",
      "employee feedback",
      "employee survey",
      "topic analysis",
      "python",
      "r",
      "shiny app"
    ],
    "contents": "\r\nA while ago, I posted about the potential of using GPT for processing open-ended feedback from employees. I simply inputted a block of text into GPT and asked for a summary of the major topics found in the feedback. Although the output was quite accurate and the information compression achieved was very useful, this approach was somewhat limited in terms of scalability and granularity of the information provided.\r\nTo address these limitations, I experimented with another approach that includes the following steps:\r\nLooping over feedback from individual employees and sending them one by one to GPT.\r\nPrompting GPT to identify all present topics in each feedback, determining their respective sentiments (positive, negative, mixed, or neutral), and extracting the relevant parts of the feedback based on which the topic was identified.\r\nPrompting GPT to categorize identified topics using a provided list of topic categories (e.g., compensation and benefits, work-life balance, collaboration and teamwork, etc.), while taking into account contextual information in the relevant parts of the feedback. Alternatively, categorizing by matching embeddings of identified topics, contextual information, and topic categories.\r\nPlotting the topic categories by the number of their occurrences and type of associated sentiment.\r\nInteractive exploration of specific topics clustered by their semantic similarity based on their respective embeddings and visualized with the help of t-SNE dimensionality reduction technique.\r\nCreating a filterable table with identified topics and all original and extracted information that may be useful for further exploration of the feedback and for checking the precision of the topic identification.\r\nI had to experiment a bit with the prompts and include some data-munging inter-steps to get useful outputs, however, it now works relatively smoothly and provides pretty good results. To test the plausibility of this approach, I tried it on publicly available feedback from more than 300 current and former employees of an unnamed company published on Glassdoor and shared on Kaggle. You can check the results of the analysis yourself in this simple dashboard.\r\n\r\nIn my opinion, it works quite well and could represent a very time- and cost-effective way to gain useful insights from employee open-ended feedback at scale, with the caveat that one has to ensure the security of the processed data, for example, by using a local LLM. Let me know what you think about this approach. And if you are interested in the Python code behind the analysis so you can play with it on your own data, here‚Äôs the link to the GitHub page with the Python code.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-28-employee-feedback-analysis-using-openai/./employeeListening.avif",
    "last_modified": "2023-04-28T18:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-24-glassdoor/",
    "title": "When flawed statistical & causal reasoning leads to a valid conclusion anyway",
    "description": "Comparison of Glassdoor ratings from current and former employees.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-24",
    "categories": [
      "glassdoor",
      "employee experience",
      "employee satisfaction",
      "employee turnover"
    ],
    "contents": "\r\nOne simple lesson from the observation that former employees tend to rate their employers more harshly on Glassdoor compared to current employees: Strive to retain your employees, and you‚Äôll likely have a more satisfied workforce and better Glassdoor ratings üòÅ\r\n\r\n\r\nShow code\r\n\r\n# uploading library\r\nlibrary(tidyverse)\r\n\r\n# uploading data (link to the original dataset: https://www.kaggle.com/datasets/davidgauthier/glassdoor-job-reviews/code)\r\n# data <- readr::read_csv(\"./glassdoor_reviews.csv\")\r\n# \r\n# # preparing data\r\n# mydata <- data %>%\r\n#   # selecting relevant vars\r\n#   dplyr::select(firm, current, overall_rating, work_life_balance, culture_values, career_opp, comp_benefits, senior_mgmt) %>%\r\n#   # keeping companies with at least 300 records\r\n#   dplyr::group_by(firm) %>%\r\n#   dplyr::mutate(n = n()) %>%\r\n#   dplyr::ungroup() %>%\r\n#   dplyr::filter(n >= 500) %>%\r\n#   dplyr::select(-n) %>%\r\n#   # renaming employee status and keeping only current and former employees\r\n#   dplyr::mutate(\r\n#     status = tolower(current),\r\n#     status = case_when(\r\n#       stringr::str_detect(status, \"\\\\bcurrent\\\\b\") ~ \"Current employee\",\r\n#       stringr::str_detect(status, \"\\\\bformer\\\\b\") ~ \"Former employee\",\r\n#       TRUE ~ \"Unknown\"\r\n#     )\r\n#     ) %>%\r\n#   dplyr::filter(status != \"Unknown\") %>%\r\n#   dplyr::select(-current) %>%\r\n#   # changing wide format to long one\r\n#   tidyr::pivot_longer(overall_rating:senior_mgmt, names_to = \"rating_dimension\", values_to = \"value\") %>%\r\n#   # removing missing values\r\n#   dplyr::filter(!is.na(value)) %>%\r\n#   # renaming rating dimensions\r\n#   dplyr::mutate(rating_dimension = case_when(\r\n#     rating_dimension == \"overall_rating\" ~ \"Overall rating\",\r\n#     rating_dimension == \"work_life_balance\" ~ \"Work-life balance\",\r\n#     rating_dimension == \"culture_values\" ~ \"Culture values\",\r\n#     rating_dimension == \"career_opp\" ~ \"Career opportunities\",\r\n#     rating_dimension == \"comp_benefits\" ~ \"Compensation & benefits\",\r\n#     rating_dimension == \"senior_mgmt\" ~ \"Senior management\",\r\n#     TRUE ~ \"Unknown\"\r\n#   )\r\n#   ) %>%\r\n#   # removing unknown rating dimensions\r\n#   dplyr::filter(rating_dimension != \"Unknown\")\r\n\r\n# to save space in my GitHub repo, I will upload already filtered dataset saved as .RDS file\r\nmydata <- readRDS(\"./glassdoor_reviews_filtered.RDS\")\r\n\r\n\r\n# dataviz \r\n# computing weighted average probability of a given rating for companies in the sample\r\nvizData <- mydata %>%\r\n  dplyr::group_by(firm, status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    n = n()\r\n  ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::group_by(firm, status, rating_dimension) %>%\r\n  dplyr::mutate(nAll = sum(n)) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    prop = n/nAll,\r\n    wprop = prop*nAll\r\n    ) %>%\r\n  dplyr::group_by(status, rating_dimension, value) %>%\r\n  dplyr::summarise(\r\n    wpropsum = sum(wprop),\r\n    w = sum(nAll)\r\n    ) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(fprop = wpropsum/w)\r\n\r\n# chart\r\nvizData %>%\r\n  dplyr::mutate(rating_dimension = factor(rating_dimension, levels = c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\"))) %>%\r\n  ggplot2::ggplot(aes(x = value, y = fprop, fill = forcats::fct_rev(status))) +\r\n  ggplot2::geom_bar(stat = \"identity\", position=\"dodge\") +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::scale_fill_manual(values = c(\"Current employee\" = \"#4E79A7\", \"Former employee\" = \"gray\")) +\r\n  ggplot2::facet_wrap(~rating_dimension, ncol = 3, scales = \"fixed\") +\r\n  ggplot2::labs(\r\n    title = \"<span style='font-size:22pt;font-weight:bold;'>**Comparison of Glassdoor ratings from** \r\n    <span style='color:#4E79A7;'>**current**<\/span> **and**\r\n    <span style='color:#999696;'>**former employees**<\/span>\r\n    <\/span>\",\r\n    caption = \"\\nBased on a sample of ratings from 792,390 individuals across 165 companies with more than 500 records each.\\nThe values represent the weighted average probability of a given rating for companies in the sample.\",\r\n    x = \"RATING\",\r\n    y = \"PROBABILITY OF RATING\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = ggtext::element_markdown(face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    strip.text.x = element_text(size = 13, face = \"bold\"),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    legend.position=\"\",\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\nOn a more serious note, it may be quite interesting and potentially useful to examine the order of estimated differences in specific areas between current and former employees as it may provide some insights on which areas to focus on when trying to retain employees within the company. We can use a multilevel ordered regression analysis on a random sample of 300 ratings per company for this purpose.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(ordinal)\r\nlibrary(broom.mixed)\r\nlibrary(parameters)\r\n\r\n# modeling responses using multilevel ordered regression analysis\r\nfits <- data.frame()\r\n# looping over individual dimensions\r\nfor(scale in c(\"Overall rating\", \"Work-life balance\", \"Culture values\", \"Career opportunities\", \"Compensation & benefits\", \"Senior management\")){\r\n  #print(scale)\r\n  set.seed(1234)\r\n  model <- ordinal::clmm(\"value ~ status + (1 | firm)\", data = mydata %>% dplyr::filter(rating_dimension == scale) %>% dplyr::mutate(value = factor(value,ordered = TRUE)) %>% dplyr::group_by(firm) %>% dplyr::sample_n(300) %>% dplyr::ungroup())\r\n  #summary(model)\r\n  \r\n  # extracting information about fitted models\r\n  supp <- broom.mixed::tidy(model) %>%\r\n    dplyr::filter(term == \"statusFormer employee\") %>%\r\n    dplyr::bind_cols(parameters::ci(model) %>% filter(Parameter == \"statusFormer employee\") %>% select(CI_low, CI_high)) %>%\r\n    dplyr::select(-coef.type) %>%\r\n    dplyr::mutate(scale = scale) %>%\r\n    dplyr::select(scale, everything())\r\n  \r\n  fits <- dplyr::bind_rows(fits, supp)\r\n  \r\n}\r\n\r\nfits %>%\r\n  arrange(estimate)\r\n\r\n                    scale                  term   estimate  std.error\r\n1          Culture values statusFormer employee -0.6750302 0.01712937\r\n2          Overall rating statusFormer employee -0.6373467 0.01707703\r\n3       Senior management statusFormer employee -0.6314480 0.01693096\r\n4    Career opportunities statusFormer employee -0.5943039 0.01693091\r\n5       Work-life balance statusFormer employee -0.5263801 0.01689702\r\n6 Compensation & benefits statusFormer employee -0.3574820 0.01691018\r\n  statistic       p.value     CI_low    CI_high\r\n1 -39.40777  0.000000e+00 -0.7086032 -0.6414573\r\n2 -37.32187 7.251441e-305 -0.6708171 -0.6038763\r\n3 -37.29547 1.943268e-304 -0.6646320 -0.5982639\r\n4 -35.10171 6.347105e-270 -0.6274878 -0.5611199\r\n5 -31.15224 4.729748e-213 -0.5594977 -0.4932626\r\n6 -21.14005  3.407214e-99 -0.3906253 -0.3243386\r\n\r\nCaveat: As the title of this post implies, readers should be aware that numerous biases can distort the portrayal of employee experiences reflected in Glassdoor ratings. Some of the most significant biases include survivorship bias, social desirability, non-response bias, self-selection, and motivated reasoning.\r\nDr.¬†Paul De Young‚Äôs personal experience in this regard is quite telling: ‚ÄúThere is often a high preponderance of phony ratings among so-called current employees on Glassdoor. Beware of bogus ‚Äúpart time‚Äù current employees giving high ratings, especially if the company does not employ a lot of part-time employees.Also, I learned from an HR executive that if you want to get ratings up on Glassdoor, encourage ALL your employees to rate the company. Most often it is the mistreated employees who post because this is an outlet for their misfortune. By getting more employees to rate, chances are your ratings will increase. Watch for actively monitored employers on Glassdoor. You can usually tell a bogus rating because there is a high rating with very few comments in jobs that do not exist. The first thing I do when looking at a company is to filter out the part time employees and look at the impact on the overall scores. If they jump down, you have to wonder about the validity of the ratings. Read the comments, they are more telling. There are all kinds of ways to game the system. Glassdoor is helpful, but doesn‚Äôt always give you a valid picture without looking at the details, which is where the devil lives.‚Äù\r\nHowever, it doesn‚Äôt mean that there is no signal in Glassdoor ratings. For example, behavioral scientists at Culture Amp investigated the relationship between Glassdoor ratings and employee engagement data collected by Culture Amp. The findings suggested a strong correlation between employee engagement and Glassdoor ratings, particularly as the number of reviews increases (r = 0.69 for 100+ reviews). Companies with higher engagement scores tended to have better Glassdoor ratings, including higher CEO approval percentages and a greater likelihood of being recommended as a workplace. The study also identified the five factors with the largest relationship to Glassdoor scores, which included Learning and Development, Service and Quality Focus, Decision-making, Leadership, and Collaboration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-24-glassdoor/./glassdoor.png",
    "last_modified": "2023-04-24T23:58:31+02:00",
    "input_file": {},
    "preview_width": 768,
    "preview_height": 595
  },
  {
    "path": "posts/2023-04-18-multilevel-correlation/",
    "title": "In need of multilevel correlations?",
    "description": "A post about a great R package to reach for when you need to calculate correlations on nested data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-18",
    "categories": [
      "correlation",
      "multilevel analysis",
      "hierarchical analysis",
      "r"
    ],
    "contents": "\r\nI am currently in the middle of a project where I am working with nested data and need to report multilevel correlations.\r\nTo my surprise, for quite a long time I couldn‚Äôt find any libraries in the R or Python ecosystems that provided an easy-to-use implementation of this type of analysis. I thought I would have to code it from scratch using Stan or JAGS.\r\nFortunately, I discovered a fantastic correlation package (part of the easystats universe) that can compute various types of correlations, including multilevel correlations, partial correlations, Bayesian correlations, polychoric correlations, biweight correlations, distance correlations, and more.\r\nSince nested data is (almost) everywhere, consider trying this package as it can make your life as an analyst a little bit easier üòâ Check out the code below to see it in action.\r\n\r\n\r\nShow code\r\n\r\n# Uploading libraries and creating custom functions\r\nlibrary(tidyverse)\r\nlibrary(correlation)\r\nlibrary(ggsci)\r\nlibrary(MASS) \r\n\r\n# Creating simulated dataset with nested data\r\n\r\n# Setting some basic parameters of the dataset\r\nnum_teams <- 7\r\nteam_ids <- LETTERS[1:num_teams]\r\nmin_rows <- 35\r\n\r\n# Defining function to generate data for a team with specified correlation\r\ngenerate_team_data <- function(team_id, correlation, job_sat_mean, agility_maturity_mean) {\r\n  \r\n  # Creating covariance matrix\r\n  covariance <- correlation * (20 * 50)\r\n  means <- c(job_sat_mean, agility_maturity_mean)\r\n  cov_matrix <- matrix(c(100, covariance, covariance, 2500), nrow = 2)\r\n  \r\n  # Generating correlated data\r\n  data <- MASS::mvrnorm(n = min_rows, mu = means, Sigma = cov_matrix)\r\n  \r\n  # Scaling the data\r\n  data[, 1] <- scale(data[, 1],center = FALSE,scale = TRUE)\r\n  data[, 2] <- scale(data[, 2],center = FALSE,scale = TRUE)\r\n  \r\n  # Putting data into dataframe\r\n  df <- data.frame(\r\n    TeamID = team_id,\r\n    JobSatisfaction = data[, 1],\r\n    AgilityMaturity = data[, 2])\r\n  \r\n  return(df)\r\n  \r\n}\r\n\r\n# Generating random means for job satisfaction and agility maturity for each of the teams within some range\r\nset.seed(42)\r\njob_sat_means <- runif(num_teams, min = -5, max = 5)\r\nagility_maturity_means <- runif(num_teams, min = 40, max = 60)\r\n\r\n# Generating random correlations for each of the teams team within some range\r\nset.seed(421)\r\ncorrelations <- runif(num_teams, min = -0.3, max = 0.4)\r\n\r\n# Generating data for each team and store in a list\r\nset.seed(123)\r\nteam_data <- mapply(generate_team_data, team_id = team_ids, correlation = correlations, job_sat_mean = job_sat_means, agility_maturity_mean = agility_maturity_means, SIMPLIFY = FALSE)\r\n\r\n# Combining team data into a single data frame\r\nsimulated_data <- do.call(rbind, team_data)\r\n\r\n# Computing multilevel Bayesian Pearson  correlation analysis\r\nc <- correlation::correlation(\r\n  simulated_data,\r\n  method = \"pearson\", \r\n  multilevel = TRUE, \r\n  bayesian = TRUE\r\n)\r\n\r\n# Extracting results of the analysis to be included in the chart defined below\r\nPearson_r = c[1,3]\r\nCI95L = c[1,4]\r\nCI95H = c[1,5]\r\n\r\n# Plotting the chart\r\nggplot2::ggplot(simulated_data, aes(y = JobSatisfaction, x = AgilityMaturity, color = TeamID)) +\r\n  ggplot2::geom_point(size = 3) +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE, size = 1.5) + \r\n  ggsci::scale_color_tron() +\r\n  ggplot2::labs(\r\n    y = \"JOB SATISFACTION\",\r\n    x = \"PERCEIVED ORGANIZATIONAL AGILITY MATURITY\",\r\n    title = \"Is organizational agility related to job satisfaction?\",\r\n    subtitle = stringr::str_glue(\"Bayesian Pearson r = {round(Pearson_r,2)}; 95% CrI: [{round(CI95L,2)}, {round(CI95H,2)}]\")\r\n  ) +\r\n \r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 15, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.line = element_line(colour = \"#E0E1E6\"),\r\n    axis.ticks = element_line(color = \"#E0E1E6\"),\r\n    strip.text.x = element_text(size = 11, face = \"plain\"),\r\n    legend.position= \"bottom\",\r\n    legend.key = element_rect(fill = \"white\"),\r\n    legend.key.width = unit(1.6, \"line\"),\r\n    legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(color = guide_legend(nrow = 1))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-18-multilevel-correlation/./plot.png",
    "last_modified": "2023-04-25T09:23:35+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2023-04-17-time-management/",
    "title": "Consequences of time management in the workplace",
    "description": "Some interesting insights from a meta-analytic review of the consequences of time management behaviors in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-17",
    "categories": [
      "time management",
      "meta-analysis",
      "job satisfaction",
      "job performance",
      "stress",
      "burnout"
    ],
    "contents": "\r\nBedi & Sass (2022) conducted a meta-analytic review of the consequences of employee time management behaviors on several employee outcomes. What are the main insights?\r\nIt may not come as a big surprise, but it is encouraging that data support the association between time management and various beneficial employee outcomes, such as increased job satisfaction, job performance, and lower levels of stress and burnout. Unfortunately, the ‚Äúproven‚Äù association is not causal, as the majority of studies were cross-sectional. In fact, there are not many studies on the causal effects of time management. The exception to this is procrastination, for which there is evidence that time management can help - see, for example, the meta-analysis by Van Eerde & Klingsieck (2018) on this topic.\r\nThe relationship between time management and employee outcomes is not only direct but also partially mediated by work-family conflict. This finding underscores the importance of work-life balance and highlights the need for organizations to help employees better address this specific issue, as it may positively affect a variety of employee outcomes.\r\n\r\nPerceived control over time, achieved through the use of time management, shows incremental validity in predicting job satisfaction, job performance, and stress with respect to the personality trait of conscientiousness. This suggests that regardless of an individual‚Äôs innate level of prudence, they may benefit from adopting time management in their professional lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-17-time-management/./tm.jpg",
    "last_modified": "2023-04-17T20:06:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-13-interpretable-ml/",
    "title": "Interpretable machine learning with modelStudio",
    "description": "There's a new kid on the block in the R ecosystem that can help analysts understand the behavior of their ML models.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-13",
    "categories": [
      "interpretability",
      "explainability",
      "machine learning",
      "predictive models",
      "r"
    ],
    "contents": "\r\nThere‚Äôs a great new R package, modelStudio, that makes it much easier for analysts to create both global and local interpretations of predictive models using an interactive interface.\r\nOnce you‚Äôve trained the model, you just get the DALEX explainer object ready and start up modelStudio that will run the following analyses (among others) and show the corresponding plots:\r\nFeature Importance: A visual representation that ranks and displays the significance of each input variable in a predictive model, helping to identify the most influential features for model predictions.\r\nPartial Dependence: A visualization that shows the relationship between a feature and the predicted outcome while averaging out the effects of all other features, to understand the marginal impact of a specific feature on the model‚Äôs predictions.\r\nAccumulated Dependence: Similar to the previous method, but reducing the influence of the assumption of uncorrelated features, providing a more robust and reliable representation of the feature‚Äôs impact on the model‚Äôs predictions.\r\nBreak Down Plot: A graphical explanation tool that demonstrates the contribution of each feature to a specific instance‚Äôs prediction, allowing for individual-level interpretation of model outcomes.\r\nShapley Values: A cooperative game theory-based approach for fairly attributing each feature‚Äôs contribution to a specific prediction, providing interpretable and consistent explanations for machine learning models.\r\nCeteris Paribus: A method that helps with understanding the influence of individual features on specific predictions by isolating the effect of a single variable while holding all other variables constant.\r\nIf you use ML in HR or any other field where it‚Äôs crucial to explain why you make specific predictions, classifications, and the resulting recommendations or decisions, definitely give it a try.\r\nWhat follows is a short demonstration of the tool using the well-known artificial IBM attrition dataset. First, let‚Äôs import the attrition dataset from the modeldata library and change the coding of the criterion variable, which will later make it easier to set up the DALEX explainer, which requires numerical data type for criterion variable even in classification tasks.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(modeldata)\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata(\"attrition\")\r\n\r\n# changing the coding of the criterion variable\r\nattrition <- attrition %>%\r\n  mutate(Attrition = recode(Attrition, \"Yes\" = \"1\", \"No\" = \"0\") %>% factor(levels = c(\"1\", \"0\")))\r\n\r\n\r\nWe now split the data into training, test, and validation datasets so that we can tune the prediction model, fit the model, and test its performance on new, previously unseen data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(rsample)\r\n\r\n# splitting data into train, validation, and test datasets\r\nset.seed(123)\r\ndata_split <- rsample::initial_split(attrition, strata = Attrition, prop = 0.8)\r\n\r\ndata_train <- rsample::training(data_split)\r\ndata_test  <- rsample::testing(data_split)\r\ndata_val <- rsample::validation_split(data_train, strata = \"Attrition\", prop = 0.8)\r\n\r\n\r\nNow let‚Äôs define the whole model training workflow, which includes the data adjustment pipeline and the specification of the model used. We will use XGBoost, presumably the best ML algorithm for tabular type of data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(recipes)\r\nlibrary(parsnip)\r\nlibrary(workflows)\r\n\r\nfmla <- as.formula(paste(\"Attrition\", \" ~ .\"))\r\n\r\n# defining recipe for adjusting data for fitting the model\r\nxgb_recipe <- \r\n  recipes::recipe(fmla, data = data_train) %>%\r\n  recipes::step_ordinalscore(recipes::all_ordered_predictors()) %>%\r\n  recipes::step_dummy(recipes::all_factor_predictors())\r\n\r\n# defining the model\r\nxgb_model <- \r\n  parsnip::boost_tree(mtry = tune(), min_n = tune(), tree_depth = tune(), trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\nxgb_workflow <- \r\n  workflows::workflow() %>% \r\n  workflows::add_model(xgb_model) %>% \r\n  workflows::add_recipe(xgb_recipe)\r\n\r\n\r\nAlthough the XGBoost algorithm works quite well with the default hyper-parameters, we will use the validation dataset to tune some of its hyper-parameters to get the best performance out of it. As can be seen below, after tuning the best model performs quite well in terms of AUC, which has a value of around 0.8.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tune)\r\nlibrary(yardstick)\r\n\r\n# tuning hyper-parameters\r\nset.seed(123)\r\nxgb_tuning <- \r\n  xgb_workflow %>% \r\n  tune::tune_grid(\r\n    data_val,\r\n    grid = 25,\r\n    control = control_grid(save_pred = TRUE),\r\n    metrics = yardstick::metric_set(roc_auc)\r\n    )\r\n\r\n# selecting the best combination of hyper-parameters \r\nxgb_best <- \r\n  xgb_tuning %>% \r\n  tune::select_best(metric = \"roc_auc\")\r\n\r\n# best model performance on validation dataset as measured by AUC \r\n(\r\n  xgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_auc(truth = Attrition,  .pred_1) \r\n)\r\n\r\n# A tibble: 1 √ó 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.802\r\n\r\nShow code\r\n\r\n# plotting the ROC curve\r\nxgb_tuning %>% \r\n  tune::collect_predictions(parameters = xgb_best) %>% \r\n  yardstick::roc_curve(truth = Attrition,  .pred_1) %>%\r\n  autoplot()\r\n\r\n\r\n\r\nNow we can set up the final model training workflow and fit the model to the entire training dataset and check its performance on out-of-sample data using k-fold cross-validation and testing dataset. As we see below, in both cases the model performance as measured by AUC is around the value of 0.8.\r\n\r\n\r\nShow code\r\n\r\n# setting the final model\r\nfinal_xgb_model <- \r\n  parsnip::boost_tree(mtry = xgb_best$mtry, min_n = xgb_best$min_n, tree_depth = xgb_best$tree_depth, trees = 1000) %>% \r\n  parsnip::set_engine(\"xgboost\") %>% \r\n  parsnip::set_mode(\"classification\")\r\n\r\n# updating the model training workflow\r\nfinal_xgb_workflow <- \r\n  xgb_workflow %>% \r\n  workflows::update_model(final_xgb_model)\r\n\r\n# fitting model on train set\r\nset.seed(123)\r\nxgb_fit <- \r\n  final_xgb_workflow %>% \r\n  fit(data_train)\r\n\r\n# checking the final model's performance (AUC) using k-fold cross-validation\r\nset.seed(123)\r\nfolds <- rsample::vfold_cv(data_train, v = 10)\r\n\r\nset.seed(123)\r\nxgb_fit_kf <- \r\n  final_xgb_workflow %>% \r\n  tune::fit_resamples(folds)\r\n\r\n(\r\n  tune::collect_metrics(xgb_fit_kf, summarize = TRUE) %>% dplyr::filter(.metric == \"roc_auc\")\r\n)\r\n\r\n# A tibble: 1 √ó 6\r\n  .metric .estimator  mean     n std_err .config             \r\n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 roc_auc binary     0.799    10  0.0166 Preprocessor1_Model1\r\n\r\nShow code\r\n\r\n# checking the final model's performance (AUC) using the testing dataset\r\nxgb_testing_pred <- \r\n  predict(xgb_fit, data_test) %>% \r\n  bind_cols(predict(xgb_fit, data_test, type = \"prob\")) %>% \r\n  dplyr::bind_cols(data_test %>% select(Attrition))\r\n\r\n(\r\n  xgb_testing_pred %>%           \r\n  yardstick::roc_auc(truth = Attrition, .pred_1)\r\n)\r\n\r\n# A tibble: 1 √ó 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.871\r\n\r\nNow we‚Äôre ready to explore the inner workings of our trained model. After setting up the explainer from the DALEX package, we simply insert this object into the modelStudio function and run it. We then get an interactive interface in our browser that we can use to easily check what our model is doing to make its predictions. You can try it out for yourself using the interactive interface below.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(DALEX)\r\nlibrary(modelStudio)\r\n\r\n# setting up the DALEX explainer object\r\n# creating predict function\r\npred <- function(model, newdata)  {\r\n  results <- (predict(model, newdata, type = \"prob\")[[1]])\r\n  return(results)\r\n}\r\n\r\nexplainer <- DALEX::explain(\r\n  model = xgb_fit,\r\n  data = data_test %>% mutate(Attrition = as.integer(Attrition)),\r\n  y = data_test %>% mutate(Attrition = as.integer(Attrition)) %>% pull(Attrition),\r\n  predict_function = pred,\r\n  type = \"classification\",\r\n  verbose = FALSE \r\n)\r\n\r\n# running ModelStudio\r\nmodelStudio::modelStudio(\r\n  explainer,\r\n  max_features = 100, # Maximum number of features to be included in BD, SV, and FI plots\r\n  N = 300, # Number of observations used for the calculation of PD and AD\r\n  new_observation_n = 3, #Number of observations to be taken from the data\r\n  show_info = TRUE,\r\n  viewer = \"browser\",\r\n  facet_dim = c(2,2) # layout of the resultiing charts\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-13-interpretable-ml/./miracle.jpg",
    "last_modified": "2023-04-25T09:27:11+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-openai-personality-interpretation/",
    "title": "Ask your personality using GPT",
    "description": "Can Generative AI like GPT meaningfully interpret personality profiles?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-04-11",
    "categories": [
      "gpt",
      "ai",
      "personality",
      "self-awareness",
      "hogan personality inventory",
      "shiny app",
      "r"
    ],
    "contents": "\r\nOne of my friends recently asked me if I could provide him with an interpretation of a free personality test he took on the internet. As a joke, I asked him if he had already tried using GPT for this.\r\nThis sparked my interest in how GPT would actually handle this kind of task. So, I provided it with my Hogan Personality Inventory (HPI) profile (in percentile scores, as requested), and to my surprise, it performed quite well - even when asked about more complex questions like interactions between my scores on selected scales or my strengths and weaknesses for specific jobs and tasks.\r\nBased on this experience, I created a simple POC app where users can input their HPI profile and some contextual information, such as their current or desired job, and ask GPT predefined or their own questions about their personality.\r\nLink to the app: https://aanalytics.shinyapps.io/ask_your_personality/\r\n\r\nI‚Äôve only tested it on a few profiles, so if you know your HPI profile (or your Big Five traits that are behind HPI), I would be happy to hear how you perceive the face validity and potential usefulness of the generated interpretations. Perhaps I have just become a victim of the well-known Barnum effect üòâ\r\nI am well aware that there are clear risks associated with using a generic GPT for such a task. However, I believe that with proper fine-tuning, an explicit disclaimer, and access to a qualified professional for possible consultation, it could be a useful tool for helping people gain self-awareness more easily - in many situations a crucial prerequisite for high-quality decisions. What do you think?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-openai-personality-interpretation/./introPic2.jfif",
    "last_modified": "2023-04-11T20:05:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-engagement-interventions/",
    "title": "Effectiveness of interventions for encreasing employee engagement",
    "description": "What evidence do we have for the effectiveness of interventions for increasing employee engagement? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-27",
    "categories": [
      "employee engagement",
      "work engagement",
      "interventions",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is some evidence that engagement of employees has positive causal impact on the bottom line of organizations (see, for example, the meta-analysis by Harter et al.¬†(2010), however, be aware of the specific definition of engagement used there, which focuses more on the contextual factors and conditions enabling engagement and less on the psychological states of engagement). Consequently, we might be naturally interested in whether we can positively influence engagement of employees through the use of certain interventions.\r\nBased on a systematic review and meta-analysis of studies with controlled workplace interventions by Knight, Patterson, and Dawson (2017), it seems the answer might be yes. The authors found a small positive effect on work engagement and each of its three sub-components: vigor, dedication, and absorption, as measured by the Utrecht Work Engagement Scale (UWES) from Bakker and Schaufeli.\r\nWhen it comes to the types of intervention (personal resource building, job resource building, leadership training, and health promotion), a moderator analysis did not find evidence for their differing effectiveness. However, there was evidence for a medium to strong effect of intervention style in favor of group interventions (vs.¬†individual), with the possible explanation being that group interventions effectively influence certain work engagement antecedents, such as social support and influence in decision-making.\r\n\r\nRegarding the sustainability of effects, there was no significant effect of time in the case of overall work engagement. However, for the vigor sub-component, there were stronger effects immediately post-intervention than at follow-up, with the opposite being true for dedication and absorption sub-components.\r\nHave you implemented any interventions to boost engagement of employees in your organization? Did you measure their effectiveness? What were your experiences and results? Feel free to share your thoughts in the comments below.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-engagement-interventions/./training.jpg",
    "last_modified": "2023-04-11T19:33:41+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-managers-overconfidence/",
    "title": "Where do managers put on their rose-tinted glasses the most?",
    "description": "In which areas are managers and leaders prone to overconfidence, and how can this overconfidence potentially impact team functioning? Let's check some data to address this question.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-23",
    "categories": [
      "team management",
      "blind spots",
      "rocket model",
      "team assessment survey"
    ],
    "contents": "\r\nThe ‚Äúillusory superiority‚Äù or ‚Äúbetter-than-average effect‚Äù causes individuals to overestimate their abilities compared to others. Unsurprisingly, even managers and leaders fall for this trap. After all, they‚Äôre human too, right? üòâ For a review of studies on this topic, see, for example, the systematic review by Heavey et al.¬†(2022).\r\nTo pinpoint particular areas where managers and leaders are prone to overconfidence, data from the Team Assessment Survey (TAS), a team assessment tool rooted in the Rocket Model of team performance, can be valuable as it enables comparisons between how team leaders and team members perceive their team‚Äôs functioning. For example, based on data from a Slovak sample of 85 teams with 835 members, it seems that team leaders rate their team‚Äôs effectiveness way better than team members in the following five areas in descending order:\r\nResources: Does the team have the budget, equipment, authority, and political capital it needs to accomplish its goals?\r\nTalent: Is the team sized correctly? Are team members‚Äô roles clear? Does the team have the right skills to succeed? Are people developing new skills? Do rewards encourage or discourage teamwork?\r\nContext: Are team members in agreement about the team‚Äôs political and economic realities, customers, competitors, suppliers, and key assumptions and challenges?\r\nMission: What is the team‚Äôs purpose? What are its goals? How does the team define winning? What are its strategies and plans for accomplishing its goals?\r\nCourage: Do team members trust each other? Is there an optimal level of tension and collaboration on this team? Do team members challenge each other in a constructive manner?\r\n\r\nWhen managers get overconfident in these areas, it can lead to all sorts of issues like underinvestment in critical resources, inefficient resource allocation, ignoring skill gaps, insufficient role clarity, making bad decisions, disjointed strategic planning, confusion, misaligned priorities, uncoordinated efforts, blocking innovation, etc.\r\nWhat‚Äôs the fix? Among other things, managers need to be aware of their own biases and work on open communication, feedback, and collaboration with team members. Easier said than done, but it‚Äôs crucial to avoid hurting the performance of the team.\r\nObviously, the size and demographics of the sample used are limited and conclusions are therefore difficult to generalize, but I may try to check with Dr.¬†Gordon Curphy, the author of the Rocket Model of team performance and TAS, to see if this pattern is also replicated in a larger, international sample.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-managers-overconfidence/./pinkGlasses.jpg",
    "last_modified": "2023-04-11T15:58:18+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-app-piloting-and-dif/",
    "title": "Estimating the impact of a new business app by piloting & method of difference-in-differences",
    "description": "What is the benefit of using the difference-in-differences method in combination with piloting a new business app, and how can this help estimate the app's effectiveness on key outcomes like time spent with prospects or closed deals?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-20",
    "categories": [
      "pilot program",
      "difference in differences",
      "data-driven decision-making"
    ],
    "contents": "\r\nWhen considering the introduction of a new business app, one of the benefits of piloting it is that it provides a great opportunity to test its causal impact on business processes or outcomes of interest. For example, in our pilots, apps are typically implemented for 3 or 6 months only in some teams and not in others. Such a situation creates ideal conditions for applying the difference-in-differences (DiD) method, which is used to approximate an experimental research design with observational data only.\r\nTo use one specific example, one of the problems addressed by our Sales Analytics app is that sales reps spend more time collaborating internally instead of communicating with prospects. The premise may be that the better visibility into time spent that the app enables will help sales reps and their managers better plan activities during their regular weekly 1-on-1 meetings, all with (hopefully) a positive impact on time spent with prospects.\r\nBy piloting the app in just one team and finding another team with a similar, parallel trend in the selected criterion, we can try to estimate its effectiveness. As shown in the attached chart, the fitted DiD model in this particular case slightly supports the effectiveness of the app, at least in terms of the amount of time spent with prospects, but can easily be switched to another criterion that is closer to the company‚Äôs bottom line, e.g., the number of closed deals. Moreover, if we are aware of certain systematic differences between the teams, such as the experience level of the sales reps, we can include relevant control variables in the model to achieve a more accurate estimation of the app‚Äôs effectiveness.\r\n\r\nSo, next time you consider introducing a new business app, consider piloting it in combination with the DiD method to better understand its impact on your organization‚Äôs goals. Happy piloting! ‚úåÔ∏è\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-app-piloting-and-dif/./impact.jpg",
    "last_modified": "2023-04-11T15:04:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-meeting-yes-men/",
    "title": "Are there meeting ‚Äúyes-men‚Äù?",
    "description": "One of our clients was struggling with meeting overload and wanted to know if the people who attend too many meetings are the kind of \"yes-men\" who just can't say no to meeting invites. You know the type - always saying \"yes\" and never protecting their precious time. What did they find?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-17",
    "categories": [
      "meeting culture",
      "meeting overload"
    ],
    "contents": "\r\nTo test this hypothesis, they looked at the monthly number of meetings people attended and the relative frequency of their responses to meeting invitations. Here‚Äôs what they found:\r\nPeople seemed to be similarly explicit in signaling their intentions about the meetings they were invited to, regardless of how many meetings they had on their plate.\r\nPeople accepted fewer meeting invites the more meetings they attended.\r\nThose who went to a bunch of meetings were more likely to say they‚Äôre not sure if they can make it or not.\r\nPeople who were busy with meetings declined more meeting invites than those who had fewer meetings.\r\n\r\nThus, contrary to initial expectations, the data showed that people who attended more meetings, on average, tended to accept fewer invites, were more likely to be unsure about their availability, and actually declined more invites than those with fewer meetings.\r\nWhile the client couldn‚Äôt rule out that there might be some individuals fitting ‚Äúyes-men‚Äù description in their company (in fact, one can easily spot a few people in the corresponding chart who attended many meetings and at the same time underutilized the option of declining the meeting invites), these results suggested that there isn‚Äôt a systematic problem in this specific area. Time for our client to explore other avenues through which the problem with meeting overload could be addressed. More on that in some of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-meeting-yes-men/./yes_man_yes.gif",
    "last_modified": "2023-04-11T14:44:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/",
    "title": "Impact of employee satisfaction at work on a company's bottom line",
    "description": "While there is evidence supporting the connection between employee satisfaction and a company's bottom line, it's essential to determine whether higher satisfaction directly causes better performance. Is there some evidence for that? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-14",
    "categories": [
      "performance",
      "employee satisfaction",
      "meta-analysis",
      "i/o psychology"
    ],
    "contents": "\r\nThere is now little doubt that employee attitudes, such as employee satisfaction or employee engagement, are to some extent related to performance - for some evidence in support of this claim, see, for example, the meta-analysis of the relationships between employee satisfaction, employee engagement and business unit-level performance conducted by Harter, Schmidt, & Hayes (2002).\r\nBut does it mean that higher employee satisfaction causes higher performance? As any statistically savvy person knows, not necessarily.\r\nFortunately, there are research designs that can help us untangle this conundrum a bit. One of these designs is path analysis of longitudinal/time-series data. This approach was also taken by Harter et al.¬†(2010) in their meta-analysis ‚ÄúCausal Impact of Employee Work Perceptions on the Bottom Line of Organizations‚Äù with the following results:\r\n‚ÄúUsing a massive longitudinal database that included 2,178 business units in 10 large organizations, we found evidence supporting the causal impact of employee perceptions on [‚Ä¶] bottom-line measures [customer loyalty, employee retention, revenue, and profit]; reverse causality of bottom-line measures on employee perceptions existed but was weaker.‚Äù\r\nThe attached figure with the two alternative path models clearly shows that the causal path from employee perceptions to outcomes is stronger than the other way around, especially when it comes to theoretically more proximal outcomes such as employee retention and customer loyalty.\r\n\r\nDespite not being free of potential biases, these results add more weight to the argument that investing in improving employee job satisfaction also makes sense for improving a company‚Äôs bottom line, rather than ‚Äúonly‚Äù improving employee well-being. The question now is what interventions to improve employee satisfaction might work. Let‚Äôs review the available evidence on this issue in one of the next posts.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-employee-satisfaction-and-company-bottom-line/./successfulEmployees.jpg",
    "last_modified": "2023-04-11T13:39:56+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-11-excel-and-python/",
    "title": "Excel + Python = Word Document",
    "description": "Using combination of Excel and Python for semi-automatic Word document generation.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-10",
    "categories": [
      "excel",
      "python",
      "document",
      "automation"
    ],
    "contents": "\r\nIn the spirit of ‚ÄúExcel isn‚Äôt dead and is actually doing well‚Äù posts, I‚Äôm sharing one practical example of combining Excel and Python to do one specific job. It‚Äôs not as exciting and sexy as ChatGPT, but it may still come in handy for someone, as it did for one of my friends.\r\nA friend of mine who works in psychological counselling has to write a large number of reports which, among other things, contain many recommendations for various compensations and interventions depending on established diagnosis.\r\nTo make his job easier, he created a simple Excel spreadsheet with a list of diagnoses and corresponding recommendations. He needed to generate a simple Word document listing and describing the appropriate compensations and interventions after he had marked the appropriate diagnoses for the client in Excel.\r\nI originally wanted to do it all in Excel, but since I‚Äôm not the best friend with VBA, I couldn‚Äôt get rid of the various text formatting issues. So I reached for Python and one of its document-related libraries and linked it via macro to Excel, which acts only as a source of input data, based on which Python generates a simple report when a button is pressed in Excel. Once generated, the document is ready for further editing and tuning.\r\nIf you are interested in technical details, you can check my GitHub page.\r\nMay the Excel be with you üôÇ\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-excel-and-python/./comboPic.png",
    "last_modified": "2023-04-11T13:06:43+02:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2023-04-11-job-attitudes-and-employee-outcomes/",
    "title": "Employee outcomes & employees' job attitudes",
    "description": "What employee outcomes are predicted by what employees' job attitudes? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-06",
    "categories": [
      "job attitudes",
      "job performance",
      "employee turnover",
      "organizational citizenship behavior"
    ],
    "contents": "\r\nIf you‚Äôve ever measured employee job attitudes (i.e.¬†constructs such as organizational commitment, procedural justice, distributive justice, job involvement, job engagement, job satisfaction, etc.), you probably won‚Äôt be surprised to learn that job attitudes are usually quite strongly correlated.\r\nThis is also the conclusion of a meta-analytic review of job attitudes by Woznyj et al.¬†(2022), which showed that job attitudes are moderately to strongly correlated with each other, with most relations falling between œÅ = .50 and .69.\r\nDespite this, relative weights and incremental validity analyses revealed that some attitudes have greater validity in predicting key employee outcomes. As shown in the table attached,\r\nperformance is most strongly predicted by job satisfaction, job engagement, and distributive justice (an employee‚Äôs perceived fairness of outcomes),\r\nturnover intentions is most strongly predicted by job satisfaction, perceived organizational support (a general evaluation regarding the extent to which employees feel their organization values their contribution and cares about their well-being), and distributive justice, and\r\norganizational citizenship behaviors is most strongly predicted by job engagement, procedural justice (perceived fairness of the means, or procedures, used to determine outcomes), and job involvement (the degree to which a person identifies psychologically with his or her work, or the importance of work in his or her total self-image).\r\n\r\nIMO, knowing this can be quite useful in planning what candidate constructs to measure in your company in an effort to support specific employee outcomes.\r\nWhat job attitudes do you regularly measure in your company? And does it pay off in any way, i.e.¬†does it have any tangible impact?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-11-job-attitudes-and-employee-outcomes/./employees.jpg",
    "last_modified": "2023-04-11T12:51:00+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-05-micro-breaks/",
    "title": "Effectiveness of micro-breaks at work",
    "description": "Quite satisfying news from a meta-analysis on the efficacy of micro-breaks for increasing well-being and performance in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "micro-break",
      "workplace",
      "well-being",
      "performance",
      "meta-analysis"
    ],
    "contents": "\r\nIf you feel you need a short break, take one - even if it is no longer than 10 minutes, it can increase your perceived well-being and some types of performance.\r\nAs the authors of the study, Albulescu et al.¬†(2022), conclude: ‚ÄúOur results revealed that micro-breaks are efficient in preserving high levels of vigor and alleviating fatigue. It seems that the effects are univocal and generalizable for the well-being outcomes. These were relatively homogeneous, and none of the included moderators were significant. Hence, the data suggest that micro-breaks may be a panacea for fostering well-being during worktime.\r\nWhen it comes to performance, the data revealed some nuances. The break duration was a significant covariate of the effect of micro-breaks: the longer the break, the better the performance. Moreover, the type of task from which participants were taking the break also emerged as a significant moderator. Micro-breaks could significantly increase performance for clerical work or creative exercises and not for a cognitively demanding task.‚Äù\r\nThere is nothing like having a meta-analysis to back up your habits, or better yet, initially bad habits üòâ\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-05-micro-breaks/./break.jpg",
    "last_modified": "2023-03-05T19:50:59+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-01-distracted-time/",
    "title": "Not-so-hidden cost of working in an office",
    "description": "Just a few data-backed thoughts on why many of us may often feel more distracted when working in an office.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-03-01",
    "categories": [
      "remote work",
      "in-office work",
      "collaboration",
      "focus time",
      "distraction"
    ],
    "contents": "\r\nWe have only recently started to measure the allocation of work time to collaborative and non-collaborative activities at Time is Ltd..\r\n\r\nWhen checking the resulting numbers in the context of where people work from, it wasn‚Äôt that much of a surprise that people spend less time (on average ~37 minutes less per person per day) on collaborative activities when working remotely vs.¬†in the office. Such a pattern is probably a good thing in many cases as it means that people have more time for focused work when working remotely, and use their time for intensive collaboration when in the office.\r\nWhat was quite surprising to me, however, was the rather large difference in the amount of distracted time. When working in the office there was much more distracted time, i.e.¬†time when people were working on their tasks while being distracted by various collaborative activities. Therefore, they didn‚Äôt have enough time to get into the flow. On average, the difference was a staggering ~69 minutes per person per day.\r\nHowever, when I thought about it a bit more (and also after I realized how we actually calculate distraction timeÔ∏è), it started to make more sense to me. After all, collaborative activities don‚Äôt just ‚Äúrob‚Äù us of time per se, they also fragment our available time into smaller chunks, which carries a cost in the form of cognitive overhead associated with task switching.\r\nI suppose it‚Äôs a trade-off that can‚Äôt be completely solved in principle, but can only be mitigated with tools like batching or timeboxing. What is your own experience with this phenomenon and what tools do you use to deal with it?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-01-distracted-time/./distraction.jpg",
    "last_modified": "2023-03-01T19:41:32+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-chatgpt-and-employee-feedback/",
    "title": "Using ChatGPT to summarize and explore employee feedback?",
    "description": "What's the potential use of tools like ChatGPT in analyzing open-ended feedback from employee engagement and satisfaction surveys? Let's take a look at the result of my little experiment in this area.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "chatgpt",
      "ai",
      "hrm",
      "people analytics",
      "employee engagement",
      "surveys"
    ],
    "contents": "\r\nIMHO, ChatGPT and similar tools may be quite useful in the near future (and probably to some extent even today) to quickly summarize and explore the feedback provided by employees through open-ended questions in employee engagement and satisfaction surveys.\r\nI just experimented with a small sample of anonymized employee feedback (just dozens of lines of text from a question about what employees would change in the company) and asked ChatGPT to summarize the feedback, identify the main topics covered in the feedback, and provide me with details about the selected topics - see the attached image of the conversation for illustration.\r\n\r\nJust based on my very limited sample, I found that there was a pretty good balance between information compression and accuracy, while the interaction was very natural, similar to asking HRBP what the results of the last ESS were for my department/team. Certainly, there are still too many known and unknown risks associated with these tools to rely blindly on them alone, but I can imagine that in the foreseeable future, when many of these risks are successfully mitigated, this will be one of the ways managers will listen to the voice of their people.\r\nHas anyone experimented with ChatGPT on similar kinds of HR data?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-chatgpt-and-employee-feedback/./chatgpt_listening.jpg",
    "last_modified": "2023-02-26T19:30:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-26-large-and-recurring-meetings/",
    "title": "Where to look first when considering meeting reset?",
    "description": "Let's briefly discuss the potential benefits of focusing on optimizing large recurring meetings to save time in the workplace.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-19",
    "categories": [
      "meeting overload",
      "meeting culture",
      "meeting reset"
    ],
    "contents": "\r\nWhen thinking about the meeting reset like they did in Asana, IMHO, it‚Äôs not a bad idea to focus on one specific category of meetings first.\r\nI am thinking of large recurring meetings that combine two big time wasters, and thus hide a greater opportunity for time savings:\r\nLarge meetings are quite often a waste of people‚Äôs time as they don‚Äôt allow everyone to meaningfully contribute. These meetings often serve only to disseminate information and can therefore be safely replaced by less intrusive asynchronous collaboration tools such as email, instant messaging or some kind of knowledge management tool.\r\nRecurring meetings have their place, but often tend to outlive their purpose over time and waste the time of everyone involved. One should therefore regularly ask oneself the following questions in this context and act accordingly: Are meetings being planned automatically, rather than out of necessity? Are attendees invited as a formality, or will they bring value? Is there still clear agenda for these meetings? Are we already sufficiently aligned or do we need some additional platform for doing so? Does the current frequency of our meetings meet our needs? etc.\r\nAs illustrated in the attached chart, large and recurring meetings can represent a relatively large chunk of time in an employee‚Äôs work month. Large meetings here account for approximately 37% of all meeting time and more than 80% of large meetings are recurring. This represents a relatively large room for optimizing the time spent in meetings.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-26-large-and-recurring-meetings/./fatigue.jpg",
    "last_modified": "2023-02-26T19:09:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-hrm-value-chain-and-sem/",
    "title": "HRM value chain and structural equation modeling - Moneyball case",
    "description": "What's the link between the HRM value chain and structural equation modeling? Let‚Äôs check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "hrm value chain",
      "people analytics",
      "path analysis",
      "structural equation modeling",
      "bayesian statistics",
      "r"
    ],
    "contents": "\r\nAs the economy cooled, more posts started appearing in my social\r\nbubble about how people analytics can prove its value, justify its\r\nexistence, and increase its resilience to layoffs.\r\nOne of the often-mentioned conceptual tools in this context is the\r\nHRM value chain which shows how people processes lead\r\nto achieving companies‚Äô desired business outcomes. There are more\r\nversions of this tool, e.g.¬†one from Jaap\r\nPaauwe and Ray Richardson or another one from Max\r\nBlumberg, but what they have in common is a kind of causal flow from\r\nHRM activities to HRM outcomes to business outcomes.\r\nExample of the HRM value chain\r\nmodel from Paauwe\r\n& Richardson (1997).\r\n\r\nExample of the HRM value chain\r\nmodel from Blumberg\r\n(2018).\r\nHowever, as potentially useful as this metaphor of the organization\r\nas a kind of ‚Äúmachine‚Äù with certain inputs, processes, and outputs is,\r\nit is still only a conceptual tool that may or may not correspond to the\r\nreality of a particular organization.\r\nIn this regard, it may help if we try to operationalize this\r\nmetaphor. In this effort, structural\r\nequation modeling can be very handy, as it allows us to\r\nformalize our ideas about the relationships between several different\r\nvariables and to assess the extent to which these ideas are consistent\r\nwith the available data. After all, no one wants to make decisions based\r\non false assumptions.\r\nTo illustrate this with a more tangible example, let‚Äôs use sabermetric data\r\nfrom the famous Moneyball\r\ncase and let‚Äôs try to formally model the Oakland A‚Äôs\r\n(OAK) as a ‚Äúmachine‚Äù that produces playoffs by trying to win more games\r\nor score more points than opposing teams, using inputs in the form of\r\nplayers‚Äô ability to play well at bat and in the field.\r\nBased on our expert knowledge of the game of baseball, our working\r\nhypotheses, and the results of some previous analyses, we can construct\r\na conceptual model of how a baseball team functions, as outlined\r\nbelow.\r\nConceptual model of how a\r\nbaseball team works.\r\nThe diagram shows clearly how this ‚Äúmachine‚Äù works: Its outputs are\r\nqualifications for the playoffs, which it achieves by trying to win more\r\ngames or score more points than the opposing teams; to do this, it uses\r\ninputs in the form of the players‚Äô ability to play well at bat and in\r\nthe field; the inputs affecting the ‚Äúmachine‚Äôs‚Äù operation are also the\r\nsimilar abilities of the opposing teams‚Äô players. This is, of course, a\r\nvery simplistic causal model of how the OAK team operates, but as the\r\nfamous statistical aphorism states, all models are\r\nwrong, but some are useful.\r\nHowever incomplete our models of how an organization works will\r\nalways be, it is essential to check that these models sufficiently\r\nreflect reality as conveyed by the available data. Only after such an\r\nassessment of the plausibility of the model it is reasonable to base\r\nfurther, e.g.¬†hiring or L&D decisions on it. And, as hinted at the\r\nbeginning of this post, we will use the statistical method of structural\r\nequation modeling to do this. So let‚Äôs apply this method to our model of\r\nthe OAK and test its plausibility.\r\nWe will use data from a publicly available database\r\nof MLB historical statistics. Specifically, we will use the\r\nfollowing variables:\r\nqualification for the playoffs in a given season\r\n(Playoffs),\r\nnumber of wins in a given season (W),\r\nnumber of points won in a given season (RS),\r\nnumber of points lost in a given season (RA),\r\naverage frequency with which a player reaches base per plate\r\nappearance in a given season (OBP - On-Base Percentage) and\r\nanalogous statistics for opponent teams (OOBP),\r\naverage number of bases players earn per at bat in a given season\r\n(SLG - Slugging Percentage) and analogous statistics for\r\nopponent teams (OSLG).\r\nIn terms of time, we will work with data from the years 1996-2001,\r\nwhich precede 2002, when the story of Moneyball mostly takes place.\r\n\r\n\r\nShow code\r\n\r\n# uploading set of libraries for data manipulation and visualization\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\nbaseball <- readr::read_csv(\"./baseball.csv\")\r\n\r\n# filtering data used for the analysis\r\nmyData <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995\r\n    ) %>%\r\n  dplyr::select(Team, OBP, SLG, OOBP, OSLG, RS, RA, W, Playoffs)\r\n\r\n# user-friendly table with data used for the analysis\r\nDT::datatable(\r\n  myData,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\nNow let‚Äôs define a formal model of the OAK and fit it to the data\r\nusing the brms R\r\npackage which allows us to make inferences about the model\r\nparameters within a Bayesian\r\ninferential framework. Specifically, we will build and fit a\r\nso-called path\r\nanalysis model, which is a special type of structural equation\r\nmodeling used to describe directed dependencies among a set of\r\nvariables. Given that we have data for a group of the same teams over\r\nseveral seasons, we must also incorporate the hierarchical nature of the\r\ndata into the model.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for fitting Bayesian generalized (non-)linear multivariate multilevel models using Stan for full Bayesian inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# specifying individual parts of the SEM (without modeling the correlation between response variables and using Student's t distribution instead of the Gaussian distribution to make the model more robust) \r\na <- brms::bf(W ~ 1 + RS + RA + (1 + RS + RA | Team), family = student())\r\nb <- brms::bf(RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team), family = student())\r\nc <- brms::bf(RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team), family = student())\r\nd <- brms::bf(Playoffs ~ 1 + W + (1 + W | Team), family = bernoulli())\r\n\r\n# fitting the model\r\nfit <- brms::brm(\r\n  a + b + c + d + set_rescor(FALSE), \r\n  data = myData,\r\n  iter = 3000,\r\n  chains = 3,\r\n  warmup = 500,\r\n  thin = 1,\r\n  seed = 123,\r\n  cores = 5,\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n  )\r\n\r\n# checking MCMC convergence\r\n# plot(fit)\r\n# summary(fit)\r\n\r\n\r\n\r\nAfter verifying that the mechanics of the MCMC algorithm work well\r\n(not shown here for brevity reasons), we should also verify how well the\r\nmodel fits the data by using the posterior predictive check for each of\r\nthe observed response variables in our model.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for arbitrarily complex composition of ggplot plots\r\nlibrary(patchwork)\r\n\r\n# posterior predictive check for all predicted observed variables \r\nplayoffsPpc <- brms::pp_check(fit, resp = \"Playoffs\", ndraws = 100) + ggplot2::labs(title = \"Playoffs variable\")\r\nwPpc <- brms::pp_check(fit, resp = \"W\", ndraws = 100) + ggplot2::labs(title = \"W variable\")\r\nrsPpc <- brms::pp_check(fit, resp = \"RS\", ndraws = 100) + ggplot2::labs(title = \"RS variable\")\r\nraPpc <- brms::pp_check(fit, resp = \"RA\", ndraws = 100) + ggplot2::labs(title = \"RA variable\")\r\n\r\nppc <- (playoffsPpc + wPpc) / (rsPpc + raPpc)\r\n\r\nprint(ppc)\r\n\r\n\r\n\r\n\r\nAs you can see, the model fits the data pretty well. Now we can look\r\nat the coefficients in the individual parts of the model. All of them\r\nshow non-zero values and are in directions that are consistent with our\r\nexpectations embodied in our conceptual model, i.e., all are positive\r\nexcept for the coefficient of the RA variable (number of points\r\nlost) as a predictor of the number of games won (W). These\r\nresults thus give us greater confidence in following our conceptual\r\nmodel of team functioning when making decisions about allocating our\r\nlimited resources.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting functions for posterior analysis, MCMC diagnostics, prior and posterior predictive checks, and other visualizations to support the applied Bayesian workflow\r\nlibrary(bayesplot)\r\n\r\n# getting overview of all parameters \r\n# get_variables(fit)\r\n# relevant parameters: \"b_RS_OBP\", \"b_RS_SLG\", \"b_RA_OOBP\", \"b_RA_OSLG\", \"b_W_RS\", \"b_W_RA\", \"b_Playoffs_W\"\r\n\r\nb_RS_OBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_OBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OBP variable in the RS ~ OBP part\"\r\n    ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n    )\r\n\r\n\r\nb_RS_SLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RS_SLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"SLG variable in the RS ~ SLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OOBP <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OOBP\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OOBP variable in the RA ~ OOBP part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_RA_OSLG <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_RA_OSLG\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"OSLG variable in the RA ~ OSLG part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RS <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RS\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RS variable in the W ~ RS part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_W_RA <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_W_RA\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"RA variable in the W ~ RA part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\nb_Playoffs_W <- bayesplot::mcmc_areas(\r\n  fit, \r\n  pars = \"b_Playoffs_W\", \r\n  prob = 0.95,\r\n  point_est = \"median\"\r\n) +\r\n  ggplot2::labs(\r\n    title = \"W variable in the Playoffs ~ W part\"\r\n  ) +\r\n  ggplot2::theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank()\r\n  )\r\n\r\n\r\ncoefCharts <- (b_RS_OBP + b_RS_SLG + b_RA_OOBP) / (b_RA_OSLG + b_W_RS + b_W_RA) / (b_Playoffs_W + patchwork::plot_spacer() + patchwork::plot_spacer()) +\r\n  plot_annotation(\r\n    title = 'Posterior interval estimations',\r\n    caption = \"The solid vertical lines represent the point (median) estimates and the shaded areas represent the 95% credible interval.\",\r\n    theme = theme(\r\n      plot.title = element_text(size = 18),\r\n      plot.caption = element_text(size = 11)\r\n      )\r\n    )\r\n\r\nprint(coefCharts)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model  \r\nsummary(fit)\r\n\r\n\r\n Family: MV(student, student, student, bernoulli) \r\n  Links: mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = identity; sigma = identity; nu = identity\r\n         mu = logit \r\nFormula: W ~ 1 + RS + RA + (1 + RS + RA | Team) \r\n         RA ~ 1 + OOBP + OSLG + (1 + OOBP + OSLG | Team) \r\n         RS ~ 1 + OBP + SLG + (1 + OBP + SLG | Team) \r\n         Playoffs ~ 1 + W + (1 + W | Team) \r\n   Data: myData (Number of observations: 90) \r\n  Draws: 3 chains, each with iter = 2500; warmup = 0; thin = 1;\r\n         total post-warmup draws = 7500\r\n\r\nGroup-Level Effects: \r\n~Team (Number of levels: 30) \r\n                                   Estimate Est.Error l-95% CI\r\nsd(W_Intercept)                        1.63      1.95     0.04\r\nsd(W_RS)                               0.00      0.00     0.00\r\nsd(W_RA)                               0.00      0.00     0.00\r\nsd(RA_Intercept)                      12.21     10.88     0.57\r\nsd(RA_OOBP)                           32.60     30.04     1.38\r\nsd(RA_OSLG)                           26.88     23.83     1.07\r\nsd(RS_Intercept)                      11.86     10.01     0.45\r\nsd(RS_OBP)                            33.58     27.10     1.55\r\nsd(RS_SLG)                            28.59     22.99     1.18\r\nsd(Playoffs_Intercept)                 2.20      2.16     0.09\r\nsd(Playoffs_W)                         0.22      0.47     0.00\r\ncor(W_Intercept,W_RS)                 -0.24      0.50    -0.96\r\ncor(W_Intercept,W_RA)                 -0.23      0.52    -0.97\r\ncor(W_RS,W_RA)                        -0.22      0.51    -0.95\r\ncor(RA_Intercept,RA_OOBP)             -0.20      0.51    -0.94\r\ncor(RA_Intercept,RA_OSLG)             -0.21      0.51    -0.95\r\ncor(RA_OOBP,RA_OSLG)                  -0.19      0.51    -0.95\r\ncor(RS_Intercept,RS_OBP)              -0.16      0.52    -0.94\r\ncor(RS_Intercept,RS_SLG)              -0.18      0.52    -0.95\r\ncor(RS_OBP,RS_SLG)                    -0.20      0.51    -0.94\r\ncor(Playoffs_Intercept,Playoffs_W)    -0.18      0.58    -0.98\r\n                                   u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsd(W_Intercept)                        7.45 1.01     1084     1547\r\nsd(W_RS)                               0.01 1.01      934     1938\r\nsd(W_RA)                               0.01 1.01     1012     3299\r\nsd(RA_Intercept)                      40.57 1.00     2271     3697\r\nsd(RA_OOBP)                          111.19 1.00     1464     2342\r\nsd(RA_OSLG)                           85.24 1.00      894     2025\r\nsd(RS_Intercept)                      38.03 1.00     1569     1794\r\nsd(RS_OBP)                           101.92 1.00      507      468\r\nsd(RS_SLG)                            86.47 1.00      654     2468\r\nsd(Playoffs_Intercept)                 7.42 1.00     2058     2623\r\nsd(Playoffs_W)                         1.80 1.07       33       16\r\ncor(W_Intercept,W_RS)                  0.79 1.00     4156     4013\r\ncor(W_Intercept,W_RA)                  0.82 1.00     1522     1654\r\ncor(W_RS,W_RA)                         0.80 1.00     3192     4270\r\ncor(RA_Intercept,RA_OOBP)              0.81 1.00     3376     3918\r\ncor(RA_Intercept,RA_OSLG)              0.80 1.00     3097     4358\r\ncor(RA_OOBP,RA_OSLG)                   0.84 1.01     1295     1547\r\ncor(RS_Intercept,RS_OBP)               0.85 1.00     1162     2565\r\ncor(RS_Intercept,RS_SLG)               0.84 1.00      983     1246\r\ncor(RS_OBP,RS_SLG)                     0.82 1.01      503     1922\r\ncor(Playoffs_Intercept,Playoffs_W)     0.93 1.01      187      219\r\n\r\nPopulation-Level Effects: \r\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nW_Intercept           85.97      5.78    74.51    97.16 1.00     4765\r\nRA_Intercept        -852.33     64.99  -982.24  -724.71 1.00     2989\r\nRS_Intercept        -949.77     67.88 -1084.15  -820.70 1.00     4559\r\nPlayoffs_Intercept  -471.68    923.11 -3529.81   -41.17 1.07       30\r\nW_RS                   0.10      0.01     0.09     0.11 1.00     3938\r\nW_RA                  -0.10      0.01    -0.11    -0.09 1.00     4087\r\nRA_OOBP             2850.08    297.29  2262.09  3420.05 1.00     2622\r\nRA_OSLG             1600.06    187.41  1241.97  1976.70 1.00     4134\r\nRS_OBP              3475.86    272.82  2955.81  4015.43 1.00     4594\r\nRS_SLG              1330.99    160.56  1014.09  1637.60 1.00     3936\r\nPlayoffs_W             5.30     10.37     0.46    39.82 1.07       30\r\n                   Tail_ESS\r\nW_Intercept            4672\r\nRA_Intercept           3771\r\nRS_Intercept           5211\r\nPlayoffs_Intercept       16\r\nW_RS                   5662\r\nW_RA                   4003\r\nRA_OOBP                2861\r\nRA_OSLG                4506\r\nRS_OBP                 3139\r\nRS_SLG                 4062\r\nPlayoffs_W               16\r\n\r\nFamily Specific Parameters: \r\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma_W      3.30      0.31     2.70     3.91 1.00     1995     1334\r\nsigma_RA    22.29      2.41    17.83    27.36 1.00     1130     1360\r\nsigma_RS    18.97      2.15    14.93    23.35 1.00     2979     3893\r\nnu_W        23.65     14.23     5.60    59.37 1.00     2419     1912\r\nnu_RA       21.98     14.02     4.93    57.35 1.00     1839     2379\r\nnu_RS       20.98     13.86     4.49    56.41 1.00     1853     2595\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nHowever, the main advantage of such a formalized model is that we can\r\nnow make specific predictions or simulations that will help us make more\r\ngranular decisions about where and how much to invest our limited\r\nresources, e.g.¬†in terms of hiring and/or L&D activities. For\r\nexample, we know from the available data that teams need to win\r\napproximately 95 games to have a high chance of making the playoffs -\r\nsee the two graphs below.\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 1\r\nset.seed(1234)\r\nmyData %>%\r\n  # creating random numbers for dispersing points across the y axis of the graph\r\n  dplyr::mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot2::ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs))) +\r\n  ggplot2::geom_point(size = 3, alpha = 0.8) +\r\n  ggplot2::scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5)) +\r\n  ggplot2::scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"The team has not made it to the playoffs\", \"The team has made it to the playoffs\")) +\r\n  ggplot2::labs(\r\n    title = \"Teams that didn't/made the playoffs from 1996-2001\",\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_blank(),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_blank(),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_blank(),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# wins to make playoffs - chart 2\r\n\r\nmyData %>%\r\n  # dividing W variable into intervals \r\n  dplyr::mutate(\r\n    WCat = cut(W, breaks = 21, right = TRUE, include.lowest = TRUE, ordered_result = TRUE)\r\n  ) %>% \r\n  dplyr::group_by(WCat) %>%\r\n  dplyr::summarise(\r\n    PlayoffsProb = mean(Playoffs)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = WCat, y = PlayoffsProb)) +\r\n  ggplot2::geom_bar(stat = \"identity\", color = NA, fill = \"lightblue\") +\r\n  ggplot2::scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1), labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"NUMBER OF GAMES WON IN THE REGULAR SEASON\",\r\n    y = \"PROBABILITY OF MAKING IT TO THE PLAYOFFS\",\r\n    title = \"Relation between the number of wins in the regular season and the probability\\nof advancing to the playoffs (1996-2001)\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"top\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nConsidering the last known OAK performance statistics from 2001, what\r\nnumber of games can OAK expect to win and what is the likelihood of\r\nmaking the playoffs next year? Let‚Äôs plug the 2001 OAK numbers into the\r\nmodel and check the prediction including all uncertainties.\r\n\r\n\r\nShow code\r\n\r\n# summary prediction of the number of matches won by OAK in 2002 using OAK's statistics from 2001 as input\r\n# set.seed(123)\r\n# predict(\r\n#   fit, \r\n#   resp = \"W\", \r\n#   newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n# )\r\n\r\n# generating predictions using OAK's statistics from 2001 as input\r\nset.seed(123)\r\nwp <- brms::posterior_predict(\r\n  fit, \r\n  resp = \"W\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n  )\r\n\r\n# actual number of matches won by OAK in 2002\r\nrealNWins <- baseball %>%\r\n  filter(Year == 2002, Team == \"OAK\") %>%\r\n  pull(W)\r\n\r\n# uploading library for visualizations of distributions and uncertainty\r\nlibrary(ggdist)\r\n\r\n# creating the graph\r\nwp %>%\r\n  as.data.frame() %>% \r\n  ggplot2::ggplot(aes(x = V1)) +\r\n  ggdist::stat_halfeye(\r\n    fill = \"lightblue\",\r\n    .width = c(0.80, 0.95),\r\n      ) +\r\n  ggplot2::geom_vline(xintercept = realNWins, linetype = \"dashed\") +\r\n  ggplot2::geom_text(x = realNWins+8.3, y = 1, label = \"Actual number of matches won by OAK in 2002\") +\r\n  ggplot2::scale_x_continuous(breaks = seq(80,120,5)) +\r\n  ggplot2::labs(\r\n    x = \"PREDICTED NUMBER OF MATCHES WON (W)\",\r\n    y = \"DENSITY\",\r\n    linetype = \"\",\r\n    title = \"Predicted number of matches won by OAK in 2002\",\r\n    caption = \"\\nThe black horizontal lines at the bottom of the graph represent the 80% and 95% probability intervals, respectively.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"none\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can see that the 95% probability interval of the predicted number\r\nof games won by the OAK in 2002 is safely above the 95-point threshold.\r\nAnd when we compare the prediction to the 2002 reality, we see that they\r\nare pretty close. Also, the projected probability of OAK advancing to\r\nthe playoffs is very high, which is consistent with the fact that OAK\r\nsuccessfully qualified for the playoffs in 2002.\r\n\r\n\r\nShow code\r\n\r\n# prediction of OAK making the playoffs in 2002 using OAK's statistics from 2001 as input \r\nset.seed(123)\r\npredict(\r\n  fit, \r\n  resp = \"Playoffs\", \r\n  newdata = baseball %>% dplyr::filter(Year == 2001, Team == \"OAK\") %>% dplyr::select(Team, RS, RA, W, Playoffs, OBP, SLG, OOBP, OSLG),\r\n  probs = c(0.025, 0.5, 0.975)\r\n)\r\n\r\n\r\n      Estimate Est.Error Q2.5 Q50 Q97.5\r\n[1,] 0.9993333  0.025813    1   1     1\r\n\r\nBased on this information, we may conclude that there is no urgent\r\nneed to invest heavily now in increasing the current quality of the\r\nteam‚Äôs play at bat and in the field (variables OBP and\r\nSLG), assuming that the opposing teams do not significantly\r\nincrease the quality of their play next year (variables OOBP\r\nand OSLG). Using the model, we can also obtain a specific range\r\nof the quality of the OAK‚Äôs play at bat and in the field that would\r\nstill allow it to reach at least 95 games won. Such information could be\r\nuseful, for example, in deciding which players can be safely traded\r\nwithout having a detrimental effect on the likelihood of making the\r\nplayoffs.\r\n\r\n\r\nShow code\r\n\r\n# simulation of the effect of SLG and OBP on the number of OAK games won in 2002\r\n\r\n# getting plausible range of values of OBP and SLG variables\r\nstats <- baseball %>%\r\n  dplyr::filter(\r\n    Year < 2002, \r\n    Year > 1995, \r\n    Team == \"OAK\"\r\n    ) %>%\r\n  dplyr::summarise(\r\n    dplyr::across(c(OBP, SLG), list(mean = mean, sd = sd))\r\n  )\r\n\r\n# creating a sequence of plausible values for OBP and SLG variables (3 SDs around the mean value)   \r\nOBPSeq <- seq(from = stats$OBP_mean + (3*stats$OBP_sd), to = stats$OBP_mean - (3*stats$OBP_sd), length.out = 20)\r\nSLGSeq <- seq(from = stats$SLG_mean + (3*stats$SLG_sd), to = stats$SLG_mean - (3*stats$SLG_sd), length.out = 20)\r\n\r\n# creating df with all possible combinations of plausible values of OBP and SLG variables\r\nsimDf <- expand.grid(OBP = OBPSeq, SLG = SLGSeq)\r\n\r\n# getting variables necessary for prediction (statistics of OAK's opponent teams from 2001)\r\nstatsO <- baseball %>%\r\n  dplyr::filter(Year == 2001, Team == \"OAK\") \r\n\r\n# adding Team and W variables\r\nsimDf <- simDf %>%\r\n  dplyr::mutate(\r\n    Team = \"OAK\",\r\n    W = NaN\r\n  )\r\n\r\n# running the simulation\r\nfor(i in 1:nrow(simDf)){\r\n  \r\n  #print(i)\r\n  \r\n  # prediction of the RS response variable\r\n  RSPred <- predict(\r\n    fit, \r\n    resp = \"RS\", \r\n    newdata = simDf[i,]\r\n  )[1]\r\n  \r\n  # prediction of the W response variable\r\n  WPred <- predict(\r\n    fit, \r\n    resp = \"W\", \r\n    newdata = data.frame(Team = \"OAK\", RS = RSPred, RA = statsO$RA)\r\n  )[1]\r\n  \r\n  simDf[i,\"W\"] <- WPred  \r\n  \r\n}\r\n\r\n# creating the graph\r\nsimDf %>%\r\n  ggplot2::ggplot(aes(x = OBP, y = SLG, color = W)) +\r\n  ggplot2::geom_point(alpha = 18, size = 3) +\r\n  ggplot2::scale_y_continuous(breaks = seq(0.36, 0.52, 0.02)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0.32, 0.38, 0.01)) +\r\n  ggplot2::scale_colour_gradient2(\r\n    low = \"blue\",\r\n    mid = \"white\",\r\n    high = \"red\",\r\n    midpoint = 95,\r\n    space = \"Lab\",\r\n    na.value = \"grey50\",\r\n    guide = \"colourbar\",\r\n    aesthetics = \"colour\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Simulation of the effect of SLG and OBP on the number of OAK games won in 2002\",\r\n    caption = \"The white color corresponds to the 95-point threshold to qualify for the playoffs.\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nAs many of you know, thanks to similar analyses, the OAK management\r\nbegan to select players for their team who, although they did not meet\r\nthe traditional criteria by which scouts judged the quality of baseball\r\nplayers, exhibited exactly those characteristics that, according to\r\nconducted analyses, predicted the number of points won and lost, and\r\nthus the likelihood of advancing to the playoffs, which was the\r\nmanagement‚Äôs main goal. Because competing teams underestimated the\r\nimportance of these player statistics and overestimated other, less\r\nimportant variables (e.g., batting average), OAK management was able to\r\npurchase players relatively inexpensively who enabled them to achieve\r\ntheir goals. As a result, the OAK won 20 more games per season than\r\nsimilarly ‚Äúpoor‚Äù teams and about the same number of games as 2 to 3\r\ntimes richer competition. The power of data in practice!\r\n\r\n\r\nShow code\r\n\r\n# salaries vs number of matches won \r\n# uploading data from the Lahman's Baseball Database, which is publicly available at https://www.seanlahman.com/baseball-archive/statistics/\r\n\r\nplayersSalaries <- readr::read_csv(\"./salaries.csv\")\r\nteamsWins <- readr::read_csv(\"./teams.csv\")\r\n\r\n# computing average sum of salaries paid by each team to its players in 1998-2001\r\nplayersSalariesAvg <- playersSalaries %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n    ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(playersSalariesAvg = sum(salary)/length(unique(yearID)))\r\n\r\n# calculating the average number of wins per season for each team from 1998-2001\r\nteamsWinsAvg <- teamsWins %>%\r\n  dplyr::filter(\r\n    yearID > 1997,\r\n    yearID < 2002\r\n  ) %>%\r\n  dplyr::group_by(teamID) %>%\r\n  dplyr::summarise(teamsWinsAvg = sum(W)/length(unique(yearID)))\r\n\r\n\r\n\r\nplayersSalariesAvg %>%\r\n  dplyr::left_join(teamsWinsAvg , \"teamID\") %>%\r\n  dplyr::mutate(OAK = ifelse(teamID == \"OAK\", \"yes\", \"no\")) %>%\r\n  ggplot2::ggplot(aes(x= playersSalariesAvg, y = teamsWinsAvg, fill = OAK)) +\r\n  ggplot2::geom_point()+\r\n  ggplot2::labs(\r\n    title = \"Player salaries and number of wins in 1998-2001\",\r\n    x = \"AVERAGE SUM OF PLAYERS' SALARIES (USD)\",\r\n    y = \"AVERAGE NUMBER OF WINS PER SEASON\"\r\n    ) +\r\n  ggrepel::geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50') +\r\n  ggplot2::scale_fill_manual(\r\n    values = c(\"#ffffff\", \"#ffd400\"), \r\n    labels = c(\"no\",\"yes\")\r\n    ) +\r\n  ggplot2::scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  ggplot2::scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07), labels = scales::number_format(scale = 0.000001, suffix = \"M\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 18, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.text.x = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.text.y = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position=\"node\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nFinally, let‚Äôs breathe some life into the dry numbers by watching a\r\nshort clip from the Moneyball film, which\r\nnicely summarizes some of the ideas presented in this blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-hrm-value-chain-and-sem/./baseball.jpg",
    "last_modified": "2023-02-09T10:09:16+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-org-chart-and-collaboration/",
    "title": "Org chart and collaboration",
    "description": "How to effectively combine information about the formal organizational structure of a company and the actual collaborative activities of its employees?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-26",
    "categories": [
      "organizational structure",
      "org chart",
      "collaboration",
      "data visualization"
    ],
    "contents": "\r\nHow to effectively combine information about the formal\r\norganizational structure of a company and the actual collaborative\r\nactivities of its employees?\r\nAt Time is Ltd. we are\r\nprimarily focused on collaboration data analytics, whether it‚Äôs\r\nmeetings, emails, chat, CRM, project management or version control\r\nsystems, but besides that we also have a product that helps companies\r\nmap their formal organisational structure. Btw, you can give it a try\r\nbecause many of its features are available for free on the Google Workspace\r\nMarketplace.\r\nWe are currently trying to connect these two ‚Äúworlds‚Äù because in\r\nsituations of organizational transformation it can be very useful to\r\nhave information about the relationship between the current and/or\r\nintended formal structure of the organization on the one hand and the\r\nactual patterns of collaboration on the other.\r\nOne option we‚Äôre considering is using a kind of heatmap, which you\r\nmay be familiar with from eye-tracking studies used in marketing, to see\r\nwhere people focus their attention when interacting with products and\r\nmaking purchasing decisions. Now imagine if we overlayed a similar\r\nheatmap showing the intensity of collaboration between a selected unit\r\nand the rest of the organization over an org chart - see the figure\r\nbelow for illustration.\r\n\r\nWould you consider such a visualization useful if you were engaged in\r\norganizational transformation? Would you suggest any other kind of\r\nvisualization or specific metric related to collaboration? Thank you in\r\nadvance for any input you may have - it will help us broaden the range\r\nof perspectives we are currently considering.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-org-chart-and-collaboration/./collaborationOrgChartOverlay.png",
    "last_modified": "2023-02-08T22:12:38+01:00",
    "input_file": {},
    "preview_width": 1661,
    "preview_height": 970
  },
  {
    "path": "posts/2023-02-08-span-of-control/",
    "title": "Span of control and collaboration data",
    "description": "How can collaboration data be used to determine the \"optimal\" scope of control?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-24",
    "categories": [
      "span of control",
      "collaboration",
      "meetings"
    ],
    "contents": "\r\nIn determining the ‚Äúoptimal‚Äù span of control (SOC) for the company\r\nand specific departments and teams, it is always advisable to consider\r\nthe context and strategy of the company, the way in which individual\r\ndepartments and teams should perform their work, and the level of\r\ncompetence of individual managers.\r\nFor example, McKinsey\r\nsuggested the following four specific aspects of managerial complexity\r\nthat should be considered in this endeavor:\r\nThe time a manager spends doing her or his own work vs.¬†managing\r\nothers.\r\nThe extent to which the work process is not standardized and\r\nformally structured.\r\nThe variety of work of the manager‚Äôs direct reports.\r\nThe amount of experience and training that team members need to do\r\ntheir jobs.\r\nThe more of the above, the smaller the SOC should be.\r\nIn addition to these factors, we can use as other useful inputs some\r\nmeasures of collaboration that can reasonably be expected to be related\r\nboth to SOC and to the obligations that managers have to their direct\r\nreports. A good example is the metric of the number of 1-on-1 meetings\r\nthat managers have with their direct reports. As the attached chart\r\nillustrates, when pitted against each other, we can look for points on\r\nthe SOC scale where managers start to fall short of the goal of a\r\ncertain minimum number of 1:1s they have with their people, e.g.¬†2\r\nmeetings per month.\r\n\r\nWhat factors do you typically consider when determining the optimal\r\nspan of control in your company? And do you regularly reassess the\r\nadequacy of the current SOC in the context of your current\r\nsituation?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-span-of-control/./scheme.jpg",
    "last_modified": "2023-02-08T18:32:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-09-slack-best-practices/",
    "title": "Attaching numbers to best practices for instant messaging",
    "description": "Slack and other instant messaging platforms can be both a blessing and a curse. Can we attach numbers to some of the recommendations on how to use them effectively? Let's take a look.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-20",
    "categories": [
      "slack",
      "instant messaging",
      "best practices"
    ],
    "contents": "\r\nAs many of you can probably confess, Slack and other instant\r\nmessaging (IM) platforms can be both a blessing and a curse. To support\r\nthe former and suppress the latter, authors of these tools and their\r\nusers themselves have come up with several recommendations on how to use\r\nthem effectively.\r\nSpecific numbers can be attached to some of these best practices to\r\nhelp teams and entire companies systematically shape their behavior on\r\nIM platforms in the desired direction. At Time Is Ltd., we currently measure\r\nthe following seven best practices:\r\nThread use: It helps create organized discussions\r\naround specific messages, and they let users discuss a topic in more\r\ndetail without adding clutter to a channel or direct message\r\nconversation.\r\nMention use: Mentioning specific people in messages\r\nin both public and private channels is one effective way to avoid\r\noverwhelming users with a large number of messages that are not relevant\r\nto them.\r\nShort messages use: Shorter messages often mean\r\nmore messages, more messages mean more notifications, and more\r\nnotifications mean more distractions, more frequent context switching,\r\nand decreased productivity.\r\nEmoji use: People should use emojis instead of\r\nshort messages as they are less distracting and more friendly to other\r\npeople‚Äôs attention.\r\nBatching: Responding to chat messages round the\r\nclock can be detrimental to employees‚Äô productivity as it can distract\r\nthem from focused work. Better strategy is checking messages every one\r\nor two hours instead of continuous handling of all incoming\r\nmessages.\r\nInactive channels: Non-archived channels that show\r\nno activity are just clutter that makes it difficult to navigate and\r\ncollaborate on the chat platform.\r\nTransparency: Direct and group messages have their\r\nplace in chat, especially when discussing sensitive issues or when\r\ntrying to avoid spamming other employees. However, when majority of chat\r\ncommunication occurs in direct and group messages, there is a higher\r\nrisk that information important for task alignment, problem-solving, or\r\ndecision will be hidden in them and out of view from relevant\r\npeople.\r\n\r\nWould you add some other best practices for IM that have worked well\r\nfor you and that would make sense to measure?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-09-slack-best-practices/./im.jpg",
    "last_modified": "2023-02-09T10:41:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-slack-batching/",
    "title": "Always messaging",
    "description": "Let's take a look at two concepts from computer science that can be used in the workplace to improve people's focus and productivity, and expose two methods for measuring their related behaviors when collaborating on instant messaging platforms.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-17",
    "categories": [
      "instant messaging",
      "collaboration",
      "focus time",
      "distractions",
      "operationalization",
      "measurement"
    ],
    "contents": "\r\nWhen dealing with a large number of tasks and frequent task\r\nswitching, two related concepts originating from computer science can be\r\nused: batch\r\nprocessing and interrupt\r\ncoalescing.\r\nIn computing, these terms refer to a situation where computers wait\r\nuntil a fixed interval and check everything, rather than contextually\r\nswitching and processing separate, uncoordinated interrupts from their\r\nvarious sub-components.\r\nWhen transposed into the world of human workers, this design\r\nprinciple can manifest in checking emails or instant messages every one\r\nor two hours instead of continuously handling all incoming emails and\r\nmessages. Such an arrangement prevents fragmentation of people‚Äôs time\r\nand provides them with more focus time they need for deep work and\r\nexperiencing flow.\r\nBut how to measure this behavior so that a number can be put on it to\r\nenable people to better shape their behavior in this regard? At Time is\r\nLtd.¬†we are currently experimenting with two different approaches:\r\nClustering of sent emails/messages using K-Means or PAM and calculation\r\nof time gaps between start/end points of identified clusters, including\r\nthe start and end of the working day. The larger the gaps, the stronger\r\nthe signal of batch behavior.\r\nPercentage of emails/messages sent during the 3 busiest working\r\nhours (defined by the number of emails/messages sent) during a given\r\nday. The higher the proportion, the stronger the signal of batch\r\nbehavior. This approach is inspired by the 2016\r\nstudy by Mark et al. ‚ÄúEmail Duration, Batching and\r\nSelf-interruption: Patterns of Email Use on Productivity and Stress‚Äù,\r\nwhere the authors used similar approach in the domain of email\r\ncommunication.\r\n\r\nThere are advantages and disadvantages to both methods (face\r\nvalidity, accuracy, sensitivity to edge cases, computational complexity,\r\netc.), but irrespective of these, which one would you prefer to see in\r\nyour collaboration report? To get a better idea of what outputs both of\r\nthe above approaches generate, you can take a look at the attached\r\ngraphs showing the prevalence of batch behavior during one of my work\r\nweeks on Slack according to these two approaches.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-slack-batching/./slack.jpg",
    "last_modified": "2023-02-08T19:02:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-overloaded-employees/",
    "title": "Warning system for overloaded employees",
    "description": "What tools and/or signals can we use to identify employees at increased risk of overload? Let's take a look at some of the options we have in this regard.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-12",
    "categories": [
      "overload",
      "layoffs",
      "retention",
      "engagement"
    ],
    "contents": "\r\nOne of the negative side-effects of layoffs and efforts to achieve\r\nthe same (or ideally more) with fewer employees can be an increased\r\nworkload for those who stay because they have to do the work of those\r\nwho have left, which may lead to an increased risk of overload,\r\ndisengagement, and voluntary quits.\r\nTo prevent this from happening, it is useful to combine active\r\nlistening (through engagement surveys, pulse surveys, stay interviews,\r\nsimple chat, etc. ) with signals that can be obtained from the traces\r\nleft by people in various digital workplace tools, such as project\r\nmanagement systems (ClickUp, Jira, Asana, etc.), version control systems\r\n(GitLab, GitHub, etc.), calendars, instant messaging, or emails.\r\nAt Time is Ltd., we are\r\ncurrently focusing on the following metrics that could be useful in this\r\nrespect:\r\nNumber of assigned tasks\r\nTask close rate\r\nNumber of assigned tasks that other people‚Äôs tasks depend on\r\nNumber of commits\r\nNumber of code reviews\r\nResponse time to received messages and e-mails\r\nAmount of focus time available\r\nAmount of distracted time\r\nAmount of time spent working after hours or on weekends\r\nBy checking the distribution of these metrics across individual team\r\nmembers and their changes over time, it is possible to identify\r\nemployees at higher risk of overload, as well as opportunities for a\r\nmore even distribution of the workload.\r\nWhat tools and/or signals do you use in your company to identify\r\nemployees at increased risk of overload?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-overloaded-employees/./overload.jpg",
    "last_modified": "2023-02-08T22:31:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/",
    "title": "Evaluation of the results of the evidence-based HRM knowledge test",
    "description": "Have we made any progress in knowledge of evidence-based HRM practices in the last 20 years? Apparently not. But let's look at the details.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-08",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I shared an app\r\nthat tests knowledge of evidence-based HRM practices using items from Rynes, Colbert, and\r\nBrown‚Äôs 2002 study on HR practitioners‚Äô beliefs about effective HR\r\npractices. The fact that more than 140 people completed the test allowed\r\nme to compare our current results with those of the participants in the\r\noriginal study (959 HR practitioners, mostly HR managers, with an\r\naverage of 13.8 years of HR experience).\r\nSo how did we do overall?\r\nOn average, we had 19.4 items out of 35 correct, i.e., we had a 55%\r\nsuccess rate, which is very close to the results of the original study\r\nwhere respondents had an average 57% success rate (and also pretty close\r\nto the 50% success rate corresponding to random choice). So these\r\nresults suggest that we have not progressed much as a group over the\r\nlast 20 years, however, see the disclaimer at the very end of the\r\npost.\r\n\r\nIn which HRM area did we have the largest & smallest\r\nknowledge gaps?\r\nThe biggest gap was in the staffing area (44% success rate) and the\r\nsmallest was in the training & employee development area (67%\r\nsuccess rate).\r\nIn which items did we do best?\r\nLeadership training is effective because good leaders are made, not\r\nborn (95% success rate).\r\nLecture-based training is not generally superior to other forms of\r\ntraining delivery (92% success rate).\r\nWhen pay must be reduced or frozen, a company can do something to\r\nreduce employee dissatisfaction and dysfunctional behaviors (90% success\r\nrate).\r\nIn which items did we do the worst?\r\nScoring positive on drug tests doesn‚Äôt mean one will be any less\r\nreliable or productive employee (11.6% success rate).\r\nSetting performance goals is, on average, more effective for\r\nimproving organizational performance than encouraging employees to\r\nparticipate in decision-making (12.3% success rate).\r\nMost errors in performance appraisals cannot be eliminated by\r\nproviding training that describes the kinds of errors managers tend to\r\nmake and suggesting ways to avoid them (15% success rate).\r\nIn which items were we most unsure?\r\nOlder adults don‚Äôt learn more from training than younger adults (38%\r\nuncertain).\r\nIntegrity tests that try to predict whether someone will steal, be\r\nabsent, or otherwise take advantage of an employer work well in practice\r\n(34% uncertain).\r\nCompanies with merit pay systems tend to have higher performance\r\nthan companies without them (17% uncertain).\r\nPlease keep in mind that the comparison presented here is not\r\nentirely an apples-to-apples comparison due to several limitations of\r\nthe data collection method (e.g., we don‚Äôt know the sociodemographics of\r\nthe new participants, new evidence may have emerged that does not match\r\nthe correct answers in the original study, some people may have taken\r\nthe test multiple times, etc.).\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-08-evidence-based-hrm-knowledge-test-results/./testEval.jpg",
    "last_modified": "2023-01-08T21:07:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-meetings-improvement/",
    "title": "How to improve effectiveness of meetings?",
    "description": "What suggestions do people have for improving the effectiveness of meetings? Let's check it out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-05",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nIn my last\r\npost I outlined the reasons why people think the meetings they\r\nattend are in/effective. Let‚Äôs now look at what they suggest can be done\r\nto improve the effectiveness of meetings. Who knows, maybe in these\r\nrecommendations you‚Äôll find an alternative to canceling most internal\r\nmeetings like they did at Shopify.\r\nThe list below is again based on Geimer\r\net al.‚Äôs 2015 paper ‚ÄúMeetings at work: Perceived effectiveness and\r\nrecommended improvements‚Äù.\r\nPEOPLE SIDE: üëâ Come prepared üëâ Arrive on time üëâ\r\nOpen to change üëâ Actively listen to what others are saying üëâ Display\r\nprofessionalism during the meeting\r\nMEETING STRUCTURE & ORGANIZATION SIDE: üëâ\r\nDistribute appropriate information via e-mail instead of in meeting üëâ\r\nAllow time to prepare for meetings üëâ Provide meaningful agenda üëâ\r\nClarify plan of action üëâ Use or rotate a facilitator/chair üëâ Invite\r\nappropriate attendees üëâ Pay attention to timing limit, start/end on\r\ntime üëâ Shorten meetings üëâ Hold meetings at appropriate intervals &\r\nmeet only when necessary üëâ Make the meeting environment more\r\ncomfortable\r\nMEETING ACTIVITIES SIDE: üëâ Make meetings more\r\ninteractive & seek input from all attendees üëâ Stay focused on the\r\ntopic üëâ Prioritize items üëâ Break into smaller groups (brainstorming,\r\netc.) üëâ Delegate responsibilities and set deadlines for assigned\r\ntasks\r\nMEETING OUTCOMES SIDE: üëâ Record and distribute\r\nmeeting minutes üëâ Follow up with proposed solutions\r\nIs there anything that you think is missing in the list, especially\r\nin the context of the fact that since 2015 the workplaces have been\r\noperating more in remote/hybrid mode?\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-meetings-improvement/./meetingsDalle.png",
    "last_modified": "2023-02-08T19:29:27+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2023-02-08-ineffective-meetings/",
    "title": "Why are meetings in/effective?",
    "description": "If you are a regular organiser or attendee of meetings, you may be interested in what people think about the reasons why the meetings they attend are in/effective, as this can give you a better chance of contributing to making your meetings more effective and meaningful for you and others.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2023-01-03",
    "categories": [
      "meetings",
      "meeting culture",
      "meeting overload",
      "collaboration"
    ],
    "contents": "\r\nThe list below is based on Geimer\r\net al.‚Äôs 2015 paper ‚ÄúMeetings at work: Perceived effectiveness and\r\nrecommended improvements‚Äù.\r\nPEOPLE SIDE: üö´ Late arrive üö´ Unprepared attendees\r\nüö´ One-way (top-down) communication üö´ Lack of open-mindedness &\r\nempathy üö´ Self-promotion, people talk just to appear to add value, and\r\nhidden agenda üö´ People interrupt/talk during meeting üö´ Interpersonal\r\nconflicts, incivility, and disrespect\r\nMEETING ORGANIZATION SIDE: ‚úÖ Agenda ‚úÖ Distribution\r\nof agenda in advance üö´ Lack of direction/goals ‚úÖ Chaired effectively\r\nüö´ Meetings held just to have them, just a routine with no real purpose\r\nüö´ Appropriate parties are not invited and inappropriate parties are\r\ninvited üö´ Too many attendees üö´ Time conflicts üö´ Meet at inappropriate\r\nintervals üö´ Meetings take too long üö´ Takes time to travel to\r\nmeeting\r\nMEETING ACTIVITIES SIDE: üö´ Insufficient\r\ninteraction, meeting activities are monotonous and boring üö´ No new\r\ninformation üö´ Discussion gets off target üö´ Core issues not discussed\r\nüö´ Lack of clarity about what the attendee is supposed to do\r\nMEETING OUTCOMES SIDE: üö´ Inaction post-meeting üö´\r\nDecisions have already been made, just a rubber stamp\r\nDo you identify with the above reasons? Is there anything that you\r\nthink is missing from the list, particularly as the workplaces have been\r\noperating more remotely/hybrid way since 2015?\r\nP.S. In the next post let‚Äôs check what suggestions people have for\r\nimproving the effectiveness of meetings.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-ineffective-meetings/./meetingsDalle.png",
    "last_modified": "2023-02-08T19:29:53+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2022-12-17-good-manager/",
    "title": "Signals of a good manager",
    "description": "Do you think it's possible to find signals in collaboration (meta)data that someone is a good manager? Let's give it some thought.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-17",
    "categories": [
      "google",
      "oxygen project",
      "collaboration data",
      "performance management"
    ],
    "contents": "\r\nMany of you have probably already heard about Google‚Äôs\r\nOxygen project and its findings on the factors differentiating the\r\nhighest and lowest-rated managers based on performance reviews, employee\r\nengagement surveys, interviews, and other sources of employee feedback.\r\nThe final list included the following eight characteristics:\r\nIs a good coach.\r\nEmpowers the team and does not micromanage.\r\nExpresses interest in and concern for team members‚Äô success and\r\npersonal well-being.\r\nIs productive and results-oriented.\r\nIs a good communicator - listens and shares information.\r\nHelps with career development.\r\nHas a clear vision and strategy for the team.\r\nHas key technical skills that help him or her advise the team.\r\nDo you think it would be possible to find any signals, however weak,\r\nof the presence of some of these managers‚Äô characteristics in the\r\ncollaboration (meta)data?\r\nOf the collaboration metrics we currently work with at Time is Ltd., I would bet on the\r\nfollowing:\r\nNumber of 1-on-1 meetings managers have with their direct reports\r\nand new hires [concern for employees‚Äô success and personal\r\nwell-being]\r\nNumber of team meetings managers have with their teams [information\r\nsharing, alignment on vision, strategy, and tactics)\r\nAfter-hours or weekend work and workday length of managers‚Äô direct\r\nreports [well-being & work-life balance]\r\nManagers‚Äô presence at meetings of their direct reports, excluding\r\n1-on-1s and team meetings [micromanagement]\r\nManagers being in CC/BCC of emails sent by their direct reports\r\n[micromanagement]\r\nTime it takes managers to respond to emails from their direct\r\nreports [interest in and concern for team members]\r\nDirect reports having skip-level meetings [career development &\r\nnew career opportunities]\r\nWould you agree? Or what other signals of a good manager would you\r\nlook for in collaboration (meta)data?\r\nP.S. In a later\r\nupdate, the list of top manager characteristics at Google also\r\nincludes the characteristic ‚ÄúCollaborates across Google‚Äù, which is\r\nrelatively straightforward to measure using collaboration data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-good-manager/./worlds-best-boss-funny-the-office-micromanagement.gif",
    "last_modified": "2022-12-17T12:36:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-evidence-based-hrm-knowledge-test/",
    "title": "Evidence-based HRM knowledge test",
    "description": "Interested in testing your knowledge of evidence-based HRM practices? If so, click and get started.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-15",
    "categories": [
      "evidence-based management",
      "hr management",
      "people management",
      "shiny app"
    ],
    "contents": "\r\n\r\n\r\n\r\nIf you are interested in testing your knowledge of evidence-based HRM practices in the following five areas‚Ä¶\r\nManagement practices\r\nGeneral employment practices\r\nTraining & Employee development\r\nStaffing\r\nCompensation & Benefits\r\n‚Ä¶ then give a try on the test, which is based on the items used in Rynes, Colbert, and Brown‚Äôs 2002 study ‚ÄúHR practitioners‚Äô beliefs about effective HR practices: a comparison of research and practice‚Äù.\r\nI built a simple shiny app that administers you the test, scores your answers, and compares your results to the results of the participants in the original study (959 HR professionals, mostly HR managers, with an average of 13.8 years of experience in HR). You can also use it to check the accuracy of your answers at the item level to fill in specific gaps in your knowledge.\r\nHere is the link to the app.\r\nFeel free to share your results in the comments. Think of it as a form of public commitment to making some progress in evidence-based HRM in the coming year üòâ To walk the talk, I attached my own results. As you can see, it‚Äôs not bad, but there is still room for improvement, especially in the general employment practices area üòÉ\r\n\r\nP.S. Keep in mind that the test is based on evidence available up to 2002, so it is possible that some correct answers or their contingencies may have changed in that time. For all of them, consider, for example, the adjustment of the estimate of the magnitude of the predictive validity of personnel selection procedures in their most recent meta-analysis by Sackett et al.¬†(2022). If you come across any such discrepancy, it would be great if you share it with others in the comments.\r\nUpdate: With more than 140 people completing the test, I was able to compare our current results with those of the participants in the original study. You can see the results of the comparison in the post Evaluation of the results of the evidence-based HRM knowledge test. Spoiler: It‚Äôs not very good üòØ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-evidence-based-hrm-knowledge-test/./manOnTheRoad.jpg",
    "last_modified": "2023-04-11T20:07:42+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-17-collaboration-during-vacations/",
    "title": "Can you really unplug?",
    "description": "With the Christmas holidays approaching, the following question is more relevant than ever, with the exception of the summer vacations: Can we really disconnect from work during the vacations? And what can collaboration data tell us about this?",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-10",
    "categories": [
      "vacation",
      "wellbeing",
      "work-life balance",
      "collaboration data"
    ],
    "contents": "\r\nAlthough most people know they need time off to stay mentally sharp,\r\nproductive, and resilient over the long term, many confess they work\r\nwhen they‚Äôre on vacation.\r\nThe data we collect and analyze at Time is Ltd. confirms this sad\r\ntruth. As the first chart illustrates, on more than 75% of vacation days\r\npeople show some collaborative activity, here measured by attending\r\nnon-recurring meetings, sending emails, and editing shared files.\r\n\r\nHowever, the same data also suggests what could be part of the remedy\r\nfor this unsatisfactory state of affairs. The second chart shows that\r\nthere is a small but not insignificant positive relationship between the\r\ntime that managers and their direct reports spend collaborating during\r\nvacation. The chart also shows that managers tend to collaborate more\r\nintensely during vacation than their direct reports, which is probably\r\nnot too much of a surprise.\r\n\r\nThe data is thus in line with the quite often mentioned suggestion\r\nthat managers should be better role models for their direct reports on\r\nhow to behave during vacation. As in other areas of people management,\r\nit is not enough to lay down the rules - the ‚Äúplaying captain‚Äù must play\r\nby those rules as well.\r\nHow are you doing in this respect? And do you have any tricks that\r\nhelp you unplug from work during vacations?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-17-collaboration-during-vacations/./outOfOffice.jpg",
    "last_modified": "2022-12-17T11:42:35+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-07-mission-scale-from-tas/",
    "title": "One does not simply do a business without getting lost",
    "description": "Breaking down one weekend association.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-07",
    "categories": [
      "team assessment survey",
      "mission",
      "purpose",
      "goals",
      "metrics",
      "review"
    ],
    "contents": "\r\n\r\n\r\n\r\nAs I was scrolling through one of my feeds over the weekend, I came across a funny meme that resonated with my recent experience on a family trip that expressed the deep truth that ‚ÄúOne does not simply do a road trip without getting lost.‚Äù üòÑ\r\n\r\nBesides that, and that‚Äôs why I‚Äôm writing about it here on my blog, it also reminded me of the results of a study I did together with Rastislav Duris and Slavka Silberg, on the characteristics of more than 80 teams from different industries and composed of more than 800 people using the Team Assessment Survey, Dr.¬†Gordon Curphy‚Äôs survey that measures some of the basic factors that determine team performance.\r\nSpecifically, I was reminded of the results on the Mission scale, which consists of the following four items:\r\nPurpose: Team members are clear about the team‚Äôs purpose.\r\nGoals: The team has a set of overall goals.\r\nMetrics: Metrics and benchmarks have been identified for each team goal.\r\nReviews: The team regularly reviews progress on team goals.\r\nAs you can see in the attached chart, the results of the ‚ÄúHeartbeat analysis‚Äù can be briefly summarized as ‚ÄúWe know where we‚Äôre going, at least some of us know the points to get there, but we‚Äôre not sure if we‚Äôre on the right track and if we should change our original plans.‚Äù\r\n\r\nThat sounds a lot like the description of our last family trip. üòÖ Do you think you‚Äôre better off in this regard in your team or company? And if so, what tips would you give others on how to improve in this respect?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-07-mission-scale-from-tas/./lost_in_desert.jpg",
    "last_modified": "2023-04-11T20:17:08+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-02-change-detection/",
    "title": "How to quickly navigate dashboard users to what they need to know?",
    "description": "Let's take a look at some tips and tricks to make dasboards more useful for their users.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-12-01",
    "categories": [
      "dashboard",
      "contextualization",
      "time series",
      "change detection",
      "python"
    ],
    "contents": "\r\nSince the beginning of our efforts at Time is Ltd. to develop a\r\ncomprehensive collaboration analytics platform, we have created several\r\nhundred collaboration metrics. As you can probably imagine, with such a\r\nhuge amount of metrics, it was quite difficult for users to get\r\nsomething useful out of the platform.\r\nTo make the metrics provided more digestible, we‚Äôve enhanced the\r\nplatform using several approaches, from simply selecting metrics with\r\nthe most straightforward call to action and creating apps for very\r\nspecific use cases to providing users with comparative, historical,\r\nintuitively scaled, and equivalent information. Brent Dykes described\r\nmany of these sense-making methods very neatly in his article Contextualized\r\nInsights: Six Ways To Put Your Numbers In Context.\r\nHowever, even after several rounds of these improvements, there was\r\nstill a need to help users quickly find areas that might be worth their\r\nattention and deeper exploration. One approach we took was based on the\r\n(validated) assumption that collaboration metrics are relatively stable\r\nover time and that the intentional and unintentional behavioral changes\r\nbehind these metrics evolve rather slowly. We therefore decided to\r\ndetect the most significant changes over the last 3 months.\r\nFor this purpose, we chose a method that is related to the Bollinger Bands\r\nmethod used in time series analysis or ‚ÄúHeartbeat Analysis‚Äù used in\r\nsurvey response analysis. Specifically, we look at the standard\r\ndeviation and average of the month-to-month changes for each metric,\r\nscale the changes to z-scores, and then identify the metrics with the\r\nhighest absolute value of average change over the last 3 months. To\r\nillustrate, see the attached chart where the metrics are sorted by\r\nmagnitude of their change in descending order from left to right and top\r\nto bottom.\r\n\r\nIf you want to try this method on your own data, you can use its\r\nPython implementation, which is on my\r\nGitHub page.\r\nWhat other methods do you find useful in identifying values or\r\nchanges that might be worth users‚Äô attention? Feel free to share them in\r\nthe comments for inspiration.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-02-change-detection/./dashboard.jpg",
    "last_modified": "2022-12-02T19:10:27+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-26-meeting-matrix/",
    "title": "Eisenhower matrix for meetings",
    "description": "Meet the Eisenhower matrix for meetings ;)",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "meeting culture",
      "meeting overload",
      "focus time",
      "collaboration culture"
    ],
    "contents": "\r\nI assume many, if not all, of you are familiar with the\r\nEisenhower matrix, a simple tool for prioritizing tasks based on a\r\ncombination of their importance and urgency. It assumes 4 types of tasks\r\n- important & urgent, important & not urgent, not important\r\n& urgent, not important & not urgent - and for each of them\r\noffers a simple recommendation on what to do with them - do it, schedule\r\nit, delegate it, and delete it. By following these rules, people should\r\nbe able to more successfully combat the ‚Äúmere-urgency‚Äù effect, eliminate\r\ntime-wasters in their lives, and create more space to make progress\r\ntoward their goals.\r\n\r\nSomething similar can be created also for meetings, which are big\r\ntime and money guzzlers and deserve to be treated accordingly. Only\r\ninstead of the dimensions of importance and urgency, we will use the\r\nsize and length of meetings. The resulting matrix assumes 4 types of\r\nmeetings with corresponding recommendations on what to do with them:\r\nSmall & Short: These meetings are a bit tricky\r\nbecause they often involve useful meetings, e.g., short syncs of teams\r\nworking on some specific task or project or one-on-one meetings between\r\nmanagers and their direct reports, however, when there is a lot of them,\r\nthey may cause calendar fragmentation and drop in available focus time;\r\nto avoid this, one can think of batching such meetings into larger\r\nblocks of two or three meetings with appropriate small breaks in between\r\nto avoid meeting fatigue and late arrivals.\r\nSmall & Long: These meetings are great for\r\nbrainstorming, problem-solving, or decision-making, so make sure you use\r\nthem primarily for that purpose and don‚Äôt waste them on low-value-added\r\nactivities.\r\nLarge & Short: These meetings often serve only\r\nto disseminate information and can therefore be safely replaced by less\r\nintrusive asynchronous collaboration tools such as email, instant\r\nmessaging or some kind of knowledge management tool.\r\nLarge & Long: These meetings are quite often a\r\nwaste of people‚Äôs time by not allowing everyone to meaningfully\r\ncontribute and by making them both physically and mentally exhausted, so\r\ntry to move these meetings into the third quadrant if your goal is\r\nsimple information dissemination or into the second quadrant if the goal\r\nis to solve some problem or to make some important decision.\r\nThe meeting matrix is by no means a panacea for meeting overload, but\r\nby following the rules above, people should be able to better protect\r\ntheir time for focused work and make meetings more efficient,\r\nmeaningful, and valuable for themselves and the company. Just be aware\r\nthat unlike the Eisenhower matrix, in the case of the meeting matrix,\r\npeople need to rely more on others to follow these rules in order for\r\nits positive effect to materialize. So get ready to update your\r\n(hopefully already existing) team agreement on your collaboration\r\nculture.\r\nBtw, what is your guess as to how much time you spend in each cell of\r\nthe meeting matrix? For comparison I attach my monthly numbers from\r\nOctober.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-meeting-matrix/./meetingMatrix.png",
    "last_modified": "2022-11-26T13:48:43+01:00",
    "input_file": {},
    "preview_width": 850,
    "preview_height": 565
  },
  {
    "path": "posts/2022-11-26-resources-on-retention-and-downsizing/",
    "title": "Some resources on staff retention and downsizing",
    "description": "Probably due to the current situation in the talent market, where many companies are laying people off and at the same time are worried about losing their key employees, a few people have contacted me in recent weeks asking for some tips on evidence-based approaches to dealing with retention and downsizing.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-26",
    "categories": [
      "employee retention",
      "employee turnover",
      "layoffs",
      "downsizing",
      "people analytics",
      "evidence-based management"
    ],
    "contents": "\r\nI referred them to the following resources - maybe some of you will\r\nfind them useful as well:\r\nEmployment\r\nDownsizing and Its Alternatives guideline from the SHRM‚Äôs Science-to-Practice\r\nSeries.\r\nRetaining\r\nTalent guideline from the SHRM‚Äôs Science-to-Practice\r\nSeries.\r\nRubenstein et al.‚Äôs meta-analysis\r\nof voluntary turnover predictors; you can also use this\r\napp visually summarizing its results.\r\nSpeer et al.‚Äôs article Here\r\nto stay or go? Connecting turnover research to applied attrition\r\nmodeling.\r\nDemo\r\ndashboard for analysis of employee turnover.\r\nFeel free to share any other resources on this topic you think might\r\nbe useful to others.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-26-resources-on-retention-and-downsizing/./retentionDownsizing.png",
    "last_modified": "2022-11-26T14:17:06+01:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 720
  },
  {
    "path": "posts/2022-11-18-interventions-reducing-gender-pay-gap/",
    "title": "Evidence-based interventions that help reduce the gender pay gap",
    "description": "Pay inequality between men and women is not only an ethical and legal issue for companies, but also a marketing issue - it can have a negative impact on their \"employer brand\" and attractiveness as an employer. This means that if companies want to attract and retain talented employees, they must be able to ensure that they treat men and women equally in this respect. Let's look at what the existing evidence tells us about what might help us with this.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-18",
    "categories": [
      "gender pay gap",
      "evidence-based management"
    ],
    "contents": "\r\nIf you are responsible for DEI initiatives in your company, you might\r\nbe interested in a paper by The\r\nBehavioural Insights Team that lists several possible interventions\r\nto close the gender pay gap, divided into three categories based on the\r\nextent to which their effectiveness is supported by empirical\r\nevidence.\r\nü•á Actions with well-documented effectiveness:\r\nIncluding more women on shortlists in recruitment and promotion. *Ô∏è\r\nUse of tasks assessing job skill levels in the selection of new\r\nemployees.\r\nUse of structured interviews in recruitment and promotion.\r\nEncouraging salary negotiation through disclosure of existing salary\r\nranges.\r\nIntroducing transparent promotion and reward processes.\r\nAppointing a manager or establishing a corporate diversity task\r\nforce.\r\nü•à Potentially promising actions, but requiring further\r\nevidence of their effectiveness:\r\nIncreasing work flexibility for men and women.\r\nSupporting shared parental leave.\r\nRecruiting former employees who have had to interrupt their careers\r\nfor a prolonged period for various personal reasons.\r\nOffering mentoring and sponsorship.\r\nOffering networking programs.\r\nSetting internal targets.\r\nü•â Actions with mixed evidence of their\r\neffectiveness:\r\nTraining on the topic of unconscious bias.\r\nDiversity training.\r\nLeadership development training.\r\nDemographically diverse selection panels in external and internal\r\nrecruitment.\r\nFor those interested, here is the original document for a closer\r\nlook.\r\n\r\n\r\nThis browser does not support PDF files. Please download the PDF file to\r\nview it: Download PDF.\r\n\r\n\r\n\r\nA final note. As useful as it is to know which interventions have a\r\ndecent chance of reducing gender pay inequality, an integral part of\r\nthis fight is to regularly check the existence of this inequality across\r\nthe organisation and within different types of organisational processes.\r\nAs Alessandro\r\nLinari aptly noted in a discussion on Linkedin, ‚ÄúThe best way to\r\nkeep the gender pay gap under control is still to do a periodic pay\r\nequity analysis across the organisation and address any identified gap.\r\nI say unfortunately because there are a thousand different ways where\r\npay differences still manage to sneak into the workforce, through hiring\r\nand promotion practices, but also retention policies, bonus allocation,\r\nperformance management, and others.‚Äù If you want to learn more\r\nabout some of the technical details of such an analysis, check out one\r\nof my previous posts on this topic (unfortunately, it is now only\r\navailable in Czech, but you can use one of the online translators to get\r\naround this limitation üòâ).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-18-interventions-reducing-gender-pay-gap/./genderPayGap.jpg",
    "last_modified": "2022-11-18T12:26:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-15-people-related-metrics-distribution/",
    "title": "It's perfectly normal not to be normal",
    "description": "And it definitely applies to the shape of the distribution of many HR metrics. Let's look at this in a little more detail.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-11-15",
    "categories": [
      "hr metrics",
      "normal distribution",
      "collaboration",
      "performance",
      "r"
    ],
    "contents": "\r\nThe fact is that many HR practitioners overestimate the frequency\r\nwith which the phenomena they commonly encounter in their practice have\r\na normal, bell-shaped, symmetrical distribution.\r\nThis is very much the case, for example, in the area of communication\r\nand collaboration that we deal with at Time is Ltd., as illustrated in\r\nthe attached chart with some of our collaboration metrics, which show a\r\nwide range of distributions from log-normal and power law to\r\nexponential, gamma, Weibull and beta.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./collaborationMetrics.rds\")\r\n\r\n# External network size metric\r\nexternalNetworkSizeG <- mydata %>%\r\n    dplyr::filter(metric == \"externalNetworkSize\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(breaks = seq(0,120,20)) +\r\n    ggplot2::labs(\r\n      x = \"EXTERNAL NETWORK SIZE / PERSON / MONTH\",\r\n      y = \"FREQUENCY\",\r\n      title = \"External network size\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n  )\r\n  \r\n\r\n# Focus rate metric\r\nfocusRatePrctG <- mydata %>%\r\n    dplyr::filter(metric == \"focusRatePrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF FOCUS TIME / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Available focus time\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n\r\n# Multitasking in meetings metric\r\nmultitaskingInMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"multitaskingInMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"MULTITASKING RATE / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Meeting participations with multitasking\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Breaks between meetings metric\r\nbreaksBetweenMeetingsMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"breaksBetweenMeetingsMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,600,120)) +\r\n    ggplot2::labs(\r\n      x = \"LENGTH OF BREAK / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Lenght of breaks between meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Collaboration time metric\r\ncollaborationTimePersonHrsDayG <- mydata %>%\r\n    dplyr::filter(metric == \"collaborationTimePersonHrsDay\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" hrs\"), breaks = seq(0,16,2)) +\r\n    ggplot2::labs(\r\n      x = \"TIME SPENT COLLABORATING / PERSON / DAY\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Time spent collaborating\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n# Supervised meetings metric\r\nmicromngMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"micromngMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format()) +\r\n    ggplot2::labs(\r\n      x = \"% OF SUPERVISED MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of supervised meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Recurring meetings metric\r\nrecurringMeetingsPrctG <- mydata %>%\r\n    dplyr::filter(metric == \"recurringMeetingsPrct\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::percent_format(), limits = c(0,1)) +\r\n    ggplot2::labs(\r\n      x = \"% OF RECURRING MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Proportion of recurring meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call ending metric\r\ncallEndingMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callEndingMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(-60,60,20)) +\r\n    ggplot2::labs(\r\n      x = \"PLANNED VS. ACTUAL END OF MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Planned vs. actual end of meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\n\r\n# Call delay metric\r\ncallDelayMinutesG <- mydata %>%\r\n    dplyr::filter(metric == \"callDelayMinutes\") %>%\r\n    ggplot2::ggplot(aes(x = metricValue)) +\r\n    ggplot2::geom_histogram(fill = \"#fdd835\", alpha = 0.9, color = \"white\") +\r\n    ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" min\"), breaks = seq(0,40,5)) +\r\n    ggplot2::labs(\r\n      x = \"DELAY OF ONLINE MEETINGS / PERSON\",\r\n      y = \"FREQUENCY\",\r\n      title = \"Late arrivals to online meetings\"\r\n    ) +\r\n    ggplot2::theme(\r\n      plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n      plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n      plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n      axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n      axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n      axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n      axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n      axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n      legend.position= \"bottom\",\r\n      legend.key = element_rect(fill = \"white\"),\r\n      legend.key.width = unit(1.6, \"line\"),\r\n      legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n      legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n      legend.background = element_rect(fill = \"transparent\"),\r\n      panel.background = element_blank(),\r\n      panel.grid.major.y = element_blank(),\r\n      panel.grid.major.x = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n      axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n      plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n      plot.title.position = \"plot\",\r\n      plot.caption.position =  \"plot\"\r\n    )\r\n\r\ng <- (externalNetworkSizeG + focusRatePrctG + multitaskingInMeetingsPrctG) / \r\n  (breaksBetweenMeetingsMinutesG + collaborationTimePersonHrsDayG + micromngMeetingsPrctG) / \r\n  (recurringMeetingsPrctG + callEndingMinutesG + callDelayMinutesG) +\r\n  patchwork::plot_annotation(\r\n    title = 'Distribution of selected collaboration metrics',\r\n    theme = theme(\r\n      plot.title = element_text(size = 26, margin=margin(20,0,12,0))\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBut it also applies to more traditional areas of HR concern such as\r\nindividual or team performance. For many job roles, especially\r\nknowledge-based ones, job performance has a power-law distribution,\r\ni.e., only a few percent of individuals or teams have disproportionately\r\nhigh performance and most have performance below the statistical average\r\n(many of you have probably heard of Pareto‚Äôs law, and the 80/20 rule in\r\nthis context). More detailed information on this particular topic can be\r\nfound, for example, in the following two articles - The\r\nBest and the Rest: Revisiting the Norm of Normality of Individual\r\nPerformance by O‚ÄôBoyle Jr.¬†& Aguinis (2012) and Team\r\nPerformance: Nature and Antecedents of Nonnormal Distributions by\r\nBradley & Aguinis (2022).\r\nWhy bother with that? Well, because, based on incorrect assumptions\r\nabout the distribution of specific phenomena, companies may make\r\ndecisions that ultimately harm them. For example, suppose a company\r\napplies the assumption of normal distribution in evaluating employees‚Äô\r\nperformance that actually has power law distribution. In that case, it\r\nwill result in underestimating the contribution of the best performers\r\nand overestimating the contribution of the worst performers, which may\r\nbe negatively reflected in various decisions regarding reward &\r\ncompensation, learning & development, promotions, succession\r\nplanning, etc.\r\nLessons learned? For frequent decisions or decisions with a\r\nsignificant expected impact, it is worth checking that our underlying\r\nassumptions match reality.\r\nBtw, this also applies to personal life. Here‚Äôs an example from mine:\r\nI thought I was in control of watching movies and TV shows on streaming\r\nplatforms, but I started getting signals from those around me that I was\r\nspending too much time there. So as a proper ‚ÄòQuantified Selfer‚Äô, I decided to\r\ntrack my daily screen time for a month. To my surprise, I found that I\r\nwas indeed spending a lot more time there than I thought and wished. I\r\nthen put in place simple solutions to prevent me from watching more than\r\nI should - I have started to pay extra fees into the family budget for\r\nwatching movies (loss aversion), I\r\nhave pre-selected time slots for watching movies (implementation\r\nintention), I‚Äôve removed streaming apps from my phone (lowering salience),\r\nI‚Äôve stopped watching movies alone (social control), and I‚Äôm also\r\nplaying with the idea of asking my wife to change the PINs (preventing\r\nimpulsive\r\nwatching).\r\nP.S. If you ever need to check the shape distribution of any of your\r\nmetrics, you should definitely try the amazing fitdistrplus\r\nR package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-15-people-related-metrics-distribution/./distributions.png",
    "last_modified": "2022-11-17T14:57:23+01:00",
    "input_file": {},
    "preview_width": 962,
    "preview_height": 670
  },
  {
    "path": "posts/2022-10-27-bayesian-belief-updating/",
    "title": "A visual introduction to Bayesian belief updating",
    "description": "Teacher: \"Bayesian belief updating involves combining existing or prior beliefs with an assessment of the strength of new evidence.\" Student: \"And could I please see this in action?\"",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-27",
    "categories": [
      "bayesian belief updating",
      "bayesian inference",
      "judgment",
      "forecasting",
      "critical thinking",
      "r"
    ],
    "contents": "\r\nWhen trying to reduce uncertainty, even very small pieces of\r\ninformation count if you are patient and have some tool to combine them\r\neffectively. I recently re-read Philip\r\nTetlock‚Äôs book Superforecasting\r\nand came across an excellent illustration of such a tool: Bayesian belief\r\nupdating.\r\n‚ÄúImagine you are sitting with your back to a billiards table. A\r\nfriend rolls a ball onto the table and it stops at a random spot. You\r\nwant to locate the ball without looking. How? Your friend rolls a second\r\nball, which stops at another random spot. You ask, ‚ÄúIs the second ball\r\nto the left or the right of the first?‚Äù Your friend says, ‚ÄúTo the left.‚Äù\r\nThat‚Äôs an almost trivial scrap of information. But it‚Äôs not nothing. It\r\ntells you that the first ball is not on the extreme left edge of the\r\ntable. And it makes it just a tad more likely that the first ball is on\r\nthe right side of the table. If your friend rolls another ball on the\r\ntable and the procedure is repeated, you get another scrap of\r\ninformation. If he says, ‚ÄúIt‚Äôs to the left,‚Äù the likelihood of the first\r\nball being on the right side of the table increases a little more. Keep\r\nrepeating the process and you slowly narrow the range of the possible\r\nlocations, zeroing in on the truth‚Äîalthough you will never eliminate\r\nuncertainty entirely.‚Äù\r\nAfter reading this paragraph, I thought it would be much more\r\npedagogically compelling (and fun üòâ) to see this update process live\r\nand in action. What a great opportunity to learn how to work with the gganimate\r\nR package.\r\nHere is the code I put together.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # data manipulation and visualization\r\nlibrary(gganimate) # animation of the charts\r\nlibrary(bayestestR) # Highest Density Interval computation\r\n\r\n# specifying the width of the billiards table  \r\nfield <- seq(0,178,1)\r\n\r\n# likelihoood function for the situation when the second ball stops to the right of the first one \r\nrightLikelihood <- (178-field)/178\r\n# likelihoood function for the situation when the second ball stops to the left of the first one \r\nleftLikelihood <- 1-rightLikelihood\r\n# flat prior for the very beginning of the updating process  \r\nfirstPrior <- rep(1/179, 179)\r\n\r\n# position where the first ball stopped\r\npoint <- 88\r\n\r\n# Bayesian belief updating\r\n# creating shell dataframe for final results\r\nposteriors <- data.frame()\r\n\r\n# setting random seed for ensuring reproducibility\r\nset.seed(1234)\r\n\r\n# specifying the number of trials\r\nfor(i in 1:500){\r\n  \r\n  # throwing the second ball\r\n  pointTrial <- runif(n = 1, min = 0, max = 178)\r\n  \r\n  # determining whether the second ball stopped to the left or to the right of the first one \r\n  side <- ifelse(pointTrial > point, \"right\", \"left\")\r\n  \r\n  # selecting appropriate prior\r\n  if(i==1){\r\n    \r\n    prior <- firstPrior\r\n    \r\n  } else{\r\n    \r\n    prior <- posteriors %>% \r\n      dplyr::filter(trial == i-1) %>% \r\n      dplyr::pull(posterior)\r\n    \r\n  }\r\n  \r\n  # combining prior and likelihood (evidence) and transforming the result into probabilities\r\n  if(side == \"right\"){\r\n    \r\n    likelihood <- rightLikelihood * prior \r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  } else{\r\n    \r\n    likelihood <- leftLikelihood * prior\r\n    probability <- likelihood/sum(likelihood)\r\n    \r\n  }\r\n  \r\n  # putting results into the dataframe\r\n  suppDf <- data.frame(\r\n    posterior = probability,\r\n    trial = i,\r\n    side = side,\r\n    pointTrial = pointTrial\r\n  )\r\n  \r\n  # computing Highest Density Interval\r\n  sampling <- sample(x = field, size = 10000, replace = TRUE, prob = probability)\r\n  \r\n  hdi <- bayestestR::hdi(sampling, ci = 0.95)\r\n  \r\n  suppDf <- suppDf %>%\r\n    dplyr::mutate(\r\n      lhdi = hdi$CI_low,\r\n      hhdi = hdi$CI_high\r\n    )\r\n  \r\n  # putting results into the shell dataframe \r\n  posteriors <- dplyr::bind_rows(posteriors, suppDf)\r\n\r\n}\r\n\r\n# adjusting data for visualization purposes\r\nposteriorsDf <- posteriors %>%\r\n  dplyr::group_by(trial) %>%\r\n  dplyr::mutate(place = dplyr::row_number()-1) %>%\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(\r\n    hdi = ifelse(place >= lhdi & place <= hhdi, \"yes\", \"no\")\r\n  )\r\n\r\n# creating charts\r\nmyAnimation <- ggplot2::ggplot(data = posteriorsDf, aes(x = place, y = posterior)) +\r\n  ggplot2::geom_area(data = posteriorsDf %>% dplyr::filter(hdi == 'yes'), fill = 'light blue') +\r\n  ggplot2::geom_line(size = 1) +\r\n  ggplot2::geom_point(aes(x = pointTrial, y = 0), size = 4) +\r\n  ggplot2::geom_point(aes(x = point, y = 0), size = 4, color = \"red\") +\r\n  ggplot2::geom_text(aes(x = 8, y = 0.1, label = stringr::str_glue(\"Trial: {trial}\\n95% HDI: [{lhdi}, {hhdi}]\")), color = \"grey\") +\r\n  ggplot2::scale_y_continuous(limits = c(0,0.105)) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" cm\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"),\r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Bayesian belief updating in the billiards table thought experiment\",\r\n    subtitle = \"Target place: 88 cm\",\r\n    x = 'PLACE ON THE BILLIARDS TABLE', \r\n    y = 'POSTERIOR PROBABILITY',\r\n    caption = \"\\n\\nThe static red point corresponds to a random and unknown location of the first ball. The moving black point then corresponds to the location where the second,\\nrepeatedly rolled ball randomly ended up. The area in blue corresponds to the 95% Highest Density Interval of the posterior distribution. All points inside this\\ninterval have a higher probability density than points outside this interval.\"\r\n    ) +\r\n  gganimate::transition_time(trial) +\r\n  gganimate::ease_aes('linear')\r\n\r\n# animating chart\r\ngganimate::animate(myAnimation, nframes = 125, fps = 5, height = 6, width = 11, units = \"in\", res = 125)\r\n\r\n# saving animated chart as a .gif file\r\ngganimate::anim_save(filename = \"./bayesianBelifUpdating.gif\", animation = last_animation())\r\n\r\n\r\n\r\nAnd here is the resulting animation of the Bayesian belief updating\r\nprocess across 500 trials.\r\n\r\nFor those who would like to incorporate Bayesian reasoning into their\r\nmanagerial decision-making under uncertainty, I can recommend this\r\narticle by Brian T.\r\nMcCann.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-27-bayesian-belief-updating/./bayesian-belief-updating.jpg",
    "last_modified": "2022-11-17T14:59:19+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-24-police-cadet-evaluation-dataset/",
    "title": "Police cadet evaluation dataset",
    "description": "A \"new\" real-world dataset useful for training in people analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-24",
    "categories": [
      "people analytics",
      "data",
      "recruitment",
      "hiring",
      "training"
    ],
    "contents": "\r\nWhile cleaning out my (very) old computer, I came across a hidden\r\ngem: a dataset with real-world data about police cadet evaluation. It\r\nwas part of a tutorial from Peltarion, an AI\r\nsoftware company providing specialized software (Synapse) for\r\ncreating, training, evaluating, and deploying artificial neural networks\r\nand other adaptive systems (recently acquired by King). AFAIK,\r\nthis dataset is not part of any publicly available database with\r\ntraining datasets, so it may add a bit to the portfolio of possibilities\r\nfor those involved and interested in people analytics.\r\nThe data were collected as part of an effort by the National\r\nPolice Services Agency and the Dutch\r\nMinistry of Justice and Security to objectively examine whether the\r\ndata collected at the time of graduation of police cadets can be used to\r\npredict the requirements for passing the standard five-year evaluation.\r\nThe main reason for the study was to find the key indicators for the\r\nthen 20% failure rate, which was considered unacceptable (data were\r\ncollected in the late 1990s), and to study the effects of lowering\r\nadmission standards (accepting cadets with past criminal records and\r\nlowering the minimum grade from 5.5 to 4.0).\r\nThe dataset has the following characteristics:\r\n2000 observations\r\n9 attributes:\r\nAge: the age at which the cadet started studying to\r\nbecome a police officer.\r\nAvG: average grade at the time of graduation (scale\r\n1-10).\r\nChdn: number of children at the time of\r\ngraduation.\r\nExEd: extra university-level or equivalent\r\neducation (years).\r\nCR: criminal record (0=No, 1=Yes).\r\nSex: sex of the cadet (0 = Male, 1 = Female).\r\nSecE: other experience in the security sector (0 =\r\nNo, 1 = Yes).\r\nAvgE: average yearly evaluation score (The average\r\nof five years. The evaluation is performed by a committee of 10 senior\r\npolice officers. Scale 1-5). This is a help attribute and not for use as\r\ninput.\r\nFinalE: final evaluation. Fail if average yearly\r\nevaluation score (Avg) < 2.0 otherwise pass. (1610 Pass / 390 Fail).\r\nThis is the target attribute.\r\n\r\nHere is a table you can use to check and download the data.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and making user-friendly data table\r\nlibrary(tidyverse)\r\nlibrary(DT)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./policeCadetEvaluation.csv\")\r\n\r\n# adjusting the data type for some variables for tabulation and visualization purposes\r\ndata <- data %>%\r\n  mutate(\r\n    CR = as.factor(CR),\r\n    Sex = as.factor(Sex),\r\n    SecE = as.factor(SecE),\r\n    FinalE = as.factor(FinalE)\r\n  )\r\n\r\n# defining the table\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy', 'csv', 'excel'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nAnd here is a pairplot showing the distribution and relationships\r\nbetween variables in the dataset.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for the pairplot data visualization\r\nlibrary(GGally)\r\n\r\n# defining custom function for diagonal continuous variable chart  \r\nmy_dens <- function(data, mapping) {\r\n  ggplot(data = data, mapping = mapping) +\r\n    geom_density(alpha = 0.6, color = NA) \r\n}\r\n\r\n# pairplot\r\nGGally::ggpairs(\r\n  data = data,\r\n  title = \"Police cadet evaluation dataset\",\r\n  mapping=ggplot2::aes(fill = FinalE),\r\n  lower=list(\r\n    combo = wrap(\"facethist\", binwidth=1, alpha = 0.6),\r\n    continuous = wrap(\"points\", alpha = 0.3, size = 0.7),\r\n    discrete = wrap(\"facetbar\", alpha = 0.6)\r\n    ),\r\n  upper=list(\r\n    discrete = wrap(\"box\", alpha = 0.6),\r\n    combo = wrap(\"box\", alpha = 0.6)\r\n  ),\r\n  diag = list(\r\n    continuous = my_dens,\r\n    discrete = wrap(\"barDiag\", alpha = 0.6)\r\n    )\r\n  ) +\r\n  ggplot2::scale_fill_manual(values=c(\"Fail\" = \"#e53935\", \"Pass\" = \"#00897b\")) +\r\n  labs(caption = \"\\nThe color indicates the pass/fail result of the final evaluation, the target attribute.\") +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 12, hjust = 0),\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you want to download the dataset, you can do so here via the table\r\nabove or via my\r\nGitHub page where you can also find more information about the\r\ndataset. Happy analysis üòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-24-police-cadet-evaluation-dataset/./Politie_Nederland_nieuw_uniform.jpg",
    "last_modified": "2022-11-17T15:01:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-conditional-inference-tree/",
    "title": "Divide and... understand",
    "description": "Finding the breakpoint when people start to score significantly higher/lower on a given criterion - the use case for the Conditional Inference Tree algorithm.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-18",
    "categories": [
      "conditional inference tree",
      "decision tree",
      "machine learning",
      "statistics",
      "interpretability",
      "prediction",
      "r"
    ],
    "contents": "\r\nWhen correlating collaboration metrics with business criteria that\r\nour clients are interested in, such as the size of the internal network\r\nof salespeople vs.¬†their sales performance, we often encounter the\r\nquestion of where the breakpoint is when people start to score\r\nsignificantly higher/lower on a given criterion.\r\nTo answer this question, I find very handy the Conditional\r\nInference Tree algorithm - a non-parametric class of decision trees\r\nthat, unlike traditional decision trees, use a significance/permutation\r\ntest (corrected for multiple testing) to select covariates to split and\r\nrecurse the variable.\r\nWhen applied to just one numerical predictor, it will provide a set\r\nof partitions that allow you to split that predictor into bins in such a\r\nway that you end up with statistically significant differences between\r\nsome of the identified bins. With this information in hand, it is much\r\neasier for you to find the ‚Äúsweet spots‚Äù (there may be more than one)\r\nwhere the criterion starts to behave differently in relation to the\r\npredictor values. See charts below for illustration.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for data manipulation and visualuzation \r\nlibrary(tidyverse)\r\n\r\n# defining normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\n# creating artificial dataset with internal network size and sales performance variables\r\ninternalNetworkSize = seq(-6, 6, 0.1)\r\nsalesPerf = 1*(internalNetworkSize**3) + 2*(internalNetworkSize**2) + 1*internalNetworkSize + 3\r\nsalesPerf_noise = 70 * rnorm(mean = 0, sd = 1, n=length(salesPerf))\r\nsalesPerformance = salesPerf + salesPerf_noise\r\n\r\n# putting data into dataframe and making some transformations of the variables\r\ndata <- data.frame(\r\n  internalNetworkSize = internalNetworkSize,\r\n  salesPerformance = salesPerformance\r\n) %>%\r\n  dplyr::mutate(\r\n    internalNetworkSize = normalize(internalNetworkSize),\r\n    salesPerformance = normalize(salesPerformance),\r\n    internalNetworkSize = internalNetworkSize*189,\r\n    salesPerformance = salesPerformance*100\r\n  )\r\n\r\n# visualizing relationship between internal network size and sales performance\r\nggplot2::ggplot(data = data, aes(x = internalNetworkSize, y = salesPerformance)) +\r\n  ggplot2::geom_point(color = \"#4d009d\", size = 3, alpha = 0.8) +\r\n  ggplot2::labs(\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE\",\r\n    y = \"SALES PERFORMANCE\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,200, 20), limits = c(0,200)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"bottom\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries for ctree algorithm and visualization of the results of statistical tests\r\nlibrary(partykit)\r\nlibrary(ggstatsplot)\r\n\r\n# defining formula\r\nfmla <- as.formula(\"salesPerformance ~ internalNetworkSize\")\r\n\r\n# binning internal network size variiablle using ctree algorithm\r\nctree <- partykit::ctree(\r\n  fmla,\r\n  data = data,\r\n  na.action = na.exclude,\r\n  control = partykit::ctree_control(minbucket = ceiling(round(0.05*nrow(data))))\r\n)\r\n\r\n# plotting resulting tree\r\n#plot(ctree)\r\n\r\n# number of identified bins\r\n#bins = partykit::width(ctree)\r\n\r\n# extracting bin borders\r\ncutvct = data.frame(matrix(ncol=0,nrow=0)) # Shell\r\nn = length(ctree) # Number of nodes\r\nfor (i in 1:n) {\r\n  cutvct = rbind(cutvct, ctree[i]$node$split$breaks)\r\n}\r\ncutvct = cutvct[order(cutvct[,1]),] # sorting / converting to an ordered vector (asc)\r\ncutvct = ifelse(cutvct<0,trunc(10000*cutvct)/10000,ceiling(10000*cutvct)/10000) # rounding to 4th decimal place to avoid borderline cases\r\n\r\n# adding minimum and maximum values\r\ncutvct <- append(cutvct, min(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct <- append(cutvct, max(data[\"internalNetworkSize\"], na.rm = TRUE))\r\ncutvct = cutvct[order(cutvct)]\r\n\r\n# creating bin categories\r\nvalueCat <- cut(x = data %>% dplyr::pull(\"internalNetworkSize\"), breaks = cutvct, include.lowest = TRUE)\r\n\r\n# creating supplementary dataframe for visualization purposes \r\nsuppDf <- data %>%\r\n  dplyr::select(internalNetworkSize, salesPerformance) %>%\r\n  dplyr::mutate(category = valueCat) %>%\r\n  dplyr::filter(category != \"NA\")\r\n\r\n# visualizing relationship between internal network size and sales performance using ggbetweenstats from ggstatsplot package\r\nggstatsplot::ggbetweenstats(\r\n  data = suppDf %>% as.data.frame(),\r\n  x = category,\r\n  y = salesPerformance,\r\n  type = \"robust\"\r\n) +\r\n  ggplot2::scale_y_continuous(labels = scales::number_format(suffix = \"%\"), breaks = seq(0,100,20)) +\r\n  ggplot2::labs(\r\n    y = \"SALES PERFORMANCE\",\r\n    x = \"INTERNAL NETWORK SIZE OF SALESPEOPLE (BINNED)\",\r\n    title = \"Relationship between internal network size of salespeople and their performance\"\r\n    ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,20,0)),\r\n                 plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 13, margin=margin(0,0,15,0)),\r\n                 plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n                 axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n                 axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n                 axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n                 axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n                 axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n                 legend.position= \"\",\r\n                 legend.key = element_rect(fill = \"white\"),\r\n                 legend.key.width = unit(1.6, \"line\"),\r\n                 legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n                 legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n                 legend.background = element_rect(fill = \"transparent\"),\r\n                 panel.background = element_blank(),\r\n                 panel.grid.major.y = element_blank(),\r\n                 panel.grid.major.x = element_blank(),\r\n                 panel.grid.minor = element_blank(),\r\n                 axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n                 axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n                 plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n                 plot.title.position = \"plot\",\r\n                 plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIf you are dealing with similar use cases, give it a try. And if you\r\nuse any other tools/approaches for this, feel free to share them in\r\nreturn.\r\nP.S. Thanks to Filip\r\nTrojan, my former boss and colleague from the Deloitte Advanced\r\nAnalytics team, who introduced me to this tool. I still benefit from it\r\nto this day üôèüí™\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-18-conditional-inference-tree/./decision-tree-analysis.jpg",
    "last_modified": "2022-10-25T10:37:02+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-11-timeboxing/",
    "title": "Timeboxing. Does it really work?",
    "description": "Checking with real-world collaboration data whether timeboxing has a protective function in terms of time available for focused work.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-10-11",
    "categories": [
      "timeboxing",
      "timeblocking",
      "regression analysis",
      "control variables"
    ],
    "contents": "\r\nIn one of my previous\r\nposts, I addressed the question of why people don‚Äôt make more use of\r\ntimeboxing, a time\r\nmanagement tool relying on a well-researched self-regulatory technique\r\ncalled implementation\r\nintention - planning what you will do, when, and how.\r\nQuite surprisingly, I found in our collaborative data that there is\r\nnot a positive but a slightly negative relationship between the amount\r\nof time for focused work and the amount of blocked working time in the\r\ncalendar, which I interpreted to mean that people who have plenty of\r\ntime for focused work usually do not have a strong need to block time\r\nfor focused work in their calendars.\r\nHowever, based on these results, one colleague wondered whether this\r\nresult actually speaks against the usefulness of this tool. To answer\r\nher question properly, we should avoid comparing apples with pears and\r\ncontrol for the effect of the number of collaborative activities people\r\nparticipate in, as it can be assumed that those who spend more time\r\ncollaborating with others have less time for focused work and also use\r\nthe timeboxing technique more.\r\nUsing this approach and our clients‚Äô collaborative data, I looked at\r\nthe relationship between the proportion of work time blocked on the\r\ncalendar and the time available for focused work (i.e.¬†no meetings, no\r\nad-hoc calls, no email or instant messaging), and found that the\r\nmarginal effect of timeboxing is in line with the positive effect of the\r\ntimeboxing technique on the time available for focused work. The effect\r\nis not huge (each percentage point of working time blocked in the\r\ncalendar yields on average .19% of focus rate), however, timeboxing\r\nseems to be saved, phew üòâ\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-11-timeboxing/./time-blocking.png",
    "last_modified": "2022-10-25T06:50:42+02:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-09-17-multilevel-modeling/",
    "title": "Multilevel modeling in people analytics",
    "description": "Don't chase (statistical) ghosts and use multilevel models instead!",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-17",
    "categories": [
      "multilevel modeling",
      "hierarchical modeling",
      "mixed models",
      "nested data",
      "bayesian inference",
      "collaboration",
      "employee engagement",
      "r"
    ],
    "contents": "\r\nIn one of our projects,\r\nwhere we were trying to find out how collaborative behavior relates to\r\nemployee engagement, we repeatedly came across patterns that reminded\r\nthe client of internally well-known differences between the behavior of\r\nteams from different parts of the company. For example, we found that\r\nmore frequent participation in short and small meetings was related to\r\nlower employee engagement. This pattern matched well the client‚Äôs\r\nobservation that one particular part of the company had regular daily\r\nstand-up meetings and also lower engagement scores compared to the rest\r\nof the company due to some other aspects of their work. As further\r\nanalysis confirmed, this pattern was really just a statistical artifact\r\ncaused by the coincidence of these two facts.\r\nOne way analysts can protect themselves from this type of misleading\r\nconclusions is by using multilevel or hierarchical\r\nmodels that take into account the fact that the data\r\nhave a nested structure, i.e.¬†that some observations are not\r\nindependent of each other because they belong to the same higher-order\r\ngroup, e.g.¬†to an organizational unit (one of the basic assumptions of\r\nmost statistical models in use).\r\nThis is well illustrated in the graphs below. They show that the\r\nrelationship between the number of monthly 1:1s that employees have with\r\ntheir line manager and their subjectively perceived support from their\r\nline manager is slightly positive across most groups of teams (shown by\r\ncolored dots and lines), but when all teams are analyzed together, the\r\nrelationship is rather negative (shown by black dots and lines).\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(scales)\r\nlibrary(patchwork)\r\nlibrary(ggtext)\r\n\r\ndata <- readr::read_csv(\"./data.csv\")\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support while taking into account differences between organizational units \r\ng1 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(aes(col = Unit), size = 2.5, alpha = 0.5) + \r\n  ggplot2::geom_smooth(aes(col = Unit), method = 'lm', alpha=0.2, se = F) +\r\n  labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\",\r\n    color = \"\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::scale_color_manual(values = c(\"Unit A\" = \"#20066b\", \"Unit B\" = \"#e56b61\", \"Unit C\" = \"#b4ba0d\", \"Unit D\" = \"#32b2c7\")) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# chart showing a relationship between 1:1s and perceived managerial support without taking into account differences between organizational units \r\ng2 <- data %>% \r\n  ggplot2::ggplot(aes(oneonones, mngsupport)) +\r\n  ggplot2::geom_point(size = 2.5, alpha = 0.5) +\r\n  ggplot2::geom_smooth(method = 'lm', alpha=0.2, linetype = \"solid\", color = \"black\", se = F) +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PROPORTION OF FAVORABLE RESPONSES\"\r\n  ) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,5,1)) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 21, margin=margin(0,0,0,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- g2 + g1\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Relationship between the number of 1:1s and perceived managerial support across**\r\n  <br>\r\n  **and within organizational units**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,12,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# fitting Bayesian hierarchical linear regression model\r\nmodel <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones + (1 + oneonones | Unit)),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(model)\r\n# plot(model)\r\n# brms::pp_check(model, ndraws = 100)\r\n\r\n\r\n# fitting Bayesian non-hierarchical linear regression model\r\nmodelNonHierarchical <- brms::brm(\r\n  brms::bf(mngsupport | trunc(lb = 0, ub = 1) ~ 1 + oneonones),\r\n  data = data,\r\n  family = gaussian(),\r\n  chains = 3, \r\n  iter = 3000, \r\n  warmup = 1000,\r\n  cores = 6, \r\n  seed = 1234, \r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2\r\n)\r\n\r\n# checking the fitted model\r\n# summary(modelNonHierarchical)\r\n# plot(modelNonHierarchical)\r\n# brms::pp_check(modelNonHierarchical, ndraws = 100)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(emmeans)\r\nlibrary(tidybayes)\r\n\r\n# marginal effect of 1:1s in the Bayesian hierarchical linear regression model\r\navg_marginal_effect <- model %>% \r\n  emmeans::emmeans(~ oneonones,\r\n                   at = list(oneonones = seq(0, 6, by = 0.1)),\r\n                   epred = TRUE,\r\n                   re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf1 <- ggplot2::ggplot(avg_marginal_effect, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Hierarchical linear regression model\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n# marginal effect of 1:1s in the Bayesian non-hierarchical linear regression model\r\navg_marginal_effect_nonHierarchical <- modelNonHierarchical %>% \r\n  emmeans::emmeans(\r\n    ~ oneonones,\r\n    at = list(oneonones = seq(0, 6, by = 0.1)),\r\n    epred = TRUE,\r\n    re_formula = NULL) %>% \r\n  tidybayes::gather_emmeans_draws()\r\n\r\ngf2 <- ggplot2::ggplot(avg_marginal_effect_nonHierarchical, aes(x = oneonones, y = .value)) +\r\n  tidybayes::stat_lineribbon() +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0.62, 0.8, 0.05), limits = c(0.62, 0.8)) +\r\n  ggplot2::scale_x_continuous(breaks = seq(0,6,1)) +\r\n  ggplot2::scale_fill_brewer(palette = \"Purples\") +\r\n  ggplot2::labs(\r\n    x = \"AVERAGE MONTHLY NUMBER OF 1:1S\",\r\n    y = \"PREDICTED % OF FAVORABLE RESPONSES\",\r\n    fill = \"Credible interval\",\r\n    title = \"Non-hierarchical linear regression model\"\r\n  ) +\r\n    ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"plain\", size = 19, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,0,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position= \"bottom\",\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(0,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        legend.background = element_rect(fill = \"transparent\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ngf <- gf2 + gf1\r\ngf <- gf + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:22pt;font-weight:bold;'>**Posterior average marginal effect of 1:1 meetings on perceived managerial support**\r\n    <\/span>\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(12,0,0,0), size = 22, face=\"bold\")\r\n  )\r\n)\r\n\r\nprint(gf)\r\n\r\n\r\n\r\n\r\nWithout the use of the hierarchical model (and/or a careful post-hoc\r\nvisual check of alternative explanations), we would reach a completely\r\nopposite (and incorrect) conclusion about the relationship between the\r\nnumber of 1:1s and perceived support from the line manager (a phenomenon\r\nknown as Simpson‚Äôs\r\nparadox). In this particular case, it is relatively easy to\r\nrecognize that something may be wrong, but the situation is not always\r\nso obvious and intuitive. In these other cases, it is advantageous to\r\nhave some tool at hand to compensate for our imperfect intuition and\r\nlimited knowledge and imagination. Hierarchical models are one such\r\ntool.\r\nIf you don‚Äôt already have it in your analytics toolbox, be sure to\r\ngive it a try. If you work with R, you can use the lme4\r\nor brms\r\npackages to implement it. In a Python\r\nenvironment, you can use the statsmodels or\r\nPyMC3 libraries to\r\ndo this. And if you‚Äôre more used to drag-and-drop tools, then JASP or jamovi (both open-source alternatives\r\nto SPSS)\r\nwill give you access to various mixed models through an easy-to-use\r\ngraphical interface.\r\nFor an accessible discussion of this topic in the context of people\r\nanalytics, including other useful tools for working with hierarchical\r\ndata, see also the excellent articles by Paul van der\r\nLaken, John\r\nLipinski, and Max\r\nBlumberg:\r\nSimpson‚Äôs\r\nParadox: Two HR examples with R code.\r\nHow\r\nto Avoid Aggregation Errors and Simpson‚Äôs Paradox in HR Analytics: Part\r\n1\r\nHow\r\nto Avoid Aggregation Errors and Simpson‚Äôs Paradox in HR Analytics: Part\r\n2\r\nWhy\r\nPeople Analytics should NOT be using regression to predict team\r\noutcomes\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-multilevel-modeling/./groupsofpeople.jpg",
    "last_modified": "2022-09-17T20:03:50+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-and-personality/",
    "title": "Collaboration and personality",
    "description": "Personality is not fate, at least when it comes to the level of engagement in corporate communication and collaboration.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-15",
    "categories": [
      "collaboration",
      "communication",
      "networking behavior",
      "personality",
      "big five"
    ],
    "contents": "\r\nOne of our clients once\r\nasked us to what extent employees‚Äô level of engagement in corporate\r\ncommunication and collaboration is driven by their personality and to\r\nwhat extent by their job role, the conditions in which they work, and\r\nother factors outside their personality.\r\nOur first answer was that the latter probably plays a more\r\nsignificant role than the former, but it was difficult to answer more\r\nspecifically because we did not yet have all the data we needed to\r\nquantify the tightness of this relationship. This motivated me to look\r\nat existing research on this topic to help us better set our apriori\r\nexpectations on this issue.\r\nWith the help of metaBus, an\r\namazing platform for curating, searching, and summarizing research\r\nfindings from the social and organizational sciences, I was able to get\r\nthe results of over 100 studies on the relationship between employees‚Äô\r\nBig\r\n5 personality traits and the amount of interaction and networking\r\nbehavior they engage in. Among the criteria for the amount of\r\ninteraction were variables like contact frequency, frequency of\r\nparticipation, communication frequency, meeting frequency, hours of\r\ninteraction, interaction duration, etc. For networking behavior there\r\nwere criteria as liaison, building networks, relationship building,\r\nnetwork activity, maintaining contacts, increasing internal visibility,\r\nnetwork ability, and informal network.\r\n\r\n\r\nShow code\r\n\r\n# The following concepts were used to search for relevant studies on the metaBus platform (their respective codes are given in brackets):\r\n# Big 5 (20443)  \r\n# Amount of interaction (20287) \r\n# Networking behavior (80017)\r\n\r\n# uploading libraries\r\nlibrary(tidyverse) # for data manipulation and visualization \r\nlibrary(ggridges) # for data visualization\r\nlibrary(ggtext) # for enabling markdown in ggplots\r\nlibrary(patchwork) #  for combining ggplots\r\n\r\n# data preparation\r\n# uploading data\r\ninteraction <- readr::read_csv(\"./interactionAmount.csv\")\r\nnetworking <- readr::read_csv(\"./networkingBehavior.csv\")\r\n\r\n# preparing dataset for amount of interaction concept\r\ninteractionPrep <- interaction %>%\r\n  dplyr::filter(\r\n    # removing non-relevant personality characteristics \r\n    !Var1 %in% c(\"Empathic concern\"),\r\n    # limiting to studies conducted at the individual level\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Extraversion\") | stringr::str_detect(Var1, \"Extroversion\") | stringr::str_detect(Var1, \"extraversion\") ~ \"Extraversion\",\r\n      stringr::str_detect(Var1, \"Openness to experience\") | stringr::str_detect(Var1, \"openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"agreeableness\") ~ \"Agreeableness\",\r\n      stringr::str_detect(Var1, \"emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# preparing dataset for networking behavior concept\r\nnetworkingPrep <- networking %>%\r\n  dplyr::filter(\r\n    Var2Unit == \"Individual\"\r\n  ) %>%\r\n  # reversing Neuroticism to Emotional Stability\r\n  dplyr::mutate(\r\n    r = case_when(\r\n      Var1 == \"Neuroticism\" ~ r*-1,\r\n      TRUE ~ r\r\n    )\r\n  ) %>%\r\n  # uniting the names of personality characteristics across the studies\r\n  dplyr::mutate(\r\n    Var1 = case_when(\r\n      stringr::str_detect(Var1, \"Openness\") ~ \"Openness\",\r\n      stringr::str_detect(Var1, \"Emotional stability\") | stringr::str_detect(Var1, \"Neuroticism\") ~ \"Emotional Stability\",\r\n      stringr::str_detect(Var1, \"Conscientious\") | stringr::str_detect(Var1, \"Consciousness\") ~ \"Conscientiousness\",\r\n      TRUE ~ Var1\r\n    )\r\n  )\r\n\r\n# data visualization\r\n# creating chart for the amount of interaction concept\r\ninteractionChart <- interactionPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#e56b61\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    axis.text.x = element_text(),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n)\r\n\r\n# creating chart for the networking behavior concept\r\nnetworkingChart <- networkingPrep %>%\r\n  ggplot2::ggplot(aes(x = r, y = Var1)) + \r\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", size = 0.56) +\r\n  ggridges::geom_density_ridges(\r\n    fill = \"#32b2c7\",\r\n    alpha = 0.5,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 0, height = 0,seed = 123),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"solid\", vline_color = \"black\", vline_size = 0.55\r\n    #quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  ggplot2::scale_x_continuous(limits = c(-1, 1), breaks = seq(-1,1,0.2)) +\r\n  ggplot2::labs(\r\n    x = \"PEARSON CORRELATION COEFFICIENT\",\r\n    y = \"\"\r\n  ) +\r\n  ggplot2::theme(\r\n    plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,12,0), hjust = 0.5),\r\n    plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n    axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n    axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 1),\r\n    legend.title = element_text(color = '#2C2F46', face = \"plain\", size = 12),\r\n    legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10),\r\n    axis.text.y = element_blank(),\r\n    axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n    legend.position = \"right\",\r\n    axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n    axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n    panel.background = element_blank(),\r\n    panel.grid.major.y = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n    axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n    plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n    plot.title.position = \"plot\",\r\n    plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts together\r\ng <- interactionChart + networkingChart \r\n\r\n# adding title and caption\r\ng <- g + patchwork::plot_annotation(\r\n  title = \"<span style='font-size:20pt;font-weight:bold;'>**Do Big 5 traits predict** \r\n    <span style='color:#e56b61;'>**the amount of interaction**<\/span> **&**\r\n    <span style='color:#32b2c7;'>**networking behavior**<\/span> **of employees?**\r\n    <\/span>\",\r\n  \r\n  caption = \"The solid vertical lines represent quartile values.\\nBased on studies found on the metaBus platform using the concepts 'Big 5' (code: 20443), 'Amount of interaction' (code: 20287), and 'Networking behavior' (code: 80017).\",\r\n  theme = theme(\r\n    plot.title = ggtext::element_markdown(lineheight = 1.1, margin=margin(10,0,12,0)),\r\n    plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 9, hjust = 0)\r\n    )\r\n  )\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nAs can be seen from the graphs above, the relationship between\r\npersonality and the amount of interaction and networking behavior goes\r\nin the expected direction. Agreeable, open, and extraverted employees\r\nand to some extent also conscientious and emotionally stable employees\r\ntend to engage more in interactions with others and in networking.\r\nHowever, the relationships found are relatively weak. The middle 80% of\r\nobserved effects range from an absolute value of .02 to .24, so across\r\nthe studies shown, small effects prevail. And even in the case of the\r\nstrongest effect (r = .38), personality ‚Äúexplains‚Äù only 14% of\r\nthe variability in the networking behavior. There is therefore ample\r\nscope for the influence of a range of other factors.\r\nHow about you? Are you able to engage in interactions and networking\r\nin a way that supports your career, work performance, or other positive\r\noutcomes, perhaps despite your natural tendencies due to your\r\npersonality setup? Feel free to share your experience and thoughts in\r\nthe comments. Btw, you can find interesting information on this topic in\r\nthe excellent book 8\r\nSteps to High Performance by Marc Effron, specifically\r\nin chapters 4 and 6.\r\nCaveat: The graphs represent only a simple summary\r\nof the effects observed in the selected studies, not a proper\r\nmeta-analysis. If you‚Äôre interested in the specific analysis steps and\r\nchoices behind the graphs shown, you can check the code above or go to\r\nmy GitHub\r\npage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-and-personality/./personalityandcollaboration.jpeg",
    "last_modified": "2022-09-17T14:19:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-collaboration-overload-and-bottlenecks/",
    "title": "Hot spots of collaboration overload and collaboration bottlenecks and how to find them",
    "description": "One of the most useful insights that can be gleaned from collaboration data is where hot spots of potential collaboration overload and/or collaboration bottlenecks may exist in a company. Such insight can be especially valuable these days, when many companies are trying to fight the upcoming economic downturn by achieving more with less.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-08",
    "categories": [
      "collaboration overload",
      "collaboration bottlenecks",
      "organizational network analysis",
      "r"
    ],
    "contents": "\r\nOne simple way to identify such hot spots is to compare the\r\noutbound and inbound collaborative activities in which\r\nteams or individuals participate. The greater the difference between the\r\ntwo in favor of inbound collaboration activities, the stronger the\r\nsignal that collaboration overload and/or bottlenecks may be a problem\r\nfor that team or individual.\r\nTo illustrate, take a look at the attached charts that show the\r\npatterns of collaboration between several teams via Slack. The distances\r\nbetween teams, the thickness, and the direction of the arrows between\r\nthem tell us who is collaborating with whom and how much. The size of\r\nthe nodes then represents the amount of inbound and outbound\r\ncollaborative activities that the teams participate in, respectively.\r\nBased on the differences between them, the bar chart below shows us\r\nwhere the inbound collaboration activities outweigh the outbound ones\r\nthe most, and therefore where the risk of collaboration overload and/or\r\nbottlenecks is greatest.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nlibrary(patchwork)\r\n\r\n# uploading data with nodes/ties based on the current frequency of communication via Slack\r\nnodesR <- readr::read_csv(\"./nodes.csv\") \r\ntiesR <- readr::read_csv(\"./ties.csv\")\r\n\r\n# specifying cut-off value for showing only the x% of strongest edges\r\nprob = 1\r\n\r\n# changing coding of individual nodes in the network\r\nties <- tiesR %>%\r\n  dplyr::left_join(nodesR, by = c(\"from\" = \"id\")) %>%\r\n  dplyr::select(-from) %>% \r\n  dplyr::rename(from = name) %>%\r\n  dplyr::left_join(nodesR, by = c(\"to\" = \"id\")) %>%\r\n  dplyr::select(-to) %>%\r\n  dplyr::rename(to = name) %>%\r\n  dplyr::select(from, to, weight) %>%\r\n  dplyr::mutate(\r\n    from = stringr::str_to_title(from),\r\n    to = stringr::str_to_title(to),\r\n    from = stringr::str_replace(from, \"Development\", \"Dev\"),\r\n    to = stringr::str_replace(to, \"Development\", \"Dev\"),\r\n    from = stringr::str_replace(from, \"Dev Management\", \"Dev - Management\"),\r\n    to = stringr::str_replace(to, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n  \r\nnodes <- nodesR %>%\r\n  dplyr::select(-id) %>%\r\n  dplyr::mutate(\r\n    name = stringr::str_to_title(name),\r\n    name = stringr::str_replace(name, \"Development\", \"Dev\"),\r\n    name = stringr::str_replace(name, \"Dev Management\", \"Dev - Management\")\r\n  )\r\n\r\n\r\n# making the network from the data frame \r\ng <- igraph::graph_from_data_frame(d = ties, vertices = nodes, directed = TRUE)\r\n\r\n# setting name of the network\r\ng$name <- \"Collaboration via Slack\"\r\n\r\n# assigning ids to nodes\r\nV(g)$id <- seq_len(vcount(g))\r\n\r\n# cutoff value for showing only the x% of strongest edges\r\ncutoff <- quantile(ties$weight, probs = prob)[[1]]\r\n\r\n# visualizing the inbound network\r\nset.seed(1234)\r\ninG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = inbound), alpha = 0.5, fill = \"#32b2c7\", color = \"#32b2c7\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n    ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"INBOUND COLLABORATION ACTIVITIES\")\r\n    )\r\n\r\n\r\n\r\n# visualizing the outbound network\r\nset.seed(1234)\r\noutG <- ggraph(g, layout = 'fr', maxiter = 50000) +\r\n  ggraph::geom_edge_link(aes(edge_width = ifelse(weight > cutoff, NA, weight), edge_color = weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(2, 'mm')) + \r\n  ggraph::geom_node_point(aes(size = outbound), alpha = 0.5, fill = \"#46c8ae\", color = \"#46c8ae\") +\r\n  ggraph::scale_edge_width(range = c(0.1, 1.8)) +\r\n  ggraph::scale_edge_color_gradient(low = \"#b8b6b6\", high = \"#000000\", guide = \"none\") +\r\n  ggplot2::scale_size(range = c(0.5, 20)) +\r\n  ggraph::geom_node_text(aes(label = name), repel = TRUE, size = 8) +\r\n  ggraph::theme_graph(background = \"white\", foreground = \"grey\" , border = TRUE) +\r\n  ggplot2::theme(\r\n    legend.position = \"\",\r\n    legend.box = \"vertical\",\r\n    legend.title=element_text(size=8),\r\n    legend.text=element_text(size=8),\r\n    legend.spacing.y = unit(-0.2, \"cm\"),\r\n    plot.title = element_text(hjust = 0.5, size = 30),\r\n    plot.caption.position =  \"plot\"\r\n  ) +\r\n  ggplot2::guides(\r\n    size = guide_legend(reverse=TRUE, order = 1),\r\n    color = guide_legend(order = 3, ncol=10, override.aes = list(size=5)),\r\n    edge_width = guide_legend(reverse=TRUE, order = 2)\r\n  ) +\r\n  ggplot2::labs(\r\n    edge_width = \"Mutual collaboration\",\r\n    edge_color = \"Mutual collaboration\",\r\n    color = \"\",\r\n    size = \"Communication intensity\",\r\n    title = stringr::str_glue(\"OUTBOUND COLLABORATION ACTIVITIES\")\r\n  )\r\n\r\n\r\n# bar chart with info about difference between inbound and outbound collaboration activities\r\ninoutDiffG <- nodes %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(name, inoutDiff), y = inoutDiff)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = ifelse(nodes$inoutDiff > 0, \"#e56b61\", \"#20066b\"), alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(breaks = seq(-200,200,50)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"DIFFERENCE BETWEEN IN/OUTBOUND COLLABORATION ACTIVITIES\",\r\n    y = \"DIFFERENCE BETWEEN THE NUMBER OF IN/OUTBOUND INSTANT MESSAGES\"\r\n  ) +\r\n  ggplot2::coord_flip() +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 30, margin=margin(0,0,12,0), hjust = 0.5),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 24, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 22, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# combining the charts\r\ng <- (inG + outG) / inoutDiffG\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nWe see that the ‚Äúhottest‚Äù hot spots are in teams Dev-Management and\r\nDev-Frontend. While this is not definitive proof that we have real\r\nproblems in these two specific teams, it should be a strong enough\r\nsignal to take notice and try to verify our suspicion with additional\r\ninformation, such as checking some relevant business metrics or simply\r\nasking a few people we know should be affected, if there is a problem.\r\nIf the initial suspicion is confirmed, appropriate action should be\r\ntaken, e.g.¬†consider the relevance of some requests, possibly redirect\r\nthem to other teams, automate some tasks, expand the team and recruit\r\nnew people, etc.\r\nFor more tips on how to leverage collaboration data in the current\r\nuncertain economic times, I recommend reading the articles Top\r\n7 Collaboration Metrics to Utilize in an Economic Crisis by Jan Rezab and Top\r\n6 Metrics to measure during an economic downturn by Shwetha Pai.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-collaboration-overload-and-bottlenecks/./Organizational-network.jpg",
    "last_modified": "2022-09-17T18:33:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-17-back-to-back-meetings/",
    "title": "Are back-to-back meetings for good or bad?",
    "description": "A short post about the practice of back-to-back meetings and how to determine when it's for bad and when it's rather for good.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-09-01",
    "categories": [
      "meeting habits",
      "back-to-back meetings"
    ],
    "contents": "\r\n‚ÄúContext, Context, Context‚Äù could be the headline of this post.\r\nWhen we address the issue of good meeting habits with our clients, the length of breaks\r\nbetween successive meetings is one of the first metrics we focus on.\r\nAs many of you probably know from your firsthand experience,\r\nconsecutive meetings with no breaks, a.k.a. back-to-back\r\nmeetings, have many detrimental effects, from overload\r\nand exhaustion to not being adequately prepared for subsequent\r\nmeetings and arriving\r\nlate to them.\r\nAs useful as the above metric is, it does not tell the whole story\r\nand can lead to invalid conclusions and see a problem where there is\r\nnone. Data from one of our teams illustrates this well.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading datasets from the platform\r\ndata1 <- readr::read_delim(\"./timeisltd-chart1.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata2 <- readr::read_delim(\"./timeisltd-chart2.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\ndata3 <- readr::read_delim(\"./timeisltd-chart3.csv\",delim = \";\") %>%\r\n  rename(cat = `...1`)\r\n\r\n# Time between successive meetings\r\ng1 <- data1 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#20066b\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time between successive meetings\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Number of back-to-back meetings in a row\r\ng2 <- data2 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = as.numeric(cat)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#e56b61\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::scale_x_continuous(labels = scales::number_format(suffix = \" mtgs\", accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Number of back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# Time spent in back-to-back meetings in a row\r\ng3 <- data3 %>%\r\n  dplyr::mutate(\r\n    all = sum(data),\r\n    prop = data/all,\r\n    cat = factor(cat, levels = c(\"31-60 Min\", \"61-90 Min\", \"91-120 Min\", \"121-150 Min\", \"151-180 Min\", \"181+ Min\"), ordered = TRUE)\r\n  ) %>%\r\n  ggplot2::ggplot(aes(x = cat, y = prop)) +\r\n  ggplot2::geom_bar(stat = \"identity\", fill = \"#46c8ae\", alpha = 0.85) +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    title = \"Time spent in back-to-back meetings in a row\",\r\n    y = \"PROPORTION OF CASES\"\r\n  ) +\r\n  ggplot2::theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", size = 20, margin=margin(0,0,20,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.15,.5),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.width = unit(1.6, \"line\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_line(color = \"#E0E1E6\", linetype = \"dashed\"),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n# putting graphs together\r\ng <- g1/g2/g3\r\n\r\nprint(g)\r\n\r\n\r\n\r\n\r\nBased purely on the time between successive meetings, we could\r\nconclude that a given team suffers from an unhealthy frequency of\r\nback-to-back meetings, as in more than a third of cases, there is no\r\nbreak between meetings. However, if we look at how long the series of\r\nback-to-back meetings tend to be (in 75% of cases it‚Äôs only 2 meetings\r\nin a row) and how much time people spend in them (in 42% of cases it‚Äôs\r\nbetween 31-60 minutes and in 30% of cases it‚Äôs between 61-90 minutes),\r\nthen the resulting picture is less pessimistic and more indicative of a\r\nrather healthy level of effort to protect time for focused\r\nwork by batching meetings into short blocks\r\nthat do not come at the cost of exhausting people and making meetings\r\nless effective.\r\nWhat is your approach to back-to-back meetings? Do you try to always\r\nhave at least a 5-minute buffer between two consecutive meetings? And\r\nhow successful are you at this? Are you aware of situations where it is\r\nappropriate to batch meetings into tight blocks without breaks? And do\r\nyou have a limit on how many meetings to put in a row? Feel free to\r\nshare your thoughts and experiences in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-17-back-to-back-meetings/./backtobackmeetings.jpg",
    "last_modified": "2022-09-17T17:37:26+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-20-people-analytics-popularity-after-covid/",
    "title": "The impact of the COVID pandemic on the popularity of people analytics",
    "description": "Many people analytics professionals think that after the COVID pandemic, organizations are more willing to listen to their insights and recommendations. Can we find any empirical support for their hunch? Let's check it out with data provided by Google Trends and segmented regression analysis of interrupted time series.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-29",
    "categories": [
      "people analytics",
      "hr analytics",
      "covid pandemic",
      "segmented regression analysis",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nPeople analytics\r\npopularity after pandemic\r\nData\r\npreparation\r\nModeling\r\nWhat do the model and data\r\ntell us?\r\n\r\nPeople analytics\r\npopularity after pandemic\r\nThere is a fairly common perception among the people analytics\r\nprofessionals with whom I am in contact that after the COVID pandemic,\r\ncompanies are much more willing to use the insights provided by people\r\nanalytics teams and incorporate them into their business-related\r\ndecision-making processes.\r\nI wondered if I could find any empirical support for this feeling in\r\nthe surge of global web search interest in ‚Äúpeople analytics‚Äù and ‚ÄúHR\r\nanalytics‚Äù terms on Google after the pandemic outbreak, assuming the\r\npandemic broke out in March 2020.\r\nLet‚Äôs start our quest with a simple visual inspection of a line chart\r\nshowing the trend of worldwide web search interest in ‚Äúpeople analytics‚Äù\r\nand ‚ÄúHR analytics‚Äù terms on Google from January 2007 to July 2022. (You\r\ncan replicate this chart using the Google Trends website and\r\nthe search terms, time range, location, and source for searches\r\ndescribed above. If you are familiar with R, you can use the code\r\nbelow.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for getting data from Google Trends\r\nlibrary(gtrendsR)\r\n# uploading libraries for data manipulation\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\n# setting parameters for Google Trends\r\nsearchTerms   <- c(\"people analytics\", \"hr analytics\")\r\nlocation      <- \"\" # global\r\ntime          <- \"2007-01-01 2022-07-31\"\r\nsource         <- \"web\"\r\n\r\n# getting Google Trends data\r\ngtrendsResult <- gtrendsR::gtrends(\r\n    keyword = searchTerms, \r\n    geo     = location, \r\n    time    = time,\r\n    gprop   = source\r\n    )\r\n\r\n# cleaning data\r\ngtrendsResultDf <- gtrendsResult %>%\r\n    purrr::pluck(\"interest_over_time\") %>%\r\n    dplyr::select(date, hits, keyword) %>%\r\n    dplyr::mutate(date = lubridate::ymd(date))\r\n\r\n\r\n\r\n\r\nTowards the end of the time series, somewhere between September 2019\r\nand March 2020, it seems that the trend stops increasing and starts to\r\nstagnate, except for the very last part of the graph, which shows a\r\nsharp increase in searches for both terms, but this may just be the\r\nresult of the improved data collection system from January 2022 onwards\r\n(as indicated by the vertical line in the graph with a note). Thus, the\r\ndata seems to suggest the opposite of what we would expect if a pandemic\r\nwere to have a positive effect on interest in people analytics.\r\nHowever, after combining the results for the two search terms and\r\nplotting them on a graph together with the unadjusted regression trend\r\nlines, the resulting picture gives a slightly different impression.\r\nThere appears to be little to no decline in interest in people analytics\r\nimmediately after the pandemic outbreak, but a steeper slope of change\r\nafter the pandemic.\r\n\r\n\r\nShow code\r\n\r\n# normalize function\r\nnormalize <- function(x) {\r\n  return ((x - min(x)) / (max(x) - min(x)))\r\n}\r\n\r\n\r\ngtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100\r\n  ) %>%\r\n    # creating new pandemic variable \r\n  dplyr::mutate(\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ \"After the pandemic outbreak\",\r\n      TRUE ~ \"Before the pandemic outbreak\"\r\n    )\r\n  )%>%\r\n  ggplot2::ggplot(aes(x = date, y = interestInPeopleAnalytics, color = pandemic)) +\r\n  ggplot2::geom_point() +\r\n  ggplot2::geom_smooth(method = \"lm\", se = FALSE) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggplot2::scale_x_date(breaks = \"1 year\", date_labels = \"%Y\") +\r\n  ggplot2::labs(\r\n    x = \"\",\r\n    y = \"Interest in people analytics\",\r\n    title = \"Interest in people analytics before and after the pandemic outbreak\",\r\n    caption = \"The solid lines represent unadjusted regression model trend lines before and after the pandemic outbreak.\"\r\n  ) +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\n\r\nTo make the picture a little bit clearer, let us take the help of\r\ninferential statistics to answer the question we are interested in. The\r\nideal analytical tool for our use case is a segmented regression\r\nanalysis of interrupted time series. It allows us to model changes in\r\nvarious processes and outcomes that follow intervention while\r\ncontrolling for the baseline level of other types of changes such as\r\ntrends and seasonality. The model used has the following general\r\nstructure:\r\n\\[Y_{t} = Œ≤_{0} + Œ≤_{1}*time_{t} +\r\nŒ≤_{2}*intervention_{t} + Œ≤_{3}*time after intervention_{t} +\r\ne_{t}\\]\r\nThe Œ≤0 coefficient estimates the baseline level\r\nof the outcome variable at time zero; Œ≤1 coefficient\r\nestimates the change in the mean of the outcome variable that occurs\r\nwith each unit of time before the intervention (i.e.¬†the baseline\r\ntrend); Œ≤2 coefficient estimates the level change in\r\nthe mean of the outcome variable immediately after the intervention\r\n(i.e.¬†from the end of the preceding segment); and Œ≤3\r\nestimates the change in the trend in the mean of the outcome variable\r\nper unit of time after the intervention, compared with the trend before\r\nthe intervention (thus, the sum of Œ≤1 and\r\nŒ≤3 equals to the post-intervention slope).\r\nSince we are dealing with correlated and truncated data, we should\r\nalso include two additional terms in our model, an autocorrelation term\r\nand a truncation term, to handle these specific properties of our\r\ndata.\r\nData preparation\r\nBut first, let‚Äôs prepare the data we will need for this type of\r\nanalysis. Specifically, we will need the following five variables:\r\nsearch interest in people analytics ‚Äì numerical\r\nvariable representing search interest in people analytics relative to\r\nthe highest point on the chart for the given region and time; this\r\nvariable is truncated within the interval between values of 0 and 100; a\r\nvalue of 100 is the peak popularity for the term; a value of 50 means\r\nthat the term is half as popular; a score of 0 means that there was not\r\nenough data for this term; search interest for two monitored terms\r\n(‚Äúpeople analytics‚Äù and ‚ÄúHR analytics‚Äù) was combined by simple summation\r\nand then normalized to a range of 0 to 100; this variable serves as a\r\ndependent (criterion) variable;\r\nelapsed time ‚Äì numerical variable representing the\r\nnumber of months that elapsed from the beginning of the time series;\r\nthis variable enables estimation of the size and direction of the\r\noverall trend in the data before the intervention;\r\npandemic ‚Äì dichotomic variable indicating the\r\npresence/absence of pandemic; as already mentioned above, for the\r\npurpose of this analysis, the beginning of the pandemic is assumed to\r\nhave started in March 2020; this variable enables estimation of the\r\nlevel change in the interest in people analytics immediately after the\r\npandemic outbreak;\r\nelapsed time after pandemic outbreak ‚Äì numerical\r\nvariable representing the number of months that elapsed from the\r\nbeginning of pandemic; this variable enables estimation of the change in\r\nthe trend in the interest in people analytics after the outbreak of\r\npandemic;\r\nmonth ‚Äì categorical variable representing specific\r\nmonth within a year; this variable enables controlling for the effect of\r\nseasonality.\r\n\r\n\r\nShow code\r\n\r\n# munging data\r\nmydata <- gtrendsResultDf %>%\r\n  tidyr::pivot_wider(names_from = \"keyword\", values_from = \"hits\") %>%\r\n  dplyr::mutate(\r\n    interestInPeopleAnalytics = `people analytics` + `hr analytics`,\r\n    interestInPeopleAnalytics = normalize(interestInPeopleAnalytics)*100,\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), ordered = FALSE)\r\n  ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(date) %>%\r\n    # creating new variables elapsed time, pandemic, and time elapsed after pandemic outbreak\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= '2020-03-01' ~ 1,\r\n      TRUE ~ 0\r\n    ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic),\r\n    pandemic = as.factor(ifelse(pandemic == 1, \"After the pandemic outbreak\", \"Before the pandemic outbreak\"))\r\n  ) %>%\r\n  # final selection of variables\r\n  dplyr::select(\r\n    date, interestInPeopleAnalytics, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n  )\r\n\r\n\r\n\r\nHere is a table with the resulting data we will use for our\r\nanalysis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making more user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  mydata,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nModeling\r\nWe can now fit the model to the data and test what it tells us about\r\nthe impact of the pandemic on people‚Äôs search interest in people\r\nanalytics. We will use the\r\nbrms r package for this, which allows us to make inferences about\r\nthe model parameters within a Bayesian inferential framework. For this\r\nreason, we must also specify some additional parameters\r\n(e.g.¬†chains, iter or warmup) of\r\nthe\r\nMarkov Chain Monte Carlo (MCMC) algorithm that generates posterior\r\nsamples of our model‚Äôs parameters.\r\nThe Bayesian framework also allows us to specify priors for the\r\nestimated parameters and use them to incorporate our domain knowledge\r\ninto the analysis. The specified priors are important for both parameter\r\nestimation and hypothesis testing because they define our initial\r\ninformation state before we consider our data. Here, we will use\r\nrelatively broad, uninformative, and only slightly regularizing priors\r\n(that is, the inference results will be very close to the results of\r\nstandard, frequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\nlibrary(cmdstanr)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors \r\npriors <- c(brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            brms::set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model\r\nmodel <- brms::brm(\r\n  interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = mydata,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  seed = 1234,\r\n  sample_prior = TRUE, \r\n  control = list(adapt_delta = 0.9),\r\n  backend = \"cmdstanr\",\r\n  refresh = 0,\r\n  silent = 2 \r\n)\r\n\r\n\r\n\r\nBefore making any inferences, we should perform several validation\r\nchecks to ensure that the mechanics of the MCMC algorithm worked well\r\nand that we can use the generated posterior samples to make inferences\r\nabout the parameters of our model. There are many ways to do this, but\r\nhere we will only use a visual check of the MCMC chains. We want the\r\nplots of these chains to look like a hairy caterpillar, indicating the\r\nconvergence of the underlying Markov chain to stationarity and the\r\nconvergence of the Monte Carlo estimates to population quantities,\r\nrespectively. As can be seen in the graph below, we can observe the\r\ncharacteristics we are looking for in the MCMC chains described above.\r\n(For additional MCMC diagnostics procedures, see for example Bayesian\r\nNotes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains\r\nbayesplot::mcmc_trace(\r\n  model,\r\n  facet_args = list(nrow = 6)\r\n) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the model parameters\"\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\n\r\nIt is also important to check how well the model fits the data. To do\r\nthis, we can use the posterior predictive check, which uses a specified\r\nnumber of selected posterior values of the model parameters to show how\r\nwell the fitted model predicts the observed data. In the graph below we\r\nsee that the model fits the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model fit\r\n# specifying the number of samples\r\nndraws = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  ndraws = ndraws\r\n) +\r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive check (using {ndraws} samples)\")\r\n  ) +\r\n  ggplot2::scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\r\n  ggthemes::theme_clean() +\r\n  ggplot2::theme(\r\n    legend.position = \"bottom\",\r\n    legend.background = element_blank()\r\n  )\r\n\r\n\r\n\r\n\r\nWhat do the model and data\r\ntell us?\r\nWe can now use the parameters of our model to obtain information\r\nabout our main question. Specifically, we are interested in the value of\r\nthe coefficient of the pandemic and the time after pandemic terms in our\r\nmodel. They represent how much and in what direction the search interest\r\nin people analytics changed after the pandemic outbreak.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the fitted model\r\nsummary(model)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: interestInPeopleAnalytics | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: mydata (Number of observations: 187) \r\n  Draws: 4 chains, each with iter = 18000; warmup = 0; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.71      0.07     0.58     0.86 1.00    25681    14624\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                           -21.18      8.49   -38.59\r\nelapsedTime                           0.47      0.05     0.40\r\npandemicBeforethepandemicoutbreak    11.33      4.94     2.09\r\nelapsedTimeAfterPandemic              0.61      0.30     0.03\r\nmonthFeb                              3.66      1.33     1.07\r\nmonthMar                              4.07      1.75     0.64\r\nmonthApr                              3.78      1.97    -0.08\r\nmonthMay                              2.45      2.11    -1.67\r\nmonthJun                              1.00      2.18    -3.24\r\nmonthJul                              1.09      2.20    -3.19\r\nmonthAug                             -0.66      2.19    -4.93\r\nmonthSep                              3.30      2.12    -0.85\r\nmonthOct                             -0.10      1.99    -3.99\r\nmonthNov                              0.09      1.76    -3.34\r\nmonthDec                             -4.69      1.35    -7.34\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            -8.07 1.00    22536    14829\r\nelapsedTime                           0.57 1.00    27594    14127\r\npandemicBeforethepandemicoutbreak    21.57 1.00    44451    44015\r\nelapsedTimeAfterPandemic              1.21 1.00    45784    29992\r\nmonthFeb                              6.27 1.00    38336    47232\r\nmonthMar                              7.49 1.00    27081    42390\r\nmonthApr                              7.67 1.00    23960    39632\r\nmonthMay                              6.61 1.00    22458    36841\r\nmonthJun                              5.27 1.00    21932    37414\r\nmonthJul                              5.41 1.00    21917    38879\r\nmonthAug                              3.63 1.00    22337    38146\r\nmonthSep                              7.46 1.00    22960    39406\r\nmonthOct                              3.79 1.00    24081    41304\r\nmonthNov                              3.54 1.00    28473    43492\r\nmonthDec                             -2.05 1.00    41154    51042\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     4.77      0.28     4.27     5.36 1.00    54179    42109\r\n\r\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nThe following graph shows the posterior distribution of the pandemic\r\nparameter. We can see that it is on the right-hand side of the zero\r\nvalue, which supports the claim that there is an effect of the pandemic\r\non people‚Äôs interest in people analytics immediately after the pandemic\r\noutbreak; however, it is on the opposite side of the zero value than we\r\nwould expect if the pandemic were to have a positive effect on people‚Äôs\r\ninterest in people analytics. Thus, this suggests that immediately after\r\nthe pandemic outbreak, interest in people analysis declined slightly\r\n(somewhere between ~1 and ~20 points, as indicated by the 95% credible\r\ninterval).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(tidybayes)\r\n\r\nparamVizBeforethepandemicoutbreak <- model %>%\r\n  tidybayes::gather_draws(b_pandemicBeforethepandemicoutbreak) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizBeforethepandemicoutbreak$value)\r\n\r\nparamVizBeforethepandemicoutbreak <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_pandemicBeforethepandemicoutbreak parameter \r\nggplot2::ggplot(paramVizBeforethepandemicoutbreak, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizBeforethepandemicoutbreak, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\",\r\n    y = \"Density\",\r\n    x = \"pandemicBeforethepandemicoutbreak\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for working with output from Bayesian models\r\nlibrary(posterior)\r\n\r\n# extracting posterior samples\r\nsamplesBeforethepandemicoutbreak <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being higher than 0\r\nsum(samplesBeforethepandemicoutbreak$b_pandemicBeforethepandemicoutbreak > 0) / nrow(samplesBeforethepandemicoutbreak)\r\n\r\n\r\n[1] 0.9925833\r\n\r\nNow let‚Äôs check the second key parameter of our model, the time after\r\nthe pandemic term. In this case, the posterior distribution is again on\r\nthe right-hand side of zero value, but now this result is consistent\r\nwith the claim that there is a positive effect of the pandemic on\r\npeople‚Äôs interest in people analytics, specifically in terms of the\r\nchange in slope after the pandemic. Compared to the pre-pandemic trend,\r\nthe post-pandemic trend is steeper by ~0 to ~1 point per month (as\r\nindicated by the 95% confidence interval). We should bear in mind,\r\nhowever, that this effect may in fact only be an artifact caused by the\r\nimproved data collection system from January 2022, as mentioned at the\r\nvery beginning of this post.\r\n\r\n\r\nShow code\r\n\r\nparamVizElapsedTimeAfterPandemic <- model %>%\r\n  tidybayes::gather_draws(b_elapsedTimeAfterPandemic) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramVizElapsedTimeAfterPandemic$value)\r\n\r\nparamVizElapsedTimeAfterPandemic <- tibble::tibble(x = dens$x, y = dens$y)\r\n\r\n# visualizing the posterior distribution of the model's b_elapsedTimeAfterPandemic parameter \r\nggplot2::ggplot(paramVizElapsedTimeAfterPandemic, aes(x,y)) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x > 0), fill = \"#fca636\"\r\n  ) +\r\n  ggplot2::geom_area(data = filter(paramVizElapsedTimeAfterPandemic, x <= 0), fill = \"grey\") +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the elapsedTimeAfterPandemic parameter\",\r\n    y = \"Density\",\r\n    x = \"elapsedTimeAfterPandemic\"\r\n    ) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamplesElapsedTimeAfterPandemic <- posterior::as_draws_df(model, seed = 1234)\r\n\r\n# probability of b_elapsedTimeAfterPandemic coefficient being higher than 0\r\nsum(samplesElapsedTimeAfterPandemic$b_elapsedTimeAfterPandemic > 0) / nrow(samplesElapsedTimeAfterPandemic)\r\n\r\n\r\n[1] 0.979625\r\n\r\nThe overall resulting picture thus partially supports the impression\r\nof many of my people analytics fellows about the growing importance of\r\npeople analytics in HR and business leaders‚Äô decision making. However,\r\ngiven that the Google search interest in people analytics is a fairly\r\ndistant proxy for its actual use in HR and business practice, we should\r\ntake these results with a grain of salt and try to find other data\r\nsources that would support our results. For example, Frank Corrigan, head of\r\nanalytics at Ponder, came up with the idea of analyzing changes in\r\npostings for people analytics job positions over time. A good\r\ninspiration for anyone willing to spend some time getting at and\r\nanalyzing such data.\r\nIn addition to the main question addressed here, we may be interested\r\nin other insights provided by the model. To this end, we can plot the\r\nmarginal effect of each predictor. Beyond what we already know, we can\r\nsee that search interest in people analytics has clearly trended upwards\r\nover the last 15 years, and within each year, search interest follows\r\nthe seasonality of work and holidays, i.e., search interest is lower\r\nduring the holiday season - June, July, August and December - and higher\r\nthroughout the rest of the year.\r\n\r\n\r\nShow code\r\n\r\n# plotting marginal effect of predictors used \r\nfigList <- plot(brms::marginal_effects(model, probs = c(0.025, 0.975)), theme = ggthemes::theme_clean(), ask = FALSE) \r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for arranging multiple ggplots\r\nlibrary(ggpubr)\r\n\r\n# putting all graphs with marginal effects together  \r\nggpubr::ggarrange(\r\n  plotlist = list(figList[[1]] + geom_line(color = \"#0d1687\", size = 1), figList[[4]] + geom_point(color = \"#0d1687\", size = 3), figList[[2]] + geom_point(color = \"#0d1687\", size = 3), figList[[3]] + geom_line(color = \"#0d1687\", size = 1)), \r\n  nrow = 4\r\n  ) %>%\r\n  ggpubr::annotate_figure(\r\n    top = text_grob(\"Marginal effect of predictors used\"),\r\n    bottom = text_grob(\"Point estimates are shown along with 95% credible intervals.\", hjust = 1, x = 1, face = \"italic\", size = 10)\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-popularity-after-covid/./people-analytics.jpg",
    "last_modified": "2023-02-08T17:37:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/",
    "title": "People Analytics Challenge from Orgnostic: Plan for high growth",
    "description": "A brief summary of my participation in Orgnostic's People Analytics Challenge.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-08-19",
    "categories": [
      "recruitment channels",
      "workforce planning",
      "descriptive statistics",
      "r"
    ],
    "contents": "\r\nDuring the last few nights, I had the opportunity to participate in\r\nan interesting People Analytics Challenge from Orgnostic, a company providing people\r\nanalytics platform that links scattered HR and finance data, run surveys\r\non top, analyses the data, and get answers to the critical questions\r\nabout organizations and their employees.\r\nOut of three possible challenges, I chose one that was quite far from\r\nwhat I‚Äôm currently usually working on or what I‚Äôve worked on in the\r\npast. The chosen task was to analyze the effectiveness of\r\nrecruiting sources for a company that plans to double in size\r\nfrom its current 750+ employees.\r\nAlthough the dummy data provided was quite limited for obvious\r\nreasons and did not allow to answer all relevant questions (but you\r\ncould also use your own data which would not suffer from this\r\nshortcoming), after combining them and enriching them slightly based on\r\nrealistic assumptions, it was possible to arrive at quite interesting\r\ninsights and recommendations. See for yourself - the resulting\r\npresentation is attached to this post below.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it:\r\nDownload\r\nPDF.\r\n\r\n\r\n\r\nIf you‚Äôd like to look more into the guts of the analyses conducted,\r\nyou can find both the data provided by Orgnostic and the R script used\r\nto analyze it on my\r\nGitHub page.\r\nP.S. Oh, I almost forgot - there are exciting prizes in the form of\r\ntickets to HRtechX Copenhagen or HR Technology Conference in Las Vegas.\r\nSo please keep your fingers crossed for me ü§ûüòâ\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-20-people-analytics-challenge-from-orgnostic/./recruitmentChannels.png",
    "last_modified": "2022-08-29T18:41:35+02:00",
    "input_file": {},
    "preview_width": 864,
    "preview_height": 515
  },
  {
    "path": "posts/2022-06-11-visual-inference-statistics/",
    "title": "Visual statistical inference",
    "description": "Visual statistical inference represents a valid alternative to standard statistical inference, and as a by-product it also helps with building intuition about the difference between signal and noise. Give it a try.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-06-13",
    "categories": [
      "statistical inference",
      "visual statistical inference",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\nDo you have any experience with visual\r\nstatistical inference? Not only is it a valid alternative\r\nto standard statistical inference, but as a by-product it helps with\r\nbuilding intuition about the\r\ndifference between signal and noise.\r\nThe basis of the method is a so-called lineup\r\nprotocol that places a graph of the actual data between arrays\r\nof graphs of null data that are generated by a method consistent with\r\nthe null hypothesis. The lineup is shown to one or more observers who\r\nare asked to identify the graph that differs. If an observer can pick\r\nout the actual data as different from the others, this gives weight to\r\nthe statistical significance of the pattern in the graph.\r\nBecause people usually have a hard time recognizing randomness and\r\ntend to see patterns even where there are none, there is also a\r\nso-called Rorschach protocol that only displays graphs\r\ncreated with null datasets and which is used by observers to calibrate\r\ntheir eyes for variation due to sampling.\r\nYou can try it for yourself in the graphs below, which show the\r\nrelationship between two variables from my current area of work\r\n(collaboration\r\nanalytics), namely between time\r\navailable for focused work and the use of timeboxing\r\n(productivity enhancing technique of assigning a fixed unit of time to\r\nan activity within which a planned activity takes place).\r\nUse the first array of graphs (the Rorschach protocol) to calibrate\r\nyour eye for randomness and then try to identify the actual data in the\r\nsecond array of graphs (the lineup protocol). What‚Äôs your guess? Which\r\ngraph matches the actual data (1-20) and what relationship do you see in\r\nthe data? You can give your guess in the comments.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(nullabor)\r\nlibrary(patchwork)\r\nlibrary(scales)\r\n\r\n# uploading data\r\nmydata <- readRDS(\"./visualInferenceData.RDS\")\r\n\r\n# lineup protocol\r\nset.seed(1234)\r\nd <- lineup(null_permute(\"propBlockedTime\"), mydata)\r\n\r\nlplot <- ggplot(data=d, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"LINEUP PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n\r\n# Rorschach protocol\r\ndr <- rorschach(null_permute(\"propBlockedTime\"), mydata, n = 20, p = 0)\r\n\r\nrplot <- ggplot(data=dr, aes(x = focusRate, y = propBlockedTime)) + \r\n  geom_point(alpha = 0.3, size = 1) + \r\n  facet_wrap(~ .sample) +\r\n  labs(\r\n    title = \"RORSCHACH PROTOCOL\",\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels = scales::percent_format())\r\n\r\n# combining plots\r\nfplot <- rplot / lplot\r\n\r\nprint(fplot)\r\n\r\n\r\n\r\n\r\nHere‚Äôs a check on your guess. The actual data are shown in chart #\r\n12. As you can see in the chart below, the relationship between time for\r\nfocused work and the use of timeboxing is slightly negative, which makes\r\npretty good sense, because people who have enough time for focused work\r\nusually don‚Äôt have such a strong need to block out time for focused work\r\nin their calendar.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot(aes(x = focusRate, y = propBlockedTime)) +\r\n  geom_point(size = 2, alpha = 0.5) +\r\n  geom_smooth(method = \"lm\", se = F) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format()) +\r\n  labs(\r\n    x = \"FOCUS RATE\",\r\n    y = \"PROPORTION OF BLOCKED WORKING TIME\",\r\n    title = \"RELATIONSHIP BETWEEN FOCUSED TIME AND TIMEBOXING USAGE\",\r\n    caption = \"\\nThe blue line in the graph represents the linear regression line.\"\r\n  )\r\n\r\n\r\n\r\n\r\nWe can also test the relationship between time for focused work and\r\nthe use of timeboxing more formally by fitting a Bayesian\r\nbeta regression model to the data. As you can see in the summary\r\ntables and charts below, the null value is safely outside the 95%\r\ncredible interval of the mean of the focus rate parameter, and the\r\nmarginal effect of focus rate clearly shows its negative relationship\r\nwith the predicted proportion of working time blocked in the calendar.\r\nNote also that the relationship is non-linear, i.e.¬†the marginal effect\r\nof the focus rate is different depending on its level.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(brms)\r\nlibrary(bayesplot)\r\nlibrary(tidybayes) \r\nlibrary(ggdist)       \r\n\r\n# fitting Bayesian beta regression model\r\nmodel <- brms::brm(\r\n  bf(\r\n    propBlockedTime ~ focusRate,\r\n    phi ~ focusRate\r\n    ),\r\n  data=mydata,\r\n  family= Beta(),\r\n  seed = 1234,\r\n  iter = 20000,\r\n  warmup = 2000,\r\n  chains = 4,\r\n  cores = 6,\r\n  control = list(\r\n    adapt_delta = 0.9,\r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# summary of the fitted model\r\nsummary(model)\r\n\r\n\r\n Family: beta \r\n  Links: mu = logit; phi = log \r\nFormula: propBlockedTime ~ focusRate \r\n         phi ~ focusRate\r\n   Data: mydata (Number of observations: 306) \r\n  Draws: 4 chains, each with iter = 20000; warmup = 2000; thin = 1;\r\n         total post-warmup draws = 72000\r\n\r\nPopulation-Level Effects: \r\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept        -0.85      0.25    -1.33    -0.35 1.00    61653\r\nphi_Intercept     2.09      0.39     1.30     2.84 1.00    59323\r\nfocusRate        -0.02      0.00    -0.03    -0.02 1.00    57209\r\nphi_focusRate     0.01      0.01    -0.01     0.02 1.00    56423\r\n              Tail_ESS\r\nIntercept        54084\r\nphi_Intercept    51459\r\nfocusRate        55167\r\nphi_focusRate    51573\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the estimated parameters\r\nposterior_beta <- model %>% \r\n  gather_draws(`b_.*`, regex = TRUE) %>% \r\n  mutate(component = ifelse(str_detect(.variable, \"phi_\"), \"Precision\", \"Mean\"),\r\n         intercept = str_detect(.variable, \"Intercept\")) %>%\r\n  filter(intercept == FALSE)\r\n\r\nggplot(posterior_beta, aes(x = .value, y = fct_rev(.variable), fill = component)) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  stat_halfeye(aes(slab_alpha = intercept), \r\n               .width = c(0.8, 0.95), point_interval = \"median_hdi\") +\r\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\r\n  scale_slab_alpha_discrete(range = c(1, 0.4)) +\r\n  guides(fill = \"none\", slab_alpha = \"none\") +\r\n  labs(\r\n    x = \"COEFFICIENT\", \r\n    y = \"\",\r\n    title = \"POSTERIOR DISTRIBUTION OF THE ESTIMATED PARAMETERS\",\r\n    caption = \"\\n80% and 95% credible intervals shown in black\") +\r\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing marginal effect of the focus rate\r\nmodel_pred <- model %>% \r\n  epred_draws(newdata = expand_grid(focusRate = seq(30, 100, by = 1)))\r\n\r\nggplot(model_pred , aes(x = focusRate, y = .epred)) +\r\n  stat_lineribbon() + \r\n  scale_fill_brewer(palette = \"Purples\") +\r\n  labs(x = \"FOCUS RATE\", \r\n       y = \"PREDICTED PROPORTION OF BLOCKED WORKING TIME\",\r\n       fill = \"Credible interval\",\r\n       title = \"MARGINAL EFFECT OF THE FOCUS RATE\"\r\n       ) +\r\n  scale_x_continuous(limits = c(NA,NA), labels = scales::number_format(suffix = \"%\")) +\r\n  scale_y_continuous(labels =  scales::percent_format(accuracy = 1)) +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nIf you would like to apply the visual statistical inference approach\r\nto your own data, you can easily do so using the nullabor R\r\npackage.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-11-visual-inference-statistics/./lineup.jpg",
    "last_modified": "2022-06-19T21:18:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-06-standard-and-trend-predictors/",
    "title": "Standard vs. trend predictors",
    "description": "When modeling a phenomenon, one usually can't get by with just raw data but must use one's domain knowledge to select and transform the most relevant variables from raw data to be able to successfully grasp regularities in the domain of one's interest. Let's look at one simple example of such feature engineering from the domain of collaboration analytics.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-02-06",
    "categories": [
      "predictive analytics",
      "feature engineering",
      "employee experience",
      "employee engagement",
      "employee satisfaction",
      "employee attrition",
      "collaboration data"
    ],
    "contents": "\r\nAs predictive analytics practitioners know, trend variables can be more useful in many situations for predicting certain phenomena than standard variables that simply refer to the state of the world at a particular time point or period.\r\nFor example, when trying to predict employee attrition, a downward trend in the use of a piece of company equipment, such as a printer/copier, over the 6 months prior to the resignation may be more predictive than the absolute number of pages printed/copied over the same period.\r\nThis is also true for our domain we focus on at Time is Ltd. where, among other things, we try to use collaboration data to infer some aspects of employee experience.\r\nTo illustrate, the attached chart shows the distribution of the typical daily amount of time people spend by collaboration for two groups of employees - one with above-average scores and the other with below-average scores on the employee satisfaction survey. As you can see, there is little difference between the two groups in terms of the average daily amount of time people spend by collaboration over the last six months (see the density plots), but there is a fairly clear difference in the trend of this metric over the same period, suggesting that less satisfied employees may be suffering from increasing collaboration overload (see the line charts with trend lines for individual employees and the estimated overall linear trend).\r\n\r\n\r\n\r\nDo you have a similar experience with or just a strong hunch about other metrics in your area of expertise? Let me know in the comments.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-06-standard-and-trend-predictors/./trendGraph.jpg",
    "last_modified": "2022-02-09T09:23:54+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-30-meeting-planning/",
    "title": "Fighting meeting overload",
    "description": "One of the most effective ways to fight meeting overload is to better plan meetings in terms of the time we spend in them. Let's look at how data can tell us how much room for improvement we have in this area.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-30",
    "categories": [
      "meeting planning",
      "meeting effectiveness"
    ],
    "contents": "\r\nOne of the recommended ways to save time in meetings is to plan them better in terms of the time we allocate for them. As in other activities, even here the well-known Parkinson‚Äôs rule applies that ‚Äúwork expands so as to fill the time available for its completion.‚Äù When this is combined with the automatic use of default meeting lengths, it leads to spending more time in meetings than is necessary.\r\nFor this reason, Steven Rogelberg suggests in his book The Surprising Science of Meetings that all meeting times should be reduced by 5-10 percent by default.\r\nTo assess whether you have room for improvement in this regard, it is useful to compare actual and planned meeting lengths. For illustration, the attached chart shows the distribution of the typical differences between actual and planned meeting lengths for each of our teams organizing online meetings over the course of a year. It clearly shows that a large proportion of teams are organizing meetings longer than necessary, by an average of 4 minutes. So in the case of our company Time is Ltd., there definitely seems to be room for implementing Steven Rogelberg‚Äôs suggestion.\r\n\r\n\r\nShow code\r\n\r\n# uploading package\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndata <- readRDS(\"./tardiness.rds\")\r\n\r\n# preparing data for density plot\r\nmydata <- with(density(data %>% pull(tardiness)), data.frame(x, y)) %>%\r\n  mutate(col = ifelse(x >= 0, \"A\", \"B\"))\r\n\r\n# visualizing data\r\nmydata %>%\r\n  ggplot() +\r\n  geom_rug(data = filter(data, tardiness >=0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 1, position = \"identity\") +\r\n  geom_rug(data = filter(data, tardiness <0), aes(x = tardiness), color = \"#4d009d\", size = 0.55, alpha = 0.5, position = \"identity\") +\r\n  \r\n  geom_area(data = filter(mydata, col == 'A'), aes(x = x, y = y), fill = '#4d009d', alpha = 1) + \r\n  geom_area(data = filter(mydata, col == 'B'), aes(x = x, y = y),  fill = '#4d009d', alpha = 0.5) +\r\n  \r\n  geom_label(aes( x=-15.25, y=0.06, label=\" Shorter than planned \"), fill = \"#a67fce\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n  geom_label(aes( x=10.5, y=0.04, label=\" Longer than planned \"), fill = \"#4d009d\", color=\"white\", size=4.5 , fontface=\"bold\",  family = \"Nunito Sans\",  label.padding = unit(0.5, \"lines\")) +\r\n\r\n  labs(\r\n    x = \"TYPICAL DIFFERENCE BETWEEN ACTUAL AND PLANNED LENGTHS OF MEETINGS\",\r\n    y = \"DENSITY\",\r\n    title = \"Do our online meetings end on time?\",\r\n    subtitle = str_glue(\"On average, our teams organize online meetings {round(abs(mean(data$tardiness)),1)} minutes longer than necessary.\"),\r\n    caption = \"\\nPositive values indicate that online meetings tend to overrun; negative values indicate that online meetings are planned longer than they need to be.\"\r\n  ) +\r\n  scale_x_continuous(labels = scales::label_number(suffix = \" min\"), breaks =  seq(-40, 20, 10)) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,12,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        axis.title.y.right = element_text(margin = margin(t = 0, r = 0, b = 0, l = 15), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        legend.position=c(.2,.98),\r\n        legend.key = element_rect(fill = \"white\"),\r\n        legend.key.size = unit(0, \"cm\"),\r\n        legend.margin = margin(-0.8,0,0,0, unit=\"cm\"),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10, lineheight = 16),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  )\r\n\r\n\r\n\r\n\r\nIt is also worth noting the reverse situation where meetings take longer than planned, as a late end to one meeting becomes a late start to the next meeting.\r\nHow do you feel about finishing meetings too early or too late? Are both similarly unpleasant for you? And isn‚Äôt actually having a shorter meeting than planned something positive?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-30-meeting-planning/./meetingPlanning.jpg",
    "last_modified": "2022-01-30T18:27:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-18-probability-words/",
    "title": "How do we perceive probability words?",
    "description": "Have you ever wondered exactly how much chance of success people give a project when they say they believe in it? If so, then you may find this post useful, as it attempts to answer that question at least in part with data.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-18",
    "categories": [
      "probability",
      "perception"
    ],
    "contents": "\r\nNowadays - probably also due to the Covid pandemic and the associated predictions - we are more and more frequently encountering various probabilistic statements, but these are often expressed not in terms of precise numerical probabilities, but in terms of relatively vague probability words such as ‚Äúprobably‚Äù, ‚Äúmaybe‚Äù, ‚Äúunlikely‚Äù, etc.\r\nSince people may imagine different probabilities under these words, it would be useful to have something like a glossary to help us decipher these words and indicate what people usually mean when they use them.\r\nFortunately, there are some studies that examine what numerical probabilities people typically associate with probability words.\r\nFor this purpose, I used a collection of 123 responses to the Wade Fagen-Ulmschneider‚Äôs internet survey and created two similar graphs based on them. The first shows the distribution of the numerical probabilities that people associate with each word, and these are sorted in the graph by the median value of the corresponding probability in descending order. The second graph then differs only in that the words are sorted by the size of the interquartile range in descending order.\r\n\r\n\r\nShow code\r\n\r\n# uploading libraries\r\nlibrary(tidyverse)\r\nlibrary(ggridges)\r\nlibrary(ggpubr)\r\n\r\n# uploading data\r\ndata <- readr::read_csv(\"./survey-results.csv\")\r\n\r\n# getting ordered list of words based on the median value of corresponding probabilities\r\nwordsMedian <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(median = median(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, median)\r\n      )\r\n  \r\nlevelsMedian <- levels(wordsMedian$word)\r\n\r\n# getting ordered list of words based on the IQR of corresponding probabilities\r\nwordsVariability <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  group_by(word) %>%\r\n  summarise(sd = IQR(probability)) %>%\r\n  mutate(\r\n    word = factor(word),\r\n    word = forcats::fct_reorder(word, sd)\r\n  )\r\n\r\nlevelsVariability <- levels(wordsVariability$word)\r\n\r\n# graph 1\r\ng1 <- data %>%\r\n  select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n  pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n  mutate(word = factor(word, levels = levelsMedian, ordered = TRUE)) %>%\r\n  ggplot(aes(x = probability, y = word)) + \r\n  geom_density_ridges(\r\n    fill = \"#4d009d\",\r\n    alpha = 0.85,\r\n    scale = 1,\r\n    jittered_points = TRUE,\r\n    position = position_points_jitter(width = 1, height = 0),\r\n    point_shape = '|', point_size = 1, point_alpha = 1, \r\n    quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n    quantile_fun=function(x,...)median(x)\r\n  ) +\r\n  scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n  labs(\r\n    fill = \"Trend size\",\r\n    x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n    y = \"\",\r\n    title = \"How do people perceive probability words?\",\r\n    caption = \"\\nThe words are sorted by the median value of the corresponding probability in descending order.\\nThe white dashed lines represent the median values.\\nSource: A collection of 123 responses to an internet survey by Wade Fagen-Ulmschneider.\"\r\n  ) +\r\n  scale_fill_gradient2(\r\n    low = \"red\",\r\n    mid = \"white\",\r\n    high = \"blue\",\r\n    midpoint = 0,\r\n    space = \"Lab\"\r\n  ) +\r\n  theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n        plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n        plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n        axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n        axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n        legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n        legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n        axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n        axis.text.x = element_text(),\r\n        legend.position = \"right\",\r\n        axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n        axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n        panel.background = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n        axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n        plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position =  \"plot\"\r\n  ) +\r\n  guides(\r\n    fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n  )\r\n\r\n\r\n# graph 2\r\ng2 <- data %>%\r\n    select(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\") %>%\r\n    pivot_longer(\"\\\"Almost Certain\\\"\":\"\\\"Chances are Slight\\\"\", names_to = \"word\", values_to = \"probability\") %>%\r\n    mutate(word = factor(word, levels = levelsVariability, ordered = TRUE)) %>%\r\n    ggplot(aes(x = probability, y = word)) + \r\n    geom_density_ridges(\r\n      fill = \"#4d009d\",\r\n      alpha = 0.85,\r\n      scale = 1,\r\n      jittered_points = TRUE,\r\n      position = position_points_jitter(width = 1, height = 0),\r\n      point_shape = '|', point_size = 1, point_alpha = 1, \r\n      quantile_lines =TRUE, vline_linetype = \"dashed\", vline_color = \"white\", vline_size = 0.55,\r\n      quantile_fun=function(x,...)median(x)\r\n    ) +\r\n    scale_x_continuous(limits = c(0, 100), breaks = seq(0,100,10), labels = scales::number_format(suffix = \"%\",accuracy = 1)) +\r\n    labs(\r\n      fill = \"Trend size\",\r\n      x = \"PROBABILITIES ASSIGNED TO WORDS\",\r\n      y = \"\",\r\n      title = \"What probability words are the most noisy?\",\r\n      caption = \"\\nThe words are sorted by the size of the interquartile range in descending order.\\n\\n\"\r\n    ) +\r\n    scale_fill_gradient2(\r\n      low = \"red\",\r\n      mid = \"white\",\r\n      high = \"blue\",\r\n      midpoint = 0,\r\n      space = \"Lab\"\r\n    ) +\r\n    theme(plot.title = element_text(color = '#2C2F46', face = \"bold\", family = \"URW Geometric\", size = 20, margin=margin(0,0,16,0)),\r\n          plot.subtitle = element_text(color = '#2C2F46', face = \"plain\", family = \"URW Geometric\", size = 16, margin=margin(0,0,20,0)),\r\n          plot.caption = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 11, hjust = 0),\r\n          axis.title.x.bottom = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 0),\r\n          axis.title.y.left = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0), color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 13, lineheight = 16, hjust = 1),\r\n          legend.title = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12),\r\n          legend.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 10),\r\n          axis.text = element_text(color = '#2C2F46', face = \"plain\", family = \"Nunito Sans\", size = 12, lineheight = 16),\r\n          axis.text.x = element_text(),\r\n          legend.position = \"right\",\r\n          axis.line.x = element_line(colour = \"#E0E1E6\"),\r\n          axis.line.y = element_line(colour = \"#E0E1E6\"),\r\n          panel.background = element_blank(),\r\n          panel.grid.major.y = element_blank(),\r\n          panel.grid.major.x = element_blank(),\r\n          panel.grid.minor = element_blank(),\r\n          axis.ticks.x = element_line(color = \"#E0E1E6\"),\r\n          axis.ticks.y = element_line(color = \"#E0E1E6\"),\r\n          plot.margin=unit(c(5,5,5,5),\"mm\"), \r\n          plot.title.position = \"plot\",\r\n          plot.caption.position =  \"plot\"\r\n    ) +\r\n    guides(\r\n      fill = guide_colourbar(barwidth = 0.75, barheight = 10)\r\n    )\r\n\r\n# combining graphs\r\nggarrange(g1, g2, ncol = 2, nrow = 1)\r\n\r\n\r\n\r\n\r\nThe first graph can thus help us to use the right word, which in the mind of the other person is most likely to evoke the same probability we want to express. The second graph can then help us to identify the most noisy probability words, for which we will know to ask for a more precise definition because we will be aware that people may imagine very different probabilities under these words.\r\nHow about your perception of probability words? Is there anything in the graphs that surprised you? Would you expect differences between cultures? And what about other demographics? Btw, the original dataset also includes some demographic variables such as age, gender, and education level, so I‚Äôll probably come back to this question in a future post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-18-probability-words/./Probability-Word-Cards.jpg",
    "last_modified": "2022-01-30T17:55:43+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/",
    "title": "Hofstede's theory of cultural dimensions",
    "description": "Cultural diversity brings both positive effects and some challenges. To deal with the latter, it is useful to have some kind of map to help people better navigate the cultural specificities of people from different societies. Hofstede's theory of cultural dimensions is useful for such a purpose. Let's check how dis/similar countries are on these cultural dimensions with a simple app that could help us better understand, manage and appreciate cultural differences a little better.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2022-01-03",
    "categories": [
      "cultural diversity",
      "cultural awareness",
      "international management",
      "crosscultural communication",
      "shiny app"
    ],
    "contents": "\r\nOne of the advantages of switch to remote working is that companies can expand the pool of talent from which they choose their employees, without being too constrained by country or even continental boundaries. However, the resulting cultural diversity can bring not only positive effects (e.g.¬†a broader set of perspectives, a more diverse skill base, local market knowledge and insight, better creativity and innovation, etc.) but also some challenges (e.g.¬†risk of prejudice or negative cultural stereotypes, misinterpretation of communication, conflicting work styles, different understanding of professional etiquette, etc.).\r\nBetter knowledge and awareness of the cultural specificities of the societies from which people come is one way of dealing with these challenges. In this respect, Hofstede‚Äôs theory of cultural dimensions may be useful to us. Just as Big-5 theory facilitates our understanding of other people‚Äôs personalities, Hofstede‚Äôs theory facilitates our understanding of their cultural background by describing their social values and releated behaviors through the following six cultural dimensions:\r\n\r\nImage source: https://corporatefinanceinstitute.com/resources/knowledge/other/hofstedes-cultural-dimensions-theory/\r\nYou can easily check how countries are doing on these six dimensions on the Hofstede Insights website. To make it easier to compare cultural differences/similarities between countries, I built a simple app that projects the cultural profiles of countries into 2D space using dimensionality reduction technique called UMAP (Uniform manifold approximation and projection). By selecting a specific cultural dimension, you can see how it is distributed across countries and continents. In addition, you can select some specific countries in the comparator and compare them across all six cultural dimensions.\r\nCheck it out here ‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/Hofstede_Cultural_Dimensions/\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-03-hofstede-theory-of-cultural-dimensions/./six-dimensions-hofstedes-cultural-dimensions-theory.jpg",
    "last_modified": "2023-04-11T20:08:25+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-19-makers-and-managers-schedule/",
    "title": "Makers' schedule and managers' schedule in collaboration data",
    "description": "Many of us have probably already heard of Paul Graham's two types of schedules - one that meets the needs of makers and one that meets the needs of managers. But can these two types of schedules be found in any real collaborative data? Let's find out.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-19",
    "categories": [
      "schedule types",
      "makers and managers",
      "collaboration data"
    ],
    "contents": "\r\nI am sure that many of you have heard of the two types of schedules\r\nas described by Paul Graham in his famous article Maker‚Äôs Schedule,\r\nManager‚Äôs Schedule:\r\nThe manager‚Äôs schedule [is] embodied in the traditional\r\nappointment book, with each day cut into one hour intervals. [By]\r\ndefault you change what you‚Äôre doing every hour. But [makers] generally\r\nprefer to use time in units of half a day at least. You can‚Äôt write or\r\nprogram well in units of an hour. That‚Äôs barely enough time to get\r\nstarted. When you‚Äôre operating on the maker‚Äôs schedule, meetings are a\r\ndisaster. A single meeting can blow a whole afternoon, by breaking it\r\ninto two pieces each too small to do anything hard in. Plus you have to\r\nremember to go to the meeting.\r\nI recently realized that I have only seen illustrative pictures on\r\nthis topic so far, but not any real data. This inspired me to look at\r\nour own collaboration data at Time\r\nIs Ltd. and see if these two schedule categories can be found\r\nthere.\r\nWhen I contrasted the data on the average number of meetings per day\r\nand the average time between meetings, there were indeed categories of\r\npeople who either have relatively more meetings with relatively shorter\r\nbreaks (managers), or have relatively fewer meetings with\r\nrelatively longer breaks (makers).\r\n\r\nBut beyond that, there was a third type, which I called\r\nbatchers - they have relatively fewer meetings with relatively\r\nshorter breaks, which is a good strategy when you have to be both\r\nmanager and creator, which may be the case for more and more people as\r\nwe move to remote working.\r\nIn the charts below you can see how typical monthly calendars of\r\nthese three types of schedulers look like.\r\n\r\n\r\nWhat we cannot see in our own data, but could theoretically be there,\r\nis a fourth category I call overtimers, who have relatively\r\nmore meetings but manage to keep relatively longer breaks in between.\r\nHowever, this can only be achieved by making the meetings more spread\r\nout over time, i.e.¬†at the cost of working after hours.\r\nHow about you? Where would you fit in? And is there anyone among you\r\nwho would fit into the fourth, missing category?\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-19-makers-and-managers-schedule/./maker-schedule-vs-manager-schedule.jpg",
    "last_modified": "2022-06-19T22:21:49+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-16-linkedin-connections-analysis/",
    "title": "R Shiny app for LinkedIn connections analysis",
    "description": "An introduction of a simple R Shiny application for analysing LinkedIn connections.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-16",
    "categories": [
      "linkedin",
      "external networks",
      "social network analysis",
      "shiny app"
    ],
    "contents": "\r\nIf you like to use the end of the year as an opportunity for deeper self-reflection, you might enjoy the following simple app I have put together over the past weekend.\r\n‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/linkedIn_connections_analysis/\r\nOnce you upload your LinkedIn connections data to the app (you can easily download the data by following the instructions in the app or in this video), it automatically generates basic descriptive statistics about your LinkedIn connections:\r\nCumulative number of connections over time\r\nNumber of established connections by years, months, and days of the week\r\nTop N companies by the number of established connections\r\nTop N positions by their frequency among your connections (based on whole position titles, bigrams and single words)\r\nProportion of connections by their gender (based on your connections‚Äô first name)\r\n\r\nUnfortunately, since there is no information about your connections‚Äô connections in the data, the app cannot perform more advanced SNA-type of analyses on it. Still, I think you may find some of the statistics useful, or at least interesting and entertaining.\r\nYou can take it as a kind of Christmas gift for my fellow LinkedIn users. Enjoy exploring your connections! And if you‚Äôd like to explore and better manage also your company‚Äôs internal collaboration networks, then check out what we do at Time is Ltd.\r\nP.S. The data you upload is not permanently stored anywhere. The app runs on the shinyapps.io server. If you don‚Äôt want to upload your own data, but would still like to see what the analysis output looks like, you can download and then upload ready-made sample data from the app.\r\nP.P.S. Big thanks to Sebastian Vorac for bringing me to this idea and for UX review. Any remaining errors are, of course, mine alone.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-16-linkedin-connections-analysis/./linkedinLogo.png",
    "last_modified": "2023-04-11T20:08:53+02:00",
    "input_file": {},
    "preview_width": 3753,
    "preview_height": 2352
  },
  {
    "path": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/",
    "title": "Overview of predictors of voluntary employee turnover",
    "description": "An introduction of a simple R Shiny application to facilitate extraction and digestion of information from meta-analysis of predictors of voluntary employee turnover.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-12-12",
    "categories": [
      "great resignation",
      "employee turnover",
      "turnover predictors",
      "meta-analysis",
      "shiny app"
    ],
    "contents": "\r\nAlthough the ‚ÄòGreat Resignation‚Äô in some parts of the world may be due in no small part to factors specific to the COVID-19 pandemic, it is still useful in this context to draw on the extensive research on employee turnover carried out in the run-up to the pandemic.\r\nA useful overview of such findings is provided, for example, by a 2017 meta-analysis by Rubenstein et al.¬†that summarizes the significance of 57 predictors of voluntary turnover from 9 different domains based on 316 studies from 1975 to 2016 involving more than 300,000 people.\r\nTo make it easier to assimilate these findings, I extracted them from the original article and visualized them in a simple shiny app that helps one to quickly explore and grasp the estimated magnitude, direction, and reliability of the effect of each factor, along with information on the degree of their actionability. The last feature is based purely on my own judgement, so please take it with a grain of salt, or adjust it in your mind using your own judgement. Try it out and let me know if you find it useful.\r\n‚û°Ô∏è https://peopleanalyticsblog.shinyapps.io/voluntary_turnover_predictors/\r\n\r\nAnd here is the original research paper on which the shiny app is based.\r\n\r\n\r\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-12-overview-of-predictors-of-voluntary-employee-turnover/./great_resignation.png",
    "last_modified": "2023-04-11T20:09:22+02:00",
    "input_file": {},
    "preview_width": 2700,
    "preview_height": 1800
  },
  {
    "path": "posts/2021-01-29-paygap/",
    "title": "Firemn√≠ audit rozd√≠lu mezi platy mu≈æ≈Ø a ≈æen",
    "description": "Platov√° nerovnost mezi mu≈æi a ≈æenami nen√≠ pro firmy jen z√°le≈æitost√≠ etickou a pr√°vn√≠, ale tak√© marketingovou - m≈Ø≈æe m√≠t toti≈æ negativn√≠ dopad na jejich \"employer brand\" a atraktivitu coby zamƒõstnavatele. To znamen√°, ≈æe pokud firmy chtƒõj√≠ p≈ôil√°kat a tak√© si udr≈æet talentovan√© zamƒõstnance, mus√≠ b√Ωt schopny zajistit, ≈æe se u nich s mu≈æi a ≈æenami bude v tomto ohledu zach√°zet stejnƒõ. Prvn√≠m krokem k tomu je zjistit, jak velk√Ω je rozd√≠l mezi platy mu≈æ≈Ø a ≈æen ve firmƒõ a do jak√© m√≠ry ho lze vysvƒõtlit jin√Ωmi faktory ne≈æ je samotn√© pohlav√≠ zamƒõstnance. V tomto ƒçl√°nku demonstruji, jak takovou anal√Ωzu prov√©st s pomoc√≠ analytick√©ho n√°stroje R a dat, kter√° m√° vƒõt≈°ina firem bƒõ≈ænƒõ k dispozici. Struƒçnƒõ se zmi≈àuji rovnƒõ≈æ o tom, jak√© mohou b√Ωt p≈ô√≠padn√© dal≈°√≠ kroky a doporuƒçen√≠ vypl√Ωvaj√≠c√≠ z v√Ωsledk≈Ø provedn√© anal√Ωzy.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2021-05-17",
    "categories": [
      "gender pay gap",
      "gender pay audit",
      "regression analysis",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nCo to je gender pay\r\ngap a jak ho mƒõ≈ôit?\r\nProƒç se\r\nzab√Ωvat platovou nerovnost√≠ ve Va≈°√≠ firmƒõ?\r\nAudit platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami\r\nPl√°n anal√Ωzy\r\nDostupn√°\r\ndata\r\nP≈ô√≠prava dat k anal√Ωze\r\nExploraƒçn√≠\r\nanal√Ωza\r\nStatistick√Ω model platov√©\r\nnerovnosti\r\nV√Ωsledky\r\nanal√Ωzy\r\nMo≈æn√© dal≈°√≠\r\nkroky\r\n\r\nCo to je gender pay\r\ngap a jak ho mƒõ≈ôit?\r\nGender pay gap (GPG), v p≈ôekladu genderov√° p≈ô√≠jmov√°\r\nnerovnost nebo p≈ô√≠jmov√° propast mezi mu≈æi a ≈æenami, oznaƒçuje\r\ntypick√Ω rozd√≠l mezi platov√Ωm ohodnocen√≠m pracuj√≠c√≠ch ≈æen a\r\nmu≈æ≈Ø. Obvykle je GPG vyjad≈ôov√°na procenty, pomƒõrem typick√©\r\nhrub√© hodinov√© (ƒçi roƒçn√≠) mzdy ≈æeny k typick√© mzdƒõ mu≈æe nebo pomƒõrem\r\nrozd√≠lu mezi typickou mzdou mu≈æ≈Ø a ≈æen v≈Øƒçi typick√© mzdƒõ mu≈æ≈Ø.\r\nBez ohledu na zp≈Øsob mƒõ≈ôen√≠ GPG, je dob≈ôe dolo≈æen√Ωm faktem, ≈æe ≈æeny\r\njsou obecnƒõ h≈Ø≈ôe placeny ne≈æ mu≈æi, jakkoli se tento rozd√≠l\r\npostupem ƒçasu zmen≈°uje. Rozd√≠ly v platech se p≈ôitom mohou v\r\njednotliv√Ωch zem√≠ch pomƒõrnƒõ dost li≈°it. N√°zornƒõ to ilustruje n√≠≈æe\r\nuveden√Ω graf, kter√Ω ukazuje v√Ωvoj (neadjustovan√©) GPG (definovan√© jako\r\npomƒõr rozd√≠lu medi√°nov√© mzdy zamƒõstnan√Ωch mu≈æ≈Ø a ≈æen a medi√°nov√© mzdy\r\nzamƒõstnan√Ωch mu≈æ≈Ø) v pr≈Øbƒõhu nƒõkolika minul√Ωch let v zem√≠ch OECD.\r\n\r\n\r\nShow code\r\n\r\n# uploading data\r\ngpgoecd <- readr::read_csv(\"./DP_LIVE_29012021212234147.csv\")\r\n\r\n# creating color palette\r\n# list of R color Brewer's palettes: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\r\nnbCols <- length(unique(gpgoecd$LOCATION))\r\nmyColors <- colorRampPalette(brewer.pal(8, \"Set1\"))(nbCols)\r\n\r\n# creating a graph\r\ng <- gpgoecd %>%\r\n  ggplot2::ggplot(aes(x = forcats::fct_reorder(LOCATION, Value), y = Value, fill = LOCATION,\r\n                      text = paste('Zemƒõ: ', LOCATION,\r\n                                 '<\/br><\/br>GPG: ', round(Value))))+\r\n  ggplot2::geom_col() +\r\n  ggplot2::facet_wrap(~ TIME, nrow = 4) +\r\n  ggplot2::labs(x = \"\",\r\n                y = \"GPG\",\r\n                title = \"Genderov√° p≈ô√≠jmov√° nerovnost v zem√≠ch OECD v letech 2016-2019\") +\r\n  ggthemes::theme_few() +\r\n  ggplot2::scale_fill_manual(values = myColors) +\r\n  ggplot2::theme(legend.position = \"\",\r\n                 legend.title = element_blank(),\r\n                 axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n\r\n\r\n# making the graph interactive\r\nplotly::ggplotly(\r\n  g, \r\n  width = 800,\r\n  height = 700,\r\n  tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nD≈Øvod≈Ø pro nevyv√°≈æenost p≈ô√≠jm≈Ø ≈æen a mu≈æ≈Ø pravdƒõpodobnƒõ existuje\r\nvƒõt≈°√≠ mno≈æstv√≠. Mezi nejƒçastƒõji uv√°dƒõn√© d≈Øvody pat≈ô√≠:\r\nDiskriminace na pracovi≈°ti. Stejn√© pr√°ce je\r\nodmƒõ≈àov√°na rozd√≠lnƒõ ƒçistƒõ na z√°kladƒõ pohlav√≠ pracovn√≠ka.\r\nGenderov√© stereotypy. P≈ôedsudky ohlednƒõ v√Ωkonnosti,\r\nschopnost√≠ a vlastnost√≠ ≈æen maj√≠ za n√°sledek oslaben√≠ jejich pozic a\r\nvytv√°≈ôen√≠ tzv. ‚Äûsklenƒõn√©ho stropu‚Äú, tj. neviditeln√© bari√©ry, na kterou\r\n≈æeny nar√°≈æ√≠ p≈ôi snaze o kari√©rn√≠ postup na l√©pe placen√© pozice.\r\nSegregace trhu. Odvƒõtv√≠, v nich≈æ je tradiƒçnƒõ\r\nzamƒõstn√°v√°no v√≠ce ≈æen ne≈æ mu≈æ≈Ø jako je zdravotnictv√≠, ≈°kolstv√≠ nebo\r\nve≈ôejn√° spr√°va, jsou spoleƒçnost√≠ vn√≠m√°na jako m√©nƒõ presti≈æn√≠, a tedy i\r\nh≈Ø≈ôe odmƒõ≈àov√°na.\r\nRodinn√Ω ≈æivot. ≈Ωeny vƒõt≈°inou nesou vƒõt≈°√≠ ƒç√°st\r\nz√°tƒõ≈æe spojen√© s rodinn√Ωm ≈æivotem (nap≈ô. p≈ôi odchodu na mate≈ôskou\r\ndovolenou, p≈ôi p√©ƒçi o nemocn√© dƒõti ƒçi jin√© ƒçleny dom√°cnosti), co≈æ jim\r\nv√Ωznamnƒõ stƒõ≈æuje jejich snahu o kari√©rn√≠ r≈Øst.\r\nV situaci, kdy p≈ôi reportov√°n√≠ GPG nerozli≈°ujeme mezi r≈Øzn√Ωmi d≈Øvody\r\npro platovou nerovnost, hovo≈ô√≠me o tzv. neadjustovan√©\r\nGPG. Pro pot≈ôeby firemn√≠ho auditu platov√© nerovnosti je v≈°ak\r\nd≈Øle≈æit√© zjistit rovnƒõ≈æ tzv. adjustovanou GPG, kter√° se\r\nsna≈æ√≠ vyj√°d≈ôit m√≠ru platov√© nerovnosti, kter√° je zp≈Øsobena ƒçistƒõ\r\npohlav√≠m zamƒõstnance. Zat√≠mco adjustovan√° GPG umo≈æ≈àuje firmƒõ\r\nidentifikovat mo≈ænou diskriminaci na pracovi≈°ti, neadjustovan√° GPG (p≈ôi\r\nneprok√°zan√© adjustovan√© GPG) m≈Ø≈æe poukazovat na existenci probl√©m≈Ø jako\r\njsou genderov√© stereotypy ƒçi nedostateƒçn√° podpora ≈æen p≈ôi snaze skloubit\r\nsv≈Øj osobn√≠ a profesn√≠ ≈æivot. Pro firmy je tak u≈æiteƒçn√© sledovat oba\r\nukazatele.\r\nProƒç se\r\nzab√Ωvat platovou nerovnost√≠ ve Va≈°√≠ firmƒõ?\r\nI kdybychom odhl√©dli od etick√Ωch ƒçi pr√°vn√≠ch aspekt≈Ø platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami, je ve velice pragmatick√©m z√°jmu ka≈æd√©\r\nfirmy, aby se tento druh nespravedlnosti v jej√≠m syst√©mu odmƒõ≈àov√°n√≠\r\nnevyskytoval. V dobƒõ soci√°ln√≠ch s√≠t√≠ a platforem na hodnocen√≠ firem\r\njejich souƒçasn√Ωmi i b√Ωval√Ωmi zamƒõstnanci (za v≈°echny zmi≈àme nap≈ô. Glassdoor nebo\r\nƒçesk√Ω Atmoskop) se toti≈æ\r\ninformace o nerovn√©m p≈ô√≠stupu m≈Ø≈æe velice snadno roz≈°√≠≈ôit mezi\r\npotenci√°ln√≠ i st√°vaj√≠c√≠ zamƒõstnance, kte≈ô√≠ ji mohou zohlednit p≈ôi sv√©m\r\nrozhodov√°n√≠, zda se v dan√© firmƒõ uch√°zet o pr√°ci, resp. zda v n√≠ i\r\nnad√°le z≈Østat.\r\nTuto skuteƒçnost dokl√°daj√≠ nap≈ô. v√Ωsledky pr≈Øzkumu\r\nproveden√©ho spoleƒçnost√≠ Glassdoor, podle kter√©ho cca 67 % (U.S.)\r\nzamƒõstnanc≈Ø by se neuch√°zelo o pr√°ci tam, kde by si myslelo, ≈æe mu≈æi a\r\n≈æeny maj√≠ nerovn√© platov√© podm√≠nky.\r\nAudit platov√©\r\nnerovnosti mezi mu≈æi a ≈æenami\r\nStejnƒõ jako p≈ôi ≈ôe≈°en√≠ jak√©hokoli jin√©ho probl√©mu, i v tomto p≈ô√≠padƒõ\r\nplat√≠, ≈æe v prvn√≠ ≈ôadƒõ je p≈ôedev≈°√≠m pot≈ôeba ovƒõ≈ôit, ≈æe nƒõjak√Ω\r\nprobl√©m k ≈ôe≈°en√≠ v≈Øbec existuje. K tomu poslou≈æ√≠\r\nfiremn√≠ audit platov√© nerovnosti mezi mu≈æi a ≈æenami.\r\nTen prost≈ôednictv√≠m anal√Ωzy platov√Ωch, demografick√Ωch a organizaƒçn√≠ch\r\ndat ovƒõ≈ô√≠, zda m√°me nƒõjak√© doklady pro to, ≈æe v dan√© spoleƒçnosti\r\nexistuj√≠ platov√© rozd√≠ly mezi zamƒõstnanci spojen√© s jejich pohlav√≠m.\r\nTeprve na z√°kladƒõ v√Ωsledk≈Ø takov√© anal√Ωzy je mo≈æn√© se zaƒç√≠t poohl√≠≈æet po\r\nmo≈æn√Ωch opat≈ôen√≠ch v oblastech n√°boru, odmƒõ≈àov√°n√≠ a/nebo povy≈°ov√°n√≠,\r\nkter√° by mohla pomoct nespravedliv√© platov√© nerovnosti odstranit nebo\r\nalespo≈à zm√≠rnit.\r\nN√≠≈æe uveden√Ω p≈ô√≠klad takov√©ho auditu vych√°z√≠ z ƒçl√°nku How\r\nto Analyze Your Gender Pay Gap: An Employer‚Äôs Guide od Andrew\r\nChamberlaina, Ph.D., hlavn√≠ho ekonoma a vedouc√≠ho v√Ωzkumu ve\r\nspoleƒçnosti Glassdoor.\r\nPl√°n anal√Ωzy\r\nAnal√Ωzu platov√© nerovnosti mezi mu≈æi a ≈æenami provedeme v\r\nn√°sleduj√≠c√≠ch nƒõkolika kroc√≠ch:\r\nNaƒçteme si data, kter√° obsahuj√≠ informace o platech vzorku\r\nzamƒõstnanc≈Ø, jejich pohlav√≠, demografick√Ωch a organizaƒçn√≠ch\r\ncharakteristik√°ch, na kter√Ωch budeme testovat na≈°e hypot√©zy. Za t√≠mto\r\n√∫ƒçelem pou≈æijeme ilustraƒçn√≠ data\r\nposkytnut√° spoleƒçnost√≠ Glassdoor.\r\nV p≈ô√≠padƒõ pot≈ôeby si uprav√≠me data tak, aby l√©pe vyhovovala pot≈ôeb√°m\r\nna≈°√≠ anal√Ωzy.\r\nProvedeme exploraƒçn√≠ anal√Ωzu, kter√° n√°m poskytne z√°kladn√≠ p≈ôedstavu\r\no na≈°ich datech.\r\nSpoƒç√≠t√°me si neadjustovanou GPG.\r\nS pomoc√≠ hierarchick√© regresn√≠ anal√Ωzy vytvo≈ô√≠me statistick√Ω model\r\nGPG, kter√Ω n√°m umo≈æn√≠ l√©pe rozli≈°it ‚Äúvliv‚Äù r≈Øzn√Ωch faktor≈Ø, vƒçetnƒõ\r\njejich interakc√≠, na pozorovan√© rozd√≠ly v platech mu≈æ≈Ø a ≈æen.\r\nOvƒõ≈ô√≠me, zda samotn√© pohlav√≠ zamƒõstance - p≈ôi zohlednƒõn√≠ ‚Äúvlivu‚Äù\r\nostatn√≠ch faktor≈Ø, ke kter√Ωm m√°me k dispozici nƒõjak√° data - hraje\r\nnƒõjakou v√Ωznamnƒõj≈°√≠ roli ve v√Ω≈°i platu, kter√Ω zamƒõstnanec dost√°v√°.\r\nOvƒõ≈ô√≠me, zda pohlav√≠ zamƒõstnance neinteraguje s nƒõkter√Ωmi dal≈°√≠mi\r\nfaktory p≈ôi predikci v√Ω≈°e jejich mzdy.\r\nDostupn√° data\r\n\r\n\r\nShow code\r\n\r\ndata <- readr::read_csv(\"./GenderPay_Data.csv\")\r\n\r\n\r\n\r\nK dispozici m√°me n√°sleduj√≠c√≠ data ke vzorku 1000 zamƒõstnanc≈Ø:\r\nTyp pozice, na kter√© zamƒõstnanec pracuje (jobTitle)\r\nPohlav√≠ zamƒõstnance (gender)\r\nVƒõk zamƒõstnance (age)\r\nHodnocen√≠ pracovn√≠ho v√Ωkonu zamƒõstnance (perfEval)\r\n√örove≈à vzdƒõl√°n√≠ zamƒõstnance (edu)\r\nOddƒõlen√≠, ve kter√©m zamƒõstnanec pracuje (dpt)\r\nM√≠ra seniority zamƒõstnance (seniority)\r\nZ√°kladn√≠ mzda zamƒõstnance (basePay)\r\nBonusov√° slo≈æka platu zamƒõstnance (bonus)\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(\r\n  data,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames = FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nP≈ô√≠prava dat k anal√Ωze\r\nZe zbƒõ≈æn√© kontroly povahy na≈°ich dat je patrn√©, ≈æe ne ka≈æd√° z\r\npromƒõnn√Ωch je v na≈°em datasetu reprezentov√°na pomoc√≠ adekv√°tn√≠ho\r\ndatov√©ho typu. P≈ôed samotnou anal√Ωzou si tedy budeme muset na≈°e data\r\nje≈°tƒõ trochu upravit.\r\n\r\n\r\nShow code\r\n\r\ndplyr::glimpse(data)\r\n\r\n\r\nRows: 1,000\r\nColumns: 9\r\n$ jobTitle  <chr> \"Graphic Designer\", \"Software Engineer\", \"Warehous~\r\n$ gender    <chr> \"Female\", \"Male\", \"Female\", \"Male\", \"Male\", \"Femal~\r\n$ age       <dbl> 18, 21, 19, 20, 26, 20, 20, 18, 33, 35, 24, 18, 19~\r\n$ perfEval  <dbl> 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,~\r\n$ edu       <chr> \"College\", \"College\", \"PhD\", \"Masters\", \"Masters\",~\r\n$ dept      <chr> \"Operations\", \"Management\", \"Administration\", \"Sal~\r\n$ seniority <dbl> 2, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 3, 3, 5, 4, 3, 5,~\r\n$ basePay   <dbl> 42363, 108476, 90208, 108080, 99464, 70890, 67585,~\r\n$ bonus     <dbl> 9938, 11128, 9268, 10154, 9319, 10126, 10541, 1024~\r\n\r\nKonkr√©tnƒõ budeme cht√≠t upravit v≈°echny textov√© promƒõnn√© (pracovn√≠\r\npozice, pohlav√≠, √∫rove≈à vzdƒõl√°n√≠ a pracovn√≠ oddƒõlen√≠) a dvƒõ numerick√©\r\npromƒõnn√© (hodnocen√≠ pracovn√≠ho v√Ωkonu a m√≠ru seniority) na faktorov√©\r\npromƒõnn√©. Ke t≈ôem z tƒõchto novƒõ vytvo≈ôen√Ωch faktorov√Ωch promƒõnn√Ωch\r\n(√∫rove≈à vzdƒõl√°n√≠, hodnocen√≠ pracovn√≠ho v√Ωkonu a m√≠ra senirotity) je\r\npotom pot≈ôeba p≈ôidat informaci o spr√°vn√©m po≈ôad√≠ jejich jednotliv√Ωch\r\nkategori√≠, proto≈æe reprezentuj√≠ ordin√°ln√≠ promƒõnn√©, u kter√Ωch lze\r\nsmysluplnƒõ hovo≈ôit o relativn√≠m po≈ôad√≠ kategori√≠ ve smyslu vy≈°≈°√≠/ni≈æ≈°√≠,\r\nresp. vƒõt≈°√≠/men≈°√≠. Takto upraven√° data ji≈æ odpov√≠daj√≠ typu informac√≠,\r\nkter√© reprezentuj√≠, a m≈Ø≈æeme je tedy zaƒç√≠t pou≈æ√≠vat pro anal√Ωzu na≈°eho\r\nprobl√©mu.\r\n\r\n\r\nShow code\r\n\r\nmydata <- data %>%\r\n  dplyr::mutate_if(is.character, as.factor) %>%\r\n  dplyr::mutate(edu = factor(edu, ordered = TRUE, levels = c(\"High School\", \"College\", \"Masters\", \"PhD\")),\r\n                perfEval = factor(as.character(perfEval), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")),\r\n                seniority = factor(as.character(seniority), ordered = TRUE, levels = c(\"1\",\"2\",\"3\",\"4\",\"5\")))\r\n\r\n\r\n\r\nExploraƒçn√≠ anal√Ωza\r\nV n√≠≈æe uveden√Ωch tabulk√°ch jsou uvedeny z√°kladn√≠ popisn√© statistiky k\r\njednotliv√Ωm promƒõnn√Ωm. M≈Ø≈æeme z nich vyƒç√≠st nap≈ô. to, ≈æe na≈°ich 1000\r\nzamƒõstnanc≈Ø je relativnƒõ rovnomƒõnƒõ rozdƒõlen√Ωch do jednotliv√Ωch kategori√≠\r\nz hlediska pracovn√≠ pozice, pohlav√≠, hodnocen√≠ pracovn√≠ho v√Ωkonu, √∫rovnƒõ\r\nvzdƒõl√°n√≠, oddƒõlen√≠, ve kter√©m pracuj√≠, i m√≠ry jejich seniority. D√°le se\r\nz nich m≈Ø≈æeme dozvƒõdƒõt, ≈æe prost≈ôedn√≠ch 50 % zamƒõstnanc≈Ø je ve vƒõku mezi\r\n29 a 54 lety, jejich roƒçn√≠ z√°kladn√≠ mzda se pohybuje od 76 850 do 111\r\n558 USD a jejich bonusy za rok ƒçin√≠ 4 849 a≈æ 8 026 USD.\r\n\r\n\r\nShow code\r\n\r\nskimr::skim(mydata)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nmydata\r\nNumber of rows\r\n1000\r\nNumber of columns\r\n9\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n6\r\nnumeric\r\n3\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\njobTitle\r\n0\r\n1\r\nFALSE\r\n10\r\nMar: 118, Sof: 109, Dat: 107, Fin:\r\n107\r\ngender\r\n0\r\n1\r\nFALSE\r\n2\r\nMal: 532, Fem: 468\r\nperfEval\r\n0\r\n1\r\nTRUE\r\n5\r\n5: 209, 4: 207, 1: 198, 3: 194\r\nedu\r\n0\r\n1\r\nTRUE\r\n4\r\nHig: 265, Mas: 256, Col: 241, PhD:\r\n238\r\ndept\r\n0\r\n1\r\nFALSE\r\n5\r\nOpe: 210, Sal: 207, Man: 198, Adm:\r\n193\r\nseniority\r\n0\r\n1\r\nTRUE\r\n5\r\n3: 219, 2: 209, 1: 195, 5: 193\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n41.39\r\n14.29\r\n18\r\n29.00\r\n41.0\r\n54.25\r\n65\r\n‚ñá‚ñá‚ñÜ‚ñÜ‚ñá\r\nbasePay\r\n0\r\n1\r\n94472.65\r\n25337.49\r\n34208\r\n76850.25\r\n93327.5\r\n111558.00\r\n179726\r\n‚ñÇ‚ñá‚ñá‚ñÉ‚ñÅ\r\nbonus\r\n0\r\n1\r\n6467.16\r\n2004.38\r\n1703\r\n4849.50\r\n6507.0\r\n8026.00\r\n11293\r\n‚ñÇ‚ñá‚ñá‚ñÜ‚ñÇ\r\n\r\n Z hlediska n√°mi analyzovan√©ho probl√©mu jsou pro n√°s ale\r\nd≈Øle≈æitƒõj≈°√≠ vztahy mezi jednotliv√Ωmi promƒõnn√Ωmi, zejm√©na mezi pohlav√≠m a\r\nostatn√≠mi promƒõnn√Ωmi a jejich r≈Øzn√Ωmi kombinacemi. Rychl√Ω p≈ôehled o\r\nnƒõkter√Ωch tƒõchto vztaz√≠ch n√°m m≈Ø≈æe poskytnout n√≠≈æe uveden√Ω graf, kter√Ω\r\nzobrazuje souvislosti mezi jednotliv√Ωmi dvojicemi promƒõnn√Ωch a s pomoc√≠\r\nbarevn√©ho k√≥dov√°n√≠ nav√≠c nese informaci o tom, jak se tyto souvislosti\r\nli≈°√≠ mezi pohlav√≠mi. V grafu m≈Ø≈æeme nap≈ô. vidƒõt, ≈æe se v p≈ô√≠padƒõ\r\nnƒõkter√Ωch pracovn√≠ch pozic v√Ωznamnƒõ li≈°√≠ relativn√≠ zastoupen√≠ mu≈æ≈Ø a\r\n≈æen. V men≈°√≠ m√≠≈ôe se zd√° tento rozd√≠l platit i v p≈ô√≠padƒõ √∫rovnƒõ\r\nvzdƒõl√°n√≠. Urƒçit√Ω rozd√≠l mezi mu≈æi a ≈æenami se zd√° existovat rovnƒõ≈æ ve\r\nv√Ω≈°i jejich z√°kladn√≠ mzdy (narozd√≠l od bonusov√© slo≈æky, kter√° se zd√° b√Ωt\r\nu mu≈æ≈Ø a ≈æen obdobnƒõ vysok√°).\r\n\r\n\r\nShow code\r\n\r\nGGally::ggpairs(mydata, aes(color = gender, alpha = 0.4)) +\r\n  ggplot2::theme(\r\n      strip.text.x = element_text(\r\n        size = 22),\r\n      strip.text.y = element_text(\r\n        size = 22)\r\n      ) +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\")\r\n\r\n\r\n\r\n\r\nVizu√°ln√≠ dojem o rozd√≠ln√© v√Ω≈°i z√°kladn√≠ mzdy u mu≈æ≈Ø a ≈æen potvrzuje i\r\ndetailnƒõj≈°√≠ anal√Ωza tohoto rozd√≠lu. Ta ukazuje, ≈æe v na≈°em vzorku\r\nmedi√°nov√° mzda ≈æen ƒçin√≠ 89913.5 USD a medi√°nov√° mzda mu≈æ≈Ø 98223 USD. To\r\nodpov√≠d√° rozd√≠lu 8309.5 USD, resp. neadjustovan√© GPG (definovan√© jako\r\npomƒõr rozd√≠lu medi√°nov√© mzdy mu≈æ≈Ø a ≈æen a medi√°nov√© mzdy mu≈æ≈Ø) 8.5 %.\r\nM√≠ra platov√© nerovnosti se tak v n√°mi sledovan√© firmƒõ zd√° b√Ωt sp√≠≈°e\r\nni≈æ≈°√≠, srovnateln√° s celkovou hodnotou tohoto ukazatele v zem√≠ch jako je\r\nnap≈ô. ≈†v√©dsko nebo Nov√Ω Z√©land (viz graf z √∫vodu tohoto ƒçl√°nku).\r\nPokud bychom chtƒõli zohlednit m√≠ru na≈°√≠ nejistoty p≈ôi odhadu\r\nvelikosti rozd√≠lu mezi typick√Ωm platem mu≈æ≈Ø a ≈æen, kter√° je dan√° t√≠m, ≈æe\r\npracujeme pouze se vzorkem zamƒõstnanc≈Ø a nikoli s celou firmou, mƒõli\r\nbychom s√°hnout po inferenƒçn√≠ statistice. P≈ôi pou≈æit√≠ bayesovsk√©ho\r\nekvivalentu t-testu pro dva nez√°visl√© v√Ωbƒõry z√≠sk√°me takto informaci o\r\nposteriorn√≠ distribuci velikosti tohoto rozd√≠lu. Na grafu n√≠≈æe m≈Ø≈æeme\r\nvidƒõt, ≈æe 95% interval kredibility se nach√°z√≠ v rozmez√≠ od 5511 do 11615\r\nUSD, s medi√°novou hodnotou 8392 USD. Z grafu tak√© m≈Ø≈æeme vyƒç√≠st, ≈æe\r\ndostupn√° data mluv√≠ silnƒõ v neprospƒõch nulov√© hypot√©zy o neexistenci\r\nrozd√≠lu mezi pr≈Ømƒõrn√Ωm platem mu≈æ≈Ø a ≈æen - viz velmi n√≠zk√° hodnota\r\nlogaritmu Bayesova\r\nfaktoru ve prospƒõch nulov√© hypot√©zu BF01.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nggstatsplot::ggbetweenstats(\r\n  data = mydata,\r\n  x = gender,\r\n  y = basePay,\r\n  type = \"bayes\",\r\n  title = \"Rozd√≠l v z√°kladn√≠ mzdƒõ mezi mu≈æi a ≈æenami\",\r\n  palette = \"Dark2\"\r\n) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::labs(x = \"\")\r\n\r\n\r\n\r\n\r\nSamotn√Ω fakt rozd√≠ln√© v√Ω≈°e z√°kladn√≠ mzdy u mu≈æ≈Ø a ≈æen ale\r\nje≈°tƒõ nemus√≠ automaticky znamenat, ≈æe by se za n√≠m skr√Ωvala diskriminace\r\n≈æen. Pozorovan√Ω rozd√≠l m≈Ø≈æe b√Ωt toti≈æ nap≈ô. zp≈Øsoben√Ω t√≠m, ≈æe\r\n≈æeny zamƒõstnan√© v n√°mi sledovan√© firmƒõ maj√≠ typicky ni≈æ≈°√≠ vzdƒõl√°n√≠ ne≈æ\r\nve stejn√© firmƒõ zamƒõstnan√≠ mu≈æi. A vzhledem k tomu, ≈æe v√Ω≈°e vzdƒõl√°n√≠ (z\r\nhlediska ‚Äúmeritokratick√© spravedlnosti‚Äù zcela neproblematicky) pozitivnƒõ\r\nkoreluje s v√Ω≈°√≠ platu, projev√≠ se tato souvislost v ni≈æ≈°√≠ typick√© mzdƒõ\r\n≈æen (ponechme nyn√≠ stranou ot√°zku, v jak√© m√≠≈ôe maj√≠ ≈æeny obecnƒõ p≈ô√≠stup\r\nk vy≈°≈°√≠mu vzdƒõl√°n√≠ ve spoleƒçnosti, kde dan√° firma p≈Øsob√≠). Tuto hypot√©zu\r\nse zdaj√≠ podporovat i dva n√≠≈æe uveden√© grafy, kter√© vizualizuj√≠ vztah\r\nmezi √∫rovn√≠ vzdƒõl√°n√≠ zamƒõstnance a v√Ω≈°√≠ jeho z√°kladn√≠ mzdy, resp.\r\nsouvislost mezi pohlav√≠m zamƒõstnance a √∫rovn√≠ jeho vzdƒõl√°n√≠.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, y = basePay)) +\r\n  PupillometryR::geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8, fill = \"#a9b2d1\") +\r\n  ggplot2::geom_point(aes(y = basePay), position = position_jitter(width = .15), size = .5, alpha = 0.8, color = \"#a9b2d1\") +\r\n  ggplot2::geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5, fill = \"#a9b2d1\") +\r\n  ggplot2::expand_limits(x = 5.25) +\r\n  ggplot2::guides(fill = FALSE) +\r\n  ggplot2::guides(color = FALSE) +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      ),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::theme(panel.border = element_blank()) +\r\n  ggplot2::labs(title = \"Vztah mezi √∫rovn√≠ vzdƒõl√°n√≠ a v√Ω≈°√≠ z√°kladn√≠ mzdy\",\r\n       x = \"\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = edu, fill = gender)) +\r\n  ggplot2::geom_bar(position = \"fill\") +\r\n  ggplot2::scale_fill_hue() +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"M√≠ra zastoupen√≠ m≈Ø≈æ≈Ø a ≈æen v jednotliv√Ωch kategori√≠ch √∫rovnƒõ vzdƒõl√°n√≠\",\r\n                x = \"\",\r\n                y = \"\",\r\n                fill = \"\") +\r\n  ggplot2::scale_fill_brewer(palette=\"Dark2\") +\r\n  ggplot2:: scale_color_brewer(palette=\"Dark2\") +\r\n  ggplot2::scale_y_continuous(labels = scales::percent_format()) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nPodobn√Ωch kombinovan√Ωch souvislost√≠ m≈Ø≈æe v na≈°ich datech (a v\r\nrealitƒõ, kterou reprezentuj√≠) existovat vƒõt≈°√≠ mno≈æstv√≠. Pokud by ƒçten√°≈ô\r\nchtƒõl vztahy mezi r≈Øzn√Ωmi kombinacemi promƒõnn√Ωch prozkoumat s√°m a\r\ndetailnƒõji, m≈Ø≈æe za t√≠mto √∫ƒçelem vyu≈æ√≠t tuto\r\ninteraktivn√≠ aplikaci, kde jsou nahran√° na≈°e data a kde lze snadno\r\nr≈Øzn√Ωm zp≈Øsobem vizualizovat zadan√© kombinace promƒõnn√Ωch. Viz n√≠≈æe\r\nuveden√° uk√°zka vyu≈æit√≠ t√©to aplikace p≈ôi vizualizaci vztahu mezi v√Ω≈°√≠\r\nplatu, pohlav√≠m a pracovn√≠ pozic√≠, vƒçetnƒõ poƒçtu zamƒõstnanc≈Ø v\r\njednotliv√Ωch kombinovan√Ωch kategori√≠ch. Z tohoto konkr√©tn√≠ho grafu je\r\ndob≈ôe patrn√©, ≈æe ≈æeny jsou ve srovn√°n√≠ s mu≈æi disproporƒçnƒõ m√©nƒõ\r\nzastoupeny na dvou nadpr≈Ømƒõrnƒõ odmƒõ≈àovan√Ωch pozic√≠ch Manager a\r\nSoftware Engineer a naopak disproporƒçnƒõ v√≠ce jsou zastoupeny na\r\npodpr≈Ømƒõrnƒõ platovƒõ ohodnocen√© pozici Marketing Associate.\r\n\r\n D≈Øle≈æitou kategori√≠ vztah≈Ø mezi promƒõnn√Ωmi, kterou bychom mƒõli\r\nprozkoumat, pokud se chceme co nejbl√≠≈æe dostat k p≈ô√≠ƒçin√°m pozorovan√Ωch\r\nnerovnost√≠ v platech mu≈æ≈Ø a ≈æen a dob≈ôe zac√≠lit p≈ô√≠padn√© intervence,\r\njsou tzv. interakce. Ty popisuj√≠ situace, kdy vztah\r\nmezi dvƒõma promƒõnn√Ωmi z√°vis√≠ na hodnotƒõ nƒõjak√© t≈ôet√≠ promƒõnn√©. N√°s zde\r\nbude konkr√©tnƒõ zaj√≠mat interakce mezi na≈°√≠ hlavn√≠ nez√°vislou promƒõnnou\r\n(prediktorem), tj. pohlav√≠m zamƒõstnance, a dal≈°√≠mi nez√°visl√Ωmi\r\npromƒõnn√Ωmi (nap≈ô. vƒõkem, √∫rovn√≠ vzdƒõl√°n√≠, hodnocen√≠m pracovn√≠ho v√Ωkonu,\r\npracovn√≠ pozic√≠ nebo oddƒõlen√≠m) ve vztahu k na≈°√≠ z√°visl√© promƒõnn√©\r\n(krit√©riu), tedy z√°kladn√≠ mzdƒõ.\r\nP≈ô√≠kladem vizualizace tohoto druhu vztahu mezi promƒõnn√Ωmi je n√≠≈æe\r\nuveden√Ω graf, ze kter√©ho m≈Ø≈æeme vyƒç√≠st, ≈æe ≈æeny maj√≠ sice v pr≈Ømƒõru\r\nni≈æ≈°√≠ z√°kladn√≠ mzdu ne≈æ mu≈æi nap≈ô√≠ƒç cel√Ωm vƒõkov√Ωm spektrem (viz n√≠≈æe\r\npolo≈æen√° regresn√≠ p≈ô√≠mka pro skupinu ≈æen), ale fakt, ≈æe zobrazen√©\r\nregresn√≠ p≈ô√≠mky jsou rovnobƒõ≈æn√©, svƒõdƒç√≠ pro to, ≈æe v r√°mci obou skupin\r\nplat√≠ stejn√Ω typ vztahu mezi vƒõkem a v√Ω≈°√≠ platu, a tedy ≈æe mezi pohlav√≠m\r\na vƒõkem ve vztahu k v√Ω≈°i mzdy nedoch√°z√≠ k ≈æ√°dn√© interakci. Pokud by se\r\nexistence takov√© interakce potvrdila i p≈ôi zohlednƒõn√≠ dal≈°√≠ch\r\nrelevantn√≠ch faktor≈Ø, mƒõlo by to pro n√°s b√Ωt podnƒõtem k dal≈°√≠ exploraci\r\ntoho, co se pozorovan√Ωm rozd√≠lem skr√Ωv√°.\r\n\r\n\r\nShow code\r\n\r\nmydata %>%\r\n  ggplot2::ggplot(aes(x = age, y = basePay, fill = gender, colour = gender, group = gender)) +\r\n  ggplot2::geom_point(size = 1L, position = \"jitter\", alpha = 0.5) +\r\n  ggplot2::geom_smooth(span = 1L, method = \"lm\") +\r\n  ggplot2::scale_fill_brewer(palette = \"Dark2\") +\r\n  ggplot2::scale_color_brewer(palette = \"Dark2\") +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(title = \"Vztah mezi vƒõkem zamƒõstnanc≈Ø a v√Ω≈°√≠ jejich z√°kladn√≠ mzdy\",\r\n                fill = \"\",\r\n                color = \"\") +\r\n  ggplot2::scale_y_continuous(\r\n    labels = scales::number_format(\r\n      accuracy = 1,\r\n      scale = 1/1000,\r\n      suffix = \"k\",\r\n      prefix = \"$\",\r\n      big.mark = \",\"),\r\n    limits = c(0,200000)\r\n    ) +\r\n  ggplot2::theme(legend.position = \"top\")\r\n\r\n\r\n\r\n\r\nStatistick√Ω model platov√©\r\nnerovnosti\r\nAbychom dok√°zali izolovat vliv samotn√©ho pohlav√≠ zamƒõstnanc≈Ø na v√Ω≈°i\r\nplatu a zohlednit p≈ôitom z√°rove≈à vliv v≈°ech ostatn√≠ch relevantn√≠ch\r\nfaktor≈Ø, vƒçetnƒõ nƒõkter√Ωch jejich interakc√≠, mus√≠me s√°hnout po\r\nkomplexnƒõj≈°√≠m n√°stroji ne≈æ je popisn√° statistika. A t√≠mto n√°strojem je\r\nstatistick√© modelov√°n√≠.\r\nStatistick√© modelov√°n√≠, podobnƒõ jako jak√©koli jin√© modelov√°n√≠ ve\r\nvƒõdƒõ, ale i v bƒõ≈æn√©m ≈æivotƒõ, nen√≠ niƒç√≠m jin√Ωm ne≈æ snahou\r\nvytvo≈ôit men≈°√≠ a zjednodu≈°en√Ω model na≈°eho svƒõta, kter√Ω v≈°ak\r\njeho chov√°n√≠ odr√°≈æ√≠ dostateƒçnƒõ vƒõrnƒõ na to, abychom s jeho pomoc√≠ mohli\r\nƒçinit √∫sudky a p≈ôedpovƒõdi o skuteƒçn√©m svƒõtƒõ a zakl√°dat na nƒõm sv√°\r\nrozhodnut√≠ (k tomuto t√©matu viz srozumitelnƒõ napsan√Ω\r\npopularizuj√≠c√≠ ƒçl√°nek Modeluji,\r\ntedy jsem od Josefa ≈†lerky).\r\nStatistick√© modelov√°n√≠ se potom od jin√Ωch druh≈Ø modelov√°n√≠ li≈°√≠ v tom,\r\n≈æe se ve vƒõt≈°√≠ m√≠≈ôe op√≠r√° o n√°stroje matematick√© statistiky a teorie\r\npravdƒõpodobnosti.\r\nP≈ôekvapivƒõ mnoho jev≈Ø na≈°eho svƒõta se d√° √∫spƒõ≈°nƒõ modelovat a\r\np≈ôedpov√≠dat pomoc√≠ relativnƒõ jednoduch√Ωch statistick√Ωch model≈Ø\r\nzobecnƒõn√© line√°rn√≠ regrese (Generalized Linear\r\nModels, GLM). Ty p≈ôedpokl√°daj√≠, ≈æe z√°visl√° promƒõnn√°, transformovan√°\r\nprost≈ôednictv√≠m tzv. linkovac√≠ funkce (link\r\nfunction), je funkc√≠ line√°rn√≠ kombinace nez√°visl√Ωch promƒõnn√Ωch.\r\nNejzn√°mƒõj≈°√≠ z t√©to rodiny statistick√Ωch model≈Ø je klasick√Ω\r\nline√°rn√≠ model, kter√Ω p≈ôedpokl√°d√° norm√°ln√≠ rozdƒõlen√≠ z√°visl√©\r\npromƒõnn√©, resp. rezidu√≠ (chyb) okolo predikovan√©/ oƒçek√°van√© st≈ôedn√≠\r\nhodnoty z√°visl√© promƒõnn√© (viz ilustrativn√≠ obr√°zek n√≠≈æe).\r\n\r\nVzhledem k tomu, ≈æe n√°mi modelovan√° promƒõnn√° z√°kladn√≠ mzdy se zd√° m√≠t\r\nnorm√°ln√≠, nebo t√©mƒõ≈ô norm√°ln√≠ rozdƒõlen√≠ (viz nƒõkter√© grafy v ƒç√°sti\r\nvƒõnovan√© exploraƒçn√≠ anal√Ωze), m≈Ø≈æeme i my s√°hnout po tomto statistick√©m\r\nmodelu. Jako nez√°visl√© promƒõnn√© v na≈°em modelu pou≈æijeme v≈°echny n√°m\r\ndostupn√© prediktory, spolu s interakcemi mezi promƒõnnou pohlav√≠ na\r\nstranƒõ jedn√© a promƒõnn√Ωmi √∫rovnƒõ vzdƒõl√°n√≠, seniority, vƒõku a hodnocen√≠\r\npracovn√≠ho v√Ωkonu na stranƒõ druh√©. Proto≈æe zamƒõstnanci tvo≈ô√≠ p≈ôirozen√©\r\nshluky v r√°mci oddƒõlen√≠, nap≈ô√≠ƒç kter√Ωmi se li≈°√≠ v√Ω≈°e mzdy a tak√© by se\r\nmohla li≈°it povaha vztahu mezi pohlav√≠m zamƒõstnance a v√Ω≈°√≠ jeho mzdy,\r\npou≈æijeme hierarchickou/v√≠ce√∫rov≈àovou variantu modelu line√°rn√≠\r\nregrese, kter√° umo≈æ≈àuje, aby hodnoty vybran√Ωch parametr≈Ø modelu\r\nvariovaly v z√°vilosti na p≈ô√≠slu≈°nosti zamƒõstnanc≈Ø do konkr√©tn√≠ho\r\noddƒõlen√≠.\r\nK odhadu hodnot parametr≈Ø na≈°eho modelu pou≈æijeme inferenƒçn√≠\r\nr√°mec bayesovsk√© statistiky, kter√° ve srovn√°n√≠ s\r\nfrekventistickou statistikou nab√≠z√≠ bohat≈°√≠ a intuitivnƒõ sn√°ze\r\nuchopiteln√© v√Ωstupy. Pro apriorn√≠ distribuci parametr≈Ø modelu pou≈æijeme\r\ndefaultn√≠, ≈°irok√© a neinformativn√≠ hodnoty, tak≈æe v√Ωsledky anal√Ωzy budou\r\nnomin√°lnƒõ podobn√© tƒõm, kter√© bychom z√≠skali p≈ôi pou≈æit√≠ tradiƒçnƒõj≈°√≠\r\nfrekventistick√© inferenƒçn√≠ statistiky.\r\n\r\n\r\nShow code\r\n\r\n# defining and running the model\r\n\r\nmodel <- brms::brm(\r\n  basePay | trunc(lb = 0) \r\n  ~ 1 \r\n  + jobTitle \r\n  + gender \r\n  + age \r\n  + perfEval \r\n  + edu \r\n  + seniority \r\n  + gender:edu \r\n  + gender:seniority \r\n  + gender:age \r\n  + gender:perfEval \r\n  + (1 + gender | dept),  \r\n  data = mydata %>% dplyr::mutate_if(is.factor, as.character),\r\n  family = gaussian(link = \"identity\"),\r\n  iter = 3000,\r\n  chains = 3,\r\n  cores = 6,\r\n  warmup = 1000,\r\n  seed = 2809,\r\n  control = list(\r\n    adapt_delta = 0.99, \r\n    max_treedepth = 20\r\n    )\r\n)\r\n\r\n\r\n\r\nV√Ωsledky anal√Ωzy\r\nD≈ô√≠ve ne≈æ p≈ôistoup√≠me k interpretaci v√Ωsledk≈Ø anal√Ωzy je dobr√© si\r\novƒõ≈ôit, ≈æe n√°≈° statistick√Ω model dok√°≈æe dostateƒçnƒõ vƒõrnƒõ napodobit ƒçi\r\nsimulovat data reprezentuj√≠c√≠ firemn√≠ realitu, na jej√≠≈æ vlastnosti\r\nchceme s pomoc√≠ tohoto modelu usuzovat. Za t√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t\r\nn√°stroj posteriorn√≠ prediktivn√≠ kontroly (posterior predictive\r\ncheck), kter√Ω ovƒõ≈ôuje, jak moc dob≈ôe n√°mi zvolen√Ω a odhadnut√Ω model\r\npredikuje pozorovan√° data na z√°kladƒõ vzorku posteriorn√≠ch hodnot jeho\r\nparametr≈Ø. Z n√≠≈æe uveden√©ho grafu je dob≈ôe patrn√©, ≈æe n√°≈° model si z\r\ntohoto hlediska nevede v≈Øbec ≈°patnƒõ.\r\nPo t√©to kontrole (a tak√© po ovƒõ≈ôen√≠ dal≈°√≠ch technick√Ωch\r\nn√°le≈æitost√≠, jako je nap≈ô. konvergence MCMC\r\n≈ôetƒõzc≈Ø, kter√© umo≈æ≈àuj√≠ odhadnout posteriorne√≠ distribuci parametr≈Ø i\r\nkomplexnƒõj≈°√≠ch statistick√Ωch model≈Ø jako je ten n√°≈°) m≈Ø≈æeme zaƒç√≠t\r\nvyu≈æ√≠vat parametry na≈°eho modelu k usuzov√°n√≠ na pravdƒõpodobn√© vlastnosti\r\nn√°mi studovan√© firemn√≠ reality.\r\n\r\n\r\nShow code\r\n\r\n# investigating the model's fit\r\n\r\n# specifying the number of samples\r\nnsamples = 100\r\n\r\nbrms::pp_check(\r\n  model, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posteriorn√≠ prediktivn√≠ kontrola modelu za pou≈æit√≠ vzorku o velikoti n = {nsamples}\")\r\n    )\r\n\r\n\r\n\r\n\r\nN√≠≈æe je uveden souhrn informac√≠ o na≈°em odhadnut√©m modelu. Prim√°rnƒõ\r\nn√°s zaj√≠m√° hodnota parametru pohlav√≠ (genderMale) v sekci\r\nvƒõnovan√© efekt≈Øm na √∫rovni cel√© populace (Population-Level\r\nEffects). 95% interval kredibility (Credible Interval),\r\nkter√Ω ud√°v√° kam v posteriorn√≠m rozdƒõlen√≠ spad√° hodnota nepozorovan√©ho\r\nparametru s 95% pravdƒõpodobnost√≠, se nach√°z√≠ v rozmez√≠ od -3750.04 USD\r\ndo 9081.92 USD, se st≈ôedn√≠ hodnotou 2717.57. Tzn., ≈æe podle na≈°eho\r\nmodelu m√° mu≈æ - p≈ôi zohlednƒõn√≠ ostatn√≠ch faktor≈Ø a jejich vybran√Ωch\r\ninterakc√≠ - typicky o cca 2700 USD vy≈°≈°√≠ z√°kladn√≠ mzdu ne≈æ jej√≠ ≈æensk√Ω\r\nprotƒõj≈°ek. Anal√Ωza na≈°ich dat tak do urƒçit√© m√≠ry podporuje hypot√©zu o\r\nexistenci platov√© diskriminace na z√°kladƒõ pohlav√≠ zamƒõstnance v n√°mi\r\nstudovan√© firmƒõ. S√≠la d≈Økazu ve prospƒõch t√©to hypot√©zy v≈°ak nen√≠ nijak\r\nv√Ωrazn√°, co≈æ vypl√Ωv√° z toho, ≈æe 95% interval kredibility zahrnuje vedle\r\nkladn√Ωch hodnot i nulovou hodnotu a z√°porn√© hodnoty parametru pohlav√≠\r\njako jeho plauzibiln√≠ hodnoty.\r\n\r\n\r\nShow code\r\n\r\nsummary(model)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: basePay | trunc(lb = 0) ~ 1 + jobTitle + gender + age + perfEval + edu + seniority + gender:edu + gender:seniority + gender:age + gender:perfEval + (1 + gender | dept) \r\n   Data: mydata %>% dplyr::mutate_if(is.factor, as.characte (Number of observations: 1000) \r\n  Draws: 3 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 6000\r\n\r\nGroup-Level Effects: \r\n~dept (Number of levels: 5) \r\n                          Estimate Est.Error l-95% CI u-95% CI Rhat\r\nsd(Intercept)              3659.02   2392.77  1181.83  9771.07 1.00\r\nsd(genderMale)             1808.55   1724.07    78.57  6119.45 1.00\r\ncor(Intercept,genderMale)     0.30      0.53    -0.83     0.98 1.00\r\n                          Bulk_ESS Tail_ESS\r\nsd(Intercept)                 2653     2634\r\nsd(genderMale)                3466     3552\r\ncor(Intercept,genderMale)     6333     4308\r\n\r\nPopulation-Level Effects: \r\n                            Estimate Est.Error  l-95% CI  u-95% CI\r\nIntercept                   29216.96   3175.03  23007.14  35319.81\r\njobTitleDriver              -3633.29   1488.22  -6452.10   -760.74\r\njobTitleFinancialAnalyst     3749.02   1419.10   1008.10   6517.61\r\njobTitleGraphicDesigner     -2832.43   1454.75  -5687.49     37.72\r\njobTitleIT                  -1869.36   1438.48  -4666.71    969.94\r\njobTitleManager             31411.39   1495.39  28444.26  34352.28\r\njobTitleMarketingAssociate -16475.88   1390.53 -19181.98 -13758.33\r\njobTitleSalesAssociate        316.91   1428.70  -2488.94   3110.02\r\njobTitleSoftwareEngineer    13286.51   1416.67  10487.13  16055.90\r\njobTitleWarehouseAssociate  -1040.96   1491.96  -3955.62   1848.02\r\ngenderMale                   2717.57   3238.37  -3750.04   9081.92\r\nage                           995.31     33.84    927.62   1061.68\r\nperfEval2                     246.70   1444.36  -2609.56   3094.73\r\nperfEval3                   -1515.48   1463.94  -4327.32   1361.02\r\nperfEval4                     183.45   1438.20  -2567.99   3065.99\r\nperfEval5                    1433.01   1470.70  -1405.12   4405.58\r\neduHighSchool                -417.81   1302.59  -2969.86   2140.64\r\neduMasters                   4149.49   1321.31   1548.67   6728.00\r\neduPhD                       7627.28   1342.16   5025.37  10270.97\r\nseniority2                   8000.31   1527.46   4989.21  10992.21\r\nseniority3                  17954.08   1486.71  15132.57  20868.40\r\nseniority4                  30596.32   1613.05  27406.56  33680.11\r\nseniority5                  39640.70   1530.88  36626.89  42651.21\r\ngenderMale:eduHighSchool    -1926.51   1868.39  -5590.15   1674.20\r\ngenderMale:eduMasters         780.52   1829.37  -2755.04   4346.52\r\ngenderMale:eduPhD           -3154.41   1842.55  -6757.49    436.31\r\ngenderMale:seniority2         898.79   2074.27  -3179.87   4874.13\r\ngenderMale:seniority3        -354.32   2018.99  -4315.71   3647.04\r\ngenderMale:seniority4       -2847.26   2095.05  -7044.18   1157.86\r\ngenderMale:seniority5       -3538.92   2098.61  -7564.69    565.50\r\ngenderMale:age                 16.69     44.85    -71.88    104.73\r\ngenderMale:perfEval2         -603.35   2084.98  -4768.78   3466.38\r\ngenderMale:perfEval3         1601.12   2062.41  -2375.63   5640.84\r\ngenderMale:perfEval4         -471.22   2020.88  -4447.73   3466.87\r\ngenderMale:perfEval5        -2795.97   2013.44  -6768.31   1080.21\r\n                           Rhat Bulk_ESS Tail_ESS\r\nIntercept                  1.00     2311     3592\r\njobTitleDriver             1.00     3819     4183\r\njobTitleFinancialAnalyst   1.00     3294     4284\r\njobTitleGraphicDesigner    1.00     3372     3844\r\njobTitleIT                 1.00     4040     4704\r\njobTitleManager            1.00     3617     4213\r\njobTitleMarketingAssociate 1.00     3495     4556\r\njobTitleSalesAssociate     1.00     3885     4347\r\njobTitleSoftwareEngineer   1.00     3327     4760\r\njobTitleWarehouseAssociate 1.00     3601     4249\r\ngenderMale                 1.00     3031     4168\r\nage                        1.00     5997     4475\r\nperfEval2                  1.00     4400     3950\r\nperfEval3                  1.00     4457     4393\r\nperfEval4                  1.00     4276     4384\r\nperfEval5                  1.00     4239     4453\r\neduHighSchool              1.00     4654     4274\r\neduMasters                 1.00     4692     4883\r\neduPhD                     1.00     4686     4832\r\nseniority2                 1.00     3890     4236\r\nseniority3                 1.00     3995     4891\r\nseniority4                 1.00     4234     4872\r\nseniority5                 1.00     4092     4550\r\ngenderMale:eduHighSchool   1.00     4494     4650\r\ngenderMale:eduMasters      1.00     4323     4943\r\ngenderMale:eduPhD          1.00     4369     4810\r\ngenderMale:seniority2      1.00     3919     4684\r\ngenderMale:seniority3      1.00     4535     5116\r\ngenderMale:seniority4      1.00     4379     4976\r\ngenderMale:seniority5      1.00     4594     4807\r\ngenderMale:age             1.00     5968     4280\r\ngenderMale:perfEval2       1.00     4211     4414\r\ngenderMale:perfEval3       1.00     4106     4449\r\ngenderMale:perfEval4       1.00     3960     4438\r\ngenderMale:perfEval5       1.00     3865     4221\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma 10095.58    236.20  9649.25 10569.40 1.00    10053     4303\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nPokud bychom chtƒõli p≈ôesnƒõji vyjad≈ôit m√≠ru, s n√≠≈æ na≈°e data v r√°mci\r\nna≈°eho modelu favorizuj√≠ hodnoty parametru pohlav√≠ vƒõt≈°√≠ ne≈æ nula (tj.\r\nhodnoty, kter√© jsou v souladu s hypot√©zou o existenci platov√©\r\ndiskriminace na z√°kladƒõ pohlav√≠ v neprospƒõch ≈æen), m≈Ø≈æeme se pod√≠vat na\r\nposteriorn√≠ distribuci tohoto parametru a jednodu≈°e na nƒõm spoƒç√≠tat, s\r\njakou pravdƒõpodobnost√≠ nab√Ωv√° kladn√Ωch hodnot.\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the model's b_genderMale parameter \r\n\r\nparamViz <- model %>%\r\n  tidybayes::gather_draws(\r\n    b_genderMale\r\n    ) %>%\r\n  dplyr::rename(value = .value)\r\n\r\ndens <- density(paramViz$value)\r\n\r\nparamViz <- tibble(x = dens$x, y = dens$y)\r\n\r\n\r\nggplot2::ggplot(\r\n  paramViz,\r\n  aes(x,y)\r\n    ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x > 0),\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::geom_area(\r\n    data = filter(paramViz, x <= 0),\r\n    fill = \"grey\"\r\n  ) +\r\n  ggplot2::geom_line(\r\n  ) +\r\n  ggplot2::scale_x_continuous(breaks = seq(-15000, 15000, 5000)) +\r\n  ggplot2::theme_minimal() +\r\n  ggplot2::labs(\r\n    title = \"Posteriorn√≠ distribuce parametru pohlav√≠ zamƒõstnance\",\r\n    y = \"Density\",\r\n    x = \"genderMale\"\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(model)\r\n\r\n# probability of b_genderMale coefficient being higher\r\nprop <- sum(samples$b_genderMale > 0) / nrow(samples)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Bayesian hypothesis test\r\nthe_test <- brms::hypothesis(model, \"genderMale > 0\")\r\n\r\n\r\n\r\nPo proveden√≠ tohoto v√Ωpoƒçtu n√°m vych√°z√≠ hodnota 80 %. To je v souladu\r\ns p≈ôedchoz√≠m tvrzen√≠m, ≈æe d≈Økaz ve prospƒõch testovan√© hypot√©zy nen√≠\r\np≈ô√≠li≈° siln√Ω. Dal≈°√≠ mo≈ænost√≠ by bylo pou≈æit√≠ tzv. Bayesova faktoru,\r\nkter√Ω vyjad≈ôuje m√≠ru s n√≠≈æ dostupn√° data favorizuj√≠ testovanou hypot√©zu\r\nve srovn√°n√≠ s modelem odpov√≠daj√≠c√≠m nulov√© hypot√©ze. Ten m√° pro na≈°i\r\nhypot√©zu hodnotu 4, co≈æ odpov√≠d√° v√Ωznamn√©mu, ale zdaleka nikoli siln√©mu\r\nƒçi rozhodn√©mu d≈Økazu ve prospƒõch na≈°√≠ hypot√©zy.\r\nVedle parametru pohlav√≠ m≈Ø≈æe b√Ωt pro n√°s potenci√°lnƒõ u≈æiteƒçn√© pod√≠vat\r\nse tak√© na vztah z√°kladn√≠ mzdy a ostatn√≠ch prediktor≈Ø pou≈æit√Ωch v na≈°em\r\nmodelu. Za t√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t vizualizaci margin√°ln√≠ch efekt≈Ø\r\njednotliv√Ωch prediktor≈Ø, kter√© vyjad≈ôuj√≠ vztah mezi prediktorem a\r\nkrit√©riem p≈ôi zohlednƒõn√≠ vlivu ostatn√≠ch prediktor≈Ø. Takto nap≈ô. m≈Ø≈æeme\r\nna jednom z graf≈Ø vidƒõt, ≈æe vztah mezi √∫rovn√≠ vzdƒõl√°n√≠ a v√Ω≈°√≠ z√°kladn√≠ho\r\nplatu se m√° tendenci u mu≈æ≈Ø a ≈æen li≈°it. Na jin√©m grafu si m≈Ø≈æeme zase\r\nv≈°imnout toho, ≈æe rozd√≠l mezi z√°kladn√≠ mzdou mu≈æ≈Ø a ≈æen m√° tendenci\r\nnar≈Østat s t√≠m, jak kles√° seniorita zamƒõstnanc≈Ø. Tyto a dal≈°√≠ podobn√©\r\nvhledy n√°m mohou pomoct p≈ôibl√≠≈æit se k d≈Øvod≈Øm za pozorovan√Ωmi\r\nnerovnostmi v platech mu≈æ≈Ø a ≈æen.\r\n\r\n\r\nShow code\r\n\r\n# plotting marginal effects of predictors used \r\n# Note: Conditional vs. Marginal Relationships: The regression coefficients in generalized linear mixed models represent conditional effects in the sense that they express comparisons holding the cluster-specific random effects (and covariates) constant. For this reason, conditional effects are sometimes referred to as cluster-specific effects. In contrast, marginal effects can be obtained by averaging the conditional expectation Œºij over the random effects distribution. Marginal effects express comparisons of entire sub-population strata defined by covariate values and are sometimes referred to as population-averaged effects.In linear mixed models (identity link), the regression coefficents can be interpreted as either conditional or marginal effects. However, conditional and marginal effects differ for most other link functions.\r\n\r\nmarginalEffplots <- plot(\r\n  brms::marginal_effects(\r\n    model, \r\n    effects = c(\"jobTitle\", \"age\", \"perfEval\", \"edu\", \"seniority\", \"gender:edu\", \"gender:seniority\", \"gender:age\", \"gender:perfEval\"),\r\n    probs = c(0.025, 0.975)),\r\n  ask = FALSE\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# putting all graphs with marginal effects together  \r\nggpubr::ggarrange(\r\n  plotlist = marginalEffplots, \r\n  nrow = 9,\r\n  ncol = 1\r\n)\r\n\r\n\r\n\r\n\r\nMo≈æn√© dal≈°√≠ kroky\r\nI v situaci, kdy anal√Ωza dat nepodpo≈ô√≠ na≈°e podez≈ôen√≠ na existenci\r\nplatov√© diskriminace na z√°kladƒõ pohlav√≠ zamƒõstnance, je st√°le mo≈æn√©, ≈æe\r\nza pozorovan√Ωm rozd√≠lem v platech mu≈æ≈Ø a ≈æen jsou jin√© faktory, kter√© s\r\npohlav√≠m zamƒõstance nƒõjak souvis√≠. Nap≈ô. skuteƒçnost, ≈æe jsou ≈æeny m√©nƒõ\r\nreprezentovan√© na l√©pe placen√Ωch seniornƒõj≈°√≠ch pozic√≠ch, by mohla\r\nsvƒõdƒçit o tom, ≈æe se ≈æeny na pracovi≈°ti mohou pot√Ωkat s genderov√Ωmi\r\nstereotypy a ≈æe p≈ôi snaze o kari√©rn√≠ postup na l√©pe placen√© pozice\r\nnar√°≈æej√≠ na tzv. ‚Äúsklenƒõn√Ω strop‚Äú. Pro uƒçinƒõn√≠ takov√©ho z√°vƒõru je v≈°ak\r\nzapot≈ôeb√≠ z√≠skat dal≈°√≠ data, a to sp√≠≈°e kvalitativn√≠ povahy, takov√°,\r\nkter√° sb√≠r√° a analyzuje nap≈ô. organizaƒçn√≠\r\nƒçi firemn√≠\r\nantropologie.\r\nV situaci, kdy m√°me dostateƒçnƒõ siln√© d≈Økazy pro to, ≈æe se za\r\npozorovanou platovou nerovnost√≠ mezi mu≈æi a ≈æenami skr√Ωvaj√≠ faktory\r\nsouvisej√≠c√≠ s pohlav√≠m zamƒõstnance, je mo≈æn√© zaƒç√≠t se poohl√≠≈æet po\r\nmo≈æn√Ωch ≈ôe≈°en√≠ch. Stejnƒõ jako p≈ôi identifikaci probl√©mu, i p≈ôi hled√°n√≠\r\nzp≈Øsobu jeho ≈ôe≈°en√≠ je dobr√© dr≈æet se z√°sad na\r\nd≈Økazech zalo≈æen√©ho managementu a volit pouze ≈ôe≈°en√≠ s dostateƒçnƒõ\r\nempiricky dolo≈æenou √∫ƒçinnost√≠, kter√° z√°rove≈à d√°vaj√≠ smysl ve specifick√©m\r\nkontextu dan√© firmy.\r\nU≈æiteƒçn√Ω p≈ôehled mo≈æn√Ωch akc√≠, kter√© zamƒõstnavatel√© mohou podniknout\r\ns c√≠lem sn√≠≈æit GPG ve sv√© organizaci, vytvo≈ôila zn√°m√° skupina odborn√≠k≈Ø\r\nna behavior√°ln√≠ vƒõdy v r√°mci tzv. The\r\nBehavioral Insights Team, kter√° sv√©ho ƒçasu vznikla pro to, aby\r\nbritsk√© vl√°dƒõ pom√°hala realizovat √∫ƒçinnou politiku zalo≈æenou na\r\nd≈Økazech. V dokumentu s n√°zvem Reducing the gender pay gap and\r\nimproving gender equality in organisations: Evidence-based actions for\r\nemployers tato skupina odborn√≠k≈Ø uv√°d√≠ nƒõkolik mo≈æn√Ωch intervenc√≠,\r\nkter√© ≈ôad√≠ do t≈ô√≠ kategori√≠ podle toho, jak dob≈ôe je jejich √∫ƒçinnost\r\npodlo≈æen√° empirick√Ωmi d≈Økazy.\r\nMezi akce s dob≈ôe dolo≈æenou √∫ƒçinnost√≠ ≈ôad√≠\r\nn√°sleduj√≠c√≠ intervence:\r\nZahrnut√≠ vƒõt≈°√≠ho poƒçtu ≈æen do u≈æ≈°√≠ch seznam≈Ø v r√°mci v√Ωbƒõru nov√Ωch\r\nzamƒõstnanc≈Ø a povy≈°ov√°n√≠.\r\nPou≈æ√≠v√°n√≠ √∫loh posuzuj√≠c√≠ch √∫rove≈à pracovn√≠ch dovednost√≠ v r√°mci\r\nv√Ωbƒõru nov√Ωch zamƒõstnanc≈Ø.\r\nPou≈æ√≠v√°n√≠ strukturovan√©ho interview v r√°mci v√Ωbƒõru nov√Ωch\r\nzamƒõstnanc≈Ø a povy≈°ov√°n√≠.\r\nPodpora vyjedn√°v√°n√≠ o v√Ω≈°i platu pomoc√≠ zvƒõ≈ôejnƒõn√≠ existuj√≠c√≠ho\r\nplatov√©ho rozmez√≠.\r\nZaveden√≠ transparentn√≠ch proces≈Ø povy≈°ov√°n√≠ a odmƒõ≈àov√°n√≠.\r\nJmenov√°n√≠ mana≈æera ƒçi z≈ô√≠zen√≠ pracovn√≠ skupiny pro firemn√≠\r\ndiverzitu.\r\nMezi potenci√°lnƒõ slibn√© akce, kter√© ale vy≈æaduj√≠ dal≈°√≠ d≈Økazy\r\no sv√© √∫ƒçinnosti, ≈ôad√≠ n√°sleduj√≠c√≠ postupy:\r\nZv√Ω≈°en√≠ pracovn√≠ flexibility pro mu≈æe a pro ≈æeny.\r\nPodporu sd√≠len√© rodiƒçovsk√© dovolen√©.\r\nN√°bor b√Ωval√Ωch zamƒõstnanc≈Ø, kte≈ô√≠ museli z r≈Øzn√Ωch osobn√≠ch d≈Øvod≈Ø\r\nna del≈°√≠ dobu p≈ôeru≈°it svou kari√©ru.\r\nNab√≠dku mentoringu and sponsorshipu.\r\nNab√≠dku networkingov√Ωch program≈Ø.\r\nNastaven√≠ intern√≠ch c√≠l≈Ø.\r\nA mezi akce se sm√≠≈°en√Ωmi doklady o jejich √∫ƒçinnosti\r\npotom ≈ôad√≠ n√°sleduj√≠c√≠ opat≈ôen√≠:\r\n≈†kolen√≠ vƒõnovan√© t√©matu nevƒõdom√Ωch p≈ôedsudk≈Ø.\r\n≈†kolen√≠ v oblasti diverzity.\r\n≈†kolen√≠ vƒõnovan√© rozvoji leadershipu.\r\nDemograficky r≈Øznorod√© v√Ωbƒõrov√© panely v r√°mci extern√≠ho i intern√≠ho\r\nn√°boru.\r\nZde je pro z√°jemce origin√°ln√≠ dokument k bli≈æ≈°√≠mu prostudov√°n√≠.\r\n\r\n\r\nTento prohl√≠≈æeƒç nepodporuje soubory PDF. Pro zobrazen√≠ si, pros√≠m, PDF\r\nsoubor st√°hnƒõte: St√°hnout PDF.\r\n\r\n\r\n\r\nSkript k anal√Ωze je k dispozici ke sta≈æen√≠ v podobƒõ Jupyter Notebooku\r\nna m√Ωch GitHub\r\nstr√°nk√°ch.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-29-paygap/./PayGap.png",
    "last_modified": "2022-08-30T17:05:43+02:00",
    "input_file": {},
    "preview_width": 1438,
    "preview_height": 897
  },
  {
    "path": "posts/2020-12-31-segmentedregression/",
    "title": "Modeling impact of the COVID-19 pandemic on people‚Äôs interest in work-life balance and well-being",
    "description": "Illustration of Bayesian segmented regression analysis of interrupted time series data with a testing hypothesis about the impact of the COVID-19 pandemic on increase in people's search interest in work-life balance and well-being.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "well-being",
      "work-life balance",
      "covid pandemic",
      "segmented regression",
      "interrupted time series data",
      "bayesian inference",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nSearch\r\ninterest in work-life balance and well-being\r\nBayesian segmented\r\nregression\r\nSome necessary sanity\r\nchecks\r\nResults of the analysis\r\n\r\nThe turn of the year, which is full of all sorts of resolutions to\r\nchange for the better in our private lives and in our organizations, is\r\na good time to remind ourselves that analytic tools can be very helpful\r\nin our efforts to make these resolutions come true. One way they can\r\nhelp us is by verifying that we have really achieved our stated goals\r\nand that we are not just fooling ourselves into believing so. We need to\r\nkeep in mind Richard\r\nFeynman‚Äôs famous principle of critical thinking‚Ä¶\r\n\r\n\r\nOne of the tools that can help us with that is segmented\r\nregression analysis of interrupted time series data (thanks to Masatake\r\nHirono for pointing me to its existence). It allows us to model\r\nchanges in various processes and outcomes that follow interventions,\r\nwhile controlling for other types of changes (e.g.¬†trends and\r\nseasonality) that may have occurred regardless of the interventions. It\r\nis thus very useful for data analysis conducted within studies with a quasi experimental\r\nstudy design that are often in the organizational context the best\r\nalternative to the ‚Äúgold standard‚Äù of randomized\r\ncontrolled trials (RCTs) that are not always realizable or\r\npolitically acceptable.\r\nSearch\r\ninterest in work-life balance and well-being\r\nFor illustration, let‚Äôs use this tool for testing hypothesis about\r\npeople‚Äôs increased interest in topics related to work-life balance and\r\nwell-being due to the COVID-19 pandemic and subsequent changes in the\r\nway people work. As a proxy measure of this interest we will use\r\nworldwide search interest data over the last 10 years from Google Trends using\r\nsearch terms work-life balance and well-being (see\r\nFig. 1 and 2 below).\r\nFig. 1: Interest in\r\n‚Äúwork-life balance‚Äù topic over the last 10 years measured as a search\r\ninterest by Google Trends. The numbers represent search interest\r\nrelative to the highest point on the chart for the given region and\r\ntime. A value of 100 is the peak popularity for the term. A value of 50\r\nmeans that the term is half as popular. A score of 0 means that there\r\nwas not enough data for this term.\r\n\r\nFig. 2: Interest in\r\n‚Äúwell-being‚Äù topic over the last 10 years measured as a search interest\r\nby Google Trends. The numbers represent search interest relative to the\r\nhighest point on the chart for the given region and time. A value of 100\r\nis the peak popularity for the term. A value of 50 means that the term\r\nis half as popular. A score of 0 means that there was not enough data\r\nfor this term.\r\n\r\nBased solely on the visual inspection of the graphs, it is pretty\r\ndifficult to tell whether there was some effect of the COVID-19 pandemic\r\nor not, especially in the case of work-life balance (for the purpose of\r\nthis analysis, the beginning of the pandemic is assumed to have started\r\nin March 2020). For sure it‚Äôs not a job for ‚Äúinter-ocular trauma test‚Äù\r\nwhen the existence of the effect hits you directly between the eyes. We\r\nneed to rely here on inferential statistics and its ability to help us\r\nwith distinguishing signal from noise.\r\nBefore conducting the analysis itself, we need to wrangle the data\r\nfrom Google Trends a little bit using the recipe presented in the\r\nWagner, Zhang, and Ross-Degnan‚Äôs paper. Specifically, we need the\r\nfollowing five variables (or six, given that we have two dependent\r\nvariables):\r\nsearch interest ‚Äì numerical variable representing\r\nsearch interest relative to the highest point on the chart for the given\r\nregion and time; this variable is truncated within the interval between\r\nvalues of 0 and 100; a value of 100 is the peak popularity for the term;\r\na value of 50 means that the term is half as popular; a score of 0 means\r\nthat there was not enough data for this term; this variable serves as a\r\ndependent (criterion) variable;\r\nelapsed time ‚Äì numerical variable representing the\r\nnumber of months that elapsed from the beginning of the time series;\r\nthis variable enables estimation of the size and direction of the\r\noverall trend in the data;\r\npandemic ‚Äì dichotomic variable indicating the\r\npresence/absence of pandemic; as already mentioned above, for the\r\npurpose of this analysis, the beginning of the pandemic is assumed to\r\nhave started in March 2020; this variable enables estimation of the\r\nlevel change in the interest in work-life balance and well-being\r\nimmediately after the pandemic outbreak;\r\nelapsed time after pandemic outbreak ‚Äì numerical\r\nvariable representing the number of months that elapsed from the\r\nbeginning of pandemic; this variable enables estimation of the change in\r\nthe trend in the interest in work-life balance and well-being after the\r\noutbreak of pandemic;\r\nmonth ‚Äì categorical variable representing specific\r\nmonth within a year; this variable enables controlling for the effect of\r\nseasonality.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for data manipulation\r\nlibrary(tidyverse)\r\n\r\n# uploading data\r\ndfWorkLifeBalance <- readr::read_csv(\"./workLifeBalanceGoogleTrendData.csv\")\r\ndfWellBeing <- readr::read_csv(\"./wellBeingGoogleTrendData.csv\")\r\n\r\ndfAll <- dfWorkLifeBalance %>%\r\n  # joining both datasets\r\n  dplyr::left_join(\r\n    dfWellBeing, by = \"Month\"\r\n    ) %>%\r\n  # changing the format and name of Month variable\r\n  dplyr::mutate(\r\n    Month = stringr::str_glue(\"{Month}-01\"),\r\n    Month = lubridate::ymd(Month)\r\n    ) %>%\r\n  dplyr::rename(\r\n    date = Month\r\n    ) %>%\r\n  # creating new variable month\r\n  dplyr::mutate(\r\n    month = lubridate::month(date,label = TRUE, abbr = TRUE),\r\n    month = factor(month, \r\n                   levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\", \"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"), \r\n                   ordered = FALSE)\r\n    ) %>%\r\n  # arranging data in ascending order by date\r\n  dplyr::arrange(\r\n    date\r\n    ) %>%\r\n  # creating new variables\r\n  dplyr::mutate(\r\n    elapsedTime = row_number(),\r\n    pandemic = case_when(\r\n      date >= \"2020-03-01\" ~ 1,\r\n      TRUE ~ 0\r\n      ),\r\n    elapsedTimeAfterPandemic = cumsum(pandemic)\r\n  ) %>%\r\n  dplyr::mutate(\r\n    pandemic = as.factor(case_when(\r\n        pandemic == 1 ~ \"After the pandemic outbreak\",\r\n        TRUE ~ \"Before the pandemic outbreak\"\r\n        ))\r\n  ) %>%\r\n  # changing order of variables in df\r\n  dplyr::select(\r\n    date, workLifeBalance, wellBeing, elapsedTime, month, pandemic, elapsedTimeAfterPandemic\r\n    )\r\n\r\n\r\n\r\n\r\nHere is a table with the resulting data we will use for testing our\r\nhypothesis.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for making user-friendly data table\r\nlibrary(DT)\r\n\r\nDT::datatable(\r\n  dfAll,\r\n  class = 'cell-border stripe', \r\n  filter = 'top',\r\n  extensions = 'Buttons',\r\n  fillContainer = FALSE,\r\n  rownames= FALSE,\r\n  options = list(\r\n    pageLength = 5, \r\n    autoWidth = TRUE,\r\n    dom = 'Bfrtip',\r\n    buttons = c('copy'), \r\n    scrollX = TRUE, \r\n    selection=\"multiple\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nTable 1: Final dataset used for testing hypothesis about impact\r\nof the COVID-19 pandemic on people‚Äôs interest in work-life balance and\r\nwell-being.\r\n\r\nBayesian segmented\r\nregression\r\nWe will model our data using common segmented regression models that\r\nhave following general structure:\r\n\\[Y_{t} = Œ≤_{0} + Œ≤_{1}*time_{t} +\r\nŒ≤_{2}*intervention_{t} + Œ≤_{3}*time after intervention_{t} +\r\ne_{t}\\]\r\nThe Œ≤0 coefficient estimates the baseline level\r\nof the outcome variable at time zero; Œ≤1 coefficient\r\nestimates the change in the mean of the outcome variable that occurs\r\nwith each unit of time before the intervention (i.e.¬†the baseline\r\ntrend); Œ≤2 coefficient estimates the level change in\r\nthe mean of the outcome variable immediately after the intervention\r\n(i.e.¬†from the end of the preceding segment); and Œ≤3\r\nestimates the change in the trend in the mean of the outcome variable\r\nper unit of time after the intervention, compared with the trend before\r\nthe intervention (thus, the sum of Œ≤1 and\r\nŒ≤3 equals to the post-intervention slope).\r\nSince we are dealing with correlated and truncated data, we should\r\nalso include two additional terms in our model, an autocorrelation term\r\nand a truncation term, to handle these specific properties of our\r\ndata.\r\nNow let‚Äôs fit the models to the data and check what they tell us\r\nabout the effect of pandemic on people‚Äôs search interest in work-life\r\nbalance and well-being. We will use brms\r\nr package that enables making inferences about statistical models‚Äô\r\nparameters within Bayesian inferential framework. Because of that, we\r\nalso need to specify some additional parameters\r\n(e.g.¬†chains, iter or warmup) of\r\nthe\r\nMarkov Chain Monte Carlo (MCMC) algorithm that will generate\r\nposterior samples of our models‚Äô parameters.\r\nBayesian framework also enables us to specify priors for estimated\r\nparameter and through them include our domain knowledge in the analysis.\r\nThe specified priors are important for both parameter estimation and\r\nhypothesis testing as they define our starting information state before\r\nwe take into account our data. Here we will use rather wide,\r\nuninformative, and only mildly regularizing priors (it means that the\r\nresults of the inference will be very close to the results of standard,\r\nfrequentist parameter estimation/hypothesis testing).\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# checking available priors for the models \r\nbrms::get_prior(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\nbrms::get_prior(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian())\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for Bayesian statistical inference\r\nlibrary(brms)\r\n\r\n# specifying wide, uninformative, and only mildly regularizing priors for predictors in both models \r\npriors <- c(set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTime\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"elapsedTimeAfterPandemic\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"pandemicBeforethepandemicoutbreak\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthApr\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthAug\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthDec\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthFeb\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJul\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthJun\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMar\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthMay\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthNov\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthOct\"),\r\n            set_prior(\"normal(0,50)\", class = \"b\", coef = \"monthSep\"))\r\n\r\n# defining the statistical model for work-life balance\r\nmodelWorkLifeBalance <- brms::brm(\r\n  workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 12345,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n# defining the statistical model for well-being\r\nmodelWellBeing <- brms::brm(\r\n  wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1),\r\n  data = dfAll,\r\n  family = gaussian(),\r\n  prior = priors,\r\n  chains = 4,\r\n  iter = 3000,\r\n  warmup = 1000,\r\n  seed = 678910,\r\n  sample_prior = TRUE\r\n  )\r\n\r\n\r\n\r\n\r\nSome necessary sanity checks\r\nBefore making any inferences, we should make some sanity checks to be\r\nsure that the mechanics of the MCMC algorithm worked well and that we\r\ncan use generated posterior samples for making inferences about our\r\nmodels‚Äô parameters. There are many ways for doing that, but here we will\r\nuse only visual check of the MCMC chains. We want plots of these chains\r\nlook like hairy caterpillar which would indicate convergence of the\r\nunderlying Markov chain to stationarity and convergence of Monte Carlo\r\nestimators to population quantities, respectively. As can be seen in\r\nGraph 1 and 2 below, in case of both models we can observe wanted\r\ncharacteristics of the MCMC chains described above. (For additional MCMC\r\ndiagnostics procedures, see for example Bayesian\r\nNotes from Jeffrey B. Arnold.)\r\n\r\n\r\nShow code\r\n\r\n# uploading library for plotting Bayesian models\r\nlibrary(bayesplot)\r\n\r\n# plotting the MCMC chains for the modelWorkLifeBalance \r\nbayesplot::mcmc_trace(\r\n  modelWorkLifeBalance,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWorkLifeBalance's parameters\"\r\n    )\r\n\r\n\r\n\r\n\r\nGraph 1: Trace plots of Markov chains for individual parameters\r\nof the modelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting the MCMC chains for the modelWellBeing \r\nbayesplot::mcmc_trace(\r\n  modelWellBeing,\r\n  facet_args = list(nrow = 6)\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Plots of the MCMC chains used for estimation of the modelWellBeing's parameters\"\r\n    )\r\n\r\n\r\n\r\n\r\nGraph 2: Trace plots of Markov chains for individual parameters\r\nof the modelWellBeing.\r\n\r\nIt is also important to check how well the models fit the data. We\r\ncan use for this purpose posterior predictive checks that use specified\r\nnumber of sampled posterior values of models‚Äô parameters and show how\r\nwell the fitted models predict observed data. We can see in Graphs 3 and\r\n4 that both models fit the observed data reasonably well.\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWorkLifeBalance fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWorkLifeBalance, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWorkLifeBalance (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\n\r\nGraph 3: Posterior predictive checks comparing\r\nsimulated/replicated data under the fitted modelWorkLifeBalance with the\r\nobserved data.\r\n\r\n\r\n\r\nShow code\r\n\r\n# investigating modelWellBeing fit\r\n\r\n# specifying the number of samples\r\nnsamples = 1000\r\n\r\nbrms::pp_check(\r\n  modelWellBeing, \r\n  nsamples = nsamples\r\n  ) + \r\n  ggplot2::labs(\r\n    title = stringr::str_glue(\"Posterior predictive checks for modelWellBeing (using {nsamples} samples)\")\r\n    )\r\n\r\n\r\n\r\n\r\nGraph 4: Posterior predictive checks comparing\r\nsimulated/replicated data under the fitted modelWellBeing with the\r\nobserved data.\r\n\r\nResults of the analysis\r\nNow, after having sufficient confidence that - using terminology from\r\nthe Richard\r\nMcElreath‚Äôs book Statistical Rethinking - our ‚Äúsmall worlds‚Äù can\r\npretty accurately mimic the data coming from our real,‚Äúbig world‚Äù, we\r\ncan use our models‚Äô parameters to learn something about our research\r\nquestions. Our primary interest is in the coefficient value of the\r\npandemicBeforethepandemicoutbreak term in our models. It\r\nexpresses how much and in what direction people‚Äôs search interest in\r\nwork-life balance and well-being changed immediately after the outbreak\r\nof pandemic.\r\nIn Graph 5 and 6 we can see posterior distribution of this parameter\r\nin our two models. In both cases the posterior distribution of the\r\npandemic term is (predominantly or completely) on the left side of the\r\nzero value, which supports the claim about existence of the effect of\r\npandemic on people‚Äôs increased search interest in work-life balance and\r\nwell-being. As is apparent from the graphs, for well-being (Graph 6)\r\nthis evidence is much stronger than for work-life balance (Graph 5),\r\nwhich corresponds to impression we might have when looking at the\r\noriginal Google Trends charts shown in Fig. 1 and 2.\r\n\r\n\r\nShow code\r\n\r\n# uploading library for \r\nlibrary(tidybayes)\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWorkLifeBalance\r\nmodelWorkLifeBalance %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWorkLifeBalance\"\r\n    )\r\n\r\n\r\n\r\n\r\nGraph 5: Visualization of the posterior distribution of the\r\npandemicBeforethepandemicoutbreak parameter in the\r\nmodelWorkLifeBalance.\r\n\r\n\r\n\r\nShow code\r\n\r\n# visualizing posterior distribution of the pandemicBeforethepandemicoutbreak parameter in the modelWellBeing\r\nmodelWellBeing %>%\r\n  tidybayes::gather_draws(\r\n    b_pandemicBeforethepandemicoutbreak\r\n    ) %>%\r\n  dplyr::mutate(\r\n    .variable = factor(\r\n      .variable, \r\n      levels = c(\"b_pandemicBeforethepandemicoutbreak\"), \r\n      ordered = TRUE\r\n      )\r\n    ) %>%\r\n  dplyr::rename(value = .value) %>%\r\n  ggplot2::ggplot(\r\n    aes(x = value)\r\n    ) +\r\n  ggplot2::geom_density(\r\n    fill = \"lightblue\"\r\n  ) +\r\n  ggplot2::labs(\r\n    title = \"Posterior distribution of the pandemicBeforethepandemicoutbreak parameter\\nin the modelWellBeing\"\r\n    )\r\n\r\n\r\n\r\n\r\nGraph 5: Visualization of the posterior distribution of the\r\npandemicBeforethepandemicoutbreak parameter in the\r\nmodelWellBeing.\r\n\r\nTo generate more summary statistics about posterior distributions\r\n(and also some diagnostic information like Rhat or\r\nESS), we can use summary() function.\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWorkLifeBalance \r\nsummary(modelWorkLifeBalance)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: workLifeBalance | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.21      0.10     0.02     0.39 1.00     6599     5352\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            69.72     11.40    47.30\r\nelapsedTime                          -0.05      0.04    -0.13\r\npandemicBeforethepandemicoutbreak   -14.01      9.97   -33.32\r\nelapsedTimeAfterPandemic              0.39      1.56    -2.69\r\nmonthFeb                              3.47      4.44    -5.26\r\nmonthMar                              4.45      4.90    -5.26\r\nmonthApr                              6.47      4.99    -3.31\r\nmonthMay                              8.77      5.01    -0.91\r\nmonthJun                             -2.82      5.00   -12.57\r\nmonthJul                             -7.40      5.05   -17.17\r\nmonthAug                             -2.48      4.97   -12.25\r\nmonthSep                             -2.24      5.03   -12.22\r\nmonthOct                              5.36      4.99    -4.31\r\nmonthNov                             12.27      4.90     2.65\r\nmonthDec                             -0.68      4.53    -9.69\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            91.75 1.00     4371     5350\r\nelapsedTime                           0.02 1.00     7331     5335\r\npandemicBeforethepandemicoutbreak     5.46 1.00     4901     5537\r\nelapsedTimeAfterPandemic              3.52 1.00     5040     5312\r\nmonthFeb                             12.15 1.00     4025     5240\r\nmonthMar                             13.95 1.00     3635     4970\r\nmonthApr                             16.25 1.00     3547     4652\r\nmonthMay                             18.42 1.00     3463     4879\r\nmonthJun                              7.10 1.00     3455     5129\r\nmonthJul                              2.86 1.00     3598     4943\r\nmonthAug                              7.29 1.00     3563     4837\r\nmonthSep                              7.55 1.00     3534     5042\r\nmonthOct                             15.09 1.00     3654     5213\r\nmonthNov                             21.95 1.00     3795     4731\r\nmonthDec                              8.11 1.00     3907     4717\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma    11.54      0.79    10.15    13.22 1.00     6162     4985\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# generating a summary of the results for modelWellBeing \r\nsummary(modelWellBeing)\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: wellBeing | trunc(lb = 0, ub = 100) ~ elapsedTime + pandemic + elapsedTimeAfterPandemic + month + ar(p = 1) \r\n   Data: dfAll (Number of observations: 132) \r\n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\r\n         total post-warmup draws = 8000\r\n\r\nCorrelation Structures:\r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nar[1]     0.24      0.10     0.05     0.44 1.00     6007     5574\r\n\r\nPopulation-Level Effects: \r\n                                  Estimate Est.Error l-95% CI\r\nIntercept                            56.17      4.74    46.72\r\nelapsedTime                           0.11      0.02     0.07\r\npandemicBeforethepandemicoutbreak   -21.39      4.22   -29.58\r\nelapsedTimeAfterPandemic              0.49      0.66    -0.82\r\nmonthFeb                              8.63      1.78     5.13\r\nmonthMar                             10.68      2.00     6.78\r\nmonthApr                              9.62      2.06     5.60\r\nmonthMay                              3.58      2.04    -0.39\r\nmonthJun                             -4.28      2.05    -8.27\r\nmonthJul                             -7.97      2.06   -11.96\r\nmonthAug                             -5.67      2.09    -9.69\r\nmonthSep                              4.61      2.04     0.74\r\nmonthOct                              8.20      2.03     4.17\r\nmonthNov                              6.80      2.01     2.95\r\nmonthDec                             -4.91      1.82    -8.53\r\n                                  u-95% CI Rhat Bulk_ESS Tail_ESS\r\nIntercept                            65.39 1.00     4436     4843\r\nelapsedTime                           0.14 1.00     7473     5438\r\npandemicBeforethepandemicoutbreak   -13.13 1.00     5052     5098\r\nelapsedTimeAfterPandemic              1.77 1.00     5222     4668\r\nmonthFeb                             12.08 1.00     3997     5511\r\nmonthMar                             14.70 1.00     3599     4540\r\nmonthApr                             13.75 1.00     3427     4832\r\nmonthMay                              7.62 1.00     3273     4478\r\nmonthJun                             -0.24 1.00     3270     4884\r\nmonthJul                             -3.85 1.00     3230     4752\r\nmonthAug                             -1.50 1.00     3253     4759\r\nmonthSep                              8.61 1.00     3241     5013\r\nmonthOct                             12.21 1.00     3305     4771\r\nmonthNov                             10.77 1.00     3580     4658\r\nmonthDec                             -1.37 1.00     4020     5232\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nsigma     4.64      0.31     4.08     5.29 1.00     6787     5839\r\n\r\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n\r\nGiven that for work-life balance model the posterior distribution of\r\npandemic term crosses the zero value, it would be useful to know how\r\nstrong is the evidence in the favor of hypothesis that pandemic term is\r\nlower than zero. For that purpose we can extract posterior samples and\r\nuse them for calculation of the proportion of values that are\r\nlarger/smaller than zero. The resulting proportions show that the vast\r\nmajority (around 92%) of posterior distribution lies below zero.\r\n\r\n\r\nShow code\r\n\r\n# extracting posterior samples\r\nsamples <- brms::posterior_samples(modelWorkLifeBalance, seed = 12345)\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being lower than 0\r\nsum(samples$b_pandemicBeforethepandemicoutbreak < 0) / nrow(samples)\r\n\r\n\r\n[1] 0.920875\r\n\r\nShow code\r\n\r\n# probability of b_pandemicBeforethepandemicoutbreak coefficient being higher than 0\r\nsum(samples$b_pandemicBeforethepandemicoutbreak > 0) / nrow(samples)\r\n\r\n\r\n[1] 0.079125\r\n\r\n\r\nAnother option would be to compute Bayes factor that\r\nexpresses degree to which available data favors our hypothesis in\r\ncomparison with the null model corresponding to normal prior\r\ndistribution with the parameters normal(0,50). We can see\r\nthat Bayes factor (Evid.Ratio in the table below) has value\r\nof 11.6 which indicates strong evidence in favor of our hypothesis, in\r\nterms of Harold\r\nJeffreys‚Äô scale for interpretation of Bayes factors. Specifically it\r\nmeans that under the fitted model for work-life balance the hypothesis\r\nabout pandemic term being lower than zero is 11.6 times more probable\r\nthan under the null model. In other words, the data should shift our\r\nbelieve pretty strongly in direction of acceptance of the existence of\r\npandemic‚Äôs positive effect on people‚Äôs search interest in work-life\r\nbalance.\r\n\r\n\r\nShow code\r\n\r\n# computing Bayes factor for hypothesis that pandemicBeforethepandemicoutbreak term in the modelWorkLifeBalance is smaller than zero  \r\nbrms::hypothesis(\r\n  modelWorkLifeBalance,\r\n  \"pandemicBeforethepandemicoutbreak < 0\",\r\n  seed = 12345\r\n  )\r\n\r\n\r\nHypothesis Tests for class b:\r\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper\r\n1 (pandemicBeforeth... < 0   -14.01      9.97   -30.28     2.57\r\n  Evid.Ratio Post.Prob Star\r\n1      11.64      0.92     \r\n---\r\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\r\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\r\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\r\nPosterior probabilities of point hypotheses assume equal prior probabilities.\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting prior and posterior distribution for pandemic term in the modelWorkLifeBalance is smaller than zero   \r\nplot(\r\n  brms::hypothesis(\r\n    modelWorkLifeBalance,\r\n    \"pandemicBeforethepandemicoutbreak < 0\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\nGraph 6: Visualization of prior and posterior distribution of\r\npandemic term in the modelWorkLifeBalance.\r\n\r\nBesides the major hypothesis we may be also interested in‚Ä¶\r\noverall trend across the last ten years,\r\ntrend after the outbreak of pandemic, and also\r\nseasonality of people‚Äôs search interest over the months within\r\nindividual years.\r\nFor that purpose we might use, besides the summary statistics and\r\ngraphs with posterior distributions depicted above, plots showing\r\nconditional effect of each of the predictors. From the plots in Graph 7\r\nit is thus clear that in case of work-life balance searches, the overall\r\ntrend across the last ten years is decreasing, after the outbreak of\r\npandemic the trend is rather stagnating, and within individual years the\r\nsearch interest follows work & holiday seasonality (lower search\r\ninterest during the holiday parts of the year - June, July, August and\r\nDecember - and higher during the rest of the year). In case of\r\nwell-being searches, the seasonality part is similar, but the trends\r\ndiffer - both of them are increasing (see Graph 8).\r\n\r\n\r\nShow code\r\n\r\n# plotting conditional effect of each predictor from modelWorkLifeBalance \r\nfigListWorkLifeBalance <- plot(brms::conditional_effects(modelWorkLifeBalance), ask = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# uploading library for arranging multiple ggplots\r\nlibrary(ggpubr)\r\n\r\n# putting all graphs with conditional effect together  \r\nggpubr::ggarrange(\r\n  plotlist = figListWorkLifeBalance, \r\n  nrow = 4\r\n  ) %>%\r\n  ggpubr::annotate_figure(\r\n    top = text_grob(\"Conditional effect of predictors from modelWorkLifeBalance\")\r\n)\r\n\r\n\r\n\r\n\r\nGraph 7: Visualization of conditional effect of predictors from\r\nmodelWorkLifeBalance. The predictors are conditioned on the mean in the\r\ncase of continuous variables and reference category in the case of\r\nfactors.\r\n\r\n\r\n\r\nShow code\r\n\r\n# plotting conditional effect of each predictor from modelWellBeing  \r\nfigListWellBeing <- plot(brms::conditional_effects(modelWellBeing), ask = FALSE)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# putting all graphs with conditional effect together  \r\nggpubr::ggarrange(\r\n  plotlist = figListWellBeing, \r\n  nrow = 4\r\n  ) %>% \r\n  ggpubr::annotate_figure(\r\n    top = text_grob(\"Conditional effect of predictors from modelWellBeing\")\r\n)\r\n\r\n\r\n\r\n\r\nGraph 8: Visualization of conditional effect of predictors from\r\nmodelWellBeing. The predictors are conditioned on the mean in the case\r\nof continuous variables and reference category in the case of\r\nfactors.\r\n\r\nIn conclusion, we can say that there is some evidence that the\r\nCOVID-19 pandemic has prompted people to be more interested in topics\r\nrelated to work-life balance and well-being. I wish us all to be able to\r\ntransform our increased interest in these topics into truly increased\r\nquality of our personal and professional lives. It would be a shame not\r\nto use that extra incentive many of us have now for making significant\r\nchange in our lives.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-31-segmentedregression/./wellBeingData.jpeg",
    "last_modified": "2022-11-17T15:59:16+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/",
    "title": "HR analytika a odchodovost zamƒõstnanc≈Ø",
    "description": "Kter√© faktory p≈ôisp√≠vaj√≠ k odchodovosti zamƒõstnanc≈Ø a u kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø je zv√Ω≈°en√© riziko, ≈æe firmu bƒõhem nƒõkolika p≈ô√≠≈°t√≠ch mƒõs√≠c≈Ø opust√≠? Na tyto ot√°zky se ƒç√≠m d√°l t√≠m v√≠ce firem sna≈æ√≠ odpovƒõdƒõt pomoc√≠ anal√Ωzy dat o sv√Ωch vlastn√≠ch zamƒõstnanc√≠ch. V tomto ƒçl√°nku se prost≈ôednictv√≠m analytick√©ho n√°stroje R a vizualizaƒçn√≠ho n√°stroje Shiny pod√≠v√°me, jak m≈Ø≈æe b√Ωt tento druh HR analytick√©ho projektu pro firmy u≈æiteƒçn√Ω.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-14",
    "categories": [
      "employee turnover",
      "evidence-based hr",
      "shiny app"
    ],
    "contents": "\r\n\r\nContents\r\nCo je to HR analytika?\r\nOdchodovost zamƒõstnanc≈Ø a jej√≠ prediktory\r\nNa d≈Økazech zalo≈æen√° pro-retenƒçn√≠ opat≈ôen√≠\r\n\r\nCo je to HR analytika?\r\nHR analytika ve sv√© podstatƒõ vych√°z√≠ ze zn√°m√© z√°sady managementu, ≈æe co nelze mƒõ≈ôit, nelze ani (efektivnƒõ) ≈ô√≠dit a zlep≈°ovat, a aplikuje tuto z√°sadu na lidsk√© zdroje. V nƒõkolika posledn√≠ch letech potom k tomu nav√≠c p≈ôid√°v√° nadstavbu v podobƒõ pokroƒçilej≈°√≠ch analytick√Ωch postup≈Ø, kter√© maj√≠ vƒõt≈°√≠ potenci√°l p≈ôij√≠t s hlub≈°√≠mi vhledy a s doporuƒçen√≠mi s vƒõt≈°√≠m efektem. Ale a≈• u≈æ vyu≈æ√≠v√°te pouze z√°kladn√≠ reporting nebo nƒõjakou pokroƒçilej≈°√≠ analytiku, c√≠l je v≈ædy stejn√Ω ‚Äì sna≈æit se s pomoc√≠ dat a jejich anal√Ωzy ≈æ√°douc√≠m zp≈Øsobem ovlivnit jednotliv√© HR procesy, kter√© organizac√≠m pom√°haj√≠ dosahovat jejich strategick√Ωch c√≠l≈Ø. N√°zornƒõ to ilustruje n√≠≈æe uveden√© sch√©ma zachycuj√≠c√≠ mechanismus propojuj√≠c√≠ HR procesy s (nejen) finanƒçn√≠mi v√Ωsledky organizace (Paauwe & Richardson, 1997).\r\n\r\nHR analytika pom√°h√° optimalizovat nastaven√≠ tohoto mechanismu t√≠m, ≈æe umo≈æ≈àuje nal√©zat odpovƒõdi na nƒõkter√© kl√≠ƒçov√© ot√°zky, jako nap≈ô.:\r\nKter√Ωmi kan√°ly se k n√°m dost√°vaj√≠ ti nejlep≈°√≠ kandid√°ti?\r\nJak√© charakteristiky od sebe odli≈°uj√≠ √∫spƒõ≈°n√© a ne√∫spƒõ≈°n√© kandid√°ty?\r\nKter√© faktory p≈ôisp√≠vaj√≠ k √∫spƒõ≈°n√©mu onboardingu?\r\nKter√° ‚Äûk√°p√©√≠ƒçka‚Äú maj√≠ nejsilnƒõj≈°√≠ vazbu na finanƒçn√≠ v√Ωsledky firmy?\r\nJak√© tr√©ninky vedou s nejvy≈°≈°√≠ pravdƒõpodobnost√≠ ke zlep≈°en√≠ pracovn√≠ho v√Ωkonu?\r\nKter√© intervence maj√≠ nejvƒõt≈°√≠ dopad na zamƒõstnanci poci≈•ovan√Ω well-being nebo work-life balance?\r\nCo u zamƒõstnanc≈Ø zvy≈°uje, nebo naopak sni≈æuje m√≠ru jejich anga≈æovanosti?\r\nKde se v organizaci nach√°z√≠ izolovan√° sila a √∫zk√° hrdla znemo≈æ≈àuj√≠c√≠ efektivn√≠ komunikaci a spolupr√°ci mezi jednotliv√Ωmi zamƒõstnanci, t√Ωmy nebo i cel√Ωmi oddƒõlen√≠mi?\r\nKdo p≈ôedstavuje skryt√Ω talent, kter√Ω je pot≈ôeba podchytit a d√°le rozv√≠jet?\r\nKde lze oƒçek√°vat odpor v souvislosti s pl√°novan√Ωmi zmƒõnami ve firmƒõ a kdo naopak m≈Ø≈æe b√Ωt jejich ambasadorem a katalyz√°torem?\r\nKter√© faktory p≈ôisp√≠vaj√≠ k odchodovosti zamƒõstnanc≈Ø a u kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø je zv√Ω≈°en√© riziko, ≈æe firmu bƒõhem nƒõkolika p≈ô√≠≈°t√≠ch mƒõs√≠c≈Ø opust√≠?\r\nOdchodovost zamƒõstnanc≈Ø a jej√≠ prediktory\r\nPr√°vƒõ poslednƒõ jmenovan√Ω zp≈Øsob vyu≈æit√≠ HR analytiky ƒçasto p≈ôedstavuje jeden z prvn√≠ch druh≈Ø HR analytick√Ωch projekt≈Ø, kter√Ωmi se ve firm√°ch s HR analytikou zaƒç√≠n√°, a to z dob≈ôe pochopiteln√©ho d≈Øvodu. S ne≈æ√°douc√≠mi odchody zamƒõstnanc≈Ø jsou toti≈æ spojen√© vysok√© p≈ô√≠m√© i nep≈ô√≠m√© n√°klady, tak≈æe i pomƒõrnƒõ m√≠rn√© sn√≠≈æen√≠ odchodovosti zamƒõstnanc≈Ø m≈Ø≈æe p≈ôedstavovat znaƒçnou √∫sporu, kterou ocen√≠ management ka≈æd√© firmy. Nal√©havost tohoto probl√©mu nav√≠c je≈°tƒõ zvy≈°uje souƒçasn√° f√°ze ekonomick√©ho cyklu s rekordnƒõ n√≠zkou m√≠rou nezamƒõstnanosti, kter√° v kombinaci s r≈Øzn√Ωmi on-line platformami na zprost≈ôedkov√°n√≠ pr√°ce motivuje mnoho lid√≠ k hled√°n√≠ nov√©ho m√≠sta, kde, jak doufaj√≠, bude pr√°ce zaj√≠mavƒõj≈°√≠, smysluplnƒõj≈°√≠ a l√©pe placen√° a kde kolegov√© budou sympatiƒçtƒõj≈°√≠ a ≈°√©fov√© inspirativnƒõj≈°√≠. Viz tak√© graf n√≠≈æe, kter√Ω na datech z USA n√°zornƒõ dokl√°d√° tƒõsnost vztahu mezi m√≠rou nezamƒõstnanosti a m√≠rou dobrovoln√© odchodovosti zamƒõstnanc≈Ø (r = -0,95, p < 0,001 ).\r\n\r\n\r\n\r\nVzhledem k palƒçivosti tohoto probl√©mu, kter√Ω tr√°p√≠ nejednu firmu, nen√≠ ≈æ√°dn√Ωm velk√Ωm p≈ôekvapen√≠m, ≈æe se t√©matu odchodovosti zamƒõstnanc≈Ø vƒõnovalo a st√°le vƒõnuje velk√© mno≈æstv√≠ r≈Øzn√Ωch studi√≠. Takto nap≈ô. na konci roku 2017 vy≈°la rozs√°hl√° meta-anal√Ωza od autor≈Ø Rubensteina, Eberlyov√© a Leeho, kte≈ô√≠ syntetizovali v√Ωsledky v√≠ce ne≈æ 300 d√≠lƒç√≠ch v√Ωzkum≈Ø t√Ωkaj√≠c√≠ch se prediktor≈Ø odchodovosti. M≈Ø≈æeme se tak opr√°vnƒõnƒõ pt√°t, co nov√©ho n√°m m≈Ø≈æe p≈ôin√©st HR analytika zamƒõ≈ôen√° na odchodovost zamƒõstnanc≈Ø realizovan√° pouze v jedin√© organizaci. Nebylo v≈°e podstatn√© k tomuto t√©matu ji≈æ objeveno? (K t√©to ot√°zce viz nap≈ô. tento inspirativn√≠ a trochu provokativn√≠ ƒçl√°nek od Thomase Rasmussena.) Je pravda, ≈æe nen√≠ p≈ô√≠li≈° pravdƒõpodobn√©, ≈æe p≈ôi anal√Ωze va≈°ich vlastn√≠ch dat naraz√≠te na nƒõjak√Ω naprosto nov√Ω faktor souvisej√≠c√≠ s odchodovost√≠. Na druhou stranu je rovnƒõ≈æ pravda, ≈æe ka≈æd√° organizace je v nƒõƒçem jedineƒçn√°, tak≈æe nƒõkter√© z retenƒçn√≠ch faktor≈Ø pro danou organizaci budou pravdƒõpodobnƒõ v√≠ce a jin√© m√©nƒõ d≈Øle≈æit√©. Tato informace o relativn√≠ d≈Øle≈æitosti jednotliv√Ωch retenƒçn√≠ch faktor≈Ø je potom kl√≠ƒçov√° p≈ôi nastavov√°n√≠ retenƒçn√≠ho pl√°nu a HR analytika m≈Ø≈æe b√Ωt p≈ôi tomto velice n√°pomocn√°.\r\nNa d≈Økazech zalo≈æen√° pro-retenƒçn√≠ opat≈ôen√≠\r\nS pomoc√≠ tohoto dashboardu - vytvo≈ôen√©ho prost≈ôednictv√≠m analytick√©ho n√°stroje R a vizualizaƒçn√≠ho n√°stroje Shiny a za vyu≈æit√≠ uk√°zkov√Ωch dat od spoleƒçnosti IBM - si m≈Ø≈æete sami vyzkou≈°et, jak u≈æiteƒçn√© by pro V√°s mohly b√Ωt v√Ωstupy z takov√©ho HR analytick√©ho projektu zamƒõ≈ôen√©ho na odchodovost zamƒõstnanc≈Ø. Dashboard obsahuje informace, kter√© pom√°haj√≠ (nejen) managementu zodpovƒõdƒõt ≈ôadu kl√≠ƒçov√Ωch ot√°zek, kter√© stoj√≠ na poƒç√°tku ka≈æd√©ho √∫ƒçinn√©ho pl√°nu na retenci zamƒõstnanc≈Ø, jako nap≈ô.:\r\nKolik zamƒõstnanc≈Ø n√°s roƒçnƒõ opou≈°t√≠?\r\nKter√© skupiny zamƒõstnanc≈Ø odch√°zej√≠ nejƒçastƒõji?\r\nJak√Ω je extern√≠ benchmark? Jsme na tom podobƒõ jako konkurence v oboru?\r\nP≈ôedstavuje pro n√°s st√°vaj√≠c√≠ √∫rove≈à odchodovosti z√°va≈æn√Ω probl√©m, a vyplat√≠ se n√°m ho tedy ≈ôe≈°it?\r\nZ jak√Ωch d≈Øvod≈Ø lid√© obecnƒõ nejƒçastƒõji odch√°zej√≠ ze zamƒõstn√°n√≠?\r\nJak√© faktory p≈ôisp√≠vaj√≠ k odchodu specificky na≈°ich zamƒõstnanc≈Ø?\r\nJak√° pro-retenƒçn√≠ opat≈ôen√≠ jsou obecnƒõ k dispozici?\r\nJak√° pro-retenƒçn√≠ opat≈ôen√≠ bychom mƒõli zvolit vzhledem k pravdƒõpodobn√Ωm d≈Øvod≈Øm odchod≈Ø na≈°ich zamƒõstnanc≈Ø?\r\nNa jak√© skupiny zamƒõstnanc≈Ø se p≈ôedev≈°√≠m zamƒõ≈ôit z hlediska prevence jejich odchodovosti?\r\nU kter√Ωch konkr√©tn√≠ch zamƒõstnanc≈Ø existuje zv√Ω≈°en√© riziko, ≈æe odejdou, a na jak√© konkr√©tn√≠ retenƒçn√≠ faktory se u nich zamƒõ≈ôit v r√°mci pravideln√©ho stay interview?\r\nJak je z v√Ω≈°e uveden√©ho v√Ωƒçtu ot√°zek patrn√©, dashboard obsahuje informace, kter√© p≈ôi sv√©m rozhodov√°n√≠ mohou vyu≈æ√≠t nejen HR mana≈æe≈ôi, ale tak√© HR business partne≈ôi nebo p≈ô√≠mo team-leade≈ôi a liniov√≠ mana≈æe≈ôi jednotliv√Ωch t√Ωm≈Ø ƒçi oddƒõlen√≠. Kromƒõ toho dashboard obsahuje tak√© ≈ôadu technick√Ωch detail≈Ø o pou≈æit√©m predikƒçn√≠m modelu a samotn√° data, kter√© stoj√≠ v pozad√≠ v≈°ech prezentovan√Ωch vizualizac√≠ a anal√Ωz. S jejich pomoc√≠ tak HR/Business analytik m≈Ø≈æe nap≈ô. hledat optim√°ln√≠ zp≈Øsob, jak nastavit sk√≥rovac√≠ algoritmus, aby se maximalizoval pozitivn√≠ efekt pro-retenƒçn√≠ch opat≈ôen√≠, nebo m≈Ø≈æe v dostupn√Ωch datech s√°m hledat nƒõjak√© dal≈°√≠ u≈æiteƒçn√© informace. V√≠ce viz ji≈æ samotn√Ω dashboard, z nƒõho≈æ m≈Ø≈æete n√≠≈æe vidƒõt nƒõkolik screenshot≈Ø.\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje r≈Øzn√© ≈ôezy odchodovost√≠ zamƒõstnanc≈Ø, a d√°v√° tak dobr√Ω p≈ôehled o tom, kter√© skupiny zamƒõstnanc≈Ø jsou odchodovost√≠ nejv√≠ce ohro≈æen√©.\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje informace o pravdƒõpodobnosti odchodu jednotliv√Ωch zamƒõstnanc≈Ø spoleƒçnƒõ s dal≈°√≠mi informacemi, kter√© mohou poslou≈æit jako podklad pro individu√°ln√≠ intervence s c√≠lem p≈ôedej√≠t ne≈æ√°douc√≠m odchod≈Øm zamƒõstnanc≈Ø.\r\n\r\nScreenshot ƒç√°sti dashboardu, kter√° obsahuje informace o v√Ωkonu/kvalitƒõ statistick√©ho modelu pou≈æit√©ho k identifikaci v√Ωznamn√Ωch prediktor≈Ø odchodovosti zamƒõstnanc≈Ø a k odhadu pravdƒõpodobnosti odchodu jednotliv√Ωch zamƒõstnanc≈Ø.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-14-hr-analytika-a-odchodovost-zamstnanc/./Types-of-Employee-Attrition.png",
    "last_modified": "2023-04-11T20:14:01+02:00",
    "input_file": {},
    "preview_width": 860,
    "preview_height": 396
  },
  {
    "path": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/",
    "title": "Moneyball v HR",
    "description": "P≈ôes popularitu t√©matu HR analytiky mezi HR profesion√°ly je st√°le relativnƒõ m√°lo spoleƒçnost√≠, kter√© HR analytiku re√°lnƒõ a systematicky vyu≈æ√≠vaj√≠. Jednou z mo≈æn√Ωch p≈ô√≠ƒçin je to, ≈æe tradiƒçn√≠ HR mnohdy postr√°d√° analytick√Ω mindset a nƒõkter√© z kompetenc√≠, kter√© jsou kl√≠ƒçov√© pro √∫spƒõ≈°nou realizaci HR analytick√Ωch projekt≈Ø. V takov√© situaci m≈Ø≈æe b√Ωt u≈æiteƒçn√© pod√≠vat se ve vƒõt≈°√≠m detailu na celkovou logiku i na konkr√©tn√≠ analytick√© kroky nƒõjak√©ho √∫spƒõ≈°n√©ho p≈ô√≠kladu vyu≈æit√≠ HR analytiky k optimalizaci nƒõkter√©ho z HR proces≈Ø s pozitivn√≠m dopadem na obchodn√≠ v√Ωsledky spoleƒçnosti. V tomto ƒçl√°nku se t√≠mto zp≈Øsobem pod√≠v√°me na zn√°m√Ω p≈ô√≠bƒõh oaklandsk√©ho baseballov√©ho t√Ωmu \"√Åƒçek\", jeho≈æ management pomƒõrnƒõ radik√°lnƒõ - a podle v≈°eho i √∫spƒõ≈°nƒõ - p≈ôehodnotil sv≈Øj dosavadn√≠ p≈ô√≠stup k v√Ωbƒõru nov√Ωch hr√°ƒç≈Ø na z√°kladƒõ v√Ωstup≈Ø statistick√© anal√Ωzy sabermetrick√Ωch dat o hern√≠m chov√°n√≠ hr√°ƒç≈Ø. Vyu≈æijeme p≈ôi tom volnƒõ dostupn√Ω statistick√Ω software R a ve≈ôejnƒõ dostupnou datab√°zi historick√Ωch √∫daj≈Ø o v√Ωsledc√≠ch v americk√© baseballov√© lize.",
    "author": [
      {
        "name": "Ludƒõk Stehl√≠k",
        "url": "https://www.linkedin.com/in/ludekstehlik/"
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "employee selection",
      "correlation analysis",
      "multivariate regression analysis",
      "structural equation modeling",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\n1.\r\nkrok: Zaƒç√≠t od konce aneb strategick√Ω r√°mec HR analytiky\r\n2. krok: Definice\r\nprobl√©mu a kvantifikace c√≠le\r\n3. krok: Kladen√≠ ot√°zek a\r\nmƒõ≈ôen√≠\r\n4. krok: Kladen√≠\r\ndal≈°√≠ch ot√°zek a dal≈°√≠ mƒõ≈ôen√≠\r\n5.\r\nkrok: Propojen√≠ d√≠lƒç√≠ch vhled≈Ø aneb organizace jako stroj\r\n6. krok:\r\nIntervence\r\nOmezen√≠ HR analytiky\r\nZ√°vƒõr\r\n\r\nHR analytika u≈æ dnes nen√≠ ve svƒõtƒõ HR ≈æ√°dnou horkou novinkou. T√©mƒõ≈ô\r\nv≈°ichni z oboru u≈æ o HR analytice nƒõco sly≈°eli, nƒõco o n√≠ vƒõd√≠ a\r\np≈ô√≠padnƒõ se j√≠ u≈æ tak√© pokou≈°√≠ ve sv√Ωch organizac√≠ch v nƒõjak√© podobƒõ\r\nzav√°dƒõt. Z√°rove≈à vƒõt≈°inou uzn√°vaj√≠ jej√≠ d≈Øle≈æitost p≈ôi transformaci HR z\r\npodp≈Ørn√© a administrativn√≠ funkce na funkci, kter√° dok√°≈æe organizac√≠m\r\nbezprost≈ôednƒõ pom√°hat dosahovat jejich strategick√Ωch c√≠l≈Ø. Navzdory\r\ntomuto v≈°eobecn√©mu povƒõdom√≠ o HR analytice a navzdory ≈ôadƒõ √∫spƒõ≈°nƒõ\r\nrealizovan√Ωch HR analytick√Ωch projekt≈Ø (viz nap≈ô. s√©rie ƒçl√°nk≈Ø od Davida Greena - ƒçl√°nek\r\n1, ƒçl√°nek\r\n2, ƒçl√°nek\r\n3, ƒçl√°nek\r\n4) p≈ôekvapivƒõ m√°lo organizac√≠ HR analytiku re√°lnƒõ a systematicky\r\nvyu≈æ√≠v√°. Tento stav reflektuj√≠ i v√Ωsledky v√Ωzkumu 2018\r\nHuman Capital Trends od spoleƒçnosti Deloitte, ze kter√Ωch vypl√Ωv√°, ≈æe\r\norganizace si vƒõt≈°inou uvƒõdomuj√≠ strategickou d≈Øle≈æitost v√Ωzvy, kterou\r\np≈ôedstavuje datifikace HR, z√°rove≈à se ale nec√≠t√≠ b√Ωt na ƒçelen√≠ t√©to\r\nv√Ωzvƒõ p≈ô√≠li≈° dob≈ôe p≈ôipraveny. U≈æ nƒõjakou dobu plat√≠, ≈æe kdy≈æ u≈æ se v\r\norganizaci s HR daty nƒõjak pracuje, tak je to vƒõt≈°inou pouze na √∫rovni\r\nnƒõjak√©ho z√°kladn√≠ho reportingu vybran√Ωch HR metrik a KPIs typu n√°klady\r\nna n√°bor, d√©lka obdob√≠ neobsazenosti voln√© pracovn√≠ pozice, m√≠ra\r\nne/dobrovoln√© odchodovosti zamƒõstnanc≈Ø, poƒçet zamƒõstnanc≈Ø na jednoho HR\r\nbusiness partnera apod. Slabinou tohoto p≈ô√≠stupu je, ≈æe takto sledovan√©\r\nmetriky jsou ƒçasto relevantn√≠ pouze pro monitorov√°n√≠ a ≈ô√≠zen√≠\r\nefektivnosti HR coby n√°kladov√©ho st≈ôediska, ale ji≈æ m√©nƒõ pro dosahov√°n√≠\r\nstrategick√Ωch c√≠l≈Ø organizace. Sp√≠≈°e v√Ωjimeƒçnƒõ se potom v tomto kontextu\r\nvyu≈æ√≠vaj√≠ nƒõjak√© pokroƒçilej≈°√≠ analytiky, kter√© obecnƒõ maj√≠ vƒõt≈°√≠\r\npotenci√°l p≈ôich√°zet s doporuƒçen√≠mi s p≈ô√≠m√Ωm dopadem na schopnost\r\norganizac√≠ dosahovat sv√Ωch strategick√Ωch c√≠l≈Ø.\r\nV√Ωsledky v√Ωzkumu\r\nproveden√©ho spoleƒçnostmi MIT Sloan Management Review a SAS\r\nnaznaƒçuj√≠, ≈æe tento nevyu≈æit√Ω potenci√°l HR analytiky m√° dvƒõ hlavn√≠\r\np≈ô√≠ƒçiny. Prvn√≠ z nich je to, ≈æe tradiƒçn√≠ HR mnohdy postr√°d√° analytick√Ω\r\nmindset a nƒõkter√© z kompetenc√≠, kter√© jsou kl√≠ƒçov√© pro √∫spƒõ≈°nou\r\nrealizaci HR analytick√Ωch projekt≈Ø (p≈ôehled tƒõchto kompetenc√≠ a d≈Øsledk≈Ø\r\njejich absence ƒçi nedostateƒçn√© √∫rovnƒõ viz nap≈ô. tento\r\nƒçl√°nek od Mortena Kamp\r\nAndersena). Ve stejn√©m duchu Josh Bersin ve sv√© zpr√°vƒõ\r\nHR\r\nTechnology Disruptions for 2018 konstatuje, ≈æe zvl√°dnut√≠ z√°kladn√≠ch\r\nanalytick√Ωch dovednost√≠ pat≈ô√≠ mezi nejd≈Øle≈æitƒõj≈°√≠ prediktory efektivn√≠\r\nimplementace HR analytiky v organizac√≠ch: ‚ÄúEquip all HR staff with\r\nbasic data literacy skills. All HR practitioners should know basic\r\nstatistical concepts, where to find data, how to slice and dice it, how\r\nto read a dashboard, and how to bring data and analytics to bear on\r\nbusiness issues. Our research reveals that such basic skills are among\r\nthe most important predictors of high-performing people\r\nanalytics.‚Äù\r\nDruhou hlavn√≠ p≈ô√≠ƒçinou je potom to, ≈æe HR analytick√© projekty\r\nneb√Ωvaj√≠ ukotveny v r√°mci nƒõjak√© ≈°ir≈°√≠ strategie, jak data systematicky\r\nvyu≈æ√≠vat p≈ôi ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø, nav√≠c zp≈Øsobem, kter√Ω by byl\r\nsladƒõn√Ω se strategick√Ωmi c√≠li spoleƒçnosti. Zde plat√≠ prax√≠ osvƒõdƒçen√°\r\npravda projektov√©ho managementu, ≈æe p≈ôi implementaci projekt≈Ø je pot≈ôeba\r\nv≈ædy zaƒç√≠nat od konce. V kontextu HR analytick√Ωch projekt≈Ø to tedy\r\nznamen√° zaƒç√≠nat nikoli od dat, ale od toho, k ƒçemu maj√≠ b√Ωt HR\r\nanalytick√© v√Ωstupy pou≈æity. A oƒçek√°v√°n√≠ managementu je, ≈æe HR analytika\r\nbude v posledku hlavnƒõ pom√°hat zlep≈°ovat obchodn√≠ v√Ωsledky spoleƒçnosti.\r\nN√°zornƒõ to ilustruje n√≠≈æe uveden√© sch√©ma (p≈ôevzat√© z ƒçl√°nku\r\nMaxe Blumberga),\r\nkter√© zachycuje p≈ôedpokl√°dan√Ω kauz√°ln√≠ ≈ôetƒõzec spojuj√≠c√≠ HR procesy s\r\nobchodn√≠mi v√Ωsledky. √ökolem HR analytiky je potom s pomoc√≠ dat a\r\nanalytick√Ωch n√°stroj≈Ø tyto dvƒõ oblasti propojit a zjistit, jak\r\noptimalizac√≠ prvn√≠ho zajistit zlep≈°en√≠ toho druh√©ho.\r\n\r\n≈òadƒõ organizac√≠ by v tomto ohledu mohl b√Ωt inspirac√≠ zn√°m√Ω p≈ô√≠bƒõh oaklandsk√©ho\r\nbaseballov√©ho t√Ωmu ‚Äû√Åƒçek‚Äú, kter√Ω se stal p≈ôedlohou pro knihu Moneyball a z\r\nn√≠ vych√°zej√≠c√≠ stejnojmenn√Ω film.\r\nPr√°vƒõ tento p≈ô√≠bƒõh jako jeden z prvn√≠ch uk√°zal a mezi ≈°irokou ve≈ôejnost√≠\r\nzpopularizoval mo≈ænosti vyu≈æit√≠ statistick√© anal√Ωzy ve svƒõtƒõ sportu a\r\npota≈æmo tak√© v r√°mci ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø. D√≠ky radik√°ln√≠ zmƒõnƒõ\r\ndosavadn√≠ho p≈ô√≠stupu k v√Ωbƒõru nov√Ωch hr√°ƒç≈Ø, kter√Ω se zaƒçal v√≠ce op√≠rat o\r\nv√Ωstupy statistick√© anal√Ωzy sabermetrick√Ωch\r\ndat o hern√≠m chov√°n√≠ hr√°ƒç≈Ø, dok√°zal management oaklandsk√©ho\r\nbaseballov√©ho t√Ωmu ‚Äû√Åƒçek‚Äú p≈ôij√≠mat rozhodnut√≠, kter√° z jednoho z\r\nnejchud≈°√≠ch t√Ωm≈Ø americk√© baseballov√© ligy uƒçinila jeden z\r\nnej√∫spƒõ≈°nƒõj≈°√≠ch t√Ωm≈Ø soutƒõ≈æe (mƒõ≈ôeno poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti\r\nsoutƒõ≈æe a poƒçtem postup≈Ø do play-off). Abychom mohli tento p≈ô√≠bƒõh plnƒõ\r\nvytƒõ≈æit coby inspiraci, jak analyzovat sv√° vlastn√≠ zamƒõstnaneck√° data,\r\nbude u≈æiteƒçn√©, kdy≈æ se na jednotliv√© analytick√© kroky, kter√© st√°ly v\r\npozad√≠ √∫spƒõchu oklandsk√Ωch ‚Äú√Åƒçek‚Äù, pod√≠v√°me trochu podrobnƒõji. A uƒçin√≠me\r\ntak za vyu≈æit√≠ volnƒõ dostupn√©ho statistick√©ho softwaru R a ve≈ôejnƒõ\r\ndostupn√© datab√°ze\r\nhistorick√Ωch √∫daj≈Ø o v√Ωsledc√≠ch v americk√© baseballov√© lize.\r\n1.\r\nkrok: Zaƒç√≠t od konce aneb strategick√Ω r√°mec HR analytiky\r\nJak bylo uvedeno v√Ω≈°e, ƒçasto podce≈àovan√Ωm krokem p≈ôi zav√°dƒõn√≠ HR\r\nanalytiky do firem a organizac√≠ je zasazen√≠ HR analytiky do nƒõjak√©ho\r\n≈°ir≈°√≠ho strategick√©ho r√°mce, ze kter√©ho by jasnƒõ vypl√Ωvalo, ƒçemu m√°\r\nvlastnƒõ HR analytika slou≈æit. HR analytika je pouze n√°stroj, konkr√©tnƒõ\r\nn√°stroj na zodpov√≠d√°n√≠ ot√°zek, resp. na testov√°n√≠ r≈Øzn√Ωch hypot√©z. To,\r\nzda bude tento n√°stroj u≈æiteƒçn√Ω, z√°vis√≠ na tom, zda si dok√°≈æeme kl√°st ty\r\nspr√°vn√© ot√°zky. To je p≈ôitom z velk√© ƒç√°sti d√°no t√≠m, zda si jsme vƒõdomi,\r\njak√© jsou strategick√© c√≠le na≈°√≠ organizace. Jen ve svƒõtle tƒõchto c√≠l≈Ø\r\nd√°v√° smysl kl√°st si nƒõjak√© ot√°zky, sb√≠rat a analyzovat nƒõjak√° data za\r\n√∫ƒçelem nalezen√≠ odpovƒõd√≠ na polo≈æen√© ot√°zky a posl√©ze ƒçinit nƒõjak√°\r\nkonkr√©tn√≠ rozhodnut√≠ na z√°kladƒõ nalezen√Ωch odpovƒõd√≠. V p≈ô√≠padƒõ\r\noaklandsk√Ωch ‚Äû√Åƒçek‚Äú byl c√≠l jasn√Ω ‚Äì kvalifikovat se do play-off.\r\n2. krok: Definice\r\nprobl√©mu a kvantifikace c√≠le\r\nPaul\r\nDePodesta, kter√©ho gener√°ln√≠ mana≈æer oaklandsk√Ωch ‚Äû√Åƒçek‚Äú Billy Beane p≈ôijal\r\ndo t√Ωmu jako statistick√©ho analytika, redukoval tento c√≠l na celkem\r\njednoduch√Ω matematick√Ω probl√©m: Kolik z√°pas≈Ø mus√≠ t√Ωm vyhr√°t, aby se\r\nkvalifikoval do play-off? K zodpovƒõzen√≠ t√©to ot√°zky DePodesta pot≈ôeboval\r\nhistorick√° data o poƒçtu v√≠tƒõzstv√≠ jednotliv√Ωch t√Ωm≈Ø v minul√Ωch sez√≥n√°ch\r\na o tom, zda se jim poda≈ôilo postoupit do play-off, ƒçi nikoli.\r\n\r\n\r\nShow code\r\n\r\n# Naƒçtƒõme si knihovnu, kter√° n√°m umo≈æn√≠ si naƒç√≠st a p≈ôedp≈ôipravit data k anal√Ωze a tak√© je i vizualizovat. \r\nlibrary(tidyverse)\r\n\r\n# Naƒçteme si na≈°e data.\r\nbaseball <- read_csv(\"baseball.csv\")\r\n\r\n# Pro z√≠sk√°n√≠ lep≈°√≠ p≈ôedstavy o nich se pod√≠vejme na jejich prvn√≠ch deset ≈ô√°dk≈Ø.\r\nhead(baseball, 10)\r\n\r\n\r\n# A tibble: 10 x 15\r\n   Team  League  Year    RS    RA     W   OBP   SLG    BA Playoffs\r\n   <chr> <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>\r\n 1 ARI   NL      2012   734   688    81 0.328 0.418 0.259        0\r\n 2 ATL   NL      2012   700   600    94 0.32  0.389 0.247        1\r\n 3 BAL   AL      2012   712   705    93 0.311 0.417 0.247        1\r\n 4 BOS   AL      2012   734   806    69 0.315 0.415 0.26         0\r\n 5 CHC   NL      2012   613   759    61 0.302 0.378 0.24         0\r\n 6 CHW   AL      2012   748   676    85 0.318 0.422 0.255        0\r\n 7 CIN   NL      2012   669   588    97 0.315 0.411 0.251        1\r\n 8 CLE   AL      2012   667   845    68 0.324 0.381 0.251        0\r\n 9 COL   NL      2012   758   890    64 0.33  0.436 0.274        0\r\n10 DET   AL      2012   726   670    88 0.335 0.422 0.268        1\r\n# ... with 5 more variables: RankSeason <dbl>, RankPlayoffs <dbl>,\r\n#   G <dbl>, OOBP <dbl>, OSLG <dbl>\r\n\r\nShow code\r\n\r\n# Pro n√°sleduj√≠c√≠ anal√Ωzy si potom vytvo≈ôme podmno≈æinu dat, kter√° mƒõli k dispozici v Oaklandu v roce 2002, kdy se dƒõj Moneyballu p≈ôev√°≈ænƒõ odehr√°v√°.\r\nmoneyball <- baseball %>%\r\n  filter(Year < 2002)\r\n\r\n\r\n\r\nPod√≠v√°me-li se na data mezi lety 1996‚Äì2001, tj. na data z relativnƒõ\r\nned√°vn√© minulosti (vzta≈æeno k roku 2002, kdy se dƒõj Moneyballu p≈ôev√°≈ænƒõ\r\nodehr√°v√°), z grafick√©ho vyj√°d≈ôen√≠ vztahu mezi poƒçtem v√≠tƒõzstv√≠ v\r\nz√°kladn√≠ ƒç√°st√≠ soutƒõ≈æe a postupem do play-off je dob≈ôe patrn√©, ≈æe ƒç√≠m\r\nv√≠ce z√°pas≈Ø t√Ωm vyhraje v z√°kladn√≠ soutƒõ≈æi, t√≠m vƒõt≈°√≠ je ≈°ance, ≈æe se\r\ntak√© dostane do play-off.\r\n\r\n\r\nShow code\r\n\r\n# Vytvo≈ôme si graf zachycuj√≠c√≠ vztah mezi poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a postupem do play-off\r\nmoneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select (W, Playoffs) %>%\r\n  mutate(rnd = runif(176,0,1)) %>%\r\n  ggplot(aes(x = W, y = rnd, color = as.factor(Playoffs)))+\r\n  geom_point(size = 2)+\r\n  scale_x_continuous(limits=c(50,120), breaks = seq(50,120,5))+\r\n  scale_color_manual(values = c(\"#9e9e9e\", \"#ff1919\"), labels = c(\"T√Ωm nepostoupil do play-off\",\"T√Ωm postoupil do play-off\"))+\r\n  ggtitle(\"Postupy t√Ωm≈Ø do play-off mezi lety 1996-2001\")+\r\n  ylab(\"\")+\r\n  xlab(\"Poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe\")+\r\n  theme(legend.position = \"bottom\",\r\n        axis.ticks.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.text.x = element_text(size=11),\r\n        axis.title.x = element_text(size=11),\r\n        legend.text = element_text(size=11),\r\n        legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nS daty, kter√° m√°me k dispozici, m√°me tu v√Ωhodu, ≈æe m≈Ø≈æeme vztah mezi\r\npoƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a ≈°anc√≠ na postup do play-off\r\np≈ôesnƒõ kvantifikovat. Provedeme-li podrobnƒõj≈°√≠ anal√Ωzu na≈°ich dat, uk√°≈æe\r\nse, ≈æe velkou (p≈ôibli≈ænƒõ 95%) ≈°anci na postup do play-off m√° t√Ωm tehdy,\r\nkdy≈æ v z√°kladn√≠ ƒç√°sti vyhraje minim√°lnƒõ 95 z√°pas≈Ø. Tƒõchto 95 v√≠tƒõzstv√≠\r\np≈ôedstavuje dob≈ôe definovan√Ω a kvantifikovan√Ω c√≠l, kter√©ho by se\r\noaklandsk√° ‚Äû√Åƒçka‚Äú mƒõla sna≈æit dos√°hnout.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si opƒõt data mezi lety 1996-2001. \r\nmoneyball2 <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995)\r\n\r\n# Vytvo≈ôme si seznam nƒõkolika r≈Øzn√Ωch hodnot poƒçtu v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe.\r\npocet_vitezstvi <- seq(60,115,5)\r\nucast_v_playoff <- vector(mode=\"numeric\", length=length(pocet_vitezstvi))\r\nplayoff_data <- data.frame(pocet_vitezstvi = pocet_vitezstvi, ucast_v_playoff = ucast_v_playoff)\r\n\r\n# Vypoƒçtƒõme si, jak√° je pravdƒõpodobnost postupu do play-off p≈ôi r≈Øzn√©m poƒçtu v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe.\r\nfor(i in 1:nrow(playoff_data)){\r\n playoff_data$ucast_v_playoff[i] <- length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i] & moneyball2$Playoffs == 1])/length(moneyball2$W[moneyball2$W >= playoff_data$pocet_vitezstvi[i]]) \r\n}\r\n\r\n# A nyn√≠ si vztah mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a pravdƒõpodobnost√≠ √∫ƒçasti v play-off vizualizujme.\r\nggplot(playoff_data, aes(x = pocet_vitezstvi, y = ucast_v_playoff))+\r\n  geom_point(size = 2)+\r\n  geom_line()+\r\n  ggtitle(\"Souvislost mezi poƒçtem v√Ωher v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a\\npravdƒõpodobnost√≠ postupu t√Ωmu do play-off (1996-2001)\")+\r\n  ylab(\"Pravdƒõpodobnost postupu t√Ωmu do play-off\")+\r\n  xlab(\"Poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe\")+\r\n  scale_x_continuous(limits=c(60,115), breaks = seq(60,115,5))+\r\n  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,0.1))+\r\n  theme(axis.text = element_text(size=11),\r\n        axis.title = element_text(size=11))\r\n\r\n\r\n\r\n\r\n3. krok: Kladen√≠ ot√°zek a\r\nmƒõ≈ôen√≠\r\nS takto definovan√Ωm a kvantifikovan√Ωm c√≠lem si potom m≈Ø≈æeme kl√°st\r\ndal≈°√≠ch ot√°zky, na kter√© kdy≈æ si dok√°≈æeme odpovƒõdƒõt, zv√Ω≈°√≠me t√≠m na≈°e\r\n≈°ance na to, ≈æe tohoto c√≠le dos√°hneme. V p≈ô√≠padƒõ oaklandsk√Ωch ‚Äû√Åƒçek‚Äú se\r\nm≈Ø≈æeme pt√°t, d√≠ky ƒçemu t√Ωm dosahuje v z√°pasech v√≠tƒõzstv√≠? Celkem zjevn√°\r\nodpovƒõƒè zn√≠, ≈æe d√≠ky tomu, ≈æe dok√°≈æe z√≠skat v√≠ce bod≈Ø ne≈æ jeho soupe≈ôi.\r\nOt√°zkou ale je, p≈ôesnƒõ o kolik bod≈Ø nav√≠c mus√≠ t√Ωm z√≠skat, aby v\r\nz√°kladn√≠ ƒç√°sti soutƒõ≈æe dos√°hl na minim√°lnƒõ 95 v√≠tƒõzstv√≠. K zodpovƒõzen√≠\r\nt√©to ot√°zky opƒõt pot≈ôebujeme historick√° data (√∫daje o vyhran√Ωch a\r\nprohran√Ωch bodech) a relativnƒõ jednoduch√Ω statistick√Ω model zvan√Ω line√°rn√≠\r\nregrese, pomoc√≠ kter√©ho m≈Ø≈æeme popsat vztah mezi poƒçtem vyhran√Ωch\r\nz√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi\r\nbody. Z n√≠≈æe uveden√©ho grafu je z≈ôejm√©, ≈æe mezi tƒõmito dvƒõma promƒõnn√Ωmi\r\nje velice tƒõsn√Ω vztah a ≈æe spolu velice silnƒõ koreluj√≠.\r\n\r\n\r\nShow code\r\n\r\n# Vypoƒçtƒõme si rozd√≠l mezi vyhran√Ωmi a prohran√Ωmi body\r\nmoneyball <- moneyball %>%\r\n  mutate(RD = RS - RA)\r\n\r\n# Graficky si zn√°zornƒõme vztah mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body\r\nlibrary(ggpubr)\r\nggplot(moneyball, aes(x = RD , y = W))+\r\n  geom_point(alpha = 0.5, size = 2)+\r\n  geom_smooth(method = \"lm\", se = FALSE)+\r\n  ggtitle(\"Vztah mezi poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a\\nrozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body\")+\r\n  xlab(\"Rozd√≠l mezi poƒçtem vyhran√Ωch a prohran√Ωch bod≈Ø\")+\r\n  ylab(\"Poƒçet v√≠tƒõzstv√≠\")+\r\n  theme(axis.title = element_text(size = 11),\r\n        axis.text = element_text(size = 11))+\r\n  scale_x_continuous(limits = c(-350,350), breaks = seq(-350,350,50))+\r\n  scale_y_continuous(limits = c(40, 120), breaks = seq(40,120,10))+\r\n  stat_cor(method = \"pearson\", label.x = 175, label.y = 45)\r\n\r\n\r\n\r\n\r\nP≈ôi pou≈æit√≠ modelu line√°rn√≠ regrese m≈Ø≈æeme vztah mezi tƒõmito dvƒõma\r\npromƒõnn√Ωmi popsat trochu podrobnƒõji.\r\n\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem vyhran√Ωch z√°pas≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a rozd√≠lem mezi vyhran√Ωmi a prohran√Ωmi body \r\nreg_model1 <- glm(W ~ RD, data = moneyball, family = \"gaussian\")\r\nsummary(reg_model1)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = W ~ RD, family = \"gaussian\", data = moneyball)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-14.2662   -2.6509    0.1234    2.9364   11.6570  \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 80.881375   0.131157  616.67   <2e-16 ***\r\nRD           0.105766   0.001297   81.55   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 15.51641)\r\n\r\n    Null deviance: 117164  on 901  degrees of freedom\r\nResidual deviance:  13965  on 900  degrees of freedom\r\nAIC: 5037\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nV√Ωsledn√° regresn√≠ rovnice n√°m ≈ô√≠k√°, ≈æe oƒçek√°van√Ω poƒçet v√≠tƒõzstv√≠\r\n= 80.88 + 0.106 x Rozd√≠lov√Ω sk√≥r. Tzn., ≈æe p≈ôi vyrovnan√©m pomƒõru\r\nvyhran√Ωch a prohran√Ωch bod≈Ø m≈Ø≈æeme oƒçek√°vat, ≈æe t√Ωm vyhraje p≈ôibli≈ænƒõ 80\r\nz√°pas≈Ø za sez√≥nu, a ≈æe kdy≈æ se rozd√≠lov√© sk√≥re nav√Ω≈°√≠ o deset bod≈Ø,\r\nm≈Ø≈æeme oƒçek√°vat, ≈æe t√Ωm vyhraje v pr≈Ømƒõru o jeden z√°pas za sez√≥nu nav√≠c.\r\nKl√≠ƒçov√© je ale pro n√°s to, ≈æe s pomoc√≠ t√©to rovnice a s trochou algebry\r\nsi m≈Ø≈æeme jednodu≈°e vypoƒç√≠tat, ≈æe k dosa≈æen√≠ minim√°lnƒõ 95 v√≠tƒõzstv√≠ za\r\nsez√≥nu pot≈ôebuje t√Ωm vyhr√°t p≈ôibli≈ænƒõ o 133 bod≈Ø v√≠ce, ne≈æ kolik jich se\r\nsoupe≈ôi prohraje ((95 - 80.88) / 0.106).\r\n4. krok: Kladen√≠\r\ndal≈°√≠ch ot√°zek a dal≈°√≠ mƒõ≈ôen√≠\r\nT√≠mto zji≈°tƒõn√≠m se n√°≈° c√≠l opƒõt trochu v√≠ce specifikuje a vyvol√°v√°\r\ndal≈°√≠ ot√°zky. Ot√°zka, kter√° se t√©mƒõ≈ô sama nab√≠z√≠, se t√Ωk√° charakteristik\r\nhr√°ƒç≈Ø, kter√© nejl√©pe p≈ôedpov√≠daj√≠ poƒçet vyhran√Ωch a prohran√Ωch bod≈Ø, a\r\nt√≠m tedy tak√© pravdƒõpodobnost postupu t√Ωmu do play-off. DePodesta na\r\nz√°kladƒõ sv√Ωch anal√Ωz zjistil, ≈æe poƒçet vyhran√Ωch bod≈Ø nejtƒõsnƒõji souvis√≠\r\ns procentem p≈ô√≠pad≈Ø, kdy se hr√°ƒç dostane na metu (tzv. On-Base\r\nPercentage - OBP), a to, jak daleko se hr√°ƒç dostane p≈ôi sv√©m odpalu\r\n(tzv. Slugging Percentage - SLG). Analogick√© statistiky pro\r\nt√Ωmy soupe≈ô≈Ø (OOBP a OSLG) potom stejnƒõ dob≈ôe p≈ôedpov√≠daj√≠ poƒçet\r\nprohran√Ωch bod≈Ø. Kdy≈æ vztah mezi tƒõmito promƒõnn√Ωmi pop√≠≈°eme opƒõt pomoc√≠\r\nmodelu line√°rn√≠ regrese, m≈Ø≈æeme se s jeho pomoc√≠ pokusit p≈ôedpovƒõdƒõt,\r\njak si t√Ωm povede p≈ô√≠≈°t√≠ sez√≥nu. Takov√° p≈ôedpovƒõƒè by p≈ôitom mohla b√Ωt\r\npotenci√°lnƒõ velice u≈æiteƒçn√°, proto≈æe na jej√≠m z√°kladƒõ bychom p≈ô√≠padnƒõ\r\nmohli upravit nƒõkter√° sv√° rozhodnut√≠ o koupi nebo prodeji vybran√Ωch\r\nhr√°ƒç≈Ø. Pojƒème tuto p≈ôedpovƒõƒè vytvo≈ôit pro t√Ωm oaklandsk√Ωch ‚Äû√Åƒçek‚Äú pro\r\nsez√≥nu 2002 na z√°kladƒõ dat z let 1962-2001. Z p≈ôedchoz√≠ anal√Ωzy ji≈æ\r\nv√≠me, ≈æe‚Ä¶\r\nPoƒçet v√≠tƒõzstv√≠ = 80.88 + 0.106 x (Poƒçet vyhran√Ωch bod≈Ø - Poƒçet\r\nprohran√Ωch bod≈Ø).\r\nNyn√≠ pot≈ôebujeme urƒçit, jak√Ω bude pravdƒõpodobn√Ω poƒçet vyhran√Ωch a\r\nprohran√Ωch bod≈Ø. Pom≈Ø≈æeme si opƒõt regresn√≠ anal√Ωzou.\r\n\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem vyhran√Ωch bod≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a dvƒõma vybran√Ωmi hr√°ƒçsk√Ωmi/t√Ωmov√Ωmi statistikami \r\nregModel2 = lm(RS ~ OBP + SLG, data=moneyball)\r\nsummary(regModel2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RS ~ OBP + SLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-70.838 -17.174  -1.108  16.770  90.036 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -804.63      18.92  -42.53   <2e-16 ***\r\nOBP          2737.77      90.68   30.19   <2e-16 ***\r\nSLG          1584.91      42.16   37.60   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 24.79 on 899 degrees of freedom\r\nMultiple R-squared:  0.9296,    Adjusted R-squared:  0.9294 \r\nF-statistic:  5934 on 2 and 899 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\n# Regresn√≠ anal√Ωza vztahu mezi mezi poƒçtem prohran√Ωch bod≈Ø v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a dvƒõma vybran√Ωmi hr√°ƒçsk√Ωmi/t√Ωmov√Ωmi statistikami \r\nregModel3 = lm(RA ~ OOBP + OSLG, data=moneyball)\r\nsummary(regModel3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = RA ~ OOBP + OSLG, data = moneyball)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-82.397 -15.178  -0.129  17.679  60.955 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -837.38      60.26 -13.897  < 2e-16 ***\r\nOOBP         2913.60     291.97   9.979 4.46e-16 ***\r\nOSLG         1514.29     175.43   8.632 2.55e-13 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.67 on 87 degrees of freedom\r\n  (812 observations deleted due to missingness)\r\nMultiple R-squared:  0.9073,    Adjusted R-squared:  0.9052 \r\nF-statistic: 425.8 on 2 and 87 DF,  p-value: < 2.2e-16\r\n\r\nS pomoc√≠ regresn√≠ anal√Ωzy jsme zjistili, ≈æe‚Ä¶\r\nPoƒçet vyhran√Ωch bod≈Ø = -804.63 + 2737.77 x OBP + 1584.91 x\r\nSLGPoƒçet prohran√Ωch bod≈Ø = -837.38 + 2913.60 x OOBP + 1514.29 x\r\nOSLG.\r\nSe znalost√≠ hr√°ƒçsk√Ωch/t√Ωmov√Ωch statistik oaklandsk√Ωch ‚Äû√Åƒçek‚Äú za rok\r\n2001 se nyn√≠ m≈Ø≈æeme pokusit p≈ôedpovƒõdƒõt nejd≈ô√≠ve poƒçet vyhran√Ωch a\r\nprohran√Ωch bod≈Ø a potom tak√© p≈ôedpokl√°dan√Ω poƒçet v√≠tƒõzstv√≠ v z√°kladn√≠\r\nƒç√°sti soutƒõ≈æe. P≈ôi formulov√°n√≠ t√©to p≈ôedpovƒõdi vych√°z√≠me z p≈ôedpokladu,\r\n≈æe se slo≈æen√≠ t√Ωmu v pr≈Øbƒõhu sez√≥ny 2002 nebude (nap≈ô. z d≈Øvodu zranƒõn√≠\r\nhr√°ƒç≈Ø) p≈ô√≠li≈° li≈°it od jeho slo≈æen√≠ v roce 2001.\r\n\r\n\r\nShow code\r\n\r\n# Hr√°ƒçsk√©/t√Ωmov√© statistiky oaklandsk√Ωch ‚Äû√Åƒçek‚Äú za rok 2001\r\nOBP_OAK <- moneyball$OBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nSLG_OAK <- moneyball$SLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOOBP_OAK <- moneyball$OOBP[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\nOSLG_OAK <- moneyball$OSLG[which(moneyball$Team==\"OAK\" & moneyball$Year == 2001)]\r\n\r\n# Pravdƒõpodobn√© hodnoty vybran√Ωch statistik oaklandsk√Ωch \"√Åƒçek\" pro rok 2002 vypoƒç√≠tan√© s pomoc√≠ odhadnut√Ωch regresn√≠ch model≈Ø\r\npocet_vyhranych_bodu_pred <- round(-804.63 + 2737.77*OBP_OAK + 1584.91*SLG_OAK)\r\npocet_prohranych_bodu_pred <- round(-837.38 + 2913.60*OOBP_OAK + 1514.29*OSLG_OAK)\r\npocet_vitezstvi_pred <- round(80.88 + 0.106 * (pocet_vyhranych_bodu_pred - pocet_prohranych_bodu_pred), 0)\r\n\r\n# Skuteƒçn√© hodnoty vybran√Ωch statistik oaklandsk√Ωch \"√Åƒçek\" pro rok 2002\r\npocet_vyhranych_bodu_real <- baseball$RS[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_prohranych_bodu_real <- baseball$RA[which(baseball$Team==\"OAK\" & baseball$Year == 2002)] \r\npocet_vitezstvi_real <- baseball$W[which(baseball$Team==\"OAK\" & baseball$Year == 2002)]\r\n\r\n# Tabulka porovn√°vaj√≠c√≠ statistick√© p≈ôedpovƒõdi se skuteƒçnost√≠ \r\npred <- c(pocet_vyhranych_bodu_pred, pocet_prohranych_bodu_pred, pocet_vitezstvi_pred)\r\nreal <- c(pocet_vyhranych_bodu_real, pocet_prohranych_bodu_real, pocet_vitezstvi_real)\r\ntable <- data.frame(\"P≈ôedpovƒõd\" = pred, \"Skuteƒçnost\" = real)\r\nrow.names(table) <- c(\"Vyhran√© body\", \"Prohran√© body\", \"Poƒçet v√≠tƒõzstv√≠\")\r\ntable\r\n\r\n\r\n                P≈ôedpovƒõd Skuteƒçnost\r\nVyhran√© body          836        800\r\nProhran√© body         635        654\r\nPoƒçet v√≠tƒõzstv√≠       102        103\r\n\r\nPorovn√°n√≠ na≈°ich p≈ôedpovƒõd√≠ s re√°ln√Ωmi v√Ωsledky za sez√≥nu 2002\r\nukazuje, ≈æe se n√°m poda≈ôilo velice p≈ôesnƒõ p≈ôedpovƒõdƒõt v√Ωsledky v\r\nnadch√°zej√≠c√≠ ligov√© sez√≥nƒõ, a v√Ωznamnƒõ tak sn√≠≈æit m√≠ru na≈°√≠ nejistoty\r\np≈ôi jej√≠m pl√°nov√°n√≠.\r\n5.\r\nkrok: Propojen√≠ d√≠lƒç√≠ch vhled≈Ø aneb organizace jako stroj\r\nMatt Dancho ve\r\nsv√© metodice k datovƒõ-analytick√Ωm projekt≈Øm doporuƒçuje, abychom se p≈ôi\r\nsnaze o pochopen√≠ obchodn√≠ho probl√©mu organizace na danou organizaci\r\nd√≠vali jako na druh stroje, kter√Ω m√° urƒçit√© vstupy, procesy a v√Ωstupy.\r\nTuto metaforu stroje m≈Ø≈æeme nyn√≠ vyu≈æ√≠t k tomu, abychom v≈°echny v√Ω≈°e\r\nuveden√© d√≠lƒç√≠ vhledy spojili do jednotn√©ho r√°mce. V nƒõm budou m√≠t\r\noaklandsk√° ‚Äú√Åƒçka‚Äù podobu jednoduch√©ho stroje na v√Ωrobu postup≈Ø do\r\nplay-off - viz obr√°zek n√≠≈æe.\r\n\r\nZe sch√©matu je dob≈ôe patrn√©, jak tento stroj funguje: Jeho v√Ωstupy\r\njsou postupy do play-off, kter√Ωch dosahuje tak, ≈æe se sna≈æ√≠ vyhr√°t v√≠ce\r\nz√°pas≈Ø, resp. z√≠skat v√≠ce bod≈Ø ne≈æ soupe≈ô√≠c√≠ t√Ωmy; k tomu vyu≈æ√≠v√° vstupy\r\nv podobƒõ schopnosti hr√°ƒç≈Ø hr√°t dob≈ôe na p√°lce a v poli; vstupem\r\novliv≈àuj√≠c√≠m chod stroje jsou rovnƒõ≈æ obdobn√© schopnosti hr√°ƒç≈Ø\r\nsoupe≈ô√≠c√≠ch t√Ωm≈Ø. Jedn√° se samoz≈ôejmƒõ o velmi zjednodu≈°en√Ω kauz√°ln√≠\r\nmodel fungov√°n√≠ t√Ωmu oakladnsk√Ωch ‚Äú√Åƒçek‚Äù, ale jak konstatuje slavn√Ω\r\nstatistick√Ω aforismus, modely jsou\r\nv≈ædy nep≈ôesn√©, ale nƒõkter√© z nich jsou u≈æiteƒçn√©.\r\nJakkoli na≈°e modely fungov√°n√≠ organizace budou v≈ædy ne√∫pln√©, je\r\nd≈Øle≈æit√© ovƒõ≈ôit, zda tyto modely i p≈ôes svou omezenost v dostateƒçn√© m√≠≈ôe\r\nodr√°≈æej√≠ realitu tak, jak n√°m ji zprost≈ôedkov√°vaj√≠ dostupn√° data. Za\r\nt√≠mto √∫ƒçelem m≈Ø≈æeme pou≈æ√≠t statistickou metodu struktur√°ln√≠ho\r\nmodelov√°n√≠, kter√° umo≈æ≈àuje formalizovat na≈°e p≈ôedstavy o vz√°jemn√Ωch\r\nvztaz√≠ch mezi nƒõkolika r≈Øzn√Ωmi promƒõnn√Ωmi a zhodnotit m√≠ru souladu\r\ntƒõchto na≈°ich p≈ôedstav s dostupn√Ωmi daty. Teprve po takov√©m zhodnocen√≠\r\nvƒõrohodnosti modelu je rozumn√© na nƒõm zakl√°dat sv√° dal≈°√≠ rozhodnut√≠.\r\nPojƒème tedy tuto metodu pou≈æ√≠t rovnƒõ≈æ na n√°≈° novƒõ vytvo≈ôen√Ω model\r\nfungov√°n√≠ t√Ωmu oaklandsk√Ωch ‚Äú√Åƒçek‚Äù a ovƒõ≈ôit m√≠ru jeho vƒõrohodnosti.\r\n\r\n\r\nShow code\r\n\r\n# Data, kter√° budeme pot≈ôebovat pro ovƒõ≈ôen√≠ vƒõrohodnosti na≈°eho modelu fungov√°n√≠ oaklandsk√Ωch \"√Åƒçek\" \r\nsem_data <- moneyball %>%\r\n  filter(Year < 2002 & Year > 1995) %>%\r\n  select(RS, RA, RD, W, Playoffs, OBP, SLG, OOBP, OSLG)\r\n\r\n# Definice modelu, kter√° je v souladu s v√Ω≈°e uveden√Ωm sch√©matem\r\nlibrary(lavaan)\r\noak_model <- '\r\n     Playoffs ~ W\r\n     W ~ RS + RA\r\n     RA ~ OOBP + OSLG\r\n     RS ~ OBP + SLG \r\n'\r\n# Odhad parametr≈Ø modelu\r\nfit_oak_model <- sem(oak_model, data = sem_data, missing = \"pairwise\", estimator = \"WLSMV\", ordered = \"Playoffs\")\r\nsummary(fit_oak_model, standardized = T, fit.measures = T, rsq = T)\r\n\r\n\r\nlavaan 0.6-9 ended normally after 148 iterations\r\n\r\n  Estimator                                       DWLS\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        14\r\n                                                      \r\n                                                  Used       Total\r\n  Number of observations                            90         176\r\n  Number of missing patterns                         1            \r\n                                                                  \r\nModel Test User Model:\r\n                                              Standard      Robust\r\n  Test Statistic                                 8.354      10.470\r\n  Degrees of freedom                                15          15\r\n  P-value (Chi-square)                           0.909       0.789\r\n  Scaling correction factor                                  1.159\r\n  Shift parameter                                            3.260\r\n       simple second-order correction                             \r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                               167.943     150.692\r\n  Degrees of freedom                                 6           6\r\n  P-value                                        0.000       0.000\r\n  Scaling correction factor                                  1.119\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    1.000       1.000\r\n  Tucker-Lewis Index (TLI)                       1.016       1.013\r\n                                                                  \r\n  Robust Comparative Fit Index (CFI)                            NA\r\n  Robust Tucker-Lewis Index (TLI)                               NA\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.000       0.000\r\n  90 Percent confidence interval - lower         0.000       0.000\r\n  90 Percent confidence interval - upper         0.040       0.067\r\n  P-value RMSEA <= 0.05                          0.964       0.903\r\n                                                                  \r\n  Robust RMSEA                                                  NA\r\n  90 Percent confidence interval - lower                     0.000\r\n  90 Percent confidence interval - upper                        NA\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  SRMR                                           0.108       0.108\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                           Robust.sem\r\n  Information                                 Expected\r\n  Information saturated (h1) model        Unstructured\r\n\r\nRegressions:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n  Playoffs ~                                                     \r\n    W                  0.234    0.025    9.181    0.000     0.234\r\n  W ~                                                            \r\n    RS                 0.093    0.006   15.189    0.000     0.093\r\n    RA                -0.094    0.006  -16.031    0.000    -0.094\r\n  RA ~                                                           \r\n    OOBP            3158.695  360.178    8.770    0.000  3158.695\r\n    OSLG            1520.258  213.163    7.132    0.000  1520.258\r\n  RS ~                                                           \r\n    OBP             3621.290  258.284   14.021    0.000  3621.290\r\n    SLG             1418.260  144.885    9.789    0.000  1418.260\r\n  Std.all\r\n         \r\n    0.993\r\n         \r\n    0.682\r\n   -0.726\r\n         \r\n    0.564\r\n    0.452\r\n         \r\n    0.606\r\n    0.425\r\n\r\nIntercepts:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.000                                0.000\r\n   .W                 96.691   11.568    8.358    0.000    96.691\r\n   .RA              -808.808  116.566   -6.939    0.000  -808.808\r\n   .RS             -1041.496   73.943  -14.085    0.000 -1041.496\r\n  Std.all\r\n    0.000\r\n    8.629\r\n   -9.367\r\n  -12.661\r\n\r\nThresholds:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs|t1       22.653    6.998    3.237    0.001    22.653\r\n  Std.all\r\n    8.578\r\n\r\nVariances:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n   .Playoffs           0.101                                0.101\r\n   .W                  7.779    2.306    3.374    0.001     7.779\r\n   .RA               536.111   97.833    5.480    0.000   536.111\r\n   .RS               449.043   80.457    5.581    0.000   449.043\r\n  Std.all\r\n    0.014\r\n    0.062\r\n    0.072\r\n    0.066\r\n\r\nScales y*:\r\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv \r\n    Playoffs           1.000                                1.000\r\n  Std.all\r\n    1.000\r\n\r\nR-Square:\r\n                   Estimate \r\n    Playoffs           0.986\r\n    W                  0.938\r\n    RA                 0.928\r\n    RS                 0.934\r\n\r\n\r\n\r\nShow code\r\n\r\n# Grafick√© zn√°zornƒõn√≠ modelu fungov√°n√≠ oaklandsk√Ωch \"√Åƒçek\" \r\nlibrary(semPlot)\r\nsemPaths(fit_oak_model, \r\n         whatLabels=\"std\", \r\n         intercepts=FALSE, \r\n         style=\"lisrel\",\r\n         nCharNodes=0,\r\n         nCharEdges=0,\r\n         curveAdjacent = TRUE,\r\n         title=TRUE,\r\n         layout=\"tree2\",\r\n         curvePivot=TRUE,\r\n         rotation =3)\r\n\r\n\r\n\r\n\r\nV√Ωstupy proveden√© tzv. pƒõ≈°inkov√©\r\nanal√Ωzy, kter√° je speci√°ln√≠m typem struktur√°ln√≠ho modelov√°n√≠,\r\nnaznaƒçuj√≠, ≈æe n√°mi navr≈æen√Ω model je v souladu s daty, kter√° m√°me k\r\ndispozici (viz ‚Äúp≈ô√≠zniv√©‚Äù hodnoty index≈Ø shody, resp. neshody jako je\r\nTLI a CFI, resp. RMSEA, a tak√© vysok√© hodnoty standardizovan√Ωch\r\nregresn√≠ch koeficient≈Ø). D√°vaj√≠ n√°m tak dobr√Ω d≈Øvod vƒõ≈ôit, ≈æe na≈°e dal≈°√≠\r\nkroky a rozhodnut√≠, kter√° zalo≈æ√≠me na tomto modelu, budou m√≠t ≈æ√°douc√≠\r\nefekt na po≈æadovan√© v√Ωstupy, tj. na postup oaklandsk√Ωch ‚Äú√Åƒçek‚Äù do\r\nplay-off.\r\n6. krok: Intervence\r\nNa z√°kladƒõ v√Ω≈°e uveden√Ωch zji≈°tƒõn√≠ zaƒçal management oaklandsk√Ωch\r\n‚Äû√Åƒçek‚Äú do sv√©ho t√Ωmu vyb√≠rat hr√°ƒçe, kte≈ô√≠ sice nevyhovovali tradiƒçn√≠m\r\nkrit√©ri√≠m, podle kter√Ωch hr√°ƒç≈°t√≠ skauti posuzovali kvalitu baseballov√Ωch\r\nhr√°ƒç≈Ø, ale za to vykazovali p≈ôesnƒõ ty charakteristiky, kter√© podle\r\nDePodestov√Ωch anal√Ωz p≈ôedpov√≠daly poƒçet vyhran√Ωch a prohran√Ωch bod≈Ø, a\r\npota≈æmo tedy tak√© pravdƒõpodobnost √∫ƒçasti v play-off, kter√° byla hlavn√≠m\r\nc√≠lem managementu. D√≠ky tomu, ≈æe konkurenƒçn√≠ t√Ωmy d≈Øle≈æitost tƒõchto\r\nhr√°ƒçsk√Ωch statistik podce≈àovaly a naopak p≈ôece≈àovaly jin√©, m√©nƒõ d≈Øle≈æit√©\r\npromƒõnn√© (nap≈ô. m√≠ru √∫spƒõ≈°nosti odpal≈Ø, tzv. Batting Average),\r\nmohl management oaklandsk√Ωch ‚Äû√Åƒçek‚Äú relativnƒõ levnƒõ skupovat hr√°ƒçe,\r\nkte≈ô√≠ jim umo≈æ≈àovali dosahovat stanoven√©ho c√≠le. V√Ωsledkem bylo to, ≈æe\r\noaklandsk√° ‚Äû√Åƒçka‚Äú vyhr√°vala zhruba o 20 z√°pas≈Ø za sez√≥nu v√≠ce ne≈æ stejnƒõ\r\n‚Äûchud√©‚Äú t√Ωmy a p≈ôibli≈ænƒõ stejnƒõ tolik z√°pas≈Ø jako 2kr√°t a≈æ 3kr√°t bohat≈°√≠\r\nkonkurence - viz graf n√≠≈æe.\r\n\r\n\r\nShow code\r\n\r\n# Naƒçtƒõme si pot≈ôebn√° data Lahmanovy baseballov√© datab√°ze, kter√° je ve≈ôejnƒõ p≈ô√≠stupn√° na adrese http://seanlahman.com/baseball-archive/statistics/\r\nmzdyHracu <- read_csv(\"salaries.csv\")\r\nvyhryTymu <- read_csv(\"teams.csv\")\r\n\r\n# Vypoƒçtƒõme si pr≈Ømƒõrnou sumu mezd vypl√°cen√Ωch jednotliv√Ωmi t√Ωmy sv√Ωm hr√°ƒç≈Øm v letech 1998-2001 \r\nprumerna_suma_MezdHracu <- mzdyHracu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypoƒçtƒõme si pro jednotliv√© t√Ωmy pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu v letech 1998-2001\r\nprumerny_pocet_vyher <- vyhryTymu %>%\r\n  filter(yearID > 1997 & yearID < 2002) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyj√°d≈ôeme si graficky vztah mezi poƒçtem v√Ωher a mno≈æstv√≠m penƒõz, kter√© t√Ωmy vynakl√°daj√≠ na mzdy sv√Ωch hr√°ƒç≈Ø \r\nlibrary(ggrepel)\r\nprumerna_suma_MezdHracu %>%\r\n  left_join(prumerny_pocet_vyher, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hr√°ƒç≈Ø a poƒçet v√≠tƒõzstv√≠ v letech 1998-2001\")+\r\n  xlab(\"Pr≈Ømƒõrn√° suma mezd hr√°ƒç≈Ø (USD)\")+\r\n  ylab(\"Pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\"))+\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(2e+07,9e+07), breaks = seq(2e+07,9e+07,1e+07))\r\n\r\n\r\n\r\n\r\nOmezen√≠ HR analytiky\r\nP≈ôes ve≈°kerou p≈ôidanou hodnotu, kterou HR analytika pro organizaci\r\nm≈Ø≈æe m√≠t, je vhodn√© si v≈Øƒçi n√≠ zachovat zdravou m√≠ru skepse a b√Ωt si\r\nvƒõdom jej√≠ch omezen√≠. N√≠≈æe uv√°d√≠m p≈ôehled nƒõkolika z nich.\r\nKvalita a u≈æiteƒçnost v√Ωstup≈Ø HR analytiky je z√°visl√° na kvalitƒõ\r\ndat, kter√° do n√≠ vstupuj√≠. Jako kdekoli jinde i zde plat√≠ ok≈ô√≠dlen√©\r\nrƒçen√≠ ‚Äûrubbish in, rubbish out‚Äú. Schopnost z√≠skat pot≈ôebn√° data\r\nvƒças, v dostateƒçn√© kvalitƒõ a v dostateƒçn√©m mno≈æstv√≠ p≈ôitom p≈ôedstavuje\r\njedno z neju≈æ≈°√≠ch hrdel cel√©ho procesu zav√°dƒõn√≠ HR analytiky v\r\norganizac√≠ch.\r\nHR analytika pracuje s historick√Ωmi daty a vych√°z√≠ z p≈ôedpokladu,\r\n≈æe minulost je dobr√Ωm prediktorem budoucnosti. Ale jak n√°s na to\r\nopakovanƒõ upozor≈àuj√≠ odborn√≠ci jako Nassim\r\nTaleb nebo Philip\r\nTetlock, tento vztah mezi minulost√≠ a budoucnost√≠ plat√≠ pouze do\r\nurƒçit√© m√≠ry a pouze v relativnƒõ kr√°tk√©m ƒçasov√©m horizontu. Na ka≈æd√©m\r\nrohu na n√°s ƒç√≠h√° nƒõjak√° potenci√°ln√≠ ƒçern√°\r\nlabu≈•, kter√° m≈Ø≈æe postavit na hlavu v≈°echno, co jsme se na z√°kladƒõ\r\nna≈°ich minul√Ωch zku≈°enost√≠ nauƒçili br√°t jako samoz≈ôejmou\r\njistotu.\r\nNe ka≈æd√© prost≈ôed√≠ je stejnƒõ p≈ôedv√≠dateln√© jako svƒõt sportu.\r\nPomƒõr sign√°lu\r\na ≈°umu se m≈Ø≈æe nap≈ô√≠ƒç r≈Øzn√Ωmi oblastmi v√Ωznamnƒõ li≈°it a ƒç√≠m v√≠ce\r\np≈ôevl√°d√° n√°hodn√Ω ≈°um nad sign√°lem, t√≠m m√©nƒõ jsou v√Ωstupy z HR analytiky\r\nu≈æiteƒçn√©. P≈ô√≠kladem zde m≈Ø≈æe b√Ωt relativnƒõ ne√∫spƒõ≈°n√° snaha p≈ôedpov√≠dat\r\nto, jak si baseballov√© t√Ωmu povedou v play-off. Na rozd√≠l od z√°kladn√≠\r\nƒç√°sti soutƒõ≈æe, kde se hraje dostatek z√°pas≈Ø na to, aby se vyru≈°il vliv\r\nn√°hodn√©ho ≈°tƒõst√≠ a sm≈Øly, v pƒõtiz√°pasov√Ωch kolech play-off hraje n√°hoda\r\ntak v√Ωznamnou roli, ≈æe souvislost mezi celkov√Ωm poƒçtem v√≠tƒõzstv√≠ v\r\nz√°kladn√≠ ƒç√°sti a po≈ôad√≠m t√Ωmu v play-off je t√©mƒõ≈ô nulov√°.\r\n\r\n\r\nShow code\r\n\r\n# Vyfiltrujme si data mezi lety 1994-2011, kdy v play-off hraje 8 t√Ωm≈Ø.\r\nmoneyball3 <- moneyball %>%\r\n  filter(Year < 2012 & Year > 1993)\r\n  \r\n# V√Ωpoƒçtƒõme si Kendallovu po≈ôadovou korelaci mezi mezi celkov√Ωmm poƒçtem v√≠tƒõzstv√≠ v z√°kladn√≠ ƒç√°sti soutƒõ≈æe a po≈ôad√≠m t√Ωmu v play-off mezi lety 1994-2011. \r\nsuppressWarnings(cor.test(~ W + RankPlayoffs, data = moneyball3, method = \"kendall\"))\r\n\r\n\r\n\r\n    Kendall's rank correlation tau\r\n\r\ndata:  W and RankPlayoffs\r\nz = -0.48318, p-value = 0.629\r\nalternative hypothesis: true tau is not equal to 0\r\nsample estimates:\r\n        tau \r\n-0.05541167 \r\n\r\nƒå√≠sla maj√≠ tu zvl√°≈°tn√≠ moc, ≈æe dok√°≈æou v ƒçlovƒõku velice snadno\r\nvzbudit dojem, ≈æe toho v√≠me mnohem v√≠ce ne≈æ je tomu ve skuteƒçnosti. Je\r\nv≈°ak dobr√© si b√Ωt vƒõdom toho, ≈æe ka≈æd√° statistick√° p≈ôedpovƒõƒè je v≈ædy\r\nzat√≠≈æena nƒõjakou m√≠rou chyby, tu vƒõt≈°√≠, tu men≈°√≠. Velkou v√Ωhodou\r\nstatistick√Ωch model≈Ø je to, ≈æe tato chyba je u nich explicitnƒõ\r\nvyƒç√≠slena, tak≈æe s n√≠ lze dop≈ôedu poƒç√≠tat a zohlednit ji p≈ôi n√°sledn√©m\r\nrozhodov√°n√≠. Tato ‚Äûup≈ô√≠mnost‚Äú ohlednƒõ sv√© vlastn√≠ omylnosti paradoxnƒõ\r\nmnohdy stav√≠ statistick√© modely do hor≈°√≠ho svƒõtla ne≈æ jinak m√©nƒõ p≈ôesn√©\r\nintuitivn√≠ √∫sudky expert≈Ø, pro kter√© podobn√© √∫daje o m√≠≈ôe jejich\r\nomylnosti vƒõt≈°inou nejsou v≈Øbec k dispozici.\r\nVelikost v√Ωhody, kterou n√°m zaveden√≠ HR analytiky d√°v√°, m≈Ø≈æe b√Ωt\r\nz√°visl√° na tom, zda podobn√© postupy vyu≈æ√≠v√° tak√© na≈°e konkurence. Opƒõt\r\nto lze celkem dob≈ôe dolo≈æit na oaklandsk√Ωch ‚Äû√Åƒçk√°ch‚Äú. Jejich v√Ωsledky se\r\nmezi lety 2002 a≈æ 2012, tj. v dobƒõ po zve≈ôejnƒõn√≠ Moneyballu, kdy ji≈æ\r\nv≈°echny t√Ωmy mƒõly p≈ô√≠le≈æitost sezn√°mit se s principy prediktivn√≠\r\nanalytiky a zav√©st ji do sv√© praxe, zaƒçaly v√≠ce p≈ôibli≈æovat v√Ωsledk≈Øm\r\npodobnƒõ ‚Äûchud√Ωch‚Äú soupe≈ô≈Ø a naopak jejich bohat≈°√≠ soupe≈ôi jim sv√Ωm\r\nv√Ωkonem zase trochu odskoƒçili - viz graf n√≠≈æe. Z toho mimo jin√© vypl√Ωv√°,\r\n≈æe s t√≠m, jak se st√°le v√≠ce spoleƒçnost√≠ bude p≈ôi ≈ô√≠zen√≠ lidsk√Ωch zdroj≈Ø\r\nspol√©hat na v√Ωstupy z HR analytiky, p≈ôestane b√Ωt HR analytika nƒõjakou\r\nz√°sadn√≠ konkurenƒçn√≠ v√Ωhodou a stane se z n√≠ nƒõco, co organizaci ‚Äúpouze‚Äù\r\numo≈æn√≠ dr≈æet krok s konkurenc√≠.\r\n\r\n\r\nShow code\r\n\r\n# Vypoƒçtƒõme si pr≈Ømƒõrnou sumu mezd vypl√°cen√Ωch jednotliv√Ωmi t√Ωmy sv√Ωm hr√°ƒç≈Øm v letech 2002-2012 \r\nprumerna_suma_MezdHracu2 <- mzdyHracu %>%\r\n  filter(yearID > 2001 & yearID <= 2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerna_suma_MezdHracu = sum(salary)/length(unique(yearID)))\r\n\r\n# Vypoƒçtƒõme si pro jednotliv√© t√Ωmy pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu v letech 2002-2012\r\nprumerny_pocet_vyher2 <- vyhryTymu %>%\r\n  filter(yearID > 2001 & yearID <=2012) %>%\r\n  group_by(teamID) %>%\r\n  summarise(prumerny_pocet_vyher = sum(W)/length(unique(yearID)))\r\n\r\n# Vyj√°d≈ôeme si graficky vztah mezi poƒçtem v√Ωher a mno≈æstv√≠m penƒõz, kter√© t√Ωmy vynakl√°daj√≠ na mzdy sv√Ωch hr√°ƒç≈Ø \r\nprumerna_suma_MezdHracu2 %>%\r\n  left_join(prumerny_pocet_vyher2, \"teamID\") %>%\r\n  mutate(OAK = ifelse(teamID == \"OAK\", \"ano\", \"ne\")) %>%\r\n  ggplot(aes(x= prumerna_suma_MezdHracu, y = prumerny_pocet_vyher, fill = OAK)) +\r\n  geom_point()+\r\n  ggtitle(\"Mzdy hr√°ƒç≈Ø a poƒçet v√≠tƒõzstv√≠ v letech 2002-2012\")+\r\n  xlab(\"Pr≈Ømƒõrn√° suma mezd hr√°ƒç≈Ø (USD)\")+\r\n  ylab(\"Pr≈Ømƒõrn√Ω poƒçet v√Ωher za sez√≥nu\")+\r\n  geom_label_repel(\r\n    aes(label = teamID),\r\n    box.padding = 0.25, point.padding = 0.25,\r\n    segment.color = 'grey50')+\r\n  theme(legend.position=\"none\")+\r\n  scale_fill_manual(values = c(\"#ffd400\", \"#ffffff\"), \r\n                        labels = c(\"ano\",\"ne\")) +\r\n  scale_y_continuous(limits=c(65,100), breaks = seq(65,100,5)) +\r\n  scale_x_continuous(limits=c(3e+07,2e+08), breaks = seq(3e+07,2e+08,2e+07))\r\n\r\n\r\n\r\n\r\nZ√°vƒõr\r\nNa p≈ô√≠kladu oaklandsk√©ho baseballov√©ho mu≈æstva jsme takto mohli\r\nsledovat obvykl√Ω postup aplikace HR analytiky na urƒçit√Ω druh probl√©mu,\r\nkter√Ω se sna≈æ√≠ v dan√© organizaci vy≈ôe≈°it. Vzhledem ke specifick√©mu\r\np≈ôedmƒõtu podnik√°n√≠ oaklandsk√Ωch ‚Äû√Åƒçek‚Äú bylo t√≠mto c√≠lem dos√°hnout\r\npostupu do play-off a to v situaci, kdy management nemƒõl dostatek\r\nfinanƒçn√≠ch prost≈ôedk≈Ø na zaplacen√≠ hr√°ƒç≈Ø pova≈æovan√Ωch dle tradiƒçn√≠ch\r\nmƒõ≈ô√≠tek za kvalitn√≠ a perspektivn√≠. Od tohoto c√≠le se potom odv√≠jela\r\n≈ôada krok≈Ø, kter√© bl√≠≈æe specifikovaly jeho povahu a identifikovaly\r\nfaktory (mimo jin√© i ty person√°ln√≠), kter√© s jeho dosa≈æen√≠m souvis√≠. Na\r\nz√°kladƒõ t√©to znalosti potom bylo mo≈æn√© formulovat urƒçit√© p≈ôedpovƒõdi a\r\nuƒçinit jist√° rozhodnut√≠, kter√° zv√Ω≈°ila pravdƒõpodobnost toho, ≈æe se\r\npoda≈ô√≠ vytƒçen√©ho c√≠le dos√°hnout. P≈ôesto≈æe tento p≈ô√≠bƒõh o vyu≈æit√≠ HR\r\nanalytiky se odehr√°l ve svƒõtƒõ sportu, jeho logika je platn√° i v kontextu\r\ntradiƒçnƒõj≈°√≠ho typu organizac√≠. Ostatnƒõ ve v≈°ech typech\r\norganizac√≠ jde nakonec p≈ôedev≈°√≠m o to m√≠t na spr√°vn√©m m√≠stƒõ a ve spr√°vn√Ω\r\nƒças ty spr√°vn√© lidi - jedinƒõ tak tyto organizace mohou\r\nsystematicky dosahovat sv√Ωch strategick√Ωch c√≠l≈Ø.\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-11-moneyball-v-hr-od-hr-analytiky-ke-sportovn-analytice-a-zpt/./Pitcher-at-the-mound.jpg",
    "last_modified": "2022-09-17T18:41:49+02:00",
    "input_file": {}
  }
]
