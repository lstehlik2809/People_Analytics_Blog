---
title: "R + Python via reticulate"
description: |
  Taking the `radix` R package for a test spin with `Scikit Learn`!
author:
  - name: Matt Dancho 
    url: www.business-science.io
    affiliation: Business Science
    affiliation_url: www.business-science.io
date: "2018-10-08"
output: radix::radix_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE)
```

## R + PYTHON, INTEGRATED MACHINE LEARNING TUTORIAL
The project we are performing comes from the “Wine Snob Machine Learning Tutorial” by Elite Data Science. We’ll perform the following:

1. (Python) Replicate the Machine Learning tutorial using Scikit Learn
2. (R) Use ggplot2 to visualize the results for model performance
3. (R) Build the report using RMarkdown and the new radix framework for scientific reporting

These are the same steps that were used to create the “R + Python with reticulate” report contained in this Machine Learning Tutorial on YouTube:

<div align="center">
   <iframe width="560" height="315" src="http://www.youtube.com/embed/YfqxICYVNtU" frameborder="0" allowfullscreen>
   </iframe>
</div>

### Setup Reticulate
```{r}
# R
library(reticulate)
```

```{r}
# R
conda_list()
```

```{r}
# R
use_condaenv("ANACON~1")
```

### Machine Learning With Scikit Learn 
```{python}
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
```

```{python}
# Python
dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(dataset_url, sep = ";")
```

We can use the print() function to output Python objects.

```{python}
# Python
print(data.head())
```

Review data using glimpse() from tidyverse universe.

```{r}
# R
library(tidyverse)

py$data %>%
  as.tibble() %>%
  glimpse()
```
Setup data into X (features) and y (target) variables.

```{python}
# Python
y = data.quality
X = data.drop("quality", axis = 1)
```

Split features into training and testing sets.

```{python}
# Python
X_train, X_test, y_train, y_test = train_test_split(
X,y,
test_size = 0.2,
random_state = 123,
stratify = y
)
```

Preprocess by calculating the scale from X_train with StandardScalar().

```{python}
# Python
scaler = preprocessing.StandardScaler().fit(X_train)
```

Apply transformation to X_test with the transform() method.

```{python}
# Python
X_test_scaled = scaler.transform(X_test)
```

Setup an ML pipeline using make_pipeline(). The pipeline consists of two steps. First, numeric values are scaled, then a random forest regression model is created.

```{python}
# Python
pipeline = make_pipeline(
preprocessing.StandardScaler(),
RandomForestRegressor(n_estimators=100)
)
```

We’ll perform Grid Search to get the optimal combination of parameters. First, set up a hyperparameters object that has the combination of attributes we want to change.

```{python}
# Python
hyperparameters = {
    "randomforestregressor__max_features" : ["auto", "sqrt", "log2"],
    "randomforestregressor__max_depth"    : [None, 5, 3, 1]
}
```

Apply grid search with cross validation using GridSearchCV().

```{python}
# Python
clf = GridSearchCV(pipeline, hyperparameters, cv = 10)
clf.fit(X_train, y_train)
```

Print the best parameters.

```{python}
# Python
print(clf.best_params_)
```

### Make Wine Predictions And Get Error Metrics

```{python}
# Python
y_pred = clf.predict(X_test)
```

```{python}
# Python
print(r2_score(y_test, y_pred))
```

```{python}
# Python
print(mean_squared_error(y_test, y_pred))
```

### Visualizing Model Quality With R

```{r}
#R 
library(tidyverse)
library(tidyquant) # for theme_tq()

# Manipulate data for ggplot
results_tbl <- tibble(
    y_test = py$y_test,
    y_pred = py$y_pred
) %>%
    rowid_to_column() %>%
    arrange(y_test) %>%
    mutate(rowid = as_factor(as.character(rowid))) %>%
    rowid_to_column("sorted_rowid") %>%
    gather(key = "key", value = "value", -c(rowid, sorted_rowid)) 

# Make ggplot
results_tbl %>%
    ggplot(aes(sorted_rowid, value, color = key)) +
    geom_point(alpha = 0.5) +
    geom_smooth() + 
    theme_tq() +
    scale_color_tq() +
    labs(
        title = "Prediction Versus Actual",
        subtitle = "Wine Quality Level",
        x = "Sorted RowID", y = "Quality Level"
    )
```

```{r}
#R
results_tbl %>%
  # Manipulation
  spread(key, value) %>%
  mutate(resid = y_pred - y_test) %>%
  # Plot
  ggplot(aes(sorted_rowid, resid, color = as.character(y_test))) +
    geom_point(alpha = 0.5) +
    theme_tq() +
    scale_color_tq() +
  theme(legend.position="bottom") +
    labs(
        title = "Residual Analysis (Prediction - Actual)",
        subtitle = "Wine Quality Level",
        x = "Sorted Row ID", y = "Residual",
        color = "Quality Level"
    )
```

```{r}
#R
library(plotly)

viz <- results_tbl %>%
  # Manipulation
  spread(key, value) %>%
  mutate(resid = y_pred - y_test) %>%
  # Plot
  ggplot(aes(sorted_rowid, resid, color = as.character(y_test))) +
    geom_point(alpha = 0.5) +
    theme_tq() +
    scale_color_tq() +
  theme(legend.title = element_blank()) +
    labs(
        title = "Residual Analysis (Prediction - Actual)",
        subtitle = "Wine Quality Level",
        x = "Sorted Row ID", y = "Residual",
        color = "Quality Level"
    )

ggplotly(viz) %>%
  layout(
legend = list(
    orientation = "h",
    x = 0.25,
    y = -0.25,
    name = "Quality Level"
  )
) %>%
  add_annotations( text="Quality Level", xref="paper", yref="paper",
                  x=0.25, xanchor="right",
                  y=-0.25, yanchor="top",
                  legendtitle=TRUE, showarrow=FALSE )
```